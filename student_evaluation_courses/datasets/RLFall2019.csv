,anon_ch,subject_ch,created_ch,bucket_order_ch,bucket_name_ch,type_ch,uid_a_ch,uid_ch,id_ch,updated_ch,config_ch,assignment
0,no,"<p>My name is Todd Neal, I&#39;m from Kentucky but live in Madison, Alabama. I&#39;ve been interested in machine learning and have been trying to formalize and round out my self study. This is my first class in the program.</p>",2019-08-19T23:08:17Z,34,Week 8/18 - 8/24,followup,,jzhl7qlwrpagr,jzj0fvskoah19g,2019-08-19T23:08:17Z,{},logistics
1,no,<p>Welcome to the class and the program!</p>,2019-08-19T23:14:04Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj0nbi02e74fv,2019-08-19T23:14:04Z,{},logistics
2,no,<p>Welcome Todd!  You will find there are a lot of folks from the Huntsville area in this program.</p>,2019-08-20T11:05:43Z,34,Week 8/18 - 8/24,feedback,,idfzuuu9fnI,jzjq2i4e8de2fh,2019-08-20T11:05:43Z,{},logistics
3,no,"<p>My name is Kai Sun. I&#39;m a biostatistician working on clinical trials for Alzheimer&#39;s Disease in San Diego. This is my first class in the program. In clinical trials, the adaptive design can be viewed as a restricted version of RL.  </p>",2019-08-19T23:20:50Z,34,Week 8/18 - 8/24,followup,,jzhrun61eu91l9,jzj0w0m98c321b,2019-08-19T23:20:50Z,{},logistics
4,no,"<p>Bandits!</p>
<p></p>
<p>Welcome to the class and the program!</p>",2019-08-19T23:39:01Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj1je8ecji3ks,2019-08-19T23:39:01Z,{},logistics
5,no,"<p>My name is Song Zheng. I am in Atlanta. This is my 8th course. I am working as a software engineer in a car insurance company, ML has become a great tool for the car insurance industry such as damage analysis.</p>",2019-08-19T23:34:37Z,34,Week 8/18 - 8/24,followup,,htgjinstby74xv,jzj1dqhekeq6ld,2019-08-19T23:34:37Z,{},logistics
6,no,<p>Welcome!</p>,2019-08-19T23:39:16Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj1jpzrur23pi,2019-08-19T23:39:16Z,{},logistics
7,no,"<p>Hi, my name is Ronny Hoesada. I live in San Francisco, CA and work for Udacity. I&#39;m interested in RL because of the idea of imitation learning by mimicking human behaviors in certain ways. </p>",2019-08-19T23:40:02Z,34,Week 8/18 - 8/24,followup,,jziehvhblig5if,jzj1koyi9fg31o,2019-08-19T23:40:02Z,{},logistics
8,no,"<p>Hello, My name is Rajesh. I live in NJ. This is my 6th class. I got interested to RL class when took summer  ML4T class and last project was related to RL. That project  get me very curious and  interested to learn deeper about RL.</p>",2019-08-19T23:46:28Z,34,Week 8/18 - 8/24,followup,,jl2gfssgneuU,jzj1sywewv76uj,2019-08-19T23:46:28Z,{},logistics
9,no,<p>I am Vishal. Located in San Jose. Got introduced to RL as part of the ML4T course. This is my 4th class.</p>,2019-08-20T00:00:31Z,34,Week 8/18 - 8/24,followup,,jl2kyqna7q54ck,jzj2b1b19b630g,2019-08-20T00:00:31Z,{},logistics
10,no,"<p>Hi, my name is Jacob Mapel and I live in Sandy Springs, GA. I grew up in Pittsburgh, PA before moving to Georgia and received my undergraduate from UGA in mathematics. I work as a data scientist/software engineer at UPS. I have always been interested in how AI systems can learn through trial and error to become effective decision-making systems. I was particularly inspired in the RL space by the popular YouTube video <a href=""https://www.youtube.com/watch?v=qv6UVOQ0F44"" target=""_blank"">MarI/O</a> and look forward to learning more about the subject. This is my first course in the OMSCS program, and I hope that it will help me to become a better developer and data scientist.</p>",2019-08-20T00:38:33Z,34,Week 8/18 - 8/24,followup,,jzht4p8dvn51c9,jzj3nyekmk12yt,2019-08-20T00:38:33Z,{},logistics
11,no,<p>Hi my name is Sharan. I live in New Jersey and I work from AT&amp;T. I chose RL because I am interested in challenging myself with the topic of Machine Learning and this is my first class in the specialization.</p>,2019-08-20T00:42:10Z,34,Week 8/18 - 8/24,followup,,jzj3l9wevx4734,jzj3sm5hiw35f9,2019-08-20T00:42:10Z,{},logistics
12,no,"<p>I live in San Diego, CA. I am through the program now 20 months with AI, ML4T, ML, AI4R, and KBAI behind me. I am a full time software developer in the P&amp;C insurance industry. I am also a TA for 6263 and am also a cyber professional and acting CISO for some insurance general agencies.</p>",2019-08-20T00:42:49Z,34,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzj3tftgw8n5un,2019-08-20T00:42:49Z,{},logistics
13,no,"<p>Hi everyone, I&#39;m in the Washington D.C area working for Capital One. I&#39;m in the fraud department for credit cards where my team uses ML in our rules engine to stop the bad guys (decline fraudulent transactions). This is my first semester. As I progress through the ML specialization, I forward to learning the theory behind all our defenses!</p>",2019-08-20T01:07:26Z,34,Week 8/18 - 8/24,followup,,jzhdgaq99za3um,jzj4p3k398l2xf,2019-08-20T01:07:26Z,{},logistics
14,no,"<p>Hey Hey! I work around DC too, and it&#39;s my first semester as well!</p>",2019-08-20T18:07:23Z,34,Week 8/18 - 8/24,feedback,,jzhbewk46d19d,jzk54rto2kr4k8,2019-08-20T18:07:23Z,{},logistics
15,no,<p>Nice! Excited to dive in.</p>,2019-08-21T23:46:26Z,34,Week 8/18 - 8/24,feedback,,jzhdgaq99za3um,jzlwommby5c1os,2019-08-21T23:46:26Z,{},logistics
16,no,"I&#39;m in DC as well. Third course in the program, but my first in the ML specialization.",2019-08-28T11:53:41Z,33,Week 8/25 - 8/31,feedback,,jqi9pwn26my6g2,jzv7azwjjlpxo,2019-08-28T11:53:41Z,{},logistics
17,no,<p>I&#39;m Tyler.  I&#39;m a Data Scientist working for one of several trucking companies in Arkansas.  This is attempt #2 at this course for me.  I&#39;m excited to get started again!</p>,2019-08-20T01:12:10Z,34,Week 8/18 - 8/24,followup,,jqmfuaidej9155,jzj4v70l2as55,2019-08-20T01:12:10Z,{},logistics
18,no,"<p>My name is Alison Jing Huang and I live in San Francisco, CA.  I&#39;m a full time Analytics Consultant working in Healthcare industry. This is my 4th course and 1st ML specialization course, previously have taken SDP, IHI, and IIS. I am excited to learn about reinforcement learning in depth and its applications, and possibly use in conjunction with medical research.</p>",2019-08-20T01:18:19Z,34,Week 8/18 - 8/24,followup,,jqy7j43dlz4di,jzj53397qudyk,2019-08-20T01:18:19Z,{},logistics
19,no,"<p>Hi all! I&#39;m Will. Posting now from Seattle where I&#39;ve been for the past 3 months, but moving back to New York City next week. I took ML previously and got very intrigued with RL and the professors, so I decided to take RL now. I&#39;ve completed 7 classes in OMSCS and am hoping to be done soon! </p>",2019-08-20T01:18:28Z,34,Week 8/18 - 8/24,followup,,j6ljeqr3a3A,jzj53a7jev1138,2019-08-20T01:18:28Z,{},logistics
20,no,"<p></p><ul><li>Alex Matthers</li><li>New York / New England</li><li>Durham, North Carolina</li><li>Reinforcement learning is fascinating because it&#39;s so similar to how we humans learn.</li><li>Half way!</li><li>Physically sticking where I am</li></ul>",2019-08-20T01:20:02Z,34,Week 8/18 - 8/24,followup,,jhqqvwpyvXsQ,jzj55anym4x26d,2019-08-20T01:20:02Z,{},logistics
21,no,<p>Fascinating indeed.</p>,2019-08-20T01:28:37Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj5gcgm7pu5sj,2019-08-20T01:28:37Z,{},logistics
22,no,"<p>Hi my name is Shailesh Tappe. I live in Atlanta metro area working as Data Analyst with State Farm Insurance. I took DBMS Concept, DVA and ML4T. I learned a bit of RL in ML4T and I like to learn more it detail on RL. This is my third semester and still more to learn.  Like to explore more possibilities in learning, Yes as much as people in my social network.</p>",2019-08-20T01:25:35Z,34,Week 8/18 - 8/24,followup,,jqlnrz9q91d6ko,jzj5cftx1cq26l,2019-08-20T01:25:35Z,{},logistics
23,no,"<p>Hi my name is Marcus Chong and I am from California. I currently work at Boeing as a Network Software Engineer. I wanted to take RL because I want to learn more about Machine Learning. This is my first semester and I am looking forward to see what RL is all about. After RL, I plan to continue my journey through the OMSCS program.</p>
<p></p>
<p>Fun fact: When I am away from my computer, I like camping/hiking, snowboarding, and longboarding. </p>",2019-08-20T01:34:09Z,34,Week 8/18 - 8/24,followup,,jzimye522wr7ey,jzj5ngcitpe7j2,2019-08-20T01:34:09Z,{},logistics
24,no,"<p></p><ul><li>What&#39;s your name?
<ul><li>Rick Sprague</li></ul>
</li><li>Where are you from?
<ul><li>Flint, MI</li></ul>
</li><li>Where are you now?
<ul><li>Walled Lake, MI</li></ul>
</li><li>Why RL?
<ul><li>Sounds like a cool subject, maybe a requirement for the ML track</li></ul>
</li><li>How far into the program?
<ul><li>This is class #6</li></ul>
</li><li>Where next?
<ul><li>Not sure.</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>No</li></ul>
</li></ul>",2019-08-20T01:35:51Z,34,Week 8/18 - 8/24,followup,,ixpclzk97jg2fo,jzj5pn63d6n108,2019-08-20T01:35:51Z,{},logistics
25,no,"<p>What&#39;s your name? -- George<br />Where are you from? -- Montana<br />Where are you now? -- Nevada, Reno<br />Why RL? -- I&#39;ve been interesting in RL from all the machine learning or artificial intelligence class&#39;s I&#39;ve taken.<br />How far into the program? -- 8 classes completed, this one, and GA after.<br />Where next? -- no plans to move.<br />Is there anything else you&#39;d like to share? -- Lived just outside the arctic circle for 2 years. Ice fishing through 6ft of ice into the ocean is fun.</p>",2019-08-20T01:38:17Z,34,Week 8/18 - 8/24,followup,,ixty1midfufhd,jzj5ss6sls77b7,2019-08-20T01:38:17Z,{},logistics
26,no,"<p>I&#39;m Dalton, I&#39;m from Connecticut and I&#39;m still there. I chose Georgia Tech for its reputation with machine learning, and I&#39;m excited to learn about RL. This is my 3rd class. </p>",2019-08-20T01:44:40Z,34,Week 8/18 - 8/24,followup,,jl284xdcifz44g,jzj60z7tsoq2wc,2019-08-20T01:44:40Z,{},logistics
27,no,<p>Exciting topic for sure.</p>,2019-08-20T01:47:57Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj657rmx7t5km,2019-08-20T01:47:57Z,{},logistics
28,no,"<p>Hey Dalton, also currently in CT (went to UCONN and been here since). Enjoy the course, RL is awesome!</p>",2019-08-28T15:14:57Z,33,Week 8/25 - 8/31,feedback,,jl1acpoc4HA9,jzvehu6i4ds5mq,2019-08-28T15:14:57Z,{},logistics
29,no,"<p>Hi, my name is Jeff Hara, and I&#39;m a machine learning engineer at Mercari in Palo Alto. I&#39;m born and raised in the Bay Area, and I did my undergrad at Stanford last year with a bachelors in CS (AI track) and Philosophy. This is my first semester in OMSCS, so I&#39;m looking forward to diving into RL! Nice to meet all of you.</p>",2019-08-20T01:46:50Z,34,Week 8/18 - 8/24,followup,,jziefl597da3my,jzj63s18um51wm,2019-08-20T01:46:50Z,{},logistics
30,no,"<p></p>
<p>What&#39;s your name?</p>
<p>Nicholas LePar</p>
<p></p>
<p>Where are you from?</p>
<p>All across the Midwest</p>
<p></p>
<p>Where are you now?</p>
<p>Madison, Wisconsin</p>
<p></p>
<p>Why RL?</p>
<p>I am fascinated with the topic and I believe it is one of the more important areas of research for machine learning as the problems it has the potential to solve are exciting.</p>
<p></p>
<p>How far into the program?</p>
<p>First semester, Taking with AI4R as well. Am I crazy?</p>
<p></p>
<p>Where next?</p>
<p>Hopefully a house with my beautiful finance!</p>
<p></p>
<p>Is there anything else you&#39;d like to share?</p>
<p>I have been looking forward to this for a year now! Can&#39;t believe it is finally here.</p>
<p></p>",2019-08-20T02:12:12Z,34,Week 8/18 - 8/24,followup,,jzg6j8k09gm3zm,jzj70edd6d07b2,2019-08-20T02:12:12Z,{},logistics
31,no,"<p>Hi, I&#39;m from Madison too!</p>",2019-08-20T03:07:31Z,34,Week 8/18 - 8/24,feedback,,jque4s9u2b82s0,jzj8zissvuz3d5,2019-08-20T03:07:31Z,{},logistics
32,no,Same here!,2019-08-20T18:15:43Z,34,Week 8/18 - 8/24,feedback,,jc6utnpzmnw1xt,jzk5fhk3klg2jd,2019-08-20T18:15:43Z,{},logistics
33,no,"<p>I am Hazel John and this is my 5th class (BD4H, KBAI, ML, AI4R). I am in the Seattle area, currently on the management track and focus more on architecture and system design than hands on software engineering. But, I have been a software engineer for a long time and I want to keep my skills up to date. I believe that AI and Machine learning will become more and more mainstream and will change the way we work and interact/use technology and I want to part of that change.</p>",2019-08-20T02:17:20Z,34,Week 8/18 - 8/24,followup,,j6pmq1sglzo35i,jzj76zkxnmv398,2019-08-20T02:17:20Z,{},logistics
34,no,<p></p><ul><li>What&#39;s your name? Aaron</li><li>Where are you from? CA</li><li>Where are you now? CA</li><li>Why RL? Because it will have a massive impact on our world in the next century and it is awesome! :)</li><li>How far into the program? 1/2</li></ul>,2019-08-20T02:27:46Z,34,Week 8/18 - 8/24,followup,,gx3c8l7z7r72zl,jzj7ken2a005tm,2019-08-20T02:27:46Z,{},logistics
35,no,<p></p><ul><li>What&#39;s your name? Siyuan Li</li><li>Where are you from? China</li><li>Where are you now? NYC</li><li>Why RL? Incoming new project in work. Will be very helpful</li><li>How far into the program? Fresh new</li></ul>,2019-08-20T02:29:26Z,34,Week 8/18 - 8/24,followup,,jzj77ezwl823lv,jzj7mjjmp05354,2019-08-20T02:29:26Z,{},logistics
36,no,<p>Nice to meet you Siyuan</p>,2019-08-20T19:02:21Z,34,Week 8/18 - 8/24,feedback,,jzk6nge7yl24dc,jzk73gk0oan6t9,2019-08-20T19:02:21Z,{},logistics
37,no,"Hello all, I&#39;m Aida and this is my first class as well. I&#39;m a little bit worried that I&#39;m taking it before Machine Learning, but hopefully I can manage it! Why RL? I&#39;m torn between Robotics and ML specializations, so RL was a natural choice for me as it&#39;s the intersection of both. Oh yeah, I&#39;m originally from Siberia, but live now in New York and I have no clue how to pin any of these areas on the map!",2019-08-20T02:29:40Z,34,Week 8/18 - 8/24,followup,,jzih0fdt4sn1cq,jzj7mv0gjoe39f,2019-08-20T02:29:40Z,{},logistics
38,no,<p>Nice... I took this before ML... Do you have coding experience? Are you afraid of neural networks?</p>,2019-08-20T03:18:06Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj9d52vvx1em,2019-08-20T03:18:06Z,{},logistics
39,no,"<p>Below the search bar in the map, there is a button to &#34;Add marker&#34;. Click on it, then pick your location.</p>
<p>Took me a couple minutes to figure that out as well...</p>
<p>Welcome to the class!</p>",2019-08-20T06:22:45Z,34,Week 8/18 - 8/24,feedback,,jl5wq8mca7o0,jzjfylt0sfl5ve,2019-08-20T06:22:45Z,{},logistics
40,no,"Thanks, Quang. Pined myself!",2019-08-24T15:46:28Z,34,Week 8/18 - 8/24,feedback,,jzih0fdt4sn1cq,jzppuyet96h5hq,2019-08-24T15:46:28Z,{},logistics
41,no,"That&#39;s great to hear, Miguel! I know Python (have been coding in it for 5 years) and know little bit about neural networks. So hopefully this will be sufficient enough!",2019-08-24T15:51:07Z,34,Week 8/18 - 8/24,feedback,,jzih0fdt4sn1cq,jzpq0xpwcx0120,2019-08-24T15:51:07Z,{},logistics
42,no,"<p>What&#39;s your name? -- Yong<br />Where are you from? -- China<br />Where are you now? -- Sunnyvale, CA<br />Why RL? -- I&#39;ve been interesting in RL from all the machine learning I&#39;ve studied.<br />How far into the program? -- 4 classes completed. This is my 5th class.</p>",2019-08-20T02:39:49Z,34,Week 8/18 - 8/24,followup,,jcbkq8bd6l2r,jzj7zwwf7sw3x0,2019-08-20T02:39:49Z,{},logistics
43,no,"<p></p>
<p>What&#39;s your name? Angel Barranco</p>
<p>Where are you from? Panama<br />Where are you now? SF Bay area</p>
<p>Why RL? $$$$$</p>
<p>How far into the program? 1st Course</p>
<p>Where next? $$$$$$$$$</p>
<p>Is there anything else you&#39;d like to share? Nope.</p>
<p></p>",2019-08-20T02:46:01Z,34,Week 8/18 - 8/24,followup,,jzj7y1ofgsro1,jzj87veehtm40d,2019-08-20T02:46:01Z,{},logistics
44,no,"<p>Hi Everyone!</p>
<p>My name is Dale Arbogast.  I grew up in Virginia but currently live in Melbourne Beach, Florida (well, currently in Honolulu but headed back home in a few weeks).  I&#39;m working with big data in my current job and would love to use RL to create some smart predictions/analytics.  This will be my 6th class in the program, counting two transfers then I hope to take Machine Learning and ML4T.  Looking forward to a great semester!</p>
<p></p>",2019-08-20T03:00:56Z,34,Week 8/18 - 8/24,followup,,jl2d544u1wcE,jzj8r20ea6p75q,2019-08-20T03:00:56Z,{},logistics
45,no,"<p></p><ul><li>What&#39;s your name? <strong>Yash Kochar</strong></li><li>Where are you from? <strong>Nagpur, India</strong></li><li>Where are you now? <strong>Madison, Wisconsin</strong></li><li>Why RL? ML specialization - </li><li>How far into the program? <strong>Second semester, 3rd course</strong></li><li>Where next? <strong>Good question :)</strong></li><li>Is there anything else you&#39;d like to share? <strong>Excited to be here</strong></li></ul>",2019-08-20T03:06:34Z,34,Week 8/18 - 8/24,followup,,jque4s9u2b82s0,jzj8yawkyuw2r3,2019-08-20T03:06:34Z,{},logistics
46,no,"<p>“Alice: Would you tell me, please, which way I ought to go from here?<br />The Cheshire Cat: That depends a good deal on where you want to get to.<br />Alice: I don&#39;t much care where.<br />The Cheshire Cat: Then it doesn&#39;t much matter which way you go.<br />Alice: ...So long as I get somewhere.<br />The Cheshire Cat: Oh, you&#39;re sure to do that, if only you walk long enough.”</p>",2019-08-20T03:19:31Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj9eyk2t8o23l,2019-08-20T03:19:31Z,{},logistics
47,no,<p>Welcome!</p>,2019-08-20T03:19:39Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj9f4f6w0r25y,2019-08-20T03:19:39Z,{},logistics
48,no,"<p><a href=""https://youtu.be/AD4b-52jtos?list=PLHBXBaEtFOncgm3N1zRhYubpf9Jv9mzhz&amp;t=1848"" target=""_blank"">https://youtu.be/AD4b-52jtos?list=PLHBXBaEtFOncgm3N1zRhYubpf9Jv9mzhz&amp;t=1848</a></p>",2019-08-20T06:59:06Z,34,Week 8/18 - 8/24,feedback,,jque4s9u2b82s0,jzjh9ckoe7m79j,2019-08-20T06:59:06Z,{},logistics
49,no,"<p>Hey everybody!</p>
<p></p>
<ul><li>What&#39;s your name?
<ul><li>Michael Schock</li></ul>
</li><li>Where are you from?
<ul><li>San Francisco Bay Area</li></ul>
</li><li>Where are you now?
<ul><li>Oakland, CA</li></ul>
</li><li>Why RL?
<ul><li>It&#39;s exciting, especially all the stuff coming out of DeepMind.</li></ul>
</li><li>How far into the program?
<ul><li>This is my 4th class.</li></ul>
</li></ul>",2019-08-20T03:08:56Z,34,Week 8/18 - 8/24,followup,,jdd22f9zpbq5sz,jzj91cmuzfz3bn,2019-08-20T03:08:56Z,{},logistics
50,no,"<p>Hi Michael. Nice to meet you! I am based in SF, working in Oakland.</p>",2019-08-20T21:39:28Z,34,Week 8/18 - 8/24,feedback,,jqy7j43dlz4di,jzkcpigbh18tu,2019-08-20T21:39:28Z,{},logistics
51,no,"<p></p><ul><li>What&#39;s your name? Gerardo Alarcon</li><li>Where are you from? Mexico</li><li>Where are you now? Saint Paul, MN</li><li>Why RL? I would like to learn more about RL from the academic perspective.</li><li>How far into the program? 1st Course</li><li>Where next? Not Sure</li><li>Is there anything else you&#39;d like to share? A little bit scared but ready for the challenge.</li></ul>",2019-08-20T03:12:22Z,34,Week 8/18 - 8/24,followup,,jzj72gbzpx37j0,jzj95rym9125e7,2019-08-20T03:12:22Z,{},logistics
52,no,<p>Welcome to the program.</p>,2019-08-20T03:20:14Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj9fvm2i8l2i3,2019-08-20T03:20:14Z,{},logistics
53,no,"<p>Hi All,</p>
<p>My name is Jennifer Abdusamad. I am from Kochi, India. Right now I am in Kennesaw, Georgia. This is my first term. I am very much interested in Machine Learning and planning my second career in Machine learning. I wanted to take ML before RL, but could not get through the waitlist. </p>",2019-08-20T03:48:25Z,34,Week 8/18 - 8/24,followup,,jzivix4hli85ch,jzjag4gd57012m,2019-08-20T03:48:25Z,{},logistics
54,no,"<p>Lots of people new to the program, welcome!</p>",2019-08-20T03:56:56Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzjar371y0l6bm,2019-08-20T03:56:56Z,{},logistics
55,no,"<p></p><ul><li>What&#39;s your name?
<ul><li>Miriah Peterson</li></ul>
</li><li>Where are you from?
<ul><li>Oklahoma City </li></ul>
</li><li>Where are you now?
<ul><li>Utah</li></ul>
</li><li>Why RL?
<ul><li>Cause it had the shortest waitlist of the machine learning related classes</li></ul>
</li><li>How far into the program?
<ul><li>first course</li></ul>
</li><li>Where next?
<ul><li>I am working in software and moving into machine learning as work opportunities arise</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>I do machine learning in Go and have given several talks on it</li></ul>
</li></ul>",2019-08-20T03:57:27Z,34,Week 8/18 - 8/24,followup,,jzj6p4ln7yi5de,jzjarqt1x6b3rs,2019-08-20T03:57:27Z,{},logistics
56,no,<p><em>&#34;Cause it had the shortest waitlist of the machine learning related classes&#34;</em> &lt;---- You can do better than that!!! ;)</p>,2019-08-20T03:58:34Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzjat6mbqbs1d8,2019-08-20T03:58:34Z,{},logistics
57,no,"<p>yeah, but why make up a story when that was the truth. I am intimidate by RL and would not have taken this class so soon otherwise</p>",2019-08-20T04:08:40Z,34,Week 8/18 - 8/24,feedback,,jzj6p4ln7yi5de,jzjb65tldio1m0,2019-08-20T04:08:40Z,{},logistics
58,no,"<p>Nah, it&#39;ll be fun. We&#39;ll help you like it.</p>
<p></p>
<p>Welcome!</p>",2019-08-20T04:16:24Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzjbg48ndwo494,2019-08-20T04:16:24Z,{},logistics
59,no,<p>thanks</p>,2019-08-20T04:19:21Z,34,Week 8/18 - 8/24,feedback,,jzj6p4ln7yi5de,jzjbjwk5qqg7k,2019-08-20T04:19:21Z,{},logistics
60,no,"<p>Hey Miriah! Looking to possibly make a Slack channel for the UT folks, let me know if you&#39;d be interested.</p>",2019-08-24T18:51:59Z,34,Week 8/18 - 8/24,feedback,,jqmjg3txqw6ji,jzpwhizgqea6r8,2019-08-24T18:51:59Z,{},logistics
61,no,"<p></p><ul><li>What&#39;s your name? Shuvalaxmi Panda</li><li>Where are you from? India</li><li>Where are you now? Bangalore, India</li><li>Why RL? I am interested to learn machine learning. I wanted to take ML but missed the Phase 1 registration. So RL.</li><li>How far into the program? 6th course. CV,KBAI,AI,IHI,EduTech completed.</li><li>Where next? Next subject will be ML.</li><li>Is there anything else you&#39;d like to share? Worried about not having any machine learning background before starting this class, but hopefully will be able to understand.  </li></ul>",2019-08-20T03:59:30Z,34,Week 8/18 - 8/24,followup,,jl2bw1yzg5w1ej,jzjaudv3411md,2019-08-20T03:59:30Z,{},logistics
62,no,<p>No background at all? Hmm.</p>,2019-08-20T04:16:50Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzjbgnrmmiy4g0,2019-08-20T04:16:50Z,{},logistics
63,no,"<p>Yes, just had some introduction while taking CV and AI course. This will be my first learning Machine learning related class. So feeling excited but nervous at the same time. </p>",2019-08-20T16:57:08Z,34,Week 8/18 - 8/24,feedback,,jl2bw1yzg5w1ej,jzk2mf5t7u3164,2019-08-20T16:57:08Z,{},logistics
64,no,<p>Great!</p>,2019-08-20T21:00:43Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkbbnux91g7i7,2019-08-20T21:00:43Z,{},logistics
65,no,<ul><li>What&#39;s your name? Anurag Tangri</li><li>Where are you from? India </li><li>Where are you now? San Francisco CA</li><li>Why RL? Learnt a bit about RL in ML4T and ML. Would love to deep dive and learn more on how to apply RL at my job.</li><li>How far into the program? 6th course.</li><li>Where next? I switched my career to be a Data Scientist early this year after being in the program for an year and it has been a great journey so far. I want to keep learning and adding to my skills to be a better Data Scientist.</li><li>Is there anything else you&#39;d like to share? I was a Big Data Engineer for almost a decade before switching to AI this year. I love to run. I did two half marathons this year and another one coming up in September end.</li></ul>,2019-08-20T04:09:33Z,34,Week 8/18 - 8/24,followup,,jfzaqnqvtQ1m,jzjb7arbfbd7b0,2019-08-20T04:09:33Z,{},logistics
66,no,"<p>Hello Anurag, I am also in SF.</p>",2019-08-20T21:40:05Z,34,Week 8/18 - 8/24,feedback,,jqy7j43dlz4di,jzkcqaj27pr1ef,2019-08-20T21:40:05Z,{},logistics
67,no,"<p>Hi Alison,</p>
<p>Nice to meet you.</p>",2019-08-23T12:53:04Z,34,Week 8/18 - 8/24,feedback,,jfzaqnqvtQ1m,jzo483j7ajl30t,2019-08-23T12:53:04Z,{},logistics
68,no,"<p>Hi! My name is Blin Kazazi and I live in Grayslake IL where I work from home as the CTO of Veritas Prep. We have been using ML for a couple of years now and I wanted to formalize my learning. RL because I want to learn about all three ML paradigms.  This is my first semester. </p>
<p></p>
<p>Excited to be taking this class with all of you! </p>",2019-08-20T04:14:43Z,34,Week 8/18 - 8/24,followup,,jzjaw2l4l6m5ns,jzjbdy53f6445f,2019-08-20T04:14:43Z,{},logistics
69,no,<p>Welcome to the program!</p>,2019-08-20T04:17:27Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzjbhguj7014tu,2019-08-20T04:17:27Z,{},logistics
70,no,"<p>Hi i&#39;m Isaac, I&#39;m from Melbourne FL. I&#39;m now in San Diego, CA.</p>
<p>I want to move my career more towards ML, and having a good in depth knowledge of the various sub topics will allow me to accomplish that.</p>
<p>This is my first course.</p>
<p>Where to next, who knows honestly...</p>
<p>I&#39;m also in Computational Photography this semester and I&#39;m very excited at the prospect of being able to post cat pics for a grade.</p>",2019-08-20T04:17:56Z,34,Week 8/18 - 8/24,followup,,jzj0p78cvcl57d,jzjbi307ugj74s,2019-08-20T04:17:56Z,{},logistics
71,no,"<p>Hello everyone.</p>
<ul><li>What&#39;s your name? Evgenii Ignatev (also Yevgeniy Ignatyev, name can be pronounced literally or as Eugene)</li><li>Where are you from? Russia, Samara</li><li>Where are you now? Same place, Software Engineer in Big Data</li><li>Why RL? Very exciting and rich domain of knowledge.</li><li>How far into the program? First course.</li><li>Where next? Hard to say.</li><li>Is there anything else you&#39;d like to share? Stay determined fellows.</li></ul>
<div>
<div>
<div></div>
<div style=""font-size:13px;background-color:#ffffff"">
<div>
<p style=""color:#000000""></p>
<p style=""color:#737373""></p>
</div>
</div>
</div>
</div>",2019-08-20T04:30:56Z,34,Week 8/18 - 8/24,followup,,jzhwdil7ssn443,jzjbysujg177jd,2019-08-20T04:30:56Z,{},logistics
72,no,"<p>Hi everyone! I&#39;m from midwest (near Chicago), but live in the SF Bay Area. I&#39;m fascinated by the art of RL. E.g. thinking about how to frame problems as MDPs, especially designing good reward functions. This is my 9th OMSCS course. Will be taking GA next, then (hopefully) graduating!</p>",2019-08-20T04:34:35Z,34,Week 8/18 - 8/24,followup,,ixnwq0s4ozg,jzjc3hkjuvn1d2,2019-08-20T04:34:35Z,{},logistics
73,no,"<p>Hi everyone!  I&#39;m Vahe and currently live in Las Vegas.  I am fascinated by the application of reinforcement learning to games such as chess, poker, and Starcraft.  This is my first class in the program.</p>",2019-08-20T04:43:51Z,34,Week 8/18 - 8/24,followup,,jzfsa4a37jf4aq,jzjcfepuw8f4he,2019-08-20T04:43:51Z,{},logistics
74,no,"<p>Vahe, are you on linkedin? wanted to connect with you. I live in Bayarea, CA.</p>",2019-12-10T15:50:00Z,18,Week 12/8 - 12/14,feedback,,j6ll2xkiDJf,k401ji041qn5jb,2019-12-10T15:50:00Z,{},logistics
75,no,"<p>Hi Kamran,</p>
<p></p>
<p>I&#39;m also in the Bay Area now, as of a few months ago.</p>
<p></p>
<p>I don&#39;t have a LinkedIn account.  My email is vhagopian3&#64;gatech.edu.</p>",2019-12-11T07:59:58Z,18,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k4106vh05sd35,2019-12-11T07:59:58Z,{},logistics
76,no,"<p></p><ul><li>What&#39;s your name?</li></ul>
<p>Tanner Lund</p>
<ul><li>Where are you from?</li></ul>
<p>&#34;From&#34; is a strong word. I&#39;ve lived all over the US and in 2 other countries.</p>
<ul><li>Where are you now?</li></ul>
<p>Lehi, Utah, as of this month</p>
<ul><li>Why RL?</li></ul>
<p>By far the most interesting form of ML I&#39;ve encountered</p>
<ul><li>How far into the program?</li></ul>
<p>If I don&#39;t screw this up I&#39;ll graduate in December</p>
<ul><li>Where next?</li></ul>
<p>Not sure.  I&#39;m also diving deep into complexity theory, statistics, and resilience engineering.</p>
<ul><li>Is there anything else you&#39;d like to share?</li></ul>
<p>Not off the top of my head but happy to chat</p>",2019-08-20T04:50:44Z,34,Week 8/18 - 8/24,followup,,ixr4jvzzg1zba,jzjco9bg9tj2tc,2019-08-20T04:50:44Z,{},logistics
77,no,"<p>Hey Tanner! Looking to possibly make a Slack channel for the UT folks, let me know if you&#39;d be interested.</p>",2019-08-24T18:51:10Z,34,Week 8/18 - 8/24,feedback,,jqmjg3txqw6ji,jzpwghf2fb663h,2019-08-24T18:51:10Z,{},logistics
78,no,"<p></p><ul><li>What&#39;s your name?: Anand Krishnamurthy</li><li>Where are you from? : India</li><li>Where are you now?: SF Bay Area</li><li>Why RL? : I am taking all the toughest courses first, RL is supposedly close to the top in this aspect. Plus I am really interested in this field: AI, ML, RL..</li><li>How far into the program? I have taken AI and Compilers before this</li><li>Where next? BD4H, ML and CV are probably my next courses</li><li>Is there anything else you&#39;d like to share? I work as eng manager currently and love to be back in school after almost 20 yeas</li></ul>",2019-08-20T05:38:47Z,34,Week 8/18 - 8/24,followup,,jqrr36mqfm8M,jzjee1xtm6d69o,2019-08-20T05:38:47Z,{},logistics
79,no,"<p>Hello! </p>
<p></p>
<p>I&#39;m Kiet and i&#39;m currently in San Francisco. This is going to be my 5th class and I&#39;m super excited to learn more about this domain.</p>",2019-08-20T05:40:20Z,34,Week 8/18 - 8/24,followup,,j71a6os8cxo6o2,jzjeg1jvys76zc,2019-08-20T05:40:20Z,{},logistics
80,no,"<p>Hi I&#39;m Amy and I&#39;m currently in Redwood City, CA. This is my first class and I currently work as a data scientist at a startup. I&#39;m originally fro San Francisco. Interested in learning about reinforcement learning for applications to my work. </p>",2019-08-20T06:01:21Z,34,Week 8/18 - 8/24,followup,,jzjdmd9smin752,jzjf737miod5pf,2019-08-20T06:01:21Z,{},logistics
81,no,"<p><strong>What&#39;s your name?</strong><br />Quang Vu<br /> <br /><strong>Where do you currently live?</strong><br />Orange county, California, Earth, Milky Way.<br /><br /><strong>What other OMS courses have you taken so far?</strong><br />GA, AI4R<br /><br /><strong>Why RL?</strong><br />I love Dr Isbell and Dr Littman joke so much, I want to take this class to keep watching their videos.</p>
<p>On a serious note, I have couple things I planned, and RL is a great class for it:</p>
<p>- I want improve my reading/writing skill. By the end of this class, I want to improve my ability to read (and understand) research papers, as well as my ability to write a good formal paper. I am currently work as software engineer. And RL class, and OMSCS in general, provide me a platform to push myself onto a different role under the academic setting.</p>
<p>- I have lot of thought and ideas of my own. But currently I don&#39;t yet have skills and foundation to explore them. I hope RL class will give me the skills and opens door for me to explore deeper into my ideas.<br /><br /><strong>What is something interesting about you?</strong><br />Love dog, soccer, rain, cool weather, fountain pen, fruity beer.<br />Speak bad English when sober, broken Japanese when drunk, and flawless Vietnamese during sleep.<br /><br /><a href=""https://www.linkedin.com/in/quang-vu"">https://www.linkedin.com/in/quang-vu</a></p>",2019-08-20T06:12:41Z,34,Week 8/18 - 8/24,followup,,jl5wq8mca7o0,jzjflnxzmxfq2,2019-08-20T06:12:41Z,{},logistics
82,no,"<p>Hello!</p>
<p></p>
<p>What&#39;s your name? <b>Kenny or simply Q (preferred, like the alphabet) </b></p>
<p></p>
<p>Where are you from? <b>Phanat Nikhom, Thailand</b></p>
<p></p>
<p>Where are you now? <b>Los Angeles</b></p>
<p></p>
<p>Why RL? <strong>On my way to finding out why everyone finds AI so sexy..</strong></p>
<p></p>
<p>How far into the program? <b>9th course.</b></p>
<p></p>
<p>Where next? <strong>10th course. Then complete and utter fear of the question: where next?</strong></p>
<p></p>
<p>Is there anything else you&#39;d like to share? <b>Both excited and scared to be here (given its reputation).</b></p>",2019-08-20T06:35:04Z,34,Week 8/18 - 8/24,followup,,j6jyvvn7Ty0,jzjgeflp44d2wi,2019-08-20T06:35:04Z,{},logistics
83,no,"<p></p><ul><li>What&#39;s your name? - <strong>Chris Cho</strong></li><li>Where are you from? -  <strong>Many places, but I call ATL my home</strong></li><li>Where are you now? - <strong>Singapore</strong></li><li>Why RL? - <strong>ML requirement</strong></li><li>How far into the program? - <b>5th course</b></li><li>Where next? -<strong> Hopefully enroll in my 6th course</strong></li><li>Is there anything else you&#39;d like to share? <strong>I dropped this course once before - very scared</strong></li></ul>",2019-08-20T07:39:12Z,34,Week 8/18 - 8/24,followup,,iv2q8g9wyas6yx,jzjiowloc9j4g5,2019-08-20T07:39:12Z,{},logistics
84,no,"<p></p><ul><li>What&#39;s your name? Zhiqi Yu (pronounced as &#39;Gichi&#39;)</li><li>Where are you from? I am from China and currently living in Fairfax, VA.</li><li>Why RL? Though I have learned ML before, RL is very new to me, plus I am very interested in RL.</li><li>How far into the program? First semester.</li><li>Where next? Earn internships, finish the program and find a full-time job.</li></ul>",2019-08-20T08:39:55Z,34,Week 8/18 - 8/24,followup,,jzhy4sbxyar5l4,jzjkuzqchbgni,2019-08-20T08:39:55Z,{},logistics
85,no,"<p>I am Justin Allen and I&#39;m from Huntsville, AL.  I have loved the RL bits that you get exposed to in ML and ML4T and I can&#39;t wait to take this class and explore RL in depth.  This is class #8 and my final &#34;core&#34; class from the Machine Learning specialization.  After this I&#39;m hoping to find a PhD program that fits into the work / life balance as well as OMSCS has.</p>",2019-08-20T11:10:21Z,34,Week 8/18 - 8/24,followup,,idfzuuu9fnI,jzjq8g6cc4p4pa,2019-08-20T11:10:21Z,{},logistics
86,no,"<p>Hey Justin,</p>
<p>Welcome to the course.</p>
<p></p>
<p>I am also exploring the possibility of a PhD program in future with a work/life balance. Let&#39;s stay in touch to find more on this.</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/anurag-tangri/"">https://www.linkedin.com/in/anurag-tangri/</a></p>
<p></p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>
<p></p>",2019-08-23T12:57:21Z,34,Week 8/18 - 8/24,feedback,,jfzaqnqvtQ1m,jzo4dm7b73q5of,2019-08-23T12:57:21Z,{},logistics
87,no,"<p>Hi All,</p>
<p>I am Arun and I live in Bangalore, India. This is my fourth course, I am doing KBAI along with this.</p>
<p>I have completed GIOS, CN, AI4R courses so far</p>
<p>I work for Broadcom Limited in Software Automotive team</p>
<p>I wanted to major in ML. As all other courses were closed by my time slot, have taken RL which was the only one available. Hopefully I can make this course with out any ML background</p>
<p> </p>
<p>I play badminton, would love to go for long rides during my free time.</p>",2019-08-20T11:25:00Z,34,Week 8/18 - 8/24,followup,,jqmcecftgff63y,jzjqraes7r8nr,2019-08-20T11:25:00Z,{},logistics
88,no,"<p>Hi,</p>
<p></p>
<p>My name is Neeza Thandi and I live in Boston MA. This is my 4th course in the program. I am an actuary by profession. I am taking RL to learn more about the technique having learnt about it in the ML class. </p>
<p>I like to cook, and hang out with my 2 dogs... and my human family.</p>",2019-08-20T12:15:35Z,34,Week 8/18 - 8/24,followup,,jl88dnp8AZzD,jzjskcqmkc69,2019-08-20T12:15:35Z,{},logistics
89,no,"<p></p>
<ul><li>What&#39;s your name?<br />
<ul><li>Daniel Vasilyonok</li></ul>
</li><li>Where are you from?
<ul><li>Boston, MA</li></ul>
</li><li>Where are you now?
<ul><li>Boston, MA</li></ul>
</li><li>Why RL?
<ul><li>It seemed like the next step after completing ML and ML4T. I&#39;m doing the ML specialization as well. Also, RL seems like the most analogous type of ML to how humans and various other biological organisms learn through experience.  </li></ul>
</li><li>How far into the program?
<ul><li>Completed 4/10 classes. ML, ML4T, AI4R, GIOS. Currently taking this and CV!</li></ul>
</li><li>Where next?
<ul><li>After this semester I&#39;m going to take HPC, GA, BD4H, and AI. If Deep Learning gets released online prior to graduating, I&#39;ll replace AI with Deep Learning!</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>Connect w me on Linkedin if you&#39;d like!</li><li>
<ul><li><a href=""https://www.linkedin.com/in/daniel-vasilyonok-18369a32/"">https://www.linkedin.com/in/daniel-vasilyonok-18369a32/</a><a href=""https://www.linkedin.com/in/daniel-vasilyonok-18369a32/""></a></li></ul>
</li><li>Trying to take more trips like this:
<ul><li><a href=""https://www.youtube.com/watch?v=5e6wNZ17s4s&amp;t="">https://www.youtube.com/watch?v=5e6wNZ17s4s&amp;t=</a></li></ul>
</li></ul>
</li></ul>
<p></p>",2019-08-20T13:35:12Z,34,Week 8/18 - 8/24,followup,,jl1b27fpaYkv,jzjveqlfytu5ox,2019-08-20T13:35:12Z,{},logistics
90,no,"<p></p><ul><li>What&#39;s your name?
<ul><li>Sahil Soni</li></ul>
</li><li>Where are you from?
<ul><li>New Jersey / Springfileld</li></ul>
</li><li>Where are you now?
<ul><li>New York </li></ul>
</li><li>Why RL?
<ul><li>Might be RL is next big thing ? Who knows </li></ul>
</li><li>How far into the program?
<ul><li>First Semester</li></ul>
</li><li>Where next?
<ul><li>ML/GA/CV/ML4T/AI</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>   Let&#39;s see if you can get job in ML after completing this. Currently working as DBA </li></ul>
</li></ul>",2019-08-20T13:51:39Z,34,Week 8/18 - 8/24,followup,,jzjvnayuw8t2br,jzjvzwc2zr828i,2019-08-20T13:51:39Z,{},logistics
91,no,"<p>My name Tom Zhang, I am from Canada. I am in Toronto now. I am interested in ML/RL currently which may change the relationship and communications between the world and human beings. I am in first semester. Next will be ML/GA/AI etc. Would like to make new friends here to help each.</p>",2019-08-20T14:18:35Z,34,Week 8/18 - 8/24,followup,,jzjabmjd3l2103,jzjwyipc2g214t,2019-08-20T14:18:35Z,{},logistics
92,no,"<p>My name is Oscar Thörn, and Örebro in Sweden (which is in Europe and is not the same as Switzerland ;) ). This is my first course in the program. In addition to OMSCS I´m currently studying part time at Örebro University where I´m also working as a research assistant in a lab focused on collaborative AI.</p>
<p></p>
<p>Why RL? Because learning from experience is interesting and since humans can do it so well, why not computers? (And because I just like all areas of AI).</p>",2019-08-20T14:27:23Z,34,Week 8/18 - 8/24,followup,,jzjwcq2u8o7110,jzjx9u50gw6ut,2019-08-20T14:27:23Z,{},logistics
93,no,"<p>- Yu Bai</p>
<p>- NY</p>
<p>- wrapping up the Machine Learning track. Excited to &#34;reinforce&#34; my RL knowledge &amp; skill formally</p>
<p>- This is my 9th class</p>
<p>- next will be either AI or CV </p>
<p></p>",2019-08-20T14:48:23Z,34,Week 8/18 - 8/24,followup,,ijalzjp38ba40l,jzjy0uwq5iqgf,2019-08-20T14:48:23Z,{},logistics
94,no,<p></p>,2019-08-20T15:07:13Z,34,Week 8/18 - 8/24,feedback,,jl2bv84c6FEX,jzjyp28wxf02kv,2019-08-20T15:07:13Z,{},logistics
95,no,"<p></p>
<ul><li>What&#39;s your name? Shimlee Sengupta</li><li>Where are you from? India</li><li>Where are you now?India</li><li>Why RL?Seems a very interesting subject.Have heard very good review about it and would like to learn the same.</li><li>How far into the program? taken 5 subjects including CV,KBAI,AI,IHI,edutech</li><li>Where next? Hoping to work in ML domain and expand my knowledge</li><li>Is there anything else you&#39;d like to share?Pretty nervous about this class as I am taking this without finishing ML .Hoping to get the concepts clear.</li></ul>",2019-08-20T15:07:55Z,34,Week 8/18 - 8/24,followup,,jl2bv84c6FEX,jzjypykwkib34e,2019-08-20T15:07:55Z,{},logistics
96,no,"<p></p>
<ul><li>What&#39;s your name?
<ul><li><strong>Ravi Kishore</strong></li></ul>
</li><li>Where are you from?
<ul><li><strong>Hyderabad, India</strong></li></ul>
</li><li>Where are you now?
<ul><li><strong>Delhi, India</strong></li></ul>
</li><li>Why RL?<strong> </strong>
<ul><li><strong>I am intrigued about the Idea of RL, want to take a deeper dive into it.</strong></li></ul>
</li><li>How far into the program?<strong> </strong>
<ul><li><strong>This is my second semester, done with AI4R and CV.</strong></li></ul>
</li><li>Where next?
<ul><li><strong>Looking forward to explore / exploit <em>Reinforcement Learning</em> in robotics</strong></li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li><strong>Planning to take this course along with AI, not sure if its a good idea. So, a bit worried about that.</strong></li></ul>
</li></ul>",2019-08-20T15:31:34Z,34,Week 8/18 - 8/24,followup,,jqi9pq31o4p6dt,jzjzkdi6rgb5ly,2019-08-20T15:31:34Z,{},logistics
97,no,"<p></p><ul><li>What&#39;s your name? <strong>Alan Cespedes</strong></li><li>Where are you from? <strong>San Antonio, Texas</strong></li><li>Where are you now? <strong>Dallas, Texas</strong></li><li>Why RL? <strong>Took Machine Learning and was interested in expanding my RL breadth (:</strong></li><li>How far into the program? <strong>Last semester! yay!</strong></li><li>Where next? <strong>Relaxing for a year with just work and possibly going for an MBA</strong></li><li>Is there anything else you&#39;d like to share? <strong>Recently been traveling more personally and for work.</strong></li></ul>",2019-08-20T15:38:55Z,34,Week 8/18 - 8/24,followup,,j6jxj5zix3746q,jzjztu6js9z35v,2019-08-20T15:38:55Z,{},logistics
98,no,"<p>Hi, I&#39;m Binod from Atlanta, Georgia. New to OMSCS, this is my first class.</p>",2019-08-20T17:47:59Z,34,Week 8/18 - 8/24,followup,,jzk2u4t3n5o5el,jzk4ftrp53h5vj,2019-08-20T17:47:59Z,{},logistics
99,no,"<p>Hi my name is Lucas Pasqualin! I work at a Chatbot/Virtual Agent startup in Orlando Florida. I&#39;ve always been interested in ML, and I&#39;m hoping I can apply some RL concepts to improve some internal products at work.</p>
<p></p>
<p>First class in OMSCS.</p>",2019-08-20T17:48:52Z,34,Week 8/18 - 8/24,followup,,jzk4afmbx073hw,jzk4gylin81uz,2019-08-20T17:48:52Z,{},logistics
100,no,"<p></p>
<p>Hello all!</p>
<p></p>
<p>Rob Cohen</p>
<p></p>
<p>RL because I want to give the ML specialization a shot. Hoping I do well enough to continue it!</p>
<p></p>
<p>I am from NYC, and I am still in NYC.</p>
<p></p>
<p>This will be my 3rd class. Was originally going to do Computing Systems.</p>
<p></p>
<p>Unfortunately I could not get into the lower level ML course (still #554 on the waitlist).</p>
<p>I know the syllabus says this isn&#39;t required, but it is frustrating because I know I&#39;ll have a substantively harder time in this course because of it.</p>
<p></p>",2019-08-20T18:08:13Z,34,Week 8/18 - 8/24,followup,,jl2dfm6lskZz,jzk55tscsuk5fn,2019-08-20T18:08:13Z,{},logistics
101,no,"<p> </p>
<ul><li>What&#39;s your name?
<ul><li>Hi hi, my name is Jay</li></ul>
</li><li>Where are you from?
<ul><li>I am from NOVA area in VA.</li></ul>
</li><li>Where are you now?
<ul><li>I am in the same area!</li></ul>
</li><li>Why RL?
<ul><li>I have some ML experience and I want to build on that.</li></ul>
</li><li>How far into the program?
<ul><li>This is my first semester, I hope it all goes well!</li></ul>
</li><li>Where next?
<ul><li>More research experience hopefully!</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>I am trying to get into traveling more often.</li></ul>
</li></ul>",2019-08-20T18:09:47Z,34,Week 8/18 - 8/24,followup,,jzhbewk46d19d,jzk57uqdebl6i2,2019-08-20T18:09:47Z,{},logistics
102,no,<p></p><ul><li>What&#39;s your name? Ruobing Zheng</li><li>Where are you from? China</li><li>Where are you now? NYC</li><li>Why RL? Interested in ML related areas and would like to use it in real world</li><li>How far into the program? Fresh new</li></ul>,2019-08-20T19:06:23Z,34,Week 8/18 - 8/24,followup,,jzk6nge7yl24dc,jzk78n8i87p7bi,2019-08-20T19:06:23Z,{},logistics
103,no,"<p></p><ul><li>Abdoul Ouedraogo</li><li>Oregon, USA</li><li>Still in Oregon</li><li>I just find the subject captivating because it is kind of mimicking </li><li>This is my 3rd course</li><li>I would like to do research and this class will probably give me the basic to start in RL</li><li>Go RL. </li></ul>",2019-08-20T19:13:36Z,34,Week 8/18 - 8/24,followup,,jl0c61kgrmsY,jzk7hwrnmw417l,2019-08-20T19:13:36Z,{},logistics
104,no,"<p>Hi everyone. My name is Omar. I&#39;m from Peru, but I recently moved to Switzerland to join Google. I have some experience in ML, but not any in RL at all. This is my second class!</p>",2019-08-20T20:04:26Z,34,Week 8/18 - 8/24,followup,,jqmfnc46kl26eg,jzk9baanzoa4rx,2019-08-20T20:04:26Z,{},logistics
105,no,"<p></p>
<ul><li>What&#39;s your name? Prashant Verma</li><li>Where are you from? India</li><li>Where are you now? Sunnyvale, California, USA</li><li>Why RL? Specializing in ML. Seems like a pretty interesting topic</li><li>How far into the program? 6th course in Gatech</li><li>Where next? Let&#39;s see where life takes me.</li></ul>
<p></p>",2019-08-20T20:54:00Z,34,Week 8/18 - 8/24,followup,,jc8bgo8obmm4kl,jzkb31hqm706nl,2019-08-20T20:54:00Z,{},logistics
106,no,"<p>Hi classmates, my name is Guilherme, I&#39;m from Brazil and live in the Bay area. This is my 8th course in OMSCS, and plan to graduate next year!</p>
<p>I&#39;m interested in RL as its amazing to see how problems can be framed in &#34;learning through experience&#34;, isn&#39;t that how we learn ourselves? </p>
<p>Good luck to all. </p>",2019-08-20T21:23:18Z,34,Week 8/18 - 8/24,followup,,is6e83bsfvk,jzkc4pdnvv74fp,2019-08-20T21:23:18Z,{},logistics
107,no,Nice analogy!,2019-08-22T02:08:56Z,34,Week 8/18 - 8/24,feedback,,jziehbrvvwc4hw,jzm1rwdh8kz5yd,2019-08-22T02:08:56Z,{},logistics
108,no,"<p></p><ul><li>What&#39;s your name? Dylan Warnock</li><li>Where are you from? Austin, TX</li><li>Where are you now? Austin, TX</li><li>Why RL? It&#39;s super interesting!</li><li>How far into the program? This is my 6th class</li><li>Where next? Maybe I&#39;ll move to NYC</li><li>Is there anything else you&#39;d like to share? I&#39;m excited for the course!</li></ul>",2019-08-20T21:23:58Z,34,Week 8/18 - 8/24,followup,,h6e6q7ftqr747f,jzkc5kzifvb3qy,2019-08-20T21:23:58Z,{},logistics
109,no,"<p>Super interesting, indeed.</p>",2019-08-20T23:01:06Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkfmhmp7fldt,2019-08-20T23:01:06Z,{},logistics
110,no,"<p>Hello everyone! My name is Luis Miguel, I&#39;m from Peru and live in Miami. I am interested in RL since I have always been interested in artificial intelligence. This is my first class.<br /><br />Happy to be here!</p>",2019-08-20T23:00:09Z,34,Week 8/18 - 8/24,followup,,jzhn3r3w5di5fn,jzkfl9s7hax2h9,2019-08-20T23:00:09Z,{},logistics
111,no,<p>Lots of people new to the program... exciting! Welcome.</p>,2019-08-20T23:00:45Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkfm1hvfpy5p,2019-08-20T23:00:45Z,{},logistics
112,no,"<p>Hey everyone! </p>
<p></p>
<p>I&#39;m Sijae, and I&#39;m originally from Missouri. I now live Denver, Colorado. This is my first class in the program, and I chose RL because 1) direct work applicability, 2) it came highly recommended, and 3) I think it&#39;s the most interesting subfield of ML, and I&#39;m super excited that there&#39;s a whole course dedicated to it! I work as a software engineer on an AI research team. What&#39;s next? Course-wise: maybe ML? Not sure! Life: No idea, but I&#39;m always game for an adventure! When I&#39;m not on a computer, my husband and I ride horses, run, hike, play music, frequent local coffee shops, and use every excuse to be outside in this beautiful state! </p>
<p></p>
<p>Cheers everyone! </p>",2019-08-20T23:49:08Z,34,Week 8/18 - 8/24,followup,,jzhnj355bzr2uu,jzkhc918b8412y,2019-08-20T23:49:08Z,{},logistics
113,no,"<p>Oh, Sijae, hmm. Welcome to Denver, the program, and RL!</p>",2019-08-21T00:55:51Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkjq29q93x24c,2019-08-21T00:55:51Z,{},logistics
114,no,"Hi, my name is Su Zhang. I work in Washington DC area. This is my 5th class. RL looks very interesting. Look forward to it.",2019-08-21T00:57:43Z,34,Week 8/18 - 8/24,followup,,j6ilcw6hxoc77h,jzkjsg4essk58e,2019-08-21T00:57:43Z,{},logistics
115,no,"<p>Greetings classmates and TA&#39;s,</p>
<p>I am Prasad Rao, a resident of Bangalore, India. <a href=""https://www.linkedin.com/in/prasadraoj/"">https://www.linkedin.com/in/prasadraoj/</a></p>
<p>I am a software professional, working in Dell R&amp;D focussing primarily on Systems Management.</p>
<p><br />Lately, we have been working on the applications of Machine learning in this area and I found myself lacking the skill necessary for a deeper dive.</p>
<p>Thus began my journey into OMSCS. RL would be my 4th course and the first in the ML specialization.</p>
<p><br />Like &#34;Miriah Peterson&#34; mentioned earlier, I am intimidated by RL, primarily since I do not meet its prerequisites of having completed ML.</p>
<p>But, I intend to meet, and exceed the other prerequisite mentioned by Dr. Isbell, that of enjoying this course...:)<br />I also hope to learn and do well in this class.</p>
<p></p>
<p>Here is wishing all of us an extremely productive and enjoyable term!!</p>
<p></p>
<p></p>",2019-08-21T01:01:47Z,34,Week 8/18 - 8/24,followup,,j6m1j8xqcvj6t6,jzkjxou26he6b7,2019-08-21T01:01:47Z,{},logistics
116,no,"<p>Hello,</p>
<p></p>
<p>I was able to enroll for ML after the waitlists were cleared earlier today.</p>
<p></p>
<p>Hence I will drop RL for this term and attempt to enroll again next term.</p>
<p></p>
<p>All the best to you&#39;ll.</p>",2019-08-23T14:57:10Z,34,Week 8/18 - 8/24,feedback,,j6m1j8xqcvj6t6,jzo8nozwv5d4pd,2019-08-23T14:57:10Z,{},logistics
117,no,"<p>Hi everyone, my name is Amanda House. I was born and raised in Austin, Tx. I am currently living in Dallas, Tx. I am interested in RL because it pertains to the work I do at McAfee where I work as a Data Scientist. I am specifically interested in how RL can be used to detect malware. This is my 3rd course in the program. I have completed KBAI and ML. I have a Bachelors in Mathematics from UT Austin (Hook &#39;Em). I am interested in talking cyber-security and machine learning with anyone so feel free to follow me at the links below. </p>
<p></p>
<p>LinkedIn: <a href=""https://www.linkedin.com/in/amanda-house-a20562a4/"">https://www.linkedin.com/in/amanda-house-a20562a4/</a></p>
<p>Twitter: &#64;TheAmandaHouse</p>
<p></p>",2019-08-21T01:37:30Z,34,Week 8/18 - 8/24,followup,,jl2bv90xkrs13q,jzkl7ly675n4jp,2019-08-21T01:37:30Z,{},logistics
118,no,"<p></p>
<ul><li>What&#39;s your name?</li></ul>
<p>           Ashot Hambardzumyan</p>
<ul><li>Where are you from?</li></ul>
<p>           Armenia</p>
<ul><li>Where are you now?</li></ul>
<p>           Los Angeles</p>
<ul><li>Why RL?</li></ul>
<p>           Game Theory is fascinating</p>
<ul><li>How far into the program?</li></ul>
<p>          Thus far CV, AI4R, ML, Compilers in that order</p>
<ul><li>Where next?</li></ul>
<p>           Don&#39;t know.</p>
<ul><li>Is there anything else you&#39;d like to share?</li></ul>
<p>          Let&#39;s suffer together. </p>",2019-08-21T01:43:40Z,34,Week 8/18 - 8/24,followup,,jl1d3uyrhAts,jzklfjwjgjn7h2,2019-08-21T01:43:40Z,{},logistics
119,no,<p>Cheers to the suffering :D !!</p>,2019-08-22T05:25:56Z,34,Week 8/18 - 8/24,feedback,,jqt6oket86zV,jzm8t8wb8q34rq,2019-08-22T05:25:56Z,{},logistics
120,no,"<p>Hello. My name is Frank Korf. I was born and raised in central Wisconsin. I now live in Oswego, IL (west of Chicago). I&#39;m interested in RL because I&#39;ve always enjoyed AI, and am excited about learning in a more formal setting. This is my first course.</p>
<p></p>",2019-08-21T01:46:43Z,34,Week 8/18 - 8/24,followup,,jzjwhmhsghg5cy,jzkljhasf6p23k,2019-08-21T01:46:43Z,{},logistics
121,no,"<p></p><ul><li>Connor Hayes </li><li>from Houston, TX and currently reside in Houston, TX (had a ~15 year gap)</li><li>I&#39;m taking RL because of how much I liked Littman&#39;s/Isbell&#39;s/Students&#39; Thriller ML acapella and banter we all enjoy. Also interested in robotic RL applications for motion planning/control</li><li>This is my 7th/8th course in the program (past = SDP, ML, AI4R, CP, CV, CN. Now= ML4T,RL)</li><li>If I can&#39;t get to the moon, or a golden astroid, I&#39;ll settle for Texas :)</li><li>Two courses at once, yippee!!</li></ul>",2019-08-21T01:52:03Z,34,Week 8/18 - 8/24,followup,,ixqeixjocgb6pp,jzklqbw7u4r22y,2019-08-21T01:52:03Z,{},logistics
122,no,<p>Texas is great. Welcome!</p>,2019-08-21T21:02:48Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzlqu76izd1126,2019-08-21T21:02:48Z,{},logistics
123,no,"<p></p><ul><li>Patrick Lind</li><li>Indianapolis, IN</li><li>Live and work around Indianapolis for Eli Lilly &amp; Co.</li><li>Reinforcement learning has always been such an interesting topic to me and will soon have some features in upcoming products that may make use of this.</li><li>This is my 4th class.</li><li>Hopefully getting to apply this information in my current day job!</li></ul>",2019-08-21T01:56:23Z,34,Week 8/18 - 8/24,followup,,jl2bqhkbsux4r,jzklvwtenqz65a,2019-08-21T01:56:23Z,{},logistics
124,no,<p></p><ul><li>Peter Shin</li><li>From Chicago</li><li>Currently in Northern Virginia</li><li>I am interested in developing learning systems in work.</li><li>This is my first course.</li><li>I am planning to take Machine Learning next.</li></ul>,2019-08-21T01:57:31Z,34,Week 8/18 - 8/24,followup,,jzj3hwdp508at,jzklxd28eck7il,2019-08-21T01:57:31Z,{},logistics
125,no,<p>Welcome to the program.</p>,2019-08-21T02:21:04Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkmrn63dp97i9,2019-08-21T02:21:04Z,{},logistics
126,no,"<p></p><ul><li>Sashank Aryal</li><li>Boston</li><li>RL because I like the question it asks</li><li>First course of OMSCS</li><li>ML Specialization</li><li>I&#39;m a software engineer at Amazon. I have a CS background, and have done some work with self-driving cars, but I don&#39;t have a rigorous theoretical ML background.</li></ul>",2019-08-21T02:50:25Z,34,Week 8/18 - 8/24,followup,,jqmu8f72foxu8,jzkntdvwjyu5jo,2019-08-21T02:50:25Z,{},logistics
127,no,"<p></p><ul><li>What&#39;s your name? <strong>Sam Sultan</strong></li><li>Where are you from? <strong>I am originally from Lebanon</strong></li><li>Where are you now? <strong>I live in San Diego, CA</strong></li><li>Why RL? <b>I am just interested in learning it</b></li><li>How far into the program? <strong>almost half way</strong></li><li>Where next? <strong>Go to the industry</strong></li><li>Is there anything else you&#39;d like to share? <strong>I am glad I am in this program, since my undergrad deg was in Biochem</strong></li></ul>",2019-08-21T02:53:44Z,34,Week 8/18 - 8/24,followup,,jl2842xr2jt3fe,jzknxn9jy4m407,2019-08-21T02:53:44Z,{},logistics
128,no,"<p>Hey all, my name is Bill Zhou. I&#39;m from China and went to Ottawa, Canada for Bachelor Degree and end up in New York City now. I&#39;ve had some experience with ML and applied to my current job, but mostly around regression and classification problems. I never had a chance to actually get chance to learn RL. This is my first term and the first class.</p>
<p></p>",2019-08-21T03:16:36Z,34,Week 8/18 - 8/24,followup,,jziegutgfx2455,jzkor2g6jw86xh,2019-08-21T03:16:36Z,{},logistics
129,no,"<p></p><ul><li><strong>What&#39;s your name?</strong> Jose Santos</li><li><strong>Where are you from?</strong> Venezuela</li><li><strong>Where are you now?</strong> Santa Monica, CA</li><li><strong>Why RL?</strong> I was curious to learn more about the subject!</li><li><strong>How far into the program?</strong> This is my first semester</li><li><strong>Where next?</strong> Space (?)</li><li><strong>Is there anything else you&#39;d like to share?</strong> If you live in Santa Monica, send me a message!.</li></ul>
<p> </p>",2019-08-21T03:38:39Z,34,Week 8/18 - 8/24,followup,,jzkpbbtdp594ln,jzkpjff83n73sy,2019-08-21T03:38:39Z,{},logistics
130,no,"<p>Hello everyone. My name is Kevin Quinn and I currently reside in Salt Lake City, Utah  I have over 12 years experience in the software industry and achieving a graduate degree is one of my life goals.  I currently work for a company creating quality and document control software for organizations such as the FDA, medical companies, etc.  This is my 7th course in OMSCS and I am taking it in conjunction with Reinforcement Learning this semester.  I enjoy fishing, soccer, DIY projects around the house, and video gaming when I can.  I am looking forward to another great class in this program.</p>",2019-08-21T03:46:30Z,34,Week 8/18 - 8/24,followup,,jc5n7m3r8yc3br,jzkpti7lkfo6ea,2019-08-21T03:46:30Z,{},logistics
131,no,"<p>Hey Kevin! Looking to possibly make a Slack channel for the UT folks, let me know if you&#39;d be interested.</p>",2019-08-24T18:51:31Z,34,Week 8/18 - 8/24,feedback,,jqmjg3txqw6ji,jzpwgx44qp96eh,2019-08-24T18:51:31Z,{},logistics
132,no,<p>Sure that sounds great!  Thanks!</p>,2019-08-27T21:41:22Z,33,Week 8/25 - 8/31,feedback,,jc5n7m3r8yc3br,jzucux0chau495,2019-08-27T21:41:22Z,{},logistics
133,no,"<p></p>
<ul><li>What&#39;s your name? <strong>Danilo Martins</strong></li><li>Where are you from? <strong>Brazil</strong></li><li>Where are you now? <strong>I live in Houston, TX</strong></li><li>Why RL? <strong>I&#39;m pursuing Machine Learning specialization and I&#39;ve heard good things about RL.</strong></li><li>How far into the program? <strong>Half way through, this is my 6th course.</strong></li><li>Where next? <strong>After surviving to this one, I&#39;ll try ML.</strong></li><li>Is there anything else you&#39;d like to share?  <strong>My linkedIn profile to connect <a href=""https://www.linkedin.com/in/danilo-martins-14144627/"">https://linkedin.com/in/danilo-martins-14144627/</a></strong></li></ul>
<p></p>",2019-08-21T03:47:50Z,34,Week 8/18 - 8/24,followup,,jc6xvgjncoey,jzkpv81so6x2r0,2019-08-21T03:47:50Z,{},logistics
134,no,<p></p><ul><li>What&#39;s your name? Ruiying Wang</li><li>Where are you from? China</li><li>Where are you now? California</li><li>Why RL? interested in Machine Learning</li><li>How far into the program? This is my 7th course</li><li>Where next? Finish the program</li><li>Is there anything else you&#39;d like to share? I have degrees in Chemistry and hope I can get my master degree in ML</li></ul>,2019-08-21T13:24:12Z,34,Week 8/18 - 8/24,followup,,j6lnhjhnzr3y5,jzlagg58ui5qy,2019-08-21T13:24:12Z,{},logistics
135,no,"<p></p><ul><li>What&#39;s your name? Kevin Stewart</li><li>Where are you from? California</li><li>Where are you now? California</li><li>Why RL? Really interested in reinforcement learning, I&#39;d like to build on what I learned in ML4T and a course that featured RL projects in my undergraduate program.</li><li>How far into the program? This is my third course in the program. So far, I&#39;ve taken ML4T and KBAI.</li><li>Where next? I&#39;d like to take CS 7641 - Machine learning</li><li>Is there anything else you&#39;d like to share? Nothing at this point!</li></ul>",2019-08-21T14:59:44Z,34,Week 8/18 - 8/24,followup,,idghbt86wqe,jzldvaybxgr795,2019-08-21T14:59:44Z,{},logistics
136,no,"<p>Hi guys! My name is Ben, I&#39;m from Toronto, Canada. This is the first course for me in the OMSCS program. I currently work on a ML-related product in a mobile SoC company, and have acquired some grasp of ML concepts through my work experience. I&#39;m interested in Reinforcement Learning because it&#39;s a very cool subject that has lots of potential applications. After this course, I&#39;d like to take 7641 (Machine Learning) or High Performance Computer Architecture. </p>",2019-08-21T16:07:44Z,34,Week 8/18 - 8/24,followup,,jzlf32odc2k20h,jzlgaqjusrs51,2019-08-21T16:07:44Z,{},logistics
137,no,"Hey guys! My name is Manish Wadhwani and I&#39;m from Bangalore, India. I&#39;m a software engineer, working in EY Product engineering division. This is my 5th course into the OMSCS program. I had taken ML4T course and the last project got me excited to take up this course. I plan to do Machine learning specialisation and if possible also cover systems specialisation. I used to do competitive programming and participate in CTFs.<div><br /></div><div>Linkedin profile: <md><br />https://linkedin.com/in/manishrw<br /></md></div>",2019-08-21T17:10:27Z,34,Week 8/18 - 8/24,followup,,jm666efxmod97,jzlijeoq474n4,2019-08-21T17:10:27Z,{},logistics
138,no,"<p>Hi, my name is Dylan McDowell and I am a modeling &amp; simulation engineer at Idaho National Laboratory (hence the nuclear icon ;)). This is my first semester in the program and I&#39;m super excited to take this course. I eventually want to pursue a PhD in Statistics and felt like there was too much division between the two fields of ML and Stats so that is why I am pursuing the OMSCS. </p>",2019-08-21T17:37:26Z,34,Week 8/18 - 8/24,followup,,jzkifxpxau95an,jzlji3fze964l8,2019-08-21T17:37:26Z,{},logistics
139,no,"<p>Hey Dylan,</p>
<p>Welcome to the course. At some point, I also might wanna do a PhD (i.e. once my wife promises not to leave me due to my studying all the time ;) ).</p>
<p></p>
<p>My manager is a PhD in Stats (coincidently from GA Tech also) and leads our AI Research team now.</p>
<p>I might ask her view on the same since she actually lived through this. I am personally more inclined to AI/ML though.</p>
<p></p>
<p>Let&#39;s stay in touch to continue discussion later in post-OMSCS life ;)</p>
<p><a href=""https://www.linkedin.com/in/anurag-tangri/"" target=""_blank"">https://www.linkedin.com/in/anurag-tangri/</a></p>
<p></p>",2019-09-01T14:40:40Z,32,Week 9/1 - 9/7,feedback,,jfzaqnqvtQ1m,k0131518t4ss2,2019-09-01T14:40:40Z,{},logistics
140,no,"<p>Hi, my name is Juan Monge I&#39;m from Costa Rica, central America. Currently I work at Dell EMC Costa Rica . I have a background in mobile and web development in .net, sadly I&#39;ve never touched python in my life, so I have a lot to catch up. I&#39;ve always liked data science and machine learning and I&#39;m super eager to start and learn as much as possible</p>",2019-08-21T19:16:50Z,34,Week 8/18 - 8/24,followup,,jziehbrvvwc4hw,jzln1xi02ftlj,2019-08-21T19:16:50Z,{},logistics
141,no,"<p></p><ul><li>What&#39;s your name? James Hickey</li><li>Where are you from? Limerick, Ireland</li><li>Where are you now? Lincolnshire, UK</li><li>Why RL? Specializing in ML, utilize RL in job as a Data Scientist</li><li>How far into the program? Half way - 5 courses down.</li><li>Where next? Hopefully Graduate Algorithms or Artificial Intelligence</li><li>Is there anything else you&#39;d like to share? Not at the moment :)</li></ul>",2019-08-21T19:52:00Z,34,Week 8/18 - 8/24,followup,,j6mesrwitve3d,jzlob5my4b42s,2019-08-21T19:52:00Z,{},logistics
142,no,"<p></p><ul><li>What&#39;s your name? Sohel Islam</li><li>Where are you from? Dhaka, Bangladesh</li><li>Where are you now? Dhaka, Bangladesh (that&#39;s GMT&#43;6 timezone)</li><li>Why RL? Specializing in ML, complement self study in ML</li><li>How far into the program? 20% - 2 courses done</li><li>Where next? Graduate Algorithms or Big Data for Health</li><li>Is there anything else you&#39;d like to share? Did undergrad long time ago, been a sw dev, now in leadership roles and still love coding. Loving being back to studies after many years. </li></ul>",2019-08-21T20:05:53Z,34,Week 8/18 - 8/24,followup,,jqsk3m8k5CJB,jzlot0p9qm2c1,2019-08-21T20:05:53Z,{},logistics
143,no,"<p></p>
<ul><li>What&#39;s your name?
<ul><li>Kamran</li></ul>
</li><li>Where are you from?
<ul><li>Bay area, California</li></ul>
</li><li>Where are you now?
<ul><li>Los Gatos, CA</li></ul>
</li><li>Why RL?
<ul><li>It is a very important aspect of ML that has many use cases in engineering and beyond.</li></ul>
</li><li>How far into the program?
<ul><li>This is my 7th course:
<ul><li>Adv Sftwr arch, Compilers, Intro to OS, AI, ML, ML for trading</li></ul>
</li></ul>
</li><li>Where next?
<ul><li>Graduate Algorithms</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>In my late fifties ( I think prof Isbell and Littman are both younger than me! ). Big company and Startup experience as founder. Management and Technical contributor roles. Hardware, Firmware, and Software experience in multiple languages.  </li></ul>
</li></ul>",2019-08-21T20:43:00Z,34,Week 8/18 - 8/24,followup,,j6ll2xkiDJf,jzlq4qmpak17l,2019-08-21T20:43:00Z,{},logistics
144,no,"<p></p><ul><li>What&#39;s your name? <strong>Ning Shi. I would be glad if you can call me Shining (Shi &#43; Ning).</strong></li></ul>
<ul><li>Where are you from? <strong>Shanghai</strong></li><li>Where are you now? <strong>Boston</strong></li><li>Why RL? <strong>One of the core courses of the ML track.</strong></li><li>How far into the program? <strong>That&#39;s my first try. </strong></li><li>Where next? <strong>Any course which allows me to be in rather than stay on the waitlist.</strong></li><li>Is there anything else you&#39;d like to share? <strong>I&#39;m a big fan of the PlayStation!</strong></li></ul>",2019-08-21T22:44:13Z,34,Week 8/18 - 8/24,followup,,jzj0om7qnbd4yf,jzlugmbvu3p6j1,2019-08-21T22:44:13Z,{},logistics
145,no,"<p>What&#39;s your name? Neeraj Sinha</p>
<p>Where are you from? India<br />Where are you now? SF Bay area</p>
<p>Why RL? ML Specialization</p>
<p>How far into the program? 5th course ( DVA, ML4T,ML,CN done)</p>
<p>Where next? Look for job as ML engineer.</p>
<p>Is there anything else you&#39;d like to share : Feel free to connect to me <a href=""https://www.linkedin.com/in/neeraj-s-9094114/"">https://www.linkedin.com/in/neeraj-s-9094114/</a></p>
<p></p>",2019-08-21T23:12:46Z,34,Week 8/18 - 8/24,followup,,jc9nkspumds4ki,jzlvhbysl525se,2019-08-21T23:12:46Z,{},logistics
146,no,"<p>Hello Everyone</p>
<p></p>
<ul><li><strong>What&#39;s your name</strong>? Mrinal Venkatesh</li><li><strong>Where are you from</strong>? India</li><li><strong>Where are you now</strong>? San Jose, California</li><li><strong>Why RL</strong>? I am pursuing ML specialization</li><li><strong>How far into the program</strong>? Completed HPCA, AI , IIS. Taking AOS along with RL this semester. </li><li><strong>Where next</strong>? Getting into ML/AI will be a domain change for me. Looking forward to make that change.</li><li><strong>Is there anything else you&#39;d like to share</strong>? I love to read a wide variety of books ranging from computer science to philosophy in my free time.</li></ul>",2019-08-22T00:55:08Z,34,Week 8/18 - 8/24,followup,,jqob6okzSK1c,jzlz4zaq9y13j1,2019-08-22T00:55:08Z,{},logistics
147,no,"<p></p><ul><li><strong>What&#39;s your name</strong>? Daniel Klingman</li><li><strong>Where are you from</strong>? Louisville, KY</li><li><strong>Where are you now</strong>? New Albany, IN</li><li><strong>Why RL</strong>? RL is extremely interesting. I was fortunate to be able to attend ICML this year and got to hear many amazing examples of RL applications. I am also considering implementing RL to solve a process I am currently working on for work.</li><li><strong>How far into the program</strong>? Completed ML, AI4R, IIS</li><li><strong>Where next</strong>? After I complete this program I would like to consider pursuing a PHD and doing some research in the applications of ML.</li><li><strong>Is there anything else you&#39;d like to share</strong>? I am a Pilot, I like to read, and I am a Software Engineer by trade.</li></ul>
<p></p>",2019-08-22T02:03:23Z,34,Week 8/18 - 8/24,followup,,jl561222orGT,jzm1krfmbbwqz,2019-08-22T02:03:23Z,{},logistics
148,no,"<p></p><ul><li>What&#39;s your name? My name is Jisoo Lily Jeong</li><li>Where are you from? I moved to the states when I was 16 from South Korea, and I grew up in New York</li><li>Where are you now? I&#39;m currently in Sunnyvale</li><li>Why RL? Because it has potential to investigate how humans make decisions independently and in groups</li><li>How far into the program? This is my first semester!</li><li>Where next? Who knows?</li><li>Is there anything else you&#39;d like to share? Hi - nice to meet you all!</li></ul>",2019-08-22T02:49:59Z,34,Week 8/18 - 8/24,followup,,jzhq0wae6o1uh,jzm38ojz14l41m,2019-08-22T02:49:59Z,{},logistics
149,no,"Hi all! I&#39;m Matt from San Diego (born and raised in southern California).<div><br /></div><div>This is my third course and semester in the program (can&#39;t believe I&#39;ve only been doing this for a year), and I&#39;m taking this course because RL was the most fascinating and fun parts of both ML4T and ML.</div><div><br /></div><div>My days are spent as a site reliability engineer -- has very little to do with ML unfortunately, but &#34;Big Data&#34; is my employer&#39;s bread and butter.</div><div><br /></div><div>My wife and I love traveling, and we usually take a trip overseas every year, but sadly we couldn&#39;t pull it off this year. Last year we spent three weeks driving around the British Isles (so much fun driving on the wrong side of the road from the wrong side of the car), and the previous year we went to China (which was an amazing experience).</div><div><br /></div><div>I&#39;m looking forward to an awesome semester with you all. Good luck to everyone!</div>",2019-08-22T05:06:01Z,34,Week 8/18 - 8/24,followup,,jl2xkw5tykhF,jzm83mq7v6gs1,2019-08-22T05:06:01Z,{},logistics
150,no,"<p></p>
<ul><li>What&#39;s your name? 
<ul><li><strong>Imroj Qamar</strong></li></ul>
</li><li>Where are you from?
<ul><li><strong>New Delhi, India</strong></li></ul>
</li><li>Where are you now?
<ul><li><strong>Bangalore, India. Software Development Engineer at Amazon.</strong></li></ul>
</li><li>Why RL?
<ul><li><strong>Took Machine Learning in my under grad. Had a brief intro to RL and wanted to explore more.</strong></li></ul>
</li><li>How far into the program?
<ul><li><strong>This is my second course and second semester. Keeping it slow right now.</strong></li></ul>
</li><li>Where next?
<ul><li><strong>Machine Learning probably.</strong> </li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li><strong>Nice to meet you all. I hope we have some fruitful discussions in the forums. Other than being a Software Dev, I enjoy playing Piano, watching Animes, TV shows and reading books (recently started reading up Philosophy ).</strong><br /><strong>Best of Luck to Everyone !!</strong></li></ul>
</li></ul>",2019-08-22T05:23:37Z,34,Week 8/18 - 8/24,followup,,jqt6oket86zV,jzm8q91ogme32v,2019-08-22T05:23:37Z,{},logistics
151,no,"<p></p><ul><li>What&#39;s your name?
<ul><li>Joel Middendorf</li></ul>
</li><li>Where are you from?
<ul><li>Connecticut</li></ul>
</li><li>Where are you now?
<ul><li>Atlanta</li></ul>
</li><li>Why RL?
<ul><li>My specialization is ML, I follow&#34; 2 Minute Papers&#34; on YouTube. I&#39;m fascinated by this space and want a deeper understanding.</li></ul>
</li><li>How far into the program?
<ul><li>This is my 6th course.</li></ul>
</li><li>Where next?
<ul><li>
<table cellspacing=""0"" cellpadding=""0"" border=""1""><tbody><tr><td>CS 6515 Intro to Graduate Algorithms</td></tr><tr><td>CS 7641 Machine Learning</td></tr><tr><td>CS 6476 Computer Vision</td></tr><tr><td>CSE 6242 Data and Visual Analytics</td></tr></tbody></table>
Any of these that I&#39;m able to get into</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>Waitlist finally freed up, so I need to play catch up. Nice meeting everyone!</li></ul>
</li></ul>",2019-08-22T08:36:21Z,34,Week 8/18 - 8/24,followup,,j6jixch7tFn,jzmfm3xqi641dg,2019-08-22T08:36:21Z,{},logistics
152,no,"<p></p>
<p>Morning/Evening all I&#39;m excited for this semester and look forward to engaging in class discussions. </p>
<p> name:  Walter Day </p>
<p></p>
<p> from: Florida Space Coast </p>
<p></p>
<p> current location: Orlando </p>
<p></p>
<p> Why RL: As a DevOps engineer i&#39;m always looking for way to  improve my CI/CD pipelines. Would love to add RL concepts to the stack to help analyze automated security scans.</p>
<p></p>
<p>How far into the program: First class</p>
<p></p>
<p>Where next: DOD contract work </p>
<p></p>",2019-08-22T20:47:47Z,34,Week 8/18 - 8/24,followup,,jzk789tu3bw1eh,jzn5qqlp7yb22h,2019-08-22T20:47:47Z,{},logistics
153,no,"<p></p><ul><li>What&#39;s your name? Vatsal Parasrampuria</li><li>Where are you from? Bangalore, India</li><li>Where are you now? Bangalore, India</li><li>Why RL? This was the only ML specialization course I could get this semester, was waitlisted for AI. Wanted to take up RL after ML </li><li>How far into the program? This is my first semester as an OMSCS student</li><li>Where next? Looking to specialize in ML stream</li><li>Is there anything else you&#39;d like to share? I am a complete fresher to ML domain, working currently in Zebra Technologies as software developer</li><li>https://www.linkedin.com/in/vatsal-parasrampuria-415408b1/</li></ul>",2019-08-22T23:43:54Z,34,Week 8/18 - 8/24,followup,,jzih70o61oy1af,jznc185mka942q,2019-08-22T23:43:54Z,{},logistics
154,no,"<p></p><ul><li><strong>What&#39;s your name?</strong> Yousra Alyanaai</li><li><strong>Where are you from?</strong> Sana&#39;a, Yemen</li><li><strong>Where are you now?</strong> Calgary, AB, Canada </li><li><strong>Why RL?</strong> I&#39;m doing the machine learning specialization  </li><li><strong>How far into the program?</strong> This is my 6th course in the program</li><li><strong>Where next?</strong> I don&#39;t know but definitely an exciting place :D </li></ul>",2019-08-23T02:14:44Z,34,Week 8/18 - 8/24,followup,,j6wok6tcfcsbc,jznhf79h2vx1gq,2019-08-23T02:14:44Z,{},logistics
155,no,"<p></p><ul><li>What&#39;s your name? Dante Perez</li><li>Where are you from? Philippines</li><li>Where are you now? Plano, TX (migrated in 1998). </li><li>Why RL? To get a solid foundation as I specialize toward ML </li><li>How far into the program? Matriculated last Spring &#39;19 but have to defer to this term due to schedule so basically this is my first term with HCI as my other course.</li><li>Where next? Not sure but definitely move from network engineering to ML/Analytics space soon.</li><li>Is there anything else you&#39;d like to share? My background is in wireless engineering (AT&amp;T), currently working on 5G so there&#39;s a lot of data to play with. Outside of work, I coach youth basketball and middle school with my now 3rd and 8th gr son. Love to travel.</li></ul>
<p></p>
<p>Heard a lot of good things about this course so I&#39;m looking forward to learning more in depth about RL.</p>",2019-08-23T03:18:02Z,34,Week 8/18 - 8/24,followup,,jqms3ntwj0e53p,jznjom74dli62k,2019-08-23T03:18:02Z,{},logistics
156,no,"<p>I lived in Plano, TX for a couple of years. Nice place!</p>",2019-08-23T03:54:15Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jznkz694diq30h,2019-08-23T03:54:15Z,{},logistics
157,no,"<p>Hi Miguel, I agree with you very nice place, where do you live now? Plano and its surrounding areas have grown significantly. </p>",2019-08-23T04:39:05Z,34,Week 8/18 - 8/24,feedback,,jqms3ntwj0e53p,jznmktxusst42r,2019-08-23T04:39:05Z,{},logistics
158,no,"<p></p><ul><li>What&#39;s your name? Justin Siddon</li><li>Where are you from? Portland, Oregon</li><li>Where are you now? Seattle Washington</li><li>Why RL? Sounds interesting</li><li>How far into the program? Halfway!</li><li>Where next? Not sure</li><li>Is there anything else you&#39;d like to share? Nothing really!</li></ul>",2019-08-23T03:41:51Z,34,Week 8/18 - 8/24,followup,,j6lnhk64vs3ym,jznkj8ucfos4fz,2019-08-23T03:41:51Z,{},logistics
159,no,"<p></p><ul><li>What&#39;s your name? <b>Bill Fargo</b></li><li>Where are you from? <strong>Honolulu, HI</strong></li><li>Where are you now? <strong>Hong Kong</strong></li><li>Why RL? <strong>I work at a proprietary trading company doing options market making and automated futures trading strategies(that vary from high frequency to more intraday holding time periods). Machine learning became an interest because of this. </strong></li><li>How far into the program? <strong>First Class!</strong></li><li>Where next? <strong>Going to a BBQ before I bury myself in study for the next few months :)</strong></li><li>Is there anything else you&#39;d like to share? <strong>Very honored to be able to get to study with such a fantastic group of people.</strong></li></ul>",2019-08-24T04:58:54Z,34,Week 8/18 - 8/24,followup,,jzozvpx25to679,jzp2q6hkyn71at,2019-08-24T04:58:54Z,{},logistics
160,no,"<p>Manoj Chaulagain</p>
<p>Seattle, WA</p>
<p>USA</p>
<p>I work at a television broadcast company in Seattle.</p>
<p>Second Semester.</p>
<p>Like reading about new technologies and keep up with the fast growing technology field.</p>
<p></p>",2019-08-24T07:46:32Z,34,Week 8/18 - 8/24,followup,,jcur38q7r8hP,jzp8pr16fin1ut,2019-08-24T07:46:32Z,{},logistics
161,no,"<p></p>
<p>Manu G.Krishnan</p>
<p>Am from Kerala, India but working in Knoxville, TN</p>
<p>I trying for ML specialization, ML/spring2019 was hard, but loved RL when it all clicked, finally :)</p>
<p>8 courses in, GA to do next</p>
<p></p>
<p></p>",2019-08-24T13:49:50Z,34,Week 8/18 - 8/24,followup,,i4t1oo4aALG,jzployvp4i07au,2019-08-24T13:49:50Z,{},logistics
162,no,"<p>Hi everyone, I am Alastair Paragas! I&#39;m a Software Engineer at Apple - Apple Media Products here at Cupertino, CA. This is my 3rd semester so far and my 4th/5th course (taking this course alongside Artificial Intelligence Techniques for Robotics). My LinkedIn is at <a href=""https://www.linkedin.com/in/aparagas/"" target=""_blank"">https://www.linkedin.com/in/aparagas/</a>. My GitHub is at <a href=""https://github.com/alastairparagas"" target=""_blank"">https://github.com/alastairparagas</a>!</p>
<p> </p>
<p>Have taken Big Data for Health during Spring and took High Performance Computing Architecture and Machine Learning for Trading both over the summer.</p>
<p> </p>
<p>I graduated from undergrad 2 years ago back in 2017 with a Computer Science major and minors in both Math and Mathematical Sciences. Have been married since (my wife is also working full-time while taking an online Masters in Computer Science degree as well through UIUC - study buddies!).</p>
<p> </p>
<p>Besides academics and work, I enjoy working out. In my senior year of high school, I used to be 230 pounds, then dipped to 115 pounds - having lost around the first 70 pounds in ~4 months and the rest of the weight in ~3 months, all through exercise and diet. I have gone to both weight extremes for my height but nowadays I&#39;m hovering at a midline weight area.</p>",2019-08-24T16:18:19Z,34,Week 8/18 - 8/24,followup,,jqnardlwW3NE,jzpqzwndu8pc8,2019-08-24T16:18:19Z,{},logistics
163,no,"<p></p><ul><li>What&#39;s your name? <strong>AJ Hotrum</strong></li><li>Where are you from? <strong>Seattle, WA, USA</strong></li><li>Where are you now? <strong>Salt Lake City, Utah, USA</strong></li><li>Why RL? <b>Thought it would be a great way to go for my first ML course. Really excited to get started!</b></li><li>How far into the program? <strong>3rd Semester</strong></li><li>Where next? <strong>Unsure! Obtaining this degree to make a career shift from mechanical engineering.</strong></li><li>Is there anything else you&#39;d like to share? <b>Hoping to meet some of my fellow classmates so feel free to ping me!</b></li></ul>",2019-08-24T18:48:24Z,34,Week 8/18 - 8/24,followup,,jqmjg3txqw6ji,jzpwcx6gfb2d4,2019-08-24T18:48:24Z,{},logistics
164,no,"<p></p><ul><li>What&#39;s your name? <strong>Andrew Wiley</strong></li><li>Where are you from? <strong>San Diego</strong><strong>, CA, USA</strong><strong></strong></li><li>Where are you now? <strong>San Diego</strong><strong>, CA, USA</strong></li><li>Why RL? <strong>Really enjoyed during ML</strong></li><li>How far into the program? <strong>5th Class</strong></li><li>Where next? <strong>Computer Vision next semester.</strong></li><li>Is there anything else you&#39;d like to share? <strong>Enjoy running and a good beer</strong></li></ul>",2019-08-24T19:24:28Z,34,Week 8/18 - 8/24,followup,,jc6mqevhagl262,jzpxnap2o2466l,2019-08-24T19:24:28Z,{},logistics
165,no,"<p>Hi All, my name is Ava Jiang, originally from China and currently based in Toronto, Canada. RL is my 1st course in OMSCS and I&#39;m interested in machine learning topics overall. Next course after this might be machine learning itself or other courses in the ml specification.</p>",2019-08-24T21:34:29Z,34,Week 8/18 - 8/24,followup,,jzj4205g7gd2fw,jzq2aichxzb4qq,2019-08-24T21:34:29Z,{},logistics
166,no,"<p>Hi everyone, <br /><br />My name is Mohammed, I live in Stockholm, Sweden but originally from Egypt. I completed 7 courses prior to this semester and I&#39;m taking KBAI this semester as well. Looking forward to learning a lot in this class.</p>",2019-08-25T18:55:46Z,33,Week 8/25 - 8/31,followup,,is1zht4sfqs775,jzrc28s0qlg5cd,2019-08-25T18:55:46Z,{},logistics
167,no,<p></p><ul><li>What&#39;s your name? Ye Li</li><li>Where are you from? China</li><li>Where are you now? New York</li><li>Why RL? Very interested in ML track.</li><li>How far into the program? This is my first course in OMSCS. </li><li>Where next? Lets see where I will stand in next semester&#39;s registration</li><li>Is there anything else you&#39;d like to share? Do we any group chat for people in NYC area?</li></ul>,2019-08-26T01:59:36Z,33,Week 8/25 - 8/31,followup,,jzhtklm0zje29x,jzrr7aqs8511i0,2019-08-26T01:59:36Z,{},logistics
168,no,<p>I am interested in a group chat for NYC metro! Let me know if you have found one or want to create one in slack :)  </p>,2019-09-08T14:41:47Z,31,Week 9/8 - 9/14,feedback,,jqr9leosvw8P,k0b35k22toj2tu,2019-09-08T14:41:47Z,{},logistics
169,no,"<p></p>
<ul><li>
<pre>Murat Yalcin</pre>
</li><li>
<pre>San Jose, CA</pre>
</li><li>
<pre>USA</pre>
</li><li>
<pre>RL is one of the best topics in ML.</pre>
</li><li>
<pre>Very beginning.</pre>
</li><li>
<pre>Good Luck to everyone.</pre>
</li></ul>
<p></p>
<p></p>
<p></p>",2019-08-26T02:40:36Z,33,Week 8/25 - 8/31,followup,,jzjzy6sav722bs,jzrso18zvok3w1,2019-08-26T02:40:36Z,{},logistics
170,no,"<p>Hi.. I am Mayank, based in Dallas, Texas. This is my fifth course and third in ML specialization. Wish all of you a great semester.</p>",2019-08-26T19:09:54Z,33,Week 8/25 - 8/31,followup,,jl2egn5k4zo4lp,jzss09q7vd92yg,2019-08-26T19:09:54Z,{},logistics
171,no,"<p>Hi, I&#39;m Kevin from Albuquerque, NM.  This is my first course in the program.  I&#39;ve been doing a large amount of DL for my job but haven&#39;t had the chance to work on anything RL yet.  I thought it&#39;d be an interesting start for my program.</p>",2019-08-26T20:48:08Z,33,Week 8/25 - 8/31,followup,,jzsv9qgvs05p3,jzsvilafwzy3bj,2019-08-26T20:48:08Z,{},logistics
172,no,"<p>Hi I am shravan from Atlanta, GA. this is my first course in the program. i am java programmer and totally new to ML and RL  I thought it&#39;d be an interesting start for my program. but i am scared now looking at topics and posts.</p>",2019-08-27T17:13:24Z,33,Week 8/25 - 8/31,followup,,jzu2o3mdsd9198,jzu3ab2y7kx5e1,2019-08-27T17:13:24Z,{},logistics
173,no,"<p>Hi, I&#39;m Getenet from New York. this is my first course in the program. I work with Java and new to Machine L. but I know python and did few projects. Looking forward to learn new staff. </p>",2019-08-28T02:00:43Z,33,Week 8/25 - 8/31,followup,,jzt6qw5thzz5af,jzum4fzioot59x,2019-08-28T02:00:43Z,{},logistics
174,no,"Hi, I&#39;m Chris. I&#39;m from North Carolina originally but live and work in DC now. I&#39;m a modelling analyst at work and in the program to broaden my knowledge of machine learning. The ML course was full when I registered, but I&#39;m very excited for RL and plan to take ML next.",2019-08-28T11:47:34Z,33,Week 8/25 - 8/31,followup,,jqi9pwn26my6g2,jzv734y2q325v8,2019-08-28T11:47:34Z,{},logistics
175,no,"<p>Hi,</p>
<p> </p>
<p>I&#39;m Haiqing.</p>
<p> </p>
<p>I live in west San Jose.</p>
<p> </p>
<p>This is my 7th class (BD4H, Database, IIS,  DVA, ML, GA). And I will graduate this semester. </p>
<p> </p>
<p>Here is my LinkedIn: <a href=""https://www.linkedin.com/in/haiqing-zhu-16a143149/"" target=""_blank"">Haiqing</a></p>",2019-08-30T18:59:22Z,33,Week 8/25 - 8/31,followup,,jci0wkjll56ad,jzyhe54lsju5pr,2019-08-30T18:59:22Z,{},logistics
176,no,"<p>Hi everyone,</p>
<p></p>
<p>My name is Juan Mendoza and I&#39;m from Mexico City. This is my first course and it looks exciting.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-08-30T23:39:04Z,33,Week 8/25 - 8/31,followup,,jzygktecb0e3i1,jzyrdu48pkh2v8,2019-08-30T23:39:04Z,{},logistics
177,no,<p>Welcome to OMSCS Juan.  It&#39;s a challenging path but really exciting to be a part of.</p>,2019-08-31T15:53:38Z,33,Week 8/25 - 8/31,feedback,,is9so9huTMp,jzzq74dp8g34hz,2019-08-31T15:53:38Z,{},logistics
178,no,"<p></p><ul><li><strong>What&#39;s your name?</strong>  Nick Pappas</li><li><strong>Where are you from?</strong>  Albuquerque, NM</li><li><strong>Where are you now?</strong>  Albuquerque, NM</li><li><strong>Why RL? </strong>  I got exposed to this in ML4T with the Q-Learner code.  I was so amazed at what had been created that I changed my specialization to ML.  Looking forward to learning more about this subject.</li><li><strong>How far into the program?</strong>  Assuming I survive this semester, I will just have 3 more.</li><li><strong>Where next?  </strong>I&#39;m not sure what to take next semester quite honestly.  Maybe CV, or something else...</li><li><strong>Is there anything else you&#39;d like to share?  </strong>While serving in the US Marines, I traveled around the world.  Didn&#39;t think I would be living in Albuquerque again, but here we are.</li></ul>",2019-08-31T15:53:03Z,33,Week 8/25 - 8/31,followup,,is9so9huTMp,jzzq6dnlrzt5w3,2019-08-31T15:53:03Z,{},logistics
179,no,"<p>Hi, I&#39;m Nan Zhou. I&#39;m from China and I&#39;m currently working in New York. This is the first class I take. I&#39;m exited into learning more fun stuff in Reinforcement Learning.</p>",2019-08-31T23:10:49Z,33,Week 8/25 - 8/31,followup,,jzkmt6mjn134mv,k005tccqudc3ji,2019-08-31T23:10:49Z,{},logistics
180,no,<p></p><ul><li>What&#39;s your name? Nithin Tammishetti</li><li>Where are you from? India</li><li>Where are you now? NJ</li><li>Why RL? I&#39;d taken/self learned a couple AI and ML courses in undergrad but never touched RL </li><li>How far into the program? first course along with OS</li><li>Where next? not sure</li></ul>,2019-08-31T23:25:40Z,33,Week 8/25 - 8/31,followup,,jzj3evi42071sd,k006cgclxx55yc,2019-08-31T23:25:40Z,{},logistics
181,no,"<p>Hi all, this is Jesmer Wong from Hong Kong, 1st class on the OMSCS journey. Great to meet you all!</p>
<p></p>
<ul><li>What&#39;s your name?
<ul><li>Jesmer WONG</li></ul>
</li><li>Where are you from?
<ul><li>Hong Kong</li></ul>
</li><li>Where are you now?
<ul><li>Hong Kong</li></ul>
</li><li>Why RL?
<ul><li>Sounds very cool subject and try to learn at my first place :D</li></ul>
</li><li>How far into the program?
<ul><li>My very first class on the OMSCS journey.</li></ul>
</li><li>Where next?
<ul><li>Focusing on Machine Learning Specialization courses</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>My LinkedIn - <a href=""https://www.linkedin.com/in/jesmerwong/"">https://www.linkedin.com/in/jesmerwong/</a></li></ul>
</li></ul>",2019-09-02T10:56:46Z,32,Week 9/1 - 9/7,followup,,jznmonjuzvi67j,k02ah2bvt8p1xo,2019-09-02T10:56:46Z,{},logistics
182,no,"<p></p><ul><li>What&#39;s your name?
<ul><li>I am Suresh Pandian</li></ul>
</li><li>Where are you from?
<ul><li>I am from Chennai, Tamilnadu, India.</li></ul>
</li><li>Where are you now?
<ul><li>I live in Gothenburg, Sweden.</li></ul>
</li><li>Why RL?
<ul><li>Cool tech interests me. </li></ul>
</li><li>How far into the program?
<ul><li>This is my fourth course in the program. A long way to go still.</li></ul>
</li><li>Where next?
<ul><li>Make safer self-driving trucks and safer roads.. Can i automate my aquarium so it takes care of it self?? That would be cool, I can enjoy my fishes but not have to work for it.. :)</li></ul>
</li><li>Is there anything else you&#39;d like to share?
<ul><li>Looking forward to an eventful class with all of you. </li></ul>
</li></ul>",2019-09-07T16:52:55Z,32,Week 9/1 - 9/7,followup,,jc9ssyte5au1hp,k09sebpr1mf5p3,2019-09-07T16:52:55Z,{},logistics
183,no,"<p></p><ul><li>What&#39;s your name? Kaylee Kohfeldt </li><li>Where are you from? Florida!</li><li>Where are you now? Jersey City (NYC metro area) </li><li>Why RL? One of my coworkers told me this was his favorite class and it seemed like an interesting challenge. </li><li>How far into the program? This is my second class!</li><li>Where next? Nooo idea. </li><li>Is there anything else you&#39;d like to share? linkedin: <a href=""https://www.linkedin.com/in/kaylee-kohfeldt-0a59255a/"">https://www.linkedin.com/in/kaylee-kohfeldt-0a59255a/</a> also, if anyone in the NYC metro area wants to meet up and study, I would be interested! </li></ul>",2019-09-08T14:40:51Z,31,Week 9/8 - 9/14,followup,,jqr9leosvw8P,k0b34ca3sdi1ur,2019-09-08T14:40:51Z,{},logistics
184,no,"<p></p>
<ul><li>What&#39;s your name? Jasmine</li><li>Where are you from? SF Bay Area</li><li>Where are you now? I split my time between NOVA and NYC. Open to meeting up to study in one of those areas.</li><li>Why RL? it was the only class I could get into</li><li>How far into the program? first semester</li><li>Where next? ¯\_(ツ)_/¯</li><li>Is there anything else you&#39;d like to share? ¯\_(ツ)_/¯</li></ul>",2019-10-06T02:01:14Z,28,Week 9/29 - 10/5,followup,,jqu95q68ljj1pn,k1ecbbpqqe63v4,2019-10-06T02:01:14Z,{},logistics
185,no,"<p></p>
<p>Hi all,</p>
<p></p>
<p>Will the Udacity lecture schedule be added to the course calendar or anywhere else?</p>
<p></p>
<p>Thank you for your help.</p>",2019-08-20T02:46:33Z,43,Week 8/18 - 8/24,followup,,gx3c8l7z7r72zl,jzj88ki03nz2p1,2019-08-20T02:46:33Z,{},logistics
186,no,<p>It should be there now... Isn&#39;t it?</p>,2019-08-20T02:46:59Z,43,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj8941w4iu2we,2019-08-20T02:46:59Z,{},logistics
187,no,"<p><a href=""https://gatech.instructure.com/calendar#view_name=month&amp;view_start=2019-08-19"">https://gatech.instructure.com/calendar#view_name=month&amp;view_start=2019-08-19</a></p>
<p></p>
<p>Make sure you enable the course calendar on the right.</p>",2019-08-20T02:48:04Z,43,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj8aibmt7o5ae,2019-08-20T02:48:04Z,{},logistics
188,no,"<p>I see it now, thank you.</p>",2019-08-20T14:13:40Z,43,Week 8/18 - 8/24,feedback,,gx3c8l7z7r72zl,jzjws7eha4w5og,2019-08-20T14:13:40Z,{},logistics
189,no,<p>Yw.</p>,2019-08-22T01:47:36Z,43,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzm10ghtupi6z9,2019-08-22T01:47:36Z,{},logistics
190,no,"<p>&#34;Submitting your code in a repository other than the one assigned, even if all TAs can access.&#34;</p>
<p></p>
<p>does this mean I can&#39;t keep my 7642 work in my GA Tech github repository?</p>",2019-08-20T01:39:24Z,34,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzj5u7n2vo36fs,2019-08-20T01:39:24Z,{},logistics
191,no,"<p>We will use <a href=""http://github.gatech.edu/"">http://github.gatech.edu</a>, but you will get assigned a repository for the course with a specific name and permissions. You must push your code to that repository.</p>
<p></p>
<p>Instructions about this will come out in a few days.</p>
<p></p>",2019-08-20T01:43:37Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj5zn0kxjb2bt,2019-08-20T01:43:37Z,{},logistics
192,no,<p>I know I am not the only one itching to get using github for storing my code... can we maybe pin this note that directions are coming soon? I know I spent a good hour looking into github enterprise permissions trying to figure out why I couldn&#39;t push. </p>,2019-09-02T20:36:08Z,32,Week 9/1 - 9/7,feedback,,jzj6p4ln7yi5de,k02v64rutbg1kg,2019-09-02T20:36:08Z,{},logistics
193,no,<p>Hmm. Good point.</p>,2019-09-02T22:02:18Z,32,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02y8y0m7oh1n6,2019-09-02T22:02:18Z,{},logistics
194,no,Dumb question alert - what does &#34;forgetting your hash in your report&#34; mean? Or what does that look like? ,2019-08-20T02:02:43Z,34,Week 8/18 - 8/24,followup,,j6vegc4sb0v33d,jzj6o6pk5nplj,2019-08-20T02:02:43Z,{},logistics
195,no,"<p>A Git hash is a unique identifier of changes made to your repository. We are asking for you to add the most recent Git hash to your report.</p>
<p></p>
<p>Instructions about this will come out in a few days.</p>
<p></p>
<p> </p>",2019-08-20T02:46:19Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj889q292b2k6,2019-08-20T02:46:19Z,{},logistics
196,no,Got it. Thank you. ,2019-08-20T03:06:59Z,34,Week 8/18 - 8/24,feedback,,j6vegc4sb0v33d,jzj8yugzfrg59i,2019-08-20T03:06:59Z,{},logistics
197,no,<p>Yw.</p>,2019-08-20T03:20:24Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzj9g3cf2ta2mx,2019-08-20T03:20:24Z,{},logistics
198,no,"<p>If external repo used (as I use private repos in BitBacket for all GaTech projects), does the note regarding hash means that I need to push all my changes into the GaTech GutHub first and then use this GaTech GitHub hash in the report?<br />No external hashes permitted?</p>",2019-08-24T19:59:48Z,34,Week 8/18 - 8/24,followup,,jqkxzdmmolGf,jzpywqd1qxh5ul,2019-08-24T19:59:48Z,{},logistics
199,no,"<p>Yes. You can always have 2 origins, then push to both. Then, the Git hash *should* be the same in both websites.</p>",2019-08-24T20:04:36Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzpz2x1sgwz23,2019-08-24T20:04:36Z,{},logistics
200,no,<p>Thank you Miguel for the clarification!</p>,2019-08-24T20:38:43Z,34,Week 8/18 - 8/24,feedback,,jqkxzdmmolGf,jzq0as8hql220i,2019-08-24T20:38:43Z,{},logistics
201,no,<p>Yw.</p>,2019-08-24T20:55:46Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzq0wpv86sh6yw,2019-08-24T20:55:46Z,{},logistics
202,no,"<p>Stupid question, but what is &#34;Forgetting <strong>your hash</strong> in your report.&#34; what is &#34;hash&#34;?</p>",2019-08-25T18:49:22Z,33,Week 8/25 - 8/31,followup,,j6ll2xkiDJf,jzrbu0ezjua6p,2019-08-25T18:49:22Z,{},logistics
203,no,<p>Maybe if you read earlier questions?</p>,2019-08-25T20:12:13Z,33,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzresk896c26y2,2019-08-25T20:12:13Z,{},logistics
204,no,"<p>got it &#34;Git hash&#34;</p>
<p></p>",2019-08-25T21:53:06Z,33,Week 8/25 - 8/31,feedback,,j6ll2xkiDJf,jzrieaq7c774j7,2019-08-25T21:53:06Z,{},logistics
205,no,<p>Correct. Edited.</p>,2019-08-25T22:17:12Z,33,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrj9apnxqk76c,2019-08-25T22:17:12Z,{},logistics
206,no,"<p>&#34;Using a Deep RL library instead of providing your own work.&#34;</p>
<p>Would you clarify what is NOT allowed? please give example of code/api available on internet that can not be used in this course.</p>",2019-08-25T18:51:20Z,33,Week 8/25 - 8/31,followup,,j6ll2xkiDJf,jzrbwjhitlt62q,2019-08-25T18:51:20Z,{},logistics
207,no,"<p>Anything that does any portion of the reinforcement learning algorithm for you is disallowed. Matrix multiplication libraries such as Numpy, data analysis such as pandas, plotting libraries such as matplotlib, differentiation packages such as PyTorch, are all allowed. If you have a doubt on a specific library, <em><strong>ask the specific question.</strong></em> </p>
<p></p>
<p>We will not give specific packages because we can not know every single package, but libraries such as keras_rl, rllib, rlkit are all disallowed. Again, if the library allows you to directly train RL agents, then it is disallowed.</p>",2019-08-25T20:17:32Z,33,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrezebg1qb6ws,2019-08-25T20:17:32Z,{},logistics
208,no,"<p>From what you described, it sounds like Tensorflow is OK. Could you confirm?</p>",2019-10-13T20:25:39Z,26,Week 10/13 - 10/19,feedback,,k0i2gsf6my96ge,k1pful1ohhc11c,2019-10-13T20:25:39Z,{},logistics
209,no,"<p>If you are just using tensorflow for the non-RL sections (such as a neural network), then it is ok. See &#64;512 and &#64;584.</p>
<p>The specific post is for Keras, but Keras uses tensorflow.  Also, see &#64;543.</p>
<p>TAs can still reconfirm.</p>",2019-10-13T22:18:36Z,26,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1pjvu03f605wn,2019-10-13T22:18:36Z,{},logistics
210,no,"<p>George got it.</p>
<p></p>
<p>Tensorflow is allowed as long as it is for building neural networks (do not use anything that does the RL-specific logic for you.)</p>
<p></p>",2019-10-14T02:29:49Z,26,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1psuwjvz5rle,2019-10-14T02:29:49Z,{},logistics
211,no,"<p>For the PDF report, do we need to use any special nomenclature?</p>",2019-09-18T22:33:32Z,30,Week 9/15 - 9/21,followup,,jc554vxmyuy3pt,k0pueql0x0ij3,2019-09-18T22:33:32Z,{},logistics
212,no,<p>just make sure git hash is on the paper somewhere.  No special naming convention or anything like that.</p>,2019-09-19T00:17:22Z,30,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0py49rtt7v28c,2019-09-19T00:17:22Z,{},logistics
213,no,"<p>Are libraries for organizing experiments like <a href=""https://github.com/IDSIA/sacred"" target=""_blank"" rel=""noopener noreferrer"">sacred</a> or <a href=""https://docs.wandb.com/getting-started"" target=""_blank"" rel=""noopener noreferrer"">weights &amp; biases</a> allowed / could these cause issues when TAs run our code? Thanks!</p>",2019-10-04T12:36:01Z,28,Week 9/29 - 10/5,followup,,jzifg1e23c29s,k1c43ytoruz606,2019-10-04T12:36:01Z,{},logistics
214,no,"<p>Wow, this is interesting! I had no idea these libraries existed!</p>
<p></p>
<p>You can use these, thanks for asking (and sharing).</p>",2019-10-04T14:48:23Z,28,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c8u68ht3q8g,2019-10-04T14:48:23Z,{},logistics
215,no,<p>This looks very interesting. I wish I saw this before project 1! It would have been very useful.</p>,2019-10-25T01:03:24Z,25,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k25fm4sn5r72,2019-10-25T01:03:24Z,{},logistics
216,no,"<p>How do you setup mobile </p>
<p></p>",2019-08-20T15:05:17Z,65,Week 8/18 - 8/24,followup,,jzj7y1ofgsro1,jzjymko1xsa2pc,2019-08-20T15:05:17Z,{},other
217,no,"<p>Do you mean the Udacity&#39;s mobile app? Don&#39;t know. But you can watch the lectures on YouTube. Here is a playlist I created a few semesters ago <a href=""https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2"">https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2</a></p>",2019-08-20T20:43:58Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkaq521x425sb,2019-08-20T20:43:58Z,{},other
218,no,"<p>Thanks, Miguel. This is very helpful.</p>
<p></p>",2019-08-21T00:58:38Z,65,Week 8/18 - 8/24,feedback,,jzhrun61eu91l9,jzkjtn00kj3tl,2019-08-21T00:58:38Z,{},other
219,no,<p>Yw!</p>,2019-08-21T00:59:13Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkjue59vxc4pi,2019-08-21T00:59:13Z,{},other
220,no,Thanks for sharing this!,2019-08-20T05:18:05Z,33,Week 8/18 - 8/24,followup,,jfzaqnqvtQ1m,jzjdnfy8qf41k6,2019-08-20T05:18:05Z,{},other
221,no,&#43;1000,2019-08-21T00:45:41Z,33,Week 8/18 - 8/24,feedback,,ixpwxv7xdgi1u6,jzkjczi9icq7ia,2019-08-21T00:45:41Z,{},other
222,no,"<p>BTW, here is a YouTube playlist with the lectures in a single-video-per-lecture format: <a href=""https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2"">https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2</a></p>",2019-08-21T00:58:24Z,33,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzkjtcdko713zg,2019-08-21T00:58:24Z,{},other
223,no,<p>Wow thank you for sharing.  This is huge.  Not sure why this link isn&#39;t in the syllabus.</p>,2019-08-21T12:31:48Z,33,Week 8/18 - 8/24,feedback,,jqqssjikI511,jzl8l20j1lk698,2019-08-21T12:31:48Z,{},other
224,no,"<p>Maybe I should do that. I also have the mp3 version of these lectures for those with long commutes.</p>
<p></p>",2019-08-21T14:30:12Z,33,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzlctbkmleq2ve,2019-08-21T14:30:12Z,{},other
225,no,That would be really nice for a lot of us daily commuters ☺️,2019-08-21T17:20:59Z,33,Week 8/18 - 8/24,feedback,,jfzaqnqvtQ1m,jzliwxyydm62gq,2019-08-21T17:20:59Z,{},other
226,no,<p>Yes the mp3s would be really useful. Can you share them?</p>,2019-08-21T17:52:05Z,33,Week 8/18 - 8/24,feedback,,jzliwdkt88m1q3,jzlk0y0sar91fa,2019-08-21T17:52:05Z,{},other
227,no,"<p>See if this works: <a href=""https://drive.google.com/open?id=1p8odveHH4Byp5mGh284iFUsdl_djij7l"">https://drive.google.com/open?id=1p8odveHH4Byp5mGh284iFUsdl_djij7l</a></p>
<p></p>",2019-08-21T20:55:13Z,33,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzlqkfwjhgj74o,2019-08-21T20:55:13Z,{},other
228,no,<p>Fantastic! Thanks for the mp3s.</p>,2019-08-23T04:32:19Z,33,Week 8/18 - 8/24,feedback,,ixnwq0s4ozg,jznmc4wq3ja50p,2019-08-23T04:32:19Z,{},other
229,no,<p>Thanks!</p>,2019-08-24T13:35:35Z,33,Week 8/18 - 8/24,feedback,,jzliwdkt88m1q3,jzpl6mpgeop620,2019-08-24T13:35:35Z,{},other
230,no,Thanks for posting! This is very helpful to watch on the way to/from work.,2019-09-10T10:49:30Z,30,Week 9/8 - 9/14,feedback,,jl3mf50dr12i,k0dpqityvd6jc,2019-09-10T10:49:30Z,{},other
231,no,"<p>BTW, we have a file on Canvas with this info: <a href=""https://gatech.instructure.com/courses/45210/files/folder/info?preview=6593017"">https://gatech.instructure.com/courses/45210/files/folder/info?preview=6593017</a></p>
<p></p>",2019-08-21T20:59:17Z,33,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzlqpoehm195ue,2019-08-21T20:59:17Z,{},other
232,no,"<p>Hey guys I&#39;m totally new in the program, and this is my first class. I&#39;m currently catching up because I had some extra stuff to do at work so my question is, how many lectures from Udacity are we suppose to watch every week?</p>",2019-08-26T02:19:08Z,32,Week 8/25 - 8/31,followup,,jziehbrvvwc4hw,jzrrwf2wn2w2m8,2019-08-26T02:19:08Z,{},other
233,no,"<p>Check the Canvas calendar.</p>
<p></p>
<p>&#64;8 is a good place to start.</p>
<p></p>
<p>Welcome!</p>
<p></p>
<p></p>",2019-08-26T03:34:26Z,32,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrul8xbpbqn0,2019-08-26T03:34:26Z,{},other
234,no,"<p>Thanks Miguel, found it!</p>",2019-08-26T03:43:31Z,32,Week 8/25 - 8/31,feedback,,jziehbrvvwc4hw,jzruwxsaczx6rv,2019-08-26T03:43:31Z,{},other
235,no,<p>Fixed.</p>,2019-08-20T21:27:44Z,65,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzkcaena5rn3kc,2019-08-20T21:27:44Z,{},other
236,no,"<p><a href=""http://www.incompleteideas.net/book/RLbook2018trimmed.pdf"">http://www.incompleteideas.net/book/RLbook2018trimmed.pdf</a></p>",2019-08-20T14:28:42Z,65,Week 8/18 - 8/24,followup,,jl2gfssgneuU,jzjxbj88lwy31,2019-08-20T14:28:42Z,{},other
237,no,"Adding to above, syllabus in canvas has suggested readings that you can find in canvas -&gt; files -&gt; readings.<div><br /></div>",2019-08-20T14:39:10Z,65,Week 8/18 - 8/24,followup,,jfzaqnqvtQ1m,jzjxozqxow9eo,2019-08-20T14:39:10Z,{},other
238,no,<p>There is no mention of udacity in the syllabus? Do we have instructions around setting it up?</p>,2019-08-20T15:06:25Z,65,Week 8/18 - 8/24,followup,,jzj7y1ofgsro1,jzjyo1d0kjc39t,2019-08-20T15:06:25Z,{},other
239,no,"Here’s the link:<div>https://www.udacity.com/course/reinforcement-learning--ud600</div><div><span style=""color:#3d7cc8""><br /></span></div>",2019-08-20T19:40:59Z,65,Week 8/18 - 8/24,feedback,,jfzaqnqvtQ1m,jzk8h56vafl542,2019-08-20T19:40:59Z,{},other
240,no,<p>Is there any slack group for this course?</p>,2019-08-20T15:10:18Z,65,Week 8/18 - 8/24,followup,,jl2bv84c6FEX,jzjyt0xrte55k3,2019-08-20T15:10:18Z,{},other
241,no,"<p>Yes! Check instructor post on Important Course information at the bottom there are important such as <a href=""https://gatech.instructure.com/courses/45210/files/folder/how_to?preview=6593015"" target=""_blank"">https://gatech.instructure.com/courses/45210/files/folder/how_to?preview=6593015</a></p>",2019-08-20T19:30:59Z,65,Week 8/18 - 8/24,feedback,,jl0c61kgrmsY,jzk849qpl0f4tb,2019-08-20T19:30:59Z,{},other
242,no,<p>Thanks :)</p>,2019-08-22T05:54:07Z,65,Week 8/18 - 8/24,feedback,,jl2bv84c6FEX,jzm9th0ipsqzn,2019-08-22T05:54:07Z,{},other
243,no,<p>How do we know which of the readings in Canvas match to certain sections in the course? Are they given in Udacity / the calendar / will be posted in Piazza posts?</p>,2019-08-21T03:50:48Z,65,Week 8/18 - 8/24,followup,,jl3oi5v7qkSk,jzkpz1ms85o1om,2019-08-21T03:50:48Z,{},other
244,no,Answered in &#64;8 under &#34;weekly announcements&#34;.,2019-08-21T04:05:24Z,65,Week 8/18 - 8/24,feedback,,jl2xkw5tykhF,jzkqhti3nwxnp,2019-08-21T04:05:24Z,{},other
245,no,My bad! &#x1f605;,2019-08-21T07:05:03Z,65,Week 8/18 - 8/24,feedback,,jl3oi5v7qkSk,jzkwwura9597nw,2019-08-21T07:05:03Z,{},other
246,no,<p>I searched the lectures repeatedly. It is only mentioned en passant when Littman introduces the Q1 &gt;= BQ1 line at the start of the policy iteration proof. There is no other mention of it. Is the Q2 &gt;= Q1 nomenclature correct??</p>,2019-08-21T01:14:12Z,34,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzkkdnhntri4ss,2019-08-21T01:14:12Z,{},other
247,no,<p>Hmm. Okay. I&#39;m thinking either the Convergence or the AAA lectures.</p>,2019-08-21T01:17:17Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkkhmgyixt6iz,2019-08-21T01:17:17Z,{},other
248,no,"<p>---------- 103 - CONTRACTION PROPERTIES - LANG_EN_VS2.SRT<br />this sequence, and F*, the fixed point.<br />a unique fixed point exists,<br />fixed point is unique is the same kind<br />two different fixed points then, right.<br />So there can&#39;t be multiple fixed points.</p>
<p></p>
<p>---------- 110 - CONVERGENCE - 1 - LANG_EN_VS2.SRT<br />That&#39;s the fixed point.</p>
<p></p>
<p>---------- 117 - GENERALIZED MDPS - SOLUTION - 3 - LANG_EN_VS2.SRT<br />a fixed point in the limit.</p>
<p></p>
<p>---------- 130 - ANOTHER PROPERTY IN POLICY ITERATION - 1 - LANG_EN_VS2.SRT<br />the fixed point of B1 and<br />has a unique fixed point?<br />for the the fixed point of Pi1,<br />Q1, it&#39;s the fixed point</p>
<p></p>
<p></p>
<p></p>",2019-08-21T01:38:40Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzkl94bc5p37ge,2019-08-21T01:38:40Z,{},other
249,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fjzkldd56ffel%2Fconvergencefixedpoint.png"" alt="""" /></p>",2019-08-21T01:42:07Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzkldk9dwmj61t,2019-08-21T01:42:07Z,{},other
250,no,"<p>I still don&#39;t see how Q2 &gt;= Q1 is the fixed point. Everything that I&#39;ve read would suggest that Q1 = BQ1 is the fixed point. I found it referenced in Sutton&#39;s first edition of his book in Chapter 4. Again, there, it&#39;s an equality not an inequality.</p>
<p></p>
<p>So, how is Q2 &gt;= Q1 the fixed point for the policy iteration?</p>",2019-08-21T02:32:28Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzkn6b5xrns5rb,2019-08-21T02:32:28Z,{},other
251,no,"<p>$$Q_2$$ is the fixed point for $$B_2$$. The [fixed point] citation in the last line is used on the left-hand side of the last two equations ($$\lim_{k \rightarrow \infty} B_2^k Q_1 = Q_2$$), while the right-hand side stays the same.</p>",2019-08-21T02:55:17Z,34,Week 8/18 - 8/24,feedback,,jzifg1e23c29s,jzknzn5ksk350s,2019-08-21T02:55:17Z,{},other
252,no,<p>OH. haha. Thanks Steph.</p>,2019-08-21T03:13:26Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzkomzc518n197,2019-08-21T03:13:26Z,{},other
253,no,"<p>So I guess I&#39;m just challenged or something, but I&#39;m not seeing this in the lectures for last week, and I&#39;ve watched them a couple of times. Where are these lectures? I hope I&#39;m not watching the wrong ones... Sorry for a bad question. </p>",2019-08-25T21:19:41Z,33,Week 8/25 - 8/31,followup,,jzhnj355bzr2uu,jzrh7bwk8895sw,2019-08-25T21:19:41Z,{},other
254,no,"<p>This is the thing with online learning, these lectures are not the ones assigned for this week.</p>
<p></p>
<p>If you are interested, it&#39;s Lesson: &#34;AAA&#34;, Video: &#34;Quiz: Policy Iteration Proof&#34;</p>",2019-08-26T00:05:04Z,33,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrn40cvbnv6mt,2019-08-26T00:05:04Z,{},other
255,no,<p>Oh okay cool cool. Thank you! I thought I was missing something drastic. Thank you!</p>,2019-08-26T02:28:33Z,33,Week 8/25 - 8/31,feedback,,jzhnj355bzr2uu,jzrs8ix93bq2ck,2019-08-26T02:28:33Z,{},other
256,no,"<p>There are no dumb questions. Usually, profs upload them under the files section in Canvas. But I don&#39;t see them there. Let&#39;s wait and see if they will upload them.</p>",2019-08-20T20:37:45Z,65,Week 8/18 - 8/24,followup,,jc8bgo8obmm4kl,jzkai4zuct614z,2019-08-20T20:37:45Z,{},other
257,stud,a bit out of topic but I believe there is no such a &#34;disadvantage here compared to other students&#34; since most of the course here is on absolute grading basis.,2019-08-20T22:24:17Z,65,Week 8/18 - 8/24,followup,a_0,,jzkeb5evbk2510,2019-08-20T22:24:17Z,{},other
258,no,<p>I think Robert was referring to a potential &#34;disadvantage [...] compared to other students&#34; that have taken the ML course.</p>,2019-08-20T22:34:19Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkeo1c4cvn4ix,2019-08-20T22:34:19Z,{},other
259,no,"<p>Many students find it useful to watch David Silver&#39;s lectures in parallel to the lectures for this course (I know I did).  They cover essentially the same material but provide a different perspective/teaching approach:</p>
<p></p>
<p><a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p>
<p></p>",2019-08-22T01:41:43Z,65,Week 8/18 - 8/24,followup,,jwc25tufjja3vh,jzm0swh08vg2qk,2019-08-22T01:41:43Z,{},other
260,no,"<p>Thank you for all of the helpful feedback. I&#39;ve compiled the resources below to get a head start.</p>
<p>I&#39;ll also review the ML course on Udacity.</p>
<p></p>
<table cellspacing=""0"" cellpadding=""0"" border=""1""><tbody><tr><td>
<div>
<div><a href=""https://github.com/mimoralea/gdrl"" target=""_blank"">https://github.com/mimoralea/gdrl</a></div>
</div>
</td></tr><tr><td>
<div>
<div><a href=""https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2"" target=""_blank"">https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2</a></div>
</div>
</td></tr><tr><td>
<div>
<div><a href=""https://spinningup.openai.com/en/latest/"" target=""_blank"">https://spinningup.openai.com/en/latest/</a></div>
</div>
</td></tr><tr><td>
<div>
<div><a href=""https://skymind.ai/wiki/deep-reinforcement-learning"" target=""_blank"">https://skymind.ai/wiki/deep-reinforcement-learning</a></div>
</div>
</td></tr><tr><td>
<div>
<div>Find an Intro Dynamic Programming</div>
</div>
</td></tr></tbody></table>
<p></p>
<p></p>",2019-08-24T20:08:03Z,65,Week 8/18 - 8/24,followup,,jl2dfm6lskZz,jzpz7cebkv7119,2019-08-24T20:08:03Z,{},other
261,no,"<p>My chapter 03 Notebook has an intro to DP: <a href=""https://github.com/mimoralea/gdrl/blob/master/notebooks/chapter_03/chapter-03.ipynb"">https://github.com/mimoralea/gdrl/blob/master/notebooks/chapter_03/chapter-03.ipynb</a></p>",2019-08-24T20:11:33Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzpzbv1qde61e1,2019-08-24T20:11:33Z,{},other
262,no,<p>Great list! Thanks for the links.</p>,2019-08-24T20:33:28Z,65,Week 8/18 - 8/24,feedback,,jziehvhblig5if,jzq040zjsee6tg,2019-08-24T20:33:28Z,{},other
263,no,"<p>Thank you for the info everyone! This is Jesmer and I just started the OMSCS journey. Nice to meet you all!</p>
<p></p>
<p>Will work real hard for this challenging course~</p>
<p></p>
<p>With best regards,</p>
<p>Jesmer.</p>",2019-09-02T10:06:15Z,63,Week 9/1 - 9/7,feedback,,jznmonjuzvi67j,k028o3d4yra66s,2019-09-02T10:06:15Z,{},other
264,stud,<p>Do you expect all HWs submitted to GitHub in .py or .ipynb and pdf? What is python refreshment you would recommend to go through for the HW1? Are there tutorials for tools for this class you would recommend to go through?</p>,2019-08-20T22:20:27Z,44,Week 8/18 - 8/24,followup,a_0,,jzke67p27t336p,2019-08-20T22:20:27Z,{},hw1
265,no,"<p>HWs do not expect any programming language. You will submit (basically) text to a website. Projects, in particular, project 2, can be more easily tackled using Python, OpenAI Gym, and an automatic differentiation package, perhaps PyTorch, Keras, TensorFlow, or the likes.</p>
<p></p>
<p>We will release tutorials in the next couple of weeks. Meanwhile, I can recommend the Notebooks here: <a href=""https://github.com/mimoralea/gdrl"">https://github.com/mimoralea/gdrl</a>. In particular for chapters 02-06.</p>",2019-08-20T22:40:06Z,44,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzkevhb79gv3eu,2019-08-20T22:40:06Z,{},hw1
266,stud,"<p>Is there a trick to launching the container? I&#39;m relatively new to docker and running on windows 10. I followed some tutorials for a sample &#39;hello-world&#39; and &#39;ubuntu&#39; containers and they seem to work so I think I&#39;m on the right path. However, when I try to spin up the container following the command from step 2 on the link above, it tells me &#34;invalid reference format&#34;. I&#39;ve googled a bunch of things but so far can&#39;t get it to start. </p>",2019-08-25T22:23:07Z,43,Week 8/25 - 8/31,feedback,a_1,,jzrjgw7tgid79,2019-08-25T22:23:07Z,{},hw1
267,no,"<p>Are you using Windows/PowerShell?? Try:</p>
<p></p>
<pre>docker run -it --rm -p 8888:8888 -v ${pwd}/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12</pre>
<p><a href=""https://stackoverflow.com/questions/45682010/docker-invalid-reference-format"">https://stackoverflow.com/questions/45682010/docker-invalid-reference-format</a></p>",2019-08-25T22:38:46Z,43,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrk111gx2t60f,2019-08-25T22:38:46Z,{},hw1
268,no,"<p>Will assignments and projects have a single submission, or a number of submissions allowed (e.g. X total submissions, one every Y hours, etc.)?</p>",2019-08-21T03:55:09Z,44,Week 8/18 - 8/24,followup,,jl3oi5v7qkSk,jzkq4mumzy4x9,2019-08-21T03:55:09Z,{},hw1
269,no,"<p>Homework assignments have 10 submissions per problem.  The homework is auto-graded so you will receive almost instant feedback.</p>
<p></p>
<p>You can submit your project as many times as you want up until the due date.  After the due date, we will pull down the latest submission and grade it.</p>",2019-08-21T11:53:34Z,44,Week 8/18 - 8/24,feedback,,hz7meu55mi8sd,jzl77vu6tf27ew,2019-08-21T11:53:34Z,{},hw1
270,no,"Excellent, thanks for the info ^_^",2019-08-22T02:45:44Z,44,Week 8/18 - 8/24,feedback,,jl3oi5v7qkSk,jzm337op3ov1pw,2019-08-22T02:45:44Z,{},hw1
271,no,"<p>Also, &#64;8 includes a link to set up the calendar. The calendar events marked as &#34;Lectures/Readings&#34; contain the specifics.</p>",2019-08-20T22:59:47Z,57,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzkfksfqsed78z,2019-08-20T22:59:47Z,{},other
272,no,"<p>Thanks Miguel.  </p>
<p>Do you recommend reading Littman&#39;s PhD thesis beyond the 2nd chapter?</p>
<p>Or reading Sutton&#39;s book front to back? </p>",2019-08-25T17:59:18Z,56,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzra1mwetz16tc,2019-08-25T17:59:18Z,{},other
273,no,<p>I take Sutton&#39;s any day.</p>,2019-08-25T20:21:57Z,56,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrf52rlia62vk,2019-08-25T20:21:57Z,{},other
274,no,"<p>Which lecture/paper are we talking about here?</p>
<p></p>
<p>BTW, this is actually a pretty important topic and will show up in the final. ;)</p>",2019-08-21T01:39:18Z,34,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzkl9xizdxh48b,2019-08-21T01:39:18Z,{},other
275,no,<p>Lesson 7 - Messing with Rewards</p>,2019-08-21T02:39:33Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzknff64r7ztz,2019-08-21T02:39:33Z,{},other
276,no,<p>What happens if you initialise everything w/ zero?  Wouldn&#39;t you be stuck at zero w/ subsequent iterations because 0 * anything = 0?</p>,2019-08-21T02:39:05Z,34,Week 8/18 - 8/24,followup,,ixpclzk97jg2fo,jzknetnsv9rk4,2019-08-21T02:39:05Z,{},other
277,no,"<p>Yes. Zero is not interesting in this case, as pointed out by Littman and Isbell in the lectures. The value in question is always &gt;0 (never zero and never negative). Negative values would invert the logic of Q.</p>",2019-08-21T02:40:29Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzknglski1f1kk,2019-08-21T02:40:29Z,{},other
278,no,<p>Are you talking about updating the Q table?  You&#39;re saying I can&#39;t init Q w/ zero?</p>,2019-08-21T02:42:36Z,34,Week 8/18 - 8/24,feedback,,ixpclzk97jg2fo,jzknjbwhfex2wr,2019-08-21T02:42:36Z,{},other
279,no,"<p>No, I am saying that, when we want to change the R(s,a) to a non-constant value for R(s,a), we have to find a way to characterize how it affects the Q(s,a). By using multiplicative properties, we can pull out the C constant from the Bellman update and show that cR(s,a) just makes Q(s,a) become cQ(S,a).</p>
<p></p>
<p>My argument is that R(s,a) &#43; B is no different than cR(s,a)</p>",2019-08-21T02:51:27Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzknupx12xw5qk,2019-08-21T02:51:27Z,{},other
280,no,"<p>Thanks Steph. The purpose of the multiplication and addition lecture series is about taking the constant R(s,a) and mutating into a mutable R(s,a). That is accomplished with a constant, either additive constant or a multiplicative constant. So the R(S,a) is constant in the equations and the C/B values are the only variants. They don&#39;t vary as a function of iteration, which would make them a function of Q.</p>
<p></p>
<p>That&#39;s my understanding of the lectures.</p>",2019-08-21T02:46:48Z,34,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzknoq8vc587l1,2019-08-21T02:46:48Z,{},other
281,no,"<p>The derivative of R&#43; B is dR, whereas the derivative of c*R = c*dR. dR != c*dR except for the special case which c = 1, which is no fun :). Therefore they are generally not equal, because the derivative of two equal functions must also be equal for all x. </p>",2019-08-21T03:59:43Z,34,Week 8/18 - 8/24,followup,,jzj4sh1p7pf5af,jzkqaip37z14ji,2019-08-21T03:59:43Z,{},other
282,no,"<p>Note the horizon truncation section. The k is the horizon. This determines how many steps you are going to use in your evaluation of the bellman iteration. In the case of infinite horizons, we are guaranteed to see convergence because of the contraction properties of the Bellman equation. Nobody has infinite horizons, so we also look at the finite horizon (k != inf) and determine if there is reasonable epsilon that determines if the iteration has converged.</p>
<p></p>
<p></p>",2019-08-21T17:14:27Z,42,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzlioj9rog968q,2019-08-21T17:14:27Z,{},other
283,no,So we&#39;re only evaluating the l = 0 and l = k?  There&#39;s nothing that says to take either case.  I get what k is but I don&#39;t get how to evaluate the limit here.,2019-08-21T17:18:47Z,42,Week 8/18 - 8/24,feedback,,jdd22f9zpbq5sz,jzliu3wwod01u6,2019-08-21T17:18:47Z,{},other
284,no,"<p>no, you&#39;re going from iteration #0 to iteration #k .... so if we let k = 100, then we are doing 100 iterations from 0 ... 99</p>",2019-08-21T17:20:35Z,42,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzliwftcrh3go,2019-08-21T17:20:35Z,{},other
285,no,"<p>a = [0,1,2,3,4,5,6,7,8,9,10,11,12]</p>
<p></p>
<p>$$\sum_{i=0}^{5}a_i = 0&#43;1&#43;2&#43;3&#43;4$$</p>",2019-08-21T17:22:25Z,42,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzliys20a0q4jw,2019-08-21T17:22:25Z,{},other
286,no,"Okay so it&#39;s discrete, thanks.  So once we&#39;ve calculated all these sums for different k&#39;s, then what, we pick the largest?",2019-08-21T17:26:59Z,42,Week 8/18 - 8/24,feedback,,jdd22f9zpbq5sz,jzlj4nhxq9o14e,2019-08-21T17:26:59Z,{},other
287,no,"<p>if you are referring to the lim(l=0..k)sum(t=0..l) r_t/l ?? then you are taking the limit from 0 ... to k, your horizon count. This equation demonstrates a uniform distribution of the r_t rather than the weighted distribution introduced later as P_l, e.g. sum(t=0..l) r_t * P_l. In the uniform case, P_l is just 1/l.</p>
<p></p>
<p>Uniform meaning that each r_t has equal merit in the Q of that state/action path. The weighted version just means that some r_t will contribute more than others as time (k) goes on. That&#39;s where the model comes into play. The T(s,a,r,s&#39;) is the RL model that determines the P_l in the formula I mentioned earlier.</p>",2019-08-21T17:28:13Z,42,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzlj68ku7645xy,2019-08-21T17:28:13Z,{},other
288,no,"<p>&#34; then what, we pick the largest?&#34; - we would pick the best, which is often times the max - when we are using the Bellman equations.</p>
<p></p>
<p>if we are doing value iteration, then we don&#39;t pick the max, we just continue to add up the values (iterate) until they stop changing. At some point ||V_k - V_k-1|| &lt; epsilon, and then we quit.</p>",2019-08-21T17:29:52Z,42,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzlj8d40fx75,2019-08-21T17:29:52Z,{},other
289,stud,"<p>I suspect there&#39;s a typo in the expression from table 1.2 as well. I&#39;m understanding the limit expression in such a way that for any finite $$k$$, $$\lim_{l=0}^{k} \sum _{t=0}^{l} r_{t}/l$$ is just supposed to give us the average of the first $$k$$ rewards. But if that&#39;s the case, there are several issues.</p>
<p></p>
<p>First, the term in the summation, $$r_{t}/l$$, is undefined for $$l = k = 0$$. It shouldn&#39;t be. We start from time-step $$t = 0$$. For $$l = k = 0$$, we should simply have the value of the first reward, $$r_{0}$$. </p>
<p></p>
<p>Similarly, when $$k = 1$$, we should have the (arithmetic) average of the first two rewards: $$\frac{r_{0} &#43; r_{1}}{2}$$. But with the expression above <em>as written</em>, we don&#39;t. We instead have their <em>sum</em>. </p>
<p></p>
<p>All of this can be fixed by adding one in the denominator of the summand: $$r_{t}/l \rightarrow r_{t}/(l &#43; 1)$$. </p>
<p></p>
<p>If $$k$$ is finite, I think we can just ignore the limit stamp. We don&#39;t need it in order to compute the (arithmetic) average of a finite number of terms.</p>
<p></p>
<p>if we allow $$k \rightarrow \infty$$, then what we&#39;re (evidently) doing is looking at the behavior of the sequence of parital sums of the form $$S_{k} = \sum _{t=0}^{k} r_{t}/(l&#43;1)$$ as $$k$$ goes to infinity. Well, that&#39;s just the definition of the sum of an infinite series. So we get the average in this latter case by looking at the value the partial sums (of the series of the above form) as $$k$$ increases without bound. Note that for this limit to exist, we clearly need to make certain assumptions about how the $$r_{t}$$ are defined, since certain choices are clearly going to produce a diveregent series (e.g., if $$r_{t} = 1$$ for all $$t$$, then the expression in the table, after fixing the typo, is just the harmonic series. And that diverges.)</p>
<p></p>
<p>So I think that&#39;s all that&#39;s going on here. The author is just trying to strive for a certain level of generality. &#34;Ignore&#34; the limit for finite $$k$$, and for infinite $$k$$, <em>do what comes naturally</em>.</p>
<p></p>
<p>Also, there&#39;s a typo.</p>",2019-08-21T20:13:18Z,42,Week 8/18 - 8/24,feedback,a_0,,jzlp2k0kqzj6qi,2019-08-21T20:13:18Z,{},other
290,no,"<p>:) Am I the only one who hears Charles Isbell&#39;s voice when reading Anonymous&#39; response? hmmmm</p>
<p></p>
<p>&#34;Also, there&#39;s a typo&#34; - de ja vu</p>",2019-08-21T23:57:04Z,42,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzlx2bc8jbd259,2019-08-21T23:57:04Z,{},other
291,no,"When you said &#34;then you are taking the limit from 0 ... to k&#34; that&#39;s where my original confusion was.  I don&#39;t recall ever having seen a limit specified like that, and it still doesn&#39;t make sense to me.  It seems that it&#39;s meant to be one value but then why the range?  It&#39;s as if it&#39;s saying the limit as l approaches k but it looks like it&#39;s saying as l approaches k from 0 which I don&#39;t believe makes sense.<div><br /></div><div>Thanks for trying but I think I&#39;m just going to have to move on.</div>",2019-08-22T01:22:52Z,42,Week 8/18 - 8/24,feedback,,jdd22f9zpbq5sz,jzm04nuinzl4f9,2019-08-22T01:22:52Z,{},other
292,no,"<p>I think that writing $$\lim_{l=0}^{k}$$ is not mathematically correct, since limits are used to express what a statement tends to become when one variable approaches a certain value, such as $$\lim_{l \to k}$$, without caring of the starting value.</p>
<p>So, in my humble opinion, $$\lim_{l=0}^{k} \sum_{t=0}^{l} r_{t} / l$$ should have been written $$\lim_{l \to k} \sum_{t=0}^{l} \frac{r_{t}}{l}$$ which is not only mathematically correct but avoids dealing with the case l = 0 since we are not interested in that at all, but when $$l \to k$$.</p>
<p>Now, it&#39;s also true that there is an indexing problem because we want $$ \frac{r_{0} &#43; r_{1}}{2}$$, so, imho, the expression in table 1.2 should have been $$\lim_{l \to k} \sum_{t=0}^{l} \frac {r_{t}}{l &#43; 1}$$</p>",2019-08-25T21:24:10Z,41,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzrhd3ewtcl765,2019-08-25T21:24:10Z,{},other
293,stud,"<p>I&#39;m not totally sure what Littman meant to do there. On page 15 of section 1, though, he writes that</p>
<p></p>
<blockquote>
<p>&#34;[The] average reward [criterion] [computes] an average of the transition values.<sup>2</sup>&#34;</p>
</blockquote>
<p></p>
<p>If you look at footnote two (bottom of the page), you&#39;ll find this:</p>
<p></p>
<blockquote>
<p>&#34;In the case of infinite-horizon average reward, it is necessary to compute the average as a limit.&#34;</p>
</blockquote>
<p></p>
<p>So whatever is going on with the funky notation in the table (whether that&#39;s an attempt to extend the usual limit notation in some way-maybe to encompass both the finite horizon and the infinite horizon cases in one compact notation-or just another typo) I <em>think</em> it&#39;s pretty clear that all that&#39;s going on is this:</p>
<p></p>
<p>When using the average reward criterion, we&#39;re either in a finite horizon scenario, or an infinite horizon scenario. If the former, our reward summary is just the (arithmetic) average of the finitely many rewards we&#39;ve received. If the latter, then we&#39;ll see infinitely many rewards. In that case, we must <em>define</em> our reward as the limit of the (arithemtic) average of all rewards accumulated thusfar as their number approaches infinity. </p>
<p></p>
<p>This last quantity can be written as $$\lim_{l \rightarrow k}S_{l} $$ where $$S_{l} = \sum^{l}_{t=0}r_{t}/(l&#43;1)$$, which is what you have above. </p>
<p></p>
<p>I&#39;m wondering if we can write this even more simply, though. I&#39;m not super clear on why we need two variables ($$k$$ and $$l$$) here. But $$\lim_{k \rightarrow \infty}S_{k} $$ where $$S_{k} = \sum^{k}_{t=0}r_{t}/(k&#43;1)$$ looks funny to me (probably because the variable $$k$$ is both in the limit and inside the summation, which is weird). </p>",2019-08-25T23:16:42Z,41,Week 8/25 - 8/31,feedback,a_0,,jzrldt7cgkz1l7,2019-08-25T23:16:42Z,{},other
294,no,<p>There&#39;s nothing wrong with having k &#39;both in the limit and inside the summation&#39; since that&#39;s what limits are about: evaluating an expression when it&#39;s variable tends to a predetermined value or ∞.</p>,2019-08-26T17:54:28Z,41,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzspb9weoqn725,2019-08-26T17:54:28Z,{},other
295,stud,"<p>Thanks for your input!</p>
<p></p>
<p>I think there&#39;s clearly a difference between the expression from Littman (1996) and the way the sum of a convergent infinite series is typically defined (i.e., as the limit of the sequence of partial sums, $$S_{N}$$ as $$N \rightarrow \infty$$).</p>
<p></p>
<p>In the typical case, we&#39;d have $$\lim_{N \rightarrow \infty}S_{N} = \lim_{N \rightarrow \infty} \sum_{t=0}^{N}a_{t}$$. So, yes, $$N$$ (here) appears both in the limit and as the upper limit of summation. But the individual <em>terms</em> in the summation depend only on the index of summation, $$t$$, and <em>not</em> on $$N$$.</p>
<p></p>
<p>More importantly, there is typically the following relationship between successive terms in the sequence of partial sums whose limit (if it exists) gives the sum of the corresponding infinite series: $$S_{N} = a_{N} &#43; S_{N-1}$$, which is a consequence of the fact that each partial sum just adds the next term of the series to the previous partial sum. </p>
<p></p>
<p>But thats <em>not</em> true of the sequence of partial sums that Littman&#39;s expression suggests. For example, we have </p>
<p></p>
<blockquote>
<p>$$S_{0} = r_{0}, \ S_{1} = \frac{r_{0}&#43;r_{1}}{2}, \ S_{2} = \frac{r_{0} &#43; r_{1} &#43; r_{2}}{3},...\ $$</p>
</blockquote>
<p>This suggests that, in Littman&#39;s sequence, we instead have $$S_{N} = \frac{(NS_{N-1} &#43; \ r_{N})}{N &#43; 1}$$ as the relation between successive terms of the sequence of partial sums. That&#39;s rather different from the typical case.</p>
<p></p>
<p>Though, now that I&#39;m looking at it, it seems fine to write it as $$\lim_{N \rightarrow \infty}S_{N} = \lim_{N \rightarrow \infty}\frac{1}{N&#43;1}\sum_{t=0}^{N}r_{t}$$. And that looks equivalent to what&#39;s above if we bring the factor $$\frac{1}{N&#43;1}$$ into the summation.</p>
<p></p>
<p>So I guess maybe it is fine to write it that way!</p>
<p></p>",2019-08-26T18:55:51Z,41,Week 8/25 - 8/31,feedback,a_0,,jzsri7eajjp3t1,2019-08-26T18:55:51Z,{},other
296,no,"<p>For instance, say we are dealing with a self-guided landing of a rocket. The landing trajectory is a very complex calculation of many variables. The simulated environment won&#39;t simulate the chaotic influence of fluid dynamics, but it can summarize it with some &#34;bins&#34; of states. Such as, direction of wind and speed of wind and local atmospheric density. This gives the RL model a chance to &#34;best guess&#34; what to do in a real life situation. Then when the RL model is introduced to the real case of a landing rocket, we watch it crash a few times, and then it starts to adjust to the parts of the model that were &#34;low resolution&#34; during the simulated training.</p>
<p></p>
<p>In the case of backgammon, Littman later on describes how the simulated environment does help with the performance because many of those esoteric states are never realized in the game. We are assuming the AI is playing as a best-in-class player, so the realized range of states is far fewer than the entire domain.</p>
<p></p>
<p>Game histories is another kind of simulation, but that&#39;s Dyna and reminiscing. In the case of Dyna the model makes up a bunch of state/action paths thinking they might be good alternatives. This helps to introduce noise into the state/action paths that would not otherwise get any love during training. Then reminiscing is when we replay past actions, sometimes obsessively (the same path over and over). This reminiscing also helps to smooth out the jagged noise that can occur in the Q solution because the state/action paths are not visited enough in high-range domains.</p>
<p></p>",2019-08-21T17:10:02Z,65,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzliivchapq1kb,2019-08-21T17:10:02Z,{},other
297,stud,<p>That&#39;s helpful. Thank you.</p>,2019-08-21T19:01:22Z,65,Week 8/18 - 8/24,feedback,a_0,,jzlmi1elaqz6v6,2019-08-21T19:01:22Z,{},other
298,no,<p>https://bkgm.com/articles/tesauro/tdl.html</p>,2019-09-11T20:18:31Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0fpi5itsak4j8,2019-09-11T20:18:31Z,{},other
299,no,"<p>&gt; How exactly does this approach help us deal with complex environments?</p>
<p></p>
<p>what is the optimal path, but the path that maximizes current and future rewards amongst a known reward structure (&#39;environment&#39;)? </p>
<p></p>
<p>how can you estimate (within some error bracket), future rewards if you have not seen the future?  dealing with an simplified version of your environment, that lets your agent correctly deduce a reasonable initial estimates as to expected utility of various likely paths, thus preloads your agent for success. </p>",2019-08-23T22:34:16Z,65,Week 8/18 - 8/24,followup,,jzivtxcbl6964n,jzoozj36s2w4vx,2019-08-23T22:34:16Z,{},other
300,no,"One option is to use Udacity app with other Gmail account and download the lessons, for some reason gatech email doesn&#39;t work with the app.",2019-08-21T18:57:07Z,65,Week 8/18 - 8/24,followup,,jzj72gbzpx37j0,jzlmcky4llr3kk,2019-08-21T18:57:07Z,{},other
301,no,"<p>Take a look on r/omscs, there is a discussion of this exact topic not too long ago.  RL is one topic among many that ML covers, and more than a few people end up taking RL first for various reasons, and are successful with that strategy.</p>",2019-08-21T19:56:43Z,65,Week 8/18 - 8/24,followup,,jcb3j96cOjkz,jzloh861uxb6w5,2019-08-21T19:56:43Z,{},hw1
302,no,"<p>thx, will do</p>",2019-08-21T22:42:00Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzludrulytn119,2019-08-21T22:42:00Z,{},hw1
303,no,"<p>Hey Jean-Pierre. I agree with Miguel&#39;s response - where he offers nothing and says quite a bit. RL stands apart from ML. ML is really about finding patterns and knowledge in data. RL is about finding patterns and knowledge in a model environment. You will be introduced to RL at the end of ML, and it&#39;s really just a very simple introduction.</p>
<p></p>
<p>You can take RL without ever having taken ML. You won&#39;t need any of the jargon/vernacular of ML to converse with RL people. The only overlaps are the mathematics of probability and some Bayesian learning.</p>
<p></p>
<p>This class will amaze you.</p>",2019-08-21T23:51:48Z,65,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzlwvj56o69qe,2019-08-21T23:51:48Z,{},hw1
304,no,<p>&#43;1.</p>,2019-08-21T23:55:42Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzlx0jz598z4h8,2019-08-21T23:55:42Z,{},hw1
305,no,"<p>I understand but I wanted to have a list of the things that have been taught in the ML course (which includes a part on RL) and will be assumed as &#39;known&#39; here, so I can learn that upfront and no be slowed down every time I meet a new term.  </p>
<p>For instance, MDP is taught in the ML course.  Am I supposed to know that right off the bat?</p>
<p>Are there other things like that? </p>
<p></p>
<p>Any help is appreciated because I&#39;m still hesitating taking the ML course before this one as is the usual recommendation.  Not sure I&#39;ll manage to grab it on Friday, but knowing what I&#39;m up against will help make the choice. </p>
<p>Thanks in advance for anything you may find helpful to share</p>
<p></p>",2019-08-22T01:10:35Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzlzoukcdvo4z4,2019-08-22T01:10:35Z,{},hw1
306,no,"<p>I see. What threw me off is the &#34;exhaustive&#34; part. I won&#39;t be able to do that.</p>
<p></p>
<p>What you&#39;re asking now is very different. I recommend you watch the first two lectures which are a re-cap from ML. Then watch the Generalization lecture which is about using function approximation (such as those found in Supervised ML) to solve reinforcement learning problem.</p>
<p></p>
<p>Questions:</p>
<p></p>
<p>1. Have you ever created a function approximation with packages such as scikit-learn, Torch, PyTorch, Keras, Tensorflow, or anything along those lines?</p>
<p>2. Have you ever trained a neural network!?</p>
<p></p>
<p>FWIW, I took RL before ML, and it went just fine. I did have prior ML and data science experience, though.</p>
<p></p>",2019-08-22T01:39:17Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzm0proefd416y,2019-08-22T01:39:17Z,{},hw1
307,no,"<p>Also, I recommend you ask more specific questions instead of asking for someone to create an &#34;exhaustive&#34; list for you.</p>
<p></p>
<p>Answering your first specific question: &#34;MDP is taught in the ML course.  Am I supposed to know that right off the bat?&#34;</p>
<p></p>
<p>No. If you check, the first ~3 lectures are all about MDPs.</p>",2019-08-22T01:42:19Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzm0tntawq4zu,2019-08-22T01:42:19Z,{},hw1
308,no,"<p>Thanks Miguel.  I used the word &#39;exhaustive&#39; to avoid someone telling &#39;oh, just look into A and B&#39; while there&#39;s also a C and a D to review.  I am an &#39;exhaustive&#39; kind of guy...</p>
<p>But, ok, I&#39;ll do what you say.</p>
<p>My experience with AI/ML is Andrew Ng&#39;s ML course on  Coursera which I found easy, it included training a NN, but it was using Matlab... I then studied Python for 10 months to prepare for this, and then a few weeks of numpy and pandas.  But I have no experience in scikit and all.  </p>
<p>I have a MS in electronics, so I&#39;m good with maths and modeling stuff, it&#39;s just the syntax of all those libraries and all those parameters for every function that throws me off.  </p>
<p></p>
<p>What do you say? Should I stick to RL?  I feel like I should after your explanations.</p>
<p></p>
<p>Sorry for asking so many questions but these are my first days here, and I&#39;m learning to juggle with piazza, canvas, slack, things being presented differently by Bayesian statistics for instance (I still can&#39;t access the videos although I linked to EdX).  It&#39;ll be much better in a few days.  </p>
<p></p>
<p>Thanks again for taking the time to help me out</p>",2019-08-22T02:08:27Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzm1ra4kyuk3mi,2019-08-22T02:08:27Z,{},hw1
309,no,"<p>Then, I expect you to ask exhaustive questions, not expect exhaustive answers.</p>
<p></p>
<p>You can watch all of the course lectures here: <a href=""https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2"">https://www.youtube.com/playlist?list=PLFihX_3MLxS-xipfAZUwzAie7AWbJQ8k2</a></p>
<p></p>
<p>Your background seems reasonable. Andrew Ng&#39;s ML course is pretty good. Matlab is horrible, but that&#39;s my personal taste. Python is what I would recommend for this course. </p>
<p></p>
<p>Clone the repository, follow the instructions, and run the Notebooks here: <a href=""https://github.com/mimoralea/gdrl"">https://github.com/mimoralea/gdrl</a></p>
<p></p>
<p>Do you understand what the code is doing?! Are you capable of modifying the code and outputting things that interest you?! Can you extend and customize the code?</p>
<p></p>
<p>I cannot tell you whether you should stay or wait, but after you play around with the Notebook you should be able to make that decision. One thing I will say, asking questions and being active in Piazza is a good thing.</p>
<p></p>",2019-08-22T16:33:29Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzmwnpw1hoh5ws,2019-08-22T16:33:29Z,{},hw1
310,no,"<p>Thanks a lot. With your advices and starting to study the content, watching videos, it seems much less frightening than it was before.  </p>",2019-08-22T21:17:03Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzn6sdjcueg7ek,2019-08-22T21:17:03Z,{},hw1
311,no,<p>Yw!</p>,2019-08-22T23:19:36Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jznb5zl2f9qqf,2019-08-22T23:19:36Z,{},hw1
312,no,"<p>That&#39;s 3am for europeans ... To be fair, we should alternate.</p>
<p>One time 9PM EST, next time 6AM EST, ie after and before work for americans.  </p>",2019-08-22T01:04:42Z,34,Week 8/18 - 8/24,followup,,jzh6k6o994a6dh,jzlzha1ncsk5zi,2019-08-22T01:04:42Z,{},office_hours
313,no,"<p>Hello Jean-Pierre,</p>
<p></p>
<p>Generally we run a study session once a week as well that is a few hours earlier on the weekend. That might work better for you.</p>",2019-08-22T01:41:34Z,34,Week 8/18 - 8/24,feedback,,hyxsfbkeit22m2,jzm0spjo363l5,2019-08-22T01:41:34Z,{},office_hours
314,no,"<p>Everything works for me since I don&#39;t have a job at the moment and I&#39;m a late night bird.</p>
<p>I was just thinking about all europeans which, like americans, also have jobs.  </p>
<p>Thx for the suggestion.</p>",2019-08-22T14:38:38Z,34,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzmsk0cpxi4nx,2019-08-22T14:38:38Z,{},office_hours
315,no,"Not to be a stickler, but for clarity you should change the timezones to reflect that it&#39;s EDT not EST -- at least until October &#x1f642; (Could also do what I do at work and say things like &#34;US/Eastern&#34;.)",2019-08-22T02:24:15Z,34,Week 8/18 - 8/24,followup,,jl2xkw5tykhF,jzm2blb7rnfn8,2019-08-22T02:24:15Z,{},office_hours
316,no,<p>&#43;1</p>,2019-08-22T02:43:59Z,34,Week 8/18 - 8/24,feedback,,jqmfuaidej9155,jzm30ypaqo41i,2019-08-22T02:43:59Z,{},office_hours
317,no,<p>This is why I always use ET/MT/PT...</p>,2019-08-22T23:47:55Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jznc6eb3iw86ia,2019-08-22T23:47:55Z,{},office_hours
318,no,"<p>Also &#39;tomorrow&#39; can be misleading when the post is posted 2 hours ago, and it&#39;s 5am on the 22nd here.</p>
<p>I guess the meeting is on the 22nd at 9PM EST, right? </p>
<div>
<div></div>
</div>",2019-08-22T03:27:15Z,34,Week 8/18 - 8/24,followup,,jzh6k6o994a6dh,jzm4km2dkq57cx,2019-08-22T03:27:15Z,{},office_hours
319,no,<p>Correct</p>,2019-08-22T04:08:20Z,34,Week 8/18 - 8/24,feedback,,i4op5p9vfbq5yz,jzm61fruu032ec,2019-08-22T04:08:20Z,{},office_hours
320,no,"<p>I am going to use this opportunity to note you can set your TZ in canvas and then all the deadlines, office hours, etc will be localized properly for you and we don&#39;t have to rely on me describing what time it is...</p>",2019-08-22T06:08:52Z,34,Week 8/18 - 8/24,followup,,hyxsfbkeit22m2,jzmacfztfiqjb,2019-08-22T06:08:52Z,{},office_hours
321,no,"True enough, but then you&#39;ll get the questions about which one is right, Canvas or Piazza.<div><br /></div><div>I&#39;m clear on when it is; I was just trying to help make sure it&#39;s clear for others. My apologies if my suggestion came across as rude.</div>",2019-08-22T14:02:25Z,34,Week 8/18 - 8/24,feedback,,jl2xkw5tykhF,jzmr9ftugvy2iu,2019-08-22T14:02:25Z,{},office_hours
322,no,"<p>Canvas is always right. Every semester this comes up. If you miss an office hour it is not such a big deal. But, all the deadlines for your assignments are set in Canvas as well. It is your responsibility to ensure you hit them. There are rather severe penalties for missing them. Nobody wants you to lose points over this sort of mistake. Make sure you understand when the deadline is in your timezone. For those of you outside of the US, I am sorry we change what time it is twice a year... For those of you inside the US, make sure you are aware of that as well.</p>",2019-08-22T17:08:04Z,34,Week 8/18 - 8/24,feedback,,hyxsfbkeit22m2,jzmxw6xm6yc623,2019-08-22T17:08:04Z,{},office_hours
323,no,"<p>&#43;1 -- This comment by Alec will be, unfortunately, referenced in the future.</p>",2019-08-22T19:09:49Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzn28qxwzsz28m,2019-08-22T19:09:49Z,{},office_hours
324,stud,"<p>Chapter 2 of Littman (1996) conatins the  following claims: &#34;In the discounted infinite horizon model, the quantity  $$1 - \beta$$ can be viewed as the probability that the agent will cease to accrue additional reward; <em>therefore</em> (italics mine), it is as if the agent always has a constant number of steps remaining: $$1/(1 - \beta)$$.&#34;</p>
<p></p>
<p>How is it that the quantity $$1 - \beta$$ comes to be intepreted in the way Littman suggests above? And how, given this intepretation, do we wind up with the conclusion that the agent behaves as though it always has $$1/(1 - \beta)$$ steps remaining. The wording here suggests that given the above interpretation of the quantity $$1 - \beta$$, we can somehow infer that the agent will see itself as always having $$1/(1 - \beta)$$ steps remaining. But I&#39;m not sure how that follows. </p>
<p></p>
<p>Is it important that the agent sees itself as always having $$1/(1 - \beta)$$ steps remaining, or just that it sees itself as always having a constant number of steps remaining (regardless of what that number turns out to be)?</p>",2019-08-22T23:00:22Z,34,Week 8/18 - 8/24,followup,a_0,,jznah8qqiyo1hg,2019-08-22T23:00:22Z,{},office_hours
325,no,The discounted infinite horizon is a geometric series that converges to 1/1-B.<div><br /></div><div>(https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/SandS/SeriesTests/geometric.html)</div><div><br /></div>,2019-08-22T23:33:58Z,34,Week 8/18 - 8/24,feedback,,i4op5p9vfbq5yz,jznboghb5jm35k,2019-08-22T23:33:58Z,{},office_hours
326,no,"<p>BTW, I really like this explanation by Sergey Levine with an example MDP: <a href=""https://youtu.be/Tol_jw5hWnI?t=2417"">https://youtu.be/Tol_jw5hWnI?t=2417</a></p>
<p></p>
<p>Watch for 6&#43; minutes.</p>",2019-08-22T23:47:20Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jznc5nix3wm68m,2019-08-22T23:47:20Z,{},office_hours
327,stud,"<p>Well, sure it is. If $$r_{t}$$ happens to equal $$1$$. But the general form of the objective function that Littman cites in chapter 1 doesn&#39;t commit to a particular value for the rewards (or to constant rewards, or to constant rewards equaling unity). And Littman himself doesn&#39;t say anything more (or at least, anything that seems relevant here) about the reward values in chapter two, either. </p>
<p></p>
<p>And I&#39;m guessing that the claim in chapter two is still supposed to obtain even in the event that we consider the general form of the objective function given by $$v(q) := \sum_{t=0}^{\infty} \beta^{t}r_{t}$$. If  $$r_{t} = c \neq 1$$ for all $$t$$ here, we&#39;d get $$\frac{r_{t}}{1 - \beta}$$ as the sum of the series. Would we then say that the agent was going to behave as though it had <em>that</em> number of steps always remaining?</p>
<p></p>
<p>I guess the question was really more conceptual than mathematical. Apologies if that wasn&#39;t clear. The quantity Littman picks out is the value of the <em>objective function</em>. In the quoted passage above, he&#39;s treating it as though it&#39;s <em>the value of the remaining number of steps the agent sees itself as having</em>. These seem like two different things. The question is re how we wind up equating them. </p>",2019-08-22T23:57:14Z,34,Week 8/18 - 8/24,feedback,a_0,,jzncideexvp5hx,2019-08-22T23:57:14Z,{},office_hours
328,no,"<p>We can view the infinite-horizon case with discounting as equivalent to the finite-horizon case without discounting that instead has some probability $$(1 - \beta)$$ of terminating at every step. For this finite case, the expected return is the sum of:</p>
<p></p>
<p>$$(1 - \beta)R_{t&#43;1} &#43; $$ (probability of terminating immediately x immediate reward)</p>
<p></p>
<p>$$(1 - \beta)\beta (R_{t&#43;1} &#43; R_{t&#43;2}) &#43; $$ (probability of terminating at the second step x two-step reward)</p>
<p></p>
<p>$$(1 - \beta)\beta^2 (R_{t&#43;1} &#43; R_{t&#43;2} &#43; R_{t&#43;3}) &#43; $$ (probability of terminating at the third step x three-step reward)</p>
<p></p>
<p>$$\cdots$$</p>
<p></p>
<p>...and so on. If you then consider the coefficient of each reward term in the summation, this collapses to $$R_{t&#43;1} &#43; \beta R_{t&#43;2} &#43; \beta^2 R_{t&#43;3} \cdots$$, which is exactly the return for the infinite discounted case. (Using Sutton&#39;s indexing here; I forget what the lectures use.)</p>
<p></p>
<p>Now that we know these cases are equivalent, we can analyze the finite version. The probability of termination is $$(1 - \beta)$$; viewing this as a geometric distribution, i.e. the probability distribution of the number of Bernoulli trials needed to get a success (or in this case a termination), the expected number of steps is $$1 / (1 - \beta)$$. So the agent thinks of itself as always having expected $$1 / (1 - \beta)$$ steps remaining.</p>
<p></p>
<p>&#64;TAs please let me know if i am blatantly making stuff up here :(</p>",2019-08-23T01:39:26Z,34,Week 8/18 - 8/24,feedback,,jzifg1e23c29s,jzng5squuq23so,2019-08-23T01:39:26Z,{},office_hours
329,no,"<p>Say at each timestep the agent receives a constant reward $$r_c$$ regardless of state or action. Then the expected discounted reward is $$\sum^\infty _{t=0} \beta^t r_c =r_c \cdot 1/(1-\beta)$$.</p>
<p></p>
<p>Notice that Littman is referring to the expected number of steps remaining, not the expected reward. The agent can view its maximum number of steps remaining as $$\sum^\infty _{t=0} \beta^t =1/(1-\beta) $$. Then (if the reward is a constant, as in our example) the expected discounted reward is equal to the expected number of steps remaining multiplied by the reward received at each timestep: $$r_c \cdot 1/(1-\beta)$$.</p>
<p></p>
<p>The agent behaves as though it has $$1/(1-\beta)$$ steps remaining, thus (in our specific example) behaves as though it has $$r_c \cdot 1/(1-\beta)$$ discounted reward in its future.</p>",2019-08-23T12:27:59Z,34,Week 8/18 - 8/24,feedback,,jzlyi4e55bz5kj,jzo3bucbkxx2sz,2019-08-23T12:27:59Z,{},office_hours
330,stud,<p>Thanks Jonathan! That&#39;s super helpful (and super clear!) </p>,2019-08-23T12:45:36Z,34,Week 8/18 - 8/24,feedback,a_0,,jzo3yhvz6og5jj,2019-08-23T12:45:36Z,{},office_hours
331,no,<p>could you please point out where i&#39;m going wrong in my reasoning? thanks!</p>,2019-08-23T12:50:19Z,34,Week 8/18 - 8/24,feedback,,jzifg1e23c29s,jzo44kmqqgo4in,2019-08-23T12:50:19Z,{},office_hours
332,no,"<p>You&#39;re welcome!<br /><br />Hey Steph, you&#39;ve got it mostly right. Though to say discounting is equivalent to having a probability $$(1-\beta)$$ of termination, both must have the same horizon. Also it looks like your calculations are a bit off for the return with a finite horizon. E.g., when the horizon is $$t&#43;2$$ (using Sutton&#39;s indexing), the return (with probability $$(1-\beta)$$ of termination) would be: $$\beta^2(R_{t&#43;1}&#43;R_{t&#43;2}&#43;R_{t&#43;3}) &#43; (1-\beta)\beta(R_{t&#43;1}&#43;R_{t&#43;2}) &#43; (1-\beta)(R_{t&#43;1}) =  \beta^2 R_{t&#43;3} &#43; \beta R_{t&#43;2} &#43; R_{t&#43;1}$$</p>
<p>When the horizon is infinite, this does simplify to the infinite discounted case, so the rest of your post is correct.</p>",2019-08-25T02:57:38Z,33,Week 8/25 - 8/31,feedback,,jzlyi4e55bz5kj,jzqdu331nr25qi,2019-08-25T02:57:38Z,{},office_hours
333,no,"<p>i was only thinking about the infinite discounted case (hence the ellipsis), but i see how the coefficient for the final sequence would have to be adjusted for the finite discounted case. thanks again! i really appreciate the clarification :)</p>",2019-08-25T03:51:26Z,33,Week 8/25 - 8/31,feedback,,jzifg1e23c29s,jzqfr99k9ye762,2019-08-25T03:51:26Z,{},office_hours
334,no,<p>I think Steph had the right idea here with modeling reward termination as &#39;success&#39; in a Bernoulli trial and then finding the mean of the resulting geometric distribution.</p>,2019-09-11T22:29:37Z,31,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0fu6qvmah51r,2019-09-11T22:29:37Z,{},office_hours
335,no,"<p>Just curious, will the recording be posted in this thread or separately?</p>
<p></p>
<p>Was all set to join live but then Murphy flexed his law at me.</p>",2019-08-23T03:44:51Z,34,Week 8/18 - 8/24,followup,,jl2xkw5tykhF,jznkn3lncp9xs,2019-08-23T03:44:51Z,{},office_hours
336,no,<p>&#43;1</p>,2019-08-23T05:22:36Z,34,Week 8/18 - 8/24,feedback,,j6m1j8xqcvj6t6,jzno4t4rinh22l,2019-08-23T05:22:36Z,{},office_hours
337,no,<p>&#43;1</p>,2019-08-23T16:25:00Z,34,Week 8/18 - 8/24,feedback,,jqmcecftgff63y,jzobsnoqyt72yn,2019-08-23T16:25:00Z,{},office_hours
338,no,"<p>Yes, the office hours are recorded.</p>
<p></p>
<p>Go to Canvas &gt; BlueJeans &gt; Recorded BlueJeans Meetings (near the bottom)</p>",2019-08-23T16:43:14Z,34,Week 8/18 - 8/24,feedback,,i4op5p9vfbq5yz,jzocg3ocb642eh,2019-08-23T16:43:14Z,{},office_hours
339,no,<p>Is there a way to see recorded session for this meeting?</p>,2019-08-23T16:12:49Z,34,Week 8/18 - 8/24,followup,,jc6s23mr74r45q,jzobczbd33b257,2019-08-23T16:12:49Z,{},office_hours
340,no,<p>Go to Canvas &gt; BlueJeans &gt; Recorded BlueJeans Meetings (near the bottom)</p>,2019-08-23T16:43:25Z,34,Week 8/18 - 8/24,feedback,,i4op5p9vfbq5yz,jzocgcfo6nf2il,2019-08-23T16:43:25Z,{},office_hours
341,no,"<p>Hey Chris,</p>
<p>I followed your instructions but there are no recording here for me. Any suggestions?</p>",2019-08-23T19:20:58Z,34,Week 8/18 - 8/24,feedback,,gx3c8l7z7r72zl,jzoi2y5r2jx4fv,2019-08-23T19:20:58Z,{},office_hours
342,no,Looking into it...,2019-08-23T20:40:59Z,34,Week 8/18 - 8/24,feedback,,i4op5p9vfbq5yz,jzokxum7zn3ko,2019-08-23T20:40:59Z,{},office_hours
343,no,"<p>We have had some trouble in the past with the GA Tech =&gt; Bluejeans thing before. We don&#39;t have any more buttons to push to get you in. But, you can file a ticket with OIT and see if you can get any traction.</p>
<p></p>
<p>In the interim here is the link:</p>
<p></p>
<p><a href=""https://bluejeans.com/s/cnp6Q/"">https://bluejeans.com/s/cnp6Q/</a></p>",2019-08-23T21:31:44Z,34,Week 8/18 - 8/24,feedback,,hyxsfbkeit22m2,jzomr44n9l92kg,2019-08-23T21:31:44Z,{},office_hours
344,no,"<p>How do you sign up for bluejeans? I just tried using gt credentials and it doesn&#39;t work, but I do not see a &#34;sign up&#34; button. </p>",2019-09-08T01:03:23Z,31,Week 9/8 - 9/14,feedback,,jqr9leosvw8P,k0a9x2r5r1p74t,2019-09-08T01:03:23Z,{},office_hours
345,no,"<p>Can you access: <a href=""http://gatech.bluejeans.com/"">http://gatech.bluejeans.com/</a>?</p>",2019-09-08T02:21:21Z,31,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0acpc58jrf6h1,2019-09-08T02:21:21Z,{},office_hours
346,no,"<p>For Lesson 2 video 22 “Quiz: Finding Polices - 3</p>
<p>Can someone explain/show the calculation for the lower square the y did in the video below:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fjzoi9i3v41zi%2FCapture.PNG"" alt="""" width=""685"" height=""373"" />Thank you for your help.</p>",2019-08-23T19:26:52Z,34,Week 8/18 - 8/24,followup,,gx3c8l7z7r72zl,jzoiajhmqvv6a3,2019-08-23T19:26:52Z,{},office_hours
347,no,<p>Sure. This is because all around it were 0s and it had -.04 reward. Plug that into equation in your screenshot.</p>,2019-08-23T22:09:46Z,34,Week 8/18 - 8/24,feedback,,jzj7y1ofgsro1,jzoo411pz8n49z,2019-08-23T22:09:46Z,{},office_hours
348,no,<p>Thanks Angel. I found there is a bit more too it than that but I got it now.</p>,2019-08-23T22:38:14Z,34,Week 8/18 - 8/24,feedback,,gx3c8l7z7r72zl,jzop4mqtwoh4cj,2019-08-23T22:38:14Z,{},office_hours
349,no,"<p>There is definitely more to it. In the video, they simplify it and somewhat assume things... They are only calculating the &#34;max&#34; action!</p>
<p></p>
<p>So the equation gets applied with the R(s), which is only -0.04, and then the expectation of the next state. But because in that cell the agent would bounce back or go to it right (NORTH) or left (SOUTH), and the value of both NORTH and SOUTH is 0 (In the previous iteration), then all of that is still 0. The total value of that state will be -0.04.</p>
<p></p>
<p>Here is the full equation applied to 1 iteration 1 state only. You would have to apply this for all 12 states, for as many iterations as necessary (you stop when there is no more change in values.</p>
<p></p>
<p>BTW, I&#39;m using U and V below, sorry about that, I&#39;m used to Vs, but ended up mixing things up.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fjzpsdtuhxgl%2Futility.png"" alt="""" width=""1113"" height=""641"" /> </p>",2019-08-24T16:59:16Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzpsgktsh5u5cs,2019-08-24T16:59:16Z,{},office_hours
350,no,"<p>so this would be:</p>
<p></p>
<p>-0.04 &#43; 0.5 max [ -0.1, 0, -0.1, -0.8 ]</p>
<p>= -0.04 &#43; 0.5 * (0)</p>
<p>= -0.04</p>
<p></p>
<p>??</p>",2019-08-24T20:07:55Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzpz76ni3i73dg,2019-08-24T20:07:55Z,{},office_hours
351,no,<p>Yep.</p>,2019-08-24T20:09:28Z,34,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzpz96lc7t4h8,2019-08-24T20:09:28Z,{},office_hours
352,no,<p>This is also discussed in &#64;40</p>,2019-08-24T22:05:29Z,34,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzq3ednj8813hn,2019-08-24T22:05:29Z,{},office_hours
353,no,<p>Thank you Miguel!</p>,2019-08-25T16:05:30Z,33,Week 8/25 - 8/31,feedback,,gx3c8l7z7r72zl,jzr5zaauqun66k,2019-08-25T16:05:30Z,{},office_hours
354,no,Hey many thanks!,2019-08-22T12:32:25Z,65,Week 8/18 - 8/24,followup,,jziehbrvvwc4hw,jzmo1p1k81s2x5,2019-08-22T12:32:25Z,{},other
355,no,<p>This is super helpful!  Thank you!</p>,2019-08-22T18:03:25Z,65,Week 8/18 - 8/24,followup,,jqmfuaidej9155,jzmzvd4i5oo72z,2019-08-22T18:03:25Z,{},other
356,no,"<p>This is great! Thanks, Quang Vu.</p>",2019-08-22T20:48:25Z,65,Week 8/18 - 8/24,followup,,jziehvhblig5if,jzn5rkbdlqr4zi,2019-08-22T20:48:25Z,{},other
357,no,"<p>Damn! You&#39;re saving me a lot of time.  I had started doing it.</p>
<p>I just hate all those pdf that come without bookmarks.  </p>
<p>Thanks a lot.  </p>",2019-08-22T20:50:11Z,65,Week 8/18 - 8/24,followup,,jzh6k6o994a6dh,jzn5ttpds6j3dh,2019-08-22T20:50:11Z,{},other
358,no,<p>Is it a good idea to read it all?</p>,2019-08-22T20:55:02Z,65,Week 8/18 - 8/24,followup,,jzh6k6o994a6dh,jzn602ogyf963a,2019-08-22T20:55:02Z,{},other
359,no,<p>You will read it all in this class.</p>,2019-08-22T21:41:35Z,65,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzn7nxa5dwkbj,2019-08-22T21:41:35Z,{},other
360,no,"<p>You mean, bit by bit, lectures after lectures I guess? Good.</p>",2019-08-22T21:53:03Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzn82o9cqht6fy,2019-08-22T21:53:03Z,{},other
361,no,<p>Yes. Or you can read it all over the weekend! You will also read Sutton and Barto&#39;s book.</p>,2019-08-22T21:58:14Z,65,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzn89cfnl5e4y,2019-08-22T21:58:14Z,{},other
362,no,<p>GREAT! I was worried I kinda had to read those books on top of the lectures.  </p>,2019-08-22T22:03:12Z,65,Week 8/18 - 8/24,feedback,,jzh6k6o994a6dh,jzn8fq7yy4v3e6,2019-08-22T22:03:12Z,{},other
363,no,"Thank you! You’re amazing, Quang Vu!",2019-08-25T03:30:47Z,64,Week 8/25 - 8/31,followup,,jl0d29plhbs6cl,jzqf0pu1f811o8,2019-08-25T03:30:47Z,{},other
364,no,<p>Regret is how much you lost by choosing path P instead of P*.</p>,2019-08-22T15:24:12Z,65,Week 8/18 - 8/24,followup,,jc554vxmyuy3pt,jzmu6meumifah,2019-08-22T15:24:12Z,{},other
365,no,<p>Indeed.</p>,2019-08-22T16:18:18Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jzmw472aqv13s1,2019-08-22T16:18:18Z,{},other
366,no,<p>Regret is how much money you haven&#39;t earned by not investing in the stock before it went up.</p>,2019-08-22T23:45:50Z,65,Week 8/18 - 8/24,feedback,,jqkxzdmmolGf,jznc3px426z34o,2019-08-22T23:45:50Z,{},other
367,no,"<p>That&#39;s why we take ML4T before RL, so we don&#39;t have that regret!</p>",2019-08-22T23:47:35Z,65,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jznc5z81zce6d0,2019-08-22T23:47:35Z,{},other
368,no,<p>:)</p>,2019-08-22T23:48:34Z,65,Week 8/18 - 8/24,feedback,,hyx9thiqa6j4nn,jznc782zhwg6up,2019-08-22T23:48:34Z,{},other
369,no,"<p>Would it be similar to opportunity cost concept?</p>
<p></p>",2019-09-02T10:23:02Z,63,Week 9/1 - 9/7,feedback,,jznmonjuzvi67j,k0299ocxqhx2f2,2019-09-02T10:23:02Z,{},other
370,no,"<p>Regret is the core of RL; at least with respect to preference of actions taken.  The whole paradigm of exploitation vs exploration, is one of pushing exploration in order to implicitly minimize regret.   From the optimistic initialization, to epsilon exploration (vs greedy),  to other variants.</p>
<p></p>
<p>Interestingly, regret occurs both in action selection; but also as a function of total accumulated reward (at least, as posited by value).</p>
<p></p>
<p>There are some very sophisticated things you can do with regret; but reward, is always limited as a feature of the environment (q*(a)).  Less modification/incorporation i.e. step-wise (iterated) mods.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-08-23T00:00:28Z,65,Week 8/18 - 8/24,followup,,jzivtxcbl6964n,jzncmj8eyjy6v3,2019-08-23T00:00:28Z,{},other
371,no,<p>&#34;There are some very sophisticated things you can do with regret;&#34; &gt; do you have any citations to share that we could explore?</p>,2019-08-23T00:07:31Z,65,Week 8/18 - 8/24,feedback,,jc554vxmyuy3pt,jzncvliltyk201,2019-08-23T00:07:31Z,{},other
372,no,"<p>read the book (it has tons of cites).</p>
<p></p>
<p>just remember, regret, is value&#39;s twin.  look for it even when its not explicitly stated as such.  its what fundamentally drives, RL.</p>",2019-08-23T21:38:10Z,65,Week 8/18 - 8/24,feedback,,jzivtxcbl6964n,jzomzecjvpw2zf,2019-08-23T21:38:10Z,{},other
373,no,"<p>jacob:</p>
<p></p>
<p>Pi is epsilon-optimal iff  |V^PI(s) - V^PI*(s)| =&lt; epsilon,</p>
<p></p>
<p>forall.s:S</p>
<p></p>
<p>&#34;bounded loss, regret&#34;</p>
<p></p>
<p>;)</p>
<p></p>
<p>note that epsilon = 0; for policies gauranted to converge. ;)</p>",2019-08-25T05:40:40Z,64,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzqjnqlpbuu4c7,2019-08-25T05:40:40Z,{},other
374,no,"<p><strong attention=""is5gzbotXmz"">&#64;tianhang zhu</strong>  Thanks for the thorough answer -- makes sense that we may still be in an environment that provides rewards at each step, but where the R function itself is difficult to estimate. That&#39;s a helpful distinction. It&#39;s not explicit (yet) in the lectures, but I get the sense different algorithms may be chosen that more efficiently solve each piece of the puzzle, so to speak.</p>
<p></p>
<p><tt>&#34;it seems significantly easier to specify or approximate a reward function than a sufficiently accurate transition function&#34; not necessarily, R function can be arbitrarily complex. Anything can happen right?</tt></p>
<p></p>
<p>Good point. My intuition is that reward functions are often at least semi-intuitive and tie directly with the human-defined goals that initially inspired training an agent (&#34;AGI safety/control&#34; problem aside), whereas modeling sufficiently representative transition matrices of real-world dynamics seems like it could be far harder (e.g. simulating relevant states in financial markets). My novice intuitions aside, your answer makes sense and is thought provoking -- thanks again.</p>",2019-08-22T23:53:01Z,65,Week 8/18 - 8/24,followup,,isde34zracb1mz,jznccyj7q6b1t,2019-08-22T23:53:01Z,{},other
375,no,<div>np</div><div><br /></div>,2019-08-23T12:21:27Z,65,Week 8/18 - 8/24,feedback,,is5gzbotXmz,jzo33fyvbhp12t,2019-08-23T12:21:27Z,{},other
376,no,"<p>for the Q form, we estimate the Q function as well without working directly on its functional form which needs T. (in model free approaches) As for reward, it is correct that the environment will tell us what the respective reward for a even action in a particular state and we do not need to know R function directly as well.</p>",2019-08-23T02:58:25Z,65,Week 8/18 - 8/24,followup,,jvfpllmsggt7p4,jznizds61zp5y3,2019-08-23T02:58:25Z,{},other
377,no,"<p><a href=""https://spinningup.openai.com/en/latest/"" target=""_blank"">OpenAI&#39;s Spinning Up course</a> is also excellent. </p>",2019-08-23T16:45:00Z,65,Week 8/18 - 8/24,followup,,i4op5p9vfbq5yz,jzocid7zoqs3lj,2019-08-23T16:45:00Z,{},other
378,no,<p>Great! Will check it out.. thanks!</p>,2019-08-23T19:18:08Z,65,Week 8/18 - 8/24,feedback,,is5h007nzB9,jzohzb09kcu3ci,2019-08-23T19:18:08Z,{},other
379,no,"<p>A couple other great resources for DRL:</p>
<p></p>
<ul><li><a href=""https://www.amazon.com/Introduction-Reinforcement-Learning-Foundations-Machine/dp/1680835386"">https://www.amazon.com/Introduction-Reinforcement-Learning-Foundations-Machine/dp/1680835386</a>
<ul><li>short and sweet; you can also find the paper free online, i&#39;m just a sucker for the tactile nature of printed books:  <a href=""https://arxiv.org/abs/1811.12560"">https://arxiv.org/abs/1811.12560</a></li></ul>
</li><li><a href=""https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"">https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893</a>
<ul><li>provides an excellent exposure to SotA DRL and <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong> teaches part of it!</li></ul>
</li></ul>",2019-08-25T20:10:04Z,64,Week 8/25 - 8/31,followup,,jdd22f9zpbq5sz,jzrepseg8g45od,2019-08-25T20:10:04Z,{},other
380,no,"<p>To pile on, you can think of those weights as representing the probability that one is chosen 60% of the time, another 30%, and the last 10%.  </p>
<p></p>
<p>So if you ran this process repeatedly over a long period of time, averaging the results there would give you the same answer.</p>
<p></p>
<p>It wasn&#39;t called out especially well that those were the weights on each process, but they did say it and gesture to them.</p>",2019-08-24T00:12:26Z,65,Week 8/18 - 8/24,followup,,jqmfuaidej9155,jzoshs2gtj657y,2019-08-24T00:12:26Z,{},other
381,no,"<p>&#34;... but they did say it and gesture to them.&#34; </p>
<p></p>
<p>lol.</p>
<p></p>
<p>I assumed they were probabilities for the streams i.e. they were seen x amount of times vs total times seen.</p>
<p></p>
<p>I did not, however, even attempt to answer the quiz.  the windowing function wasn&#39;t well-explained (that you were supposed to be windowing each of the series, and then averaging across the three series within each window).</p>
<p></p>
<p>I imagine that the key for comparison of streams is to window so that the average comparisons are roughly equal (i.e. infinite to finite) allowing for cross-stream comparison.   But there is no real mention of how to combine averaged multiple windows... nor is there a mention of what types of windows you would wish to use (arbitrary? partition)?</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-08-24T11:33:19Z,65,Week 8/18 - 8/24,feedback,,jzivtxcbl6964n,jzpgteel94x3y2,2019-08-24T11:33:19Z,{},other
382,no,"<p>Agreed!  I love the energy and the playfulness of these lectures, but I&#39;m afraid it&#39;s coming at the expense of conciseness in presentation.  I made another post about the pseudocode presented for TD(λ) in a later lecture and I suspect it&#39;s the same thing going on.  </p>
<p></p>
<p>Can&#39;t fault Dr. Littman for his enthusiasm though, and Dr. Isbell does a great straight man for him to play off of.</p>
<p></p>
<p></p>",2019-08-24T15:49:20Z,65,Week 8/18 - 8/24,feedback,,jqmfuaidej9155,jzppymn6euy708,2019-08-24T15:49:20Z,{},other
383,no,"<p>they get better as they start digging into the material ;) </p>
<p></p>
<p>I think the first couple of lectures are taken from the end of ML which might explain why some things might be &#39;presumed&#39; i.e. they may have been recorded in the ML context.</p>",2019-08-26T05:48:55Z,64,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzrze72xght4xf,2019-08-26T05:48:55Z,{},other
384,no,"In fact we repeatedly refer to things from 7641. Context, people, context. ",2019-08-29T13:25:10Z,64,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jzwq0hse1ddya,2019-08-29T13:25:10Z,{},other
385,no,"<p>&#64;John-Pierre Bianchi, I have to do some serious traveling in September so I&#39;m trying to get to a place where I can stay caught up, which right now means front-loading the content as much as possible.  Besides that, this is my second attempt at this course.</p>
<p></p>
<p>I expect to be roughly back on the course schedule around the end of September.  The best strategy for you is the best strategy for you.</p>
<p></p>
<p></p>",2019-08-27T02:03:59Z,64,Week 8/25 - 8/31,followup,,jqmfuaidej9155,jzt6ssgpmov4ry,2019-08-27T02:03:59Z,{},other
386,no,"<p>Aaah, ok, I get it.  It&#39;s my first term so I&#39;m asking all sorts of questions to adapt the best I can.</p>
<p>Can you share what went wrong the first time if it&#39;s ok with you?</p>
<p>Thanks for taking the time to answer.  </p>",2019-08-27T23:21:51Z,64,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzugg4knq1q24u,2019-08-27T23:21:51Z,{},other
387,no,"<p>Sure!  For me it was spending too much time at work, and not enough on school, and then burning out when it came time to catch up on the schoolwork I had let pile up.  </p>
<p></p>
<p>So my strategies focus on avoiding that situation.</p>
<p></p>
<p></p>",2019-08-28T21:25:08Z,64,Week 8/25 - 8/31,feedback,,jqmfuaidej9155,jzvrpvna94331m,2019-08-28T21:25:08Z,{},other
388,no,"<p>I started trying to sketch out a proof by induction a couple times, but have had to cut off the effort twice now. I wanted to be able to break down a finite series of states s0..sn in S into the sub-sequences s0 and s1..sn using the stationarity of preferences property, and then claim that by induction this generalizes to sequences of arbitrary, but finite length.</p>
<p></p>
<p>I think there are a couple problems with this approach. One is that as stated, the stationarity of preferences property doesn&#39;t seem restricted to sequences of finite, but arbitrarily long length, and my proof wouldn&#39;t have applied to infinite sequences. Another issue was that the stationarity of preferences property didn&#39;t give me as much power to break down a finite sequence into subsequences as I wanted, and I think I need to start from something that looks more like the Bellman equations instead.</p>",2019-08-25T05:58:35Z,64,Week 8/25 - 8/31,followup,,jqwygbqmHAiE,jzqkas9j5m57g4,2019-08-25T05:58:35Z,{},other
389,no,"<p></p>
<p>I was interested in the (potential other) mathematical properties implied by stationary properties with respect to operators    it would be interesting, to see the actual proof referenced ;) and to read it in the historical context in which it was proved.  I think the technical term might be &#34;markov preference&#34;.<span style=""font-family:sans-serif""></span></p>",2019-08-25T06:25:12Z,64,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzql90ivc4f60x,2019-08-25T06:25:12Z,{},other
390,no,"<p>Anurag or Michael would love to join the slack channel.My name is Jorge Garcia.  I work in SF and live in Palo Alto. Would love interact with Bay Area classmates. </p>
<p></p>",2019-08-28T07:09:12Z,64,Week 8/25 - 8/31,followup,,jzo9ju5fppz31r,jzux55n93r73sw,2019-08-28T07:09:12Z,{},other
391,no,"<p>Hello,</p>
<p>Sorry about the delay. I was traveling and just got back.</p>
<p></p>
<p>This is the link for the slack:</p>
<p><a href=""https://join.slack.com/t/bayarea-omscs/shared_invite/enQtNzE4ODMzMzM2MTYxLWE0NTZkMmNlNTRkNmIyYjUwYjQxNGRmYTMzMzYzZDlmN2I2Y2M4YjdkODZjYzJlMDk5NjBiZmYyMDc5MjEwMGE"">https://join.slack.com/t/bayarea-omscs/shared_invite/enQtNzE4ODMzMzM2MTYxLWE0NTZkMmNlNTRkNmIyYjUwYjQxNGRmYTMzMzYzZDlmN2I2Y2M4YjdkODZjYzJlMDk5NjBiZmYyMDc5MjEwMGE</a><a href=""https://join.slack.com/t/bayarea-omscs/shared_invite/enQtNzE4ODMzMzM2MTYxLWE0NTZkMmNlNTRkNmIyYjUwYjQxNGRmYTMzMzYzZDlmN2I2Y2M4YjdkODZjYzJlMDk5NjBiZmYyMDc5MjEwMGE""></a></p>
<p></p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>",2019-09-01T14:32:44Z,63,Week 9/1 - 9/7,feedback,,jfzaqnqvtQ1m,k012qxoln0f7c3,2019-09-01T14:32:44Z,{},other
392,no,<p>I am also from bay area. Can i join this slack as well.</p>,2019-09-27T06:32:42Z,60,Week 9/22 - 9/28,feedback,,k111ttxx3g36uj,k11r1rkbaqs14p,2019-09-27T06:32:42Z,{},other
393,no,"<p>I&#39;m Davis Robinson in Manchester NH, although originally from Tennessee. Reinforcement learning seemed neat. This is my 2nd class in the program after intro to AI or something. I&#39;m currently a working as a data scientist and am looking forward to learning some new things!</p>",2019-08-29T02:37:34Z,64,Week 8/25 - 8/31,followup,,jr46k9bbb1g5ju,jzw2vor0vq36s,2019-08-29T02:37:34Z,{},other
394,no,"<p>i have posted a series of posts to the slack #cs7642 on omscs-study.slack.com</p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzivtxcbl6964n%2Fjzqax39kyxfz%2Fcalcs_on_slack_for_mdp__20190824_213531.jpg"" target=""_blank"">calcs_on_slack_for_mdp__20190824_213531.jpg</a></p>
<p></p>
<p>this is what I need.   calcs, in an excel sheet, showing the exact procedure out past the trivial V1(s11) case.    Please visit the slack for &#39;expandable&#39; pictures. <em> I have several questions in the slack that are open, if you would like to opine.</em></p>
<p></p>
<p>I am sure, it something, extremely stupid.  It looks like RL has several very similar variants on the same algorithms that allow feed-forward, estimation and backpropogation.  I am simply not sure which ones we are calculating for this lesson.   I have done, a backpropogation from a goal state, on the fact that it has the highest utility at V0(s) (i.e. we are &#39;coincidentally&#39; starting at s11, right next to a goal state of s12).  There are several decisions that seem arbitrary/forced, key details that I am unsure of with respect to the genericized algorithm.</p>
<p></p>
<p>With the respect to the answers above referencing &#34;its always better to move right&#34;; this invokes either a static policy, or a ranking policy.  It is not clear to me how &#34;always moving right&#34; is evident in the problem declartion, other than a fiat declaration during the solution, and a back reference to a pre-existing optimized plan per state, earlier in lecture - said plan to be the result of iteration, and thus while available to the audience; is not available to the agent at time = 0.</p>
<p></p>
<p>I am unsure in particular, of the properties of crossing thresholds in an absorbing state, with respect to back-propogation of information along all possible paths.   Nor am I sure of how to handle cases where choices are technically not possible (0 probability) but in effect, cloud actual actions that can be taken due to a negative reward (max (0 , -.1 -.1) is in fact 0, --&gt; although in certain cases this in fact, the correct choice (semantically, if not in a syntactic fashion).   Nor am I sure how to directly address stochastic transitions, that due to geometrical conditions in the environment, themselves are verboten (in a clean fashion).</p>
<p></p>
<p>lastly, it seems that the notion of a planned path, with respect to stochastic transitions and declared or implicit policy selection of actions, is causing me some degree of discomfort.   I would like to have a conversation on this as well.</p>
<p></p>
<p></p>
<p></p>",2019-08-25T01:54:15Z,52,Week 8/18 - 8/24,followup,,jzivtxcbl6964n,jzqbkkkv3hr35j,2019-08-25T01:54:15Z,{},other
395,no,"<p>it looks like my answer was confusing, so let me try to clear it up.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzifg1e23c29s%2Fjzqcdyyaddmh%2Fexcelgridworld.PNG"" alt="""" width=""408"" height=""100"" /></p>
<p></p>
<p>this is from your excel screenshot and consists of $$v_1(s)$$, the value function for each state after the first update. we now make the second round of updates to get to $$v_2$$. focusing again on $$s_{11}$$, we begin by calculating the value for each action: $$\sum_{s&#39;} T(s, a, s&#39;) v(s&#39;)$$</p>
<p></p>
<p>right:</p>
<p>$$0.8*1.0$$ (if the move succeeds)</p>
<p>$$&#43; 0.1*0.36$$ (if the move fails 90 degrees upwards, and the agent returns to $$s_{11}$$)</p>
<p>$$&#43; 0.1*(-0.04)$$ (if the move fails 90 degrees downwards)</p>
<p>$$= 0.832$$</p>
<p></p>
<p>down:</p>
<p>$$0.8*(-0.04)$$ (if the move succeeds)</p>
<p>$$&#43; 0.1*1.0$$ (if the move fails 90 degrees into the goal state)</p>
<p>$$&#43; 0.1*(-0.04)$$ (if the move fails into the non-goal state)</p>
<p>$$=0.064$$</p>
<p></p>
<p>up: </p>
<p>$$0.8*0.36$$ (if the move succeeds, sending the agent back to $$s_{11}$$ due to the wall)</p>
<p>$$&#43; 0.1*1.0$$ (if the move fails 90 degrees into the goal state)</p>
<p>$$&#43; 0.1*(-0.04)$$ (if the move fails into the non-goal state)</p>
<p>$$=0.384$$</p>
<p></p>
<p>left:</p>
<p>$$0.8*(-0.04)$$ (if the move succeeds)</p>
<p>$$&#43; 0.1*0.36$$ (if the move fails 90 degrees upwards, sending the agent back to $$s_{11}$$)</p>
<p>$$&#43; 0.1*(-0.04)$$ (if the move fails 90 degrees downwards)</p>
<p>$$=0$$</p>
<p></p>
<p>this gives us $$\sum_{s&#39;} T(s, a, s&#39;) v(s&#39;)$$ for each of the four actions. if we look at the update rule, $$v(s) = r(s) &#43; \gamma \max_a \sum_{s&#39;} T(s, a, s&#39;) v(s&#39;)$$, the next step is to take the max over these values. this is $$\max[0.832, 0.064, 0.384, 0.0] = 0.832$$.</p>
<p></p>
<p>so now we have $$\max_a \sum_{s&#39;} T(s, a, s&#39;) v(s&#39;) = 0.832$$, which is most of the update. plugging this in, along with $$r(s) = 0.04$$ and $$\gamma = 0.5$$, we get $$v(s) = -0.04 &#43; 0.5*0.832 = 0.376$$.</p>
<p></p>
<p>&gt;&gt;With the respect to the answers above referencing &#34;its always better to move right&#34;...</p>
<p></p>
<p>this is just a shortcut. in the gridworld, for the first two updates, it&#39;s intuitively clear that going right at state $$s_{11}$$ is correct, so it speeds up the calculation (when we&#39;re doing it by hand) to only calculate $$\sum_{s&#39;} T(s, a, s&#39;) v(s&#39;)$$ for [right] and plug it into the update. i think it&#39;s also done this way in the lecture. but you&#39;re correct that the value iteration algorithm doesn&#39;t know this -- when you code it up, the algo will check all possible actions (as we&#39;ve done above) and select the max value from these.</p>
<p></p>
<p>regarding actions that are not possible: are you referring to the agent walking into a wall? if you ignore the gridworld setup and think of a generic MDP, this is just an action that transitions you back to the same state. it doesn&#39;t have to be treated differently from any other action.</p>
<p></p>
<p>hope that clarified a bit. it&#39;s late so there&#39;s a chance i added wrong or something in the calcs, but that&#39;s the general idea.</p>",2019-08-25T03:01:25Z,52,Week 8/18 - 8/24,feedback,,jzifg1e23c29s,jzqdyxqpsu93z3,2019-08-25T03:01:25Z,{},other
396,no,"<p>Steph Lin:</p>
<p></p>
<p>excellent presentation!</p>
<p></p>
<p>cleared it up beautifully; I was seeing the problem as a propogation outward from a initial state with the propogation happening at the tips of tree (in effect resetting the &#39;current node&#39; to the nodes at the edge of the tree); but I see here you are instead using a broadcast method from the initial node, and only concerned with the optimal at s11.   and the succession of utility updates to S11, depends only on the estimates updated in timestep t-1 at the nodes directly adjacent to S11 (and so-forth on successive iterations for each of the adjacent nodes to S11).</p>
<p></p>
<p>also I was not handling the up case (sending back to self) properly.  I considered this, but since the probability of transition is effectively zero; the expected utility in staying put would also be zero (plus the reward of the movement).   I was not sure how to handle this (this is the reference to semantic vs syntactic utilities on prohibited state transitions).</p>
<p></p>
<p>one question of clarification:  do the values of nodes adjacent to S11, on each step, take a piece of the utility from S11 on the previous iteration (depending on how/if they are able to scholastically reach s11)?   That is, is V(S11) essentially accumulating a fed-back version of its own utility at time T, from time T-1 (assuming the action selected is in fact, to arrive back at S11 from S07/S10)?</p>
<p></p>
<p>&gt; so it speeds up the calculation (when we&#39;re doing it by hand) to only calculate <span style=""font-size:120%"">∑</span><span style=""font-style:italic"">s</span>′<span style=""font-style:italic"">T</span>(<span style=""font-style:italic"">s</span>,<span style=""font-style:italic"">a</span>,<span style=""font-style:italic"">s</span>′)<span style=""font-style:italic"">v</span>(<span style=""font-style:italic"">s</span>′) for [right] and plug it into the update. </p>
<p></p>
<p>Yes I see where/how it interacts now. ;)  The implicit planning in the equations (select best action at time t) versus the explicit policy (greedy selection of optimal action at time T) vs the optimal (learned policy) vs. a planned walk, was confusing the issue to my mind.</p>
<p></p>
<p>&gt; In comparison, policy evaluation (which is part of policy iteration) calculates the expected utility of states when following a specific, known (and possibly non-optimal) policy <span style=""font-size:120%""><span style=""font-style:italic"">π</span></span>.</p>
<p></p>
<p>.... and now this makes sense ;)  utility vs policy iteration (see above) :)<br /></p>
<p></p>
<p>&gt; this is just an action that transitions you back to the same state. it doesn&#39;t have to be treated differently from any other action.</p>
<p></p>
<p>yes I was thinking programmatically that a transition to self would capture failed movements better (make the program slighty more tractable by pre-computing possible sets of actions at any given state).  the problem is, that the probability of that movement occurring, within the stochastic transitions provided, is essentially zero; and not-zero -- Zero in the sense that the movement to the intended final state, does not take place; and not-zero in the sense, that the movement, can take place (resulting in it returning to its own state).</p>
<p></p>
<p></p>
<p>thank you, for taking the time to clear that up :)</p>",2019-08-25T03:31:59Z,52,Week 8/18 - 8/24,feedback,,jzivtxcbl6964n,jzqf292lpmq6ia,2019-08-25T03:31:59Z,{},other
397,no,"<p>Steph Lin:</p>
<p></p>
<p>I have one other question besides the question above (reproduced here):</p>
<p></p>
<p>q[1] - one question of clarification:  do the values of nodes adjacent to S11, on each step, take a piece of the utility from S11 on the previous iteration (depending on how/if they are able to scholastically reach s11)?   That is, is V(S11) essentially accumulating a fed-back version of its own utility at time T, from time T-1 (assuming the action selected is in fact, to arrive back at S11 from S07/S10)?</p>
<p></p>
<p>q[2] - with regard to updating expected utility of terminal states; since entry into each of the terminal states has its own reward; is the utility of the terminal state, updated along with the rest of the other states?  that is, is there a discounting of subsequent utilities of the terminal state? </p>
<p></p>
<p>... or is the utility of the terminal state set to its reward and maintained as such?  the question asks, in essence, does the terminal state maintain a special status among states excluded from utility discounting; or does it in turn, with respect to s11 simply exist as a &#34;reward&#34; or &#34;punishment&#34;</p>
<p></p>
<p>I ask this because it looks like the updating of states in this example, happens in effect as a what-if (prospective state transition * possible utility) rather than an actual movement.   If this is this the case, then the &#39;terminal state&#39; is never reached; rather convergence determines when the algorithm ends.   And this means that as S11 is an adjacent state undergoing iterative value updates, then so to must the goal states utility with respect to the discounting of utility at time step T(1..k) s.t. that it would keep in &#34;step&#34; with the rest of the utility updates....  Or am I mistaken?</p>
<p></p>
<p>[q3] - presuming that terminal states, themselves are not considered special, and are subject to discounting -- how does one determine the maximal action?  as there are no transitions out from S12, this means that it is a virtual certainty (1.0) that you will stay in S12; that is the utility of all such actions, are identical, and thus arbitrary actions will be taken during the update sequence? </p>
<p></p>
<p>I believe, that this might be what is termed an &#34;absorbing state&#39;, in the sense that one could easily picture an MDP where the feedback is delayed i.e. entering the goal state, does not terminate the program until k time steps have occurred past entry (or alternatively, dwelling in the goal state for k steps).</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-08-25T03:49:55Z,52,Week 8/18 - 8/24,feedback,,jzivtxcbl6964n,jzqfpbbq9sa2v5,2019-08-25T03:49:55Z,{},other
398,no,"<p>glad the discussion helped :)</p>
<p></p>
<p>q1: yes (if i&#39;m reading your question correctly). when you run the $$v_2$$ update for $$s_7$$ and $$s_{10}$$, they&#39;ll use the 0.36 value of $$v_1(s_{11})$$ in their calcs. (we&#39;re doing synchronous updating here, like miguel said, so 0.376 doesn&#39;t get used until the next iteration.) then when computing $$v_3$$ for $$s_{11}$$, it&#39;ll use the $$v_2$$ values for $$s_7$$ and $$s_{10}$$, which depend on $$v_1(s_{11})$$. so there&#39;s a kind of feedback loop going on here, as you said.</p>
<p></p>
<p>q2 / 3: if you don&#39;t want to treat the goal states separately, you can create an artificial absorbing state. let&#39;s call the goal states $$s_g$$ and $$s_r$$ (for green and red, respectively). we&#39;ll add a state $$s_*$$ that has only one action, which always returns to $$s_*$$ with a reward of 0, and initialize it with a value of 0. in addition, we give $$s_g$$ and $$s_r$$ both a single action that transitions deterministically to $$s_*$$.</p>
<p></p>
<p>if you think about the value updates:</p>
<p></p>
<p>$$v(s_g) = r(s) &#43; \gamma \max_a \sum_{s&#39;} T(s, a, s&#39;) v(s&#39;) = 1 &#43; 0 = 1$$: you get &#43;1 reward for that goal state, then move to a state with value 0.</p>
<p></p>
<p>$$v(s_r) = r(s) &#43; \gamma \max_a \sum_{s&#39;} T(s, a, s&#39;) v(s&#39;) = -1 &#43; 0 = -1$$: you get -1 reward for that goal state, then move to a state with value 0.</p>
<p></p>
<p>$$v(s_*) = r(s) &#43; \gamma \max_a \sum_{s&#39;} T(s, a, s&#39;) v(s&#39;) = 0 &#43; 0 = 0$$: you get 0 reward and transition back to $$s_*$$, which has 0 value.</p>
<p></p>
<p>so by using this absorbing state, you can treat the goal &#43; terminal states the same as any other state, and make updates continuously (that don&#39;t affect their values). this approach also allows you to treat an episodic task as a continuing task, which is sometimes useful for intuition / notation.</p>
<p></p>
<p>if you don&#39;t want to use the absorbing state, you can always just say for this example that there are no actions from those two goal states, so that entire $$v(s&#39;)$$ term in the update drops out. (that particular term represents the value of the next state. for goal states in this setup, there is no next state. more specifically, $$v(s)$$ is the expected return starting from state $$s$$. if $$s$$ is a goal state, the return is identical to the immediate reward in this gridworld, since after this step the episode ends and the agent can&#39;t accumulate any more costs / rewards.)</p>",2019-08-25T12:51:15Z,51,Week 8/25 - 8/31,feedback,,jzifg1e23c29s,jzqz1hav6pc458,2019-08-25T12:51:15Z,{},other
399,no,"<p>q1 - excellent; this is a very interesting result.</p>
<p></p>
<p>q2 - yes ok; I understand -- they are &#39;special&#39;; but need not be treated as such if one modifies the underlying environments simulation response to create artificial state.  this is very clean btw. I think this will be the way I go. ;)</p>
<p></p>
<p>...and so then the correct answer to &#34;discounting (goal/punishment) rewards&#34; with respect to utility, it only happens at adjacent nodess to goal/punishment states. </p>
<p>Goal/punishment utilties remain fixed to the initial reward &#43; 0; either through extension to an absorbing state; or termination due to lack of actions available.</p>
<p></p>
<p>&gt; this approach also allows you to treat an episodic task as a continuing task</p>
<p></p>
<p>I have not seen this yet. :)  a short definition, if possible?  what is a task in this context?  Does this refer to discontinuing updates when switching from exploitation to exploring (i.e. backpropogating from final path utility back up to the initiating &#34;exploring node&#34; in order to enhance the signal received from exploring tasks rather than exploiting tasks)?  or do you mean the more general case of finite vs infinite mdp&#39;s?</p>
<p></p>
<p>&gt; if you don&#39;t want to use the absorbing state, you can always just say that there are no actions from those two goal states</p>
<p></p>
<p>yes I forget this in the grid world where the actions are supposed to be available at any given node (but end up having transition probabilities to different states as evaluated by the sim).  this is an excellent point.  </p>
<p></p>
<p>I am still thinking on how this plays out in practice with respect to implicit policies e.g. pi(s) vs pi(s,a) vs explicit policies pi e.g. if pi requires a union of all possible actions in order to specify a rank/preference or if I can just leave it attached to s or s,a (i.e. supplied by model or sim/environment) and do it as a condition of update. <br /></p>
<p></p>
<p>Thank you much again for clarifying!   My apologies on the lateness of reply, I am still adjusting to how to located &#34;new replies&#34; in Piazza!</p>
<p></p>",2019-08-26T05:26:59Z,51,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzrylzprrpr384,2019-08-26T05:26:59Z,{},other
400,no,<p>stephen lin:  :)</p>,2019-08-28T06:06:01Z,51,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzuuvw0dva21lh,2019-08-28T06:06:01Z,{},other
401,no,"<p>oops, missed your comment. yes, by episodic vs continuing tasks, i just mean finite (where none of the agent&#39;s actions in the previous episode affect the current episode) vs infinite. </p>
<p></p>
<p></p>",2019-08-28T10:52:43Z,51,Week 8/25 - 8/31,feedback,,jzifg1e23c29s,jzv54ln0sl13uw,2019-08-28T10:52:43Z,{},other
402,no,<p>np :) I ran across it about a day later ;)  I keep forgetting &#34;unread&#34; does not mean &#34;update&#34; on Piazza . :)</p>,2019-09-02T21:10:08Z,50,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k02wdv3nt634zk,2019-09-02T21:10:08Z,{},other
403,no,<p>He was our student a couple of semesters back. Nice!</p>,2019-08-24T22:24:06Z,65,Week 8/18 - 8/24,followup,,hyx9thiqa6j4nn,jzq42bda4bx1ql,2019-08-24T22:24:06Z,{},other
404,no,<p>I hope he got a good grade...</p>,2019-08-24T22:42:29Z,65,Week 8/18 - 8/24,feedback,,hyxsfbkeit22m2,jzq4py1nbbu48t,2019-08-24T22:42:29Z,{},other
405,no,I have strong feelings about this. Multiple discount factors is a better approach. ,2019-08-25T12:12:19Z,64,Week 8/25 - 8/31,followup,,gph3si6rKEb,jzqxnewfdam5tw,2019-08-25T12:12:19Z,{},other
406,no,"<p>How do we determine which discount factor to choose? At some point, as n approaches infinity, we are approaching a continuous discount factor. Now we consider gamma = f(S,A,unknowns) ..... doesn&#39;t this mean that we can no longer do the multiplication and addition tricks we learned about?</p>",2019-08-25T16:12:30Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzr68ai5jhj2ef,2019-08-25T16:12:30Z,{},other
407,no,"<p>The discounted future rewards also changes. We can&#39;t factor out the gamma now, so the expansion of the future rewards can&#39;t be reduced when gamma is a function of some non-static state.</p>
<p></p>
<p>You didn&#39;t say non-static state, so I am just assuming such is the case.</p>",2019-08-25T16:14:15Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzr6aji3w7q38o,2019-08-25T16:14:15Z,{},other
408,no,"<p>thx for that</p>
<p>me too I need to see the whole plan laid out in front of me</p>",2019-08-25T18:35:33Z,40,Week 8/25 - 8/31,followup,,jzh6k6o994a6dh,jzrbc8m7yk26d0,2019-08-25T18:35:33Z,{},other
409,no,<p>Thank you very much. It&#39;s very kind of you to share.</p>,2019-08-26T12:18:50Z,40,Week 8/25 - 8/31,followup,,jl88dnp8AZzD,jzsdbmnv9ld7by,2019-08-26T12:18:50Z,{},other
410,no,<p>midterm? i don&#39;t see any midterm on the calendar .....</p>,2019-08-25T14:50:57Z,38,Week 8/25 - 8/31,followup,,jc554vxmyuy3pt,jzr3becqdqj55d,2019-08-25T14:50:57Z,{},final_exam
411,no,"<p>I guess this is hidden reminder that most of the requested info is already in syllabus on Canvas. :) Unless the initial course schedule is going to change soon.</p>
<div>
<div>
<div></div>
<div style=""font-size:13px;background-color:#ffffff"">
<div>
<p style=""color:#000000""></p>
<p style=""color:#737373""></p>
</div>
</div>
</div>
</div>",2019-08-25T14:59:18Z,38,Week 8/25 - 8/31,feedback,,jzhwdil7ssn443,jzr3m4z42xv4la,2019-08-25T14:59:18Z,{},final_exam
412,no,<p>The lack of a midterm was probably a typo.</p>,2019-08-25T15:03:23Z,38,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzr3recx6do2uf,2019-08-25T15:03:23Z,{},final_exam
413,no,"<p>There´s no midterm, he is joking.</p>",2019-08-25T17:00:40Z,38,Week 8/25 - 8/31,feedback,,jzjwcq2u8o7110,jzr7y84zrwtoy,2019-08-25T17:00:40Z,{},final_exam
414,no,<p>I&#39;m pretty sure there is no midterm unless he just added one... which he&#39;s entitled to do. We&#39;ll verify.</p>,2019-08-25T20:19:19Z,38,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrf1p4k38lc9,2019-08-25T20:19:19Z,{},final_exam
415,no,There’s no midterm!,2019-08-25T20:21:34Z,38,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jzrf4l2zvxs7gi,2019-08-25T20:21:34Z,{},final_exam
416,no,<p>We just saved 500 lives... there!</p>,2019-08-25T20:33:10Z,38,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrfjicobgh6zv,2019-08-25T20:33:10Z,{},final_exam
417,no,"<p><strong attention=""gph3si6rKEb"">&#64;Charles Isbell</strong>  Very well played.  :-)</p>",2019-08-26T02:26:53Z,38,Week 8/25 - 8/31,feedback,,jqmfuaidej9155,jzrs6e3z51vv7,2019-08-26T02:26:53Z,{},final_exam
418,no,<p>LMAO at Isbell&#39;s answer. </p>,2019-08-25T16:46:19Z,38,Week 8/25 - 8/31,followup,,jcb3j96cOjkz,jzr7frhtuxz6ae,2019-08-25T16:46:19Z,{},final_exam
419,no,Isbell&#39;s lighting them up early this semester.,2019-08-27T02:15:10Z,38,Week 8/25 - 8/31,feedback,,ixpwxv7xdgi1u6,jzt7764zdcs7hw,2019-08-27T02:15:10Z,{},final_exam
420,no,"<p>Hi Professor &amp; TAs,</p>
<p></p>
<p>In regards to final exam, just would like to confirm -</p>
<p></p>
<p>1. There&#39;ll be a ProctorTrack link to access. Is it kinda 3-4 hour online, closed book exam?</p>
<p>2. Exam scope = All videos &#43; RL book &#43; reference papers?</p>
<p>3. Question type = Multiple choice / short questions / long questions?</p>
<p></p>
<p>Many thanks for your help!</p>
<p>With best regards,</p>
<p>Jesmer Wong.</p>",2019-11-07T09:23:13Z,28,Week 11/3 - 11/9,followup,,jznmonjuzvi67j,k2oi6zbso4j6z8,2019-11-07T09:23:13Z,{},final_exam
421,no,This is a great initiative. Thank you!,2019-08-25T17:20:04Z,64,Week 8/25 - 8/31,followup,,jfzaqnqvtQ1m,jzr8n68cmv922q,2019-08-25T17:20:04Z,{},office_hours
422,no,<p>Yw.</p>,2019-08-27T14:23:43Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jztx830odx110r,2019-08-27T14:23:43Z,{},office_hours
423,stud,"<p>This was very helpful. Thanks, Miguel!</p>",2019-08-25T18:09:28Z,64,Week 8/25 - 8/31,followup,a_0,,jzraep0hax61nt,2019-08-25T18:09:28Z,{},office_hours
424,no,<p>Yw.</p>,2019-08-27T14:23:55Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jztx8cujuet165,2019-08-27T14:23:55Z,{},office_hours
425,no,"<p>In section 1.2 of littman-1996.pdf, can someone explain the difference between “synchronous vs. asynchronous” and “fixed vs. dynamic” environments?</p>
<p> </p>
<p>They seem to be the same thing, but maybe I am missing something very subtle here?</p>
<p></p>
<p>Thank you for your help.</p>
<p></p>",2019-08-25T18:28:18Z,64,Week 8/25 - 8/31,followup,,gx3c8l7z7r72zl,jzrb2xldwor1r9,2019-08-25T18:28:18Z,{},office_hours
426,stud,"<p>I agree. It&#39;s confusing. </p>
<p></p>
<p>Here&#39;s what I <em>think</em> is happening. </p>
<p></p>
<p>The static vs. dynamic distinction is meant to capture scenarios where the agent&#39;s best most recent guess about its present state is already <em>stale</em>. If, say, the agent senses its state at time $$t_{0}$$, and then deliberates for a length of time $$\Delta t$$, and <em>then</em> finally acts (at time $$t_{1} = t_{0} &#43; \Delta t$$) based on the information accquired at time $$t_{0}$$ (and its deliberation) in a dynamic environment, then (presumably) the <em>acutal state</em> of the environment when it finally gets around to taking an action is <em>not</em> the one described by the information it had at time $$t_{0}$$. So the agent in such scenarios is acting on dated information which no longer necessarily completely (or veridically) describes the enviroment as it is <em>at the very moment it takes action</em>. </p>
<p></p>
<p>Now, one might think that an agent that finds itself in <em>that</em> sort of position is essentially in the very same position as one which lives in an asynchronous environment. But I don&#39;t think that&#39;s right. Littman tells us that, in a <em>synchronous</em> environment, &#34;time advances only when the agent takes an action.&#34; That suggests that in an <em>asynchronus</em> environment, time <em>may</em> advance even when the agent is not taking an action. So say we consider a scenario where the clock &#34;freezes&#34; every time the agent begins to deliberate on its next action, and where it only starts up again when the agent executes the action it decides upon. Thereafter (we&#39;ll suppose), the environment is allowed to change continuously <em>until</em> the agent again stops the clock to deliberate over its <em>next</em> choice of action. Well, if that&#39;s a possible scenario, then this scenario describes an environment which is <em>asynchronous</em> but <em>static</em>. The agent is never forced to act on stale information (the clock stops during periods of deliberation), which makes the environment static. But the environment changes continuously between episodes of deliberation/action taking, which makes it asynchronous. </p>
<p></p>
<p>There&#39;s clearly some overlap here, but consideration of these sorts of cases seems to show that these are <em>in principle</em> different sets of distinctions. </p>
<p></p>
<p>Hope that helps!</p>",2019-08-25T19:06:41Z,64,Week 8/25 - 8/31,feedback,a_0,,jzrcg9vowen21o,2019-08-25T19:06:41Z,{},office_hours
427,no,<p>Thank you.</p>,2019-08-25T21:16:22Z,64,Week 8/25 - 8/31,feedback,,gx3c8l7z7r72zl,jzrh3297w6l13n,2019-08-25T21:16:22Z,{},office_hours
428,stud,<p>Welcome!</p>,2019-08-25T23:21:01Z,64,Week 8/25 - 8/31,feedback,a_0,,jzrljd8zlz6250,2019-08-25T23:21:01Z,{},office_hours
429,no,"<p>Here&#39;s a fun topic that relates to the synchronous versus asynchronous model environments.</p>
<p></p>
<p><a href=""http://lis.csail.mit.edu/pubs/kim-aaai19.pdf"">http://lis.csail.mit.edu/pubs/kim-aaai19.pdf</a></p>
<p></p>
<p><a href=""https://openreview.net/pdf?id=Hk3mPK5gg"">https://openreview.net/pdf?id=Hk3mPK5gg</a></p>
<p></p>
<p><a href=""http://proceedings.mlr.press/v84/perolat18a/perolat18a.pdf"">http://proceedings.mlr.press/v84/perolat18a/perolat18a.pdf</a></p>
<p></p>
<p><a href=""https://arxiv.org/pdf/1602.01783.pdf"">https://arxiv.org/pdf/1602.01783.pdf</a></p>
<p></p>
<p>This was applied to adversarial game AIs some time ago (recently, I guess). I don&#39;t remember the citation there. Essentially, asynchronous environments like live action games where the game AI has to react to the player&#39;s actions, are very difficult but not impossible to learn. There has to be some kind of plan abandonment during the online learning of the AI so that it doesn&#39;t obsess about stale policies.</p>
<p></p>
<p>The static environment is like playing a board game, backgammon is a great example. Every time step we can model nearly all of the possible &lt;s,a,s&#39;,t&gt; model conditions and prepare predictions (t) that best reflect our deliberation.</p>
<p></p>
<p>I think of asynchronous learning as online reactive planning, and synchronous learning as deliberation.</p>",2019-08-26T01:21:54Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzrputqtwg45t3,2019-08-26T01:21:54Z,{},office_hours
430,no,"<p>Miguel;</p>
<p>I enjoyed your ad-hoc office hours this past weekend. Is it possible to publish the times in advance so that we can plan to attend. I understand it is &#34;ad-hoc&#34; but a little bit of advance notice will improve the attendance!</p>",2019-08-27T00:23:39Z,64,Week 8/25 - 8/31,followup,,j6ll2xkiDJf,jzt37rd4uj0mv,2019-08-27T00:23:39Z,{},office_hours
431,no,"<p>Absolutely. Will do, Kamran.</p>",2019-08-27T00:49:37Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzt4553bqn02kw,2019-08-27T00:49:37Z,{},office_hours
432,no,"<p>Hi Miguel,</p>
<p></p>
<p>Thanks much for the bonus office hours, that was hugely helpful. I watched the recording last night.</p>
<p></p>
<p>Thanks again.</p>
<p></p>
<p>Binod</p>",2019-08-27T12:41:50Z,64,Week 8/25 - 8/31,followup,,jzk2u4t3n5o5el,jzttl24zb0p2ux,2019-08-27T12:41:50Z,{},office_hours
433,no,"<p>Glad it was useful, we&#39;ll have more coming soon.</p>",2019-08-27T14:23:30Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jztx7t9y41juv,2019-08-27T14:23:30Z,{},office_hours
434,no,<p>Where did you find the recording link?</p>,2019-08-27T22:57:45Z,64,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzufl534uwa1br,2019-08-27T22:57:45Z,{},office_hours
435,no,<p>Where it says &#34;Recording:&#34;?</p>,2019-08-27T23:04:41Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzufu2e3uhh68h,2019-08-27T23:04:41Z,{},office_hours
436,no,<p>When is the next &#34;ad-hoc&#34; office hour? eager to attend! What would be covered?</p>,2019-09-01T00:32:41Z,63,Week 9/1 - 9/7,followup,,j6ll2xkiDJf,k008qmsv63u565,2019-09-01T00:32:41Z,{},office_hours
437,no,"<p>Not scheduled today either, sorry. I&#39;ll try to help with that once my travel schedule slows down. We are starting in a few minutes, Kamran. &#64;89</p>",2019-09-01T14:26:03Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k012ich5by34tp,2019-09-01T14:26:03Z,{},office_hours
438,stud,<p>Super! I agree-the notation in the 2nd edition makes things much clearer. </p>,2019-08-25T21:18:58Z,59,Week 8/25 - 8/31,followup,a_0,,jzrh6e4diaq3nj,2019-08-25T21:18:58Z,{},other
439,no,"<p>I&#39;ve found the Sutton book very readable.  Some of the other texts on this topic (or similar) would put me to sleep (e.g., Mitchell).  Really enjoying the Sutton book though, in case some of you haven&#39;t obtained it I would highly recommend it.</p>
<p></p>
<p> <a href=""https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"">https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf</a></p>
<p></p>
<p>I purchased a hard copy of the book too.</p>",2019-08-26T02:31:32Z,59,Week 8/25 - 8/31,followup,,is9so9huTMp,jzrscd7od3x4o0,2019-08-26T02:31:32Z,{},other
440,no,"<p>Use Sutton&#39;s link: <a href=""http://incompleteideas.net/book/the-book-2nd.html"">http://incompleteideas.net/book/the-book-2nd.html</a></p>
<p></p>",2019-08-26T03:36:08Z,59,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrunfz5gp11vo,2019-08-26T03:36:08Z,{},other
441,no,"<p>Thank you for a quick response!</p>
<p></p>
<p>So, to put it another way, if some oracle gave me the function $$V(s)$$, I couldn&#39;t generate a policy without more information, namely the transitions and rewards, but if the oracle gave me $$Q(s, a)$$, I can by setting my policy to be $$argmax_a Q(s, a)$$. Is this the right way to think about it?</p>",2019-08-25T22:54:34Z,40,Week 8/25 - 8/31,followup,,jziefl597da3my,jzrklcgk8vdk,2019-08-25T22:54:34Z,{},hw1
442,no,Exactly so,2019-08-25T22:55:00Z,40,Week 8/25 - 8/31,feedback,,i4op5p9vfbq5yz,jzrklwomwkx6lv,2019-08-25T22:55:00Z,{},hw1
443,no,"<p>Great, thanks a lot. Makes more sense why the professors were connecting this to reinforcement learning.</p>",2019-08-25T22:55:10Z,40,Week 8/25 - 8/31,feedback,,jziefl597da3my,jzrkm41icv8ox,2019-08-25T22:55:10Z,{},hw1
444,stud,<p>Many thanks to both of you for this exchange. I had been wondering about this myself. </p>,2019-08-25T23:20:30Z,40,Week 8/25 - 8/31,feedback,a_0,,jzrlip3t2217w,2019-08-25T23:20:30Z,{},hw1
445,no,"<p>Quick clarification, Q is better than V <em>for value-based control</em>. That doesn&#39;t mean V is useless, that doesn&#39;t mean V is not adequate (or better than Q) for other things. State-of-the-art methods such as A3C/A2C ( and many others) use V. Nothing we have to worry about in this course, but it is good for you to know.</p>
<p></p>",2019-08-26T01:01:37Z,40,Week 8/25 - 8/31,followup,,hyx9thiqa6j4nn,jzrp4qdyw37qe,2019-08-26T01:01:37Z,{},hw1
446,no,"<p>Hi Miguel, thanks for the clarification. Can you describe what value-based control means in this context?</p>",2019-08-26T17:15:33Z,40,Week 8/25 - 8/31,feedback,,jziefl597da3my,jzsnx7m2phm1sb,2019-08-26T17:15:33Z,{},hw1
447,no,"<p>Sure. It means that you are using a value function (in this case Q) to optimize agents&#39; behavior (policy). You could also have a policy based, which optimizes the policy directly, or model based, which optimizes the policy through learning a model of the environment.</p>
<p></p>",2019-08-26T17:20:52Z,40,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzso421eb5u509,2019-08-26T17:20:52Z,{},hw1
448,no,"<p>You can construct a policy from the V(s) value iteration solution. In the case where s-s&#39; can imply an action (2D grid world), you can construct an A* like policy by following the optimal values in your value iteration matrix.</p>
<p></p>
<pre>[ -1 | -0.5 | -0.2 |  *  ]
[ -1 | 99   | -0.5 |  99 ]
[ -1 | 99   | -0.6 | -0.8]
[ &#43;&#43; | 99   | -0.7 | -0.9]</pre>
<p>From this given matrix, you can surmise the policy:</p>
<p></p>
<pre>[ &gt; | &gt; | &gt; | * ]<br />[ ^ |---| ^ |---]<br />[ ^ |---| ^ | &lt; |<br />[ ^ |---| ^ | &lt; |</pre>
<p>This policy happens solely because each state transition has a deterministic mapping for actions.</p>",2019-08-26T01:42:44Z,40,Week 8/25 - 8/31,followup,,jc554vxmyuy3pt,jzrqllyte7t6ho,2019-08-26T01:42:44Z,{},hw1
449,no,"<p>&#34;by following the optimal values in your value iteration matrix.&#34; -- You are assuming that it is a grid world, so you are &#34;seeing&#34; an MDP. If I give you a similar problem with randomly assigned state ids, you wouldn&#39;t be able to tell how to &#34;follow&#34; the optimal values.</p>",2019-08-26T03:31:47Z,40,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzruhuo3yed5lw,2019-08-26T03:31:47Z,{},hw1
450,no,"<p>Yes, that is correct. This only works with an environment where actions can be inferred from state transitions.</p>",2019-08-26T12:13:31Z,40,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzsd4sg3u5d43r,2019-08-26T12:13:31Z,{},hw1
451,no,"<p>So there was a lecture regarding this reading also. Nice, how do I know that there is a lecture?</p>",2019-08-28T01:24:18Z,40,Week 8/25 - 8/31,feedback,,jzlwxqhkqj2e1,jzuktlm3zj07cf,2019-08-28T01:24:18Z,{},hw1
452,no,<p>Lectures are assigned in the schedule.</p>,2019-08-28T03:01:52Z,40,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzuob2hle1b4h4,2019-08-28T03:01:52Z,{},hw1
453,no,<p></p>,2019-08-28T01:23:59Z,40,Week 8/25 - 8/31,followup,,jzlwxqhkqj2e1,jzukt72ufm773i,2019-08-28T01:23:59Z,{},hw1
454,no,"<p>I found the below guest lecture to be helpful. &#34;The difference between the value function and the Q function is that the value function specifies the goodness of a state, while a Q function specifies the goodness of an action in a state.&#34;</p>
<p><a href=""https://www.aimlmarketplace.com/read-ai-ml-blogs/the-markov-decision-process-mdp"">https://www.aimlmarketplace.com/read-ai-ml-blogs/the-markov-decision-process-mdp</a></p>
<p></p>
<p></p>",2019-08-31T02:56:29Z,40,Week 8/25 - 8/31,feedback,,i4ksxcsic7S,jzyyfpn13rb7dt,2019-08-31T02:56:29Z,{},hw1
455,no,"<p>as I recall its a weighted combination; with 0 implying single-step lookahead and 1 implying looking forward throughout the entire path (&#39;infinite&#39;).   [0..1] being a continuum, you can specify a degree of participation from the simplest case TD(0), all the way to the maximal case TD(1).   This being the factor TD(lambda). </p>
<p></p>
<p>its in essence, a mixing model.  between myopic, to far-sighted.</p>
<p></p>
<p>thats off the top of my head.</p>
<p></p>
<p>...</p>
<p>addendum: go to the next lesson ;)</p>",2019-08-26T03:24:35Z,64,Week 8/25 - 8/31,followup,,jzivtxcbl6964n,jzru8lar1sz312,2019-08-26T03:24:35Z,{},other
456,no,"<p>I think I get it now, when there is an infinite amount of transitions $$S_{t-1} \to S_t$$, the V(s) updates gets added inifinitely for each $$S_{t-1} \to S_t$$ and it becomes $$E_\infty $$. Just like this example for 3 transitions.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj4205g7gd2fw%2Fjzt4w4v8rzhl%2FCapture.PNG"" alt="""" width=""496"" height=""246"" /></p>
<p></p>
<p></p>",2019-08-27T01:11:37Z,64,Week 8/25 - 8/31,followup,,jzj4205g7gd2fw,jzt4xg6udzn3l2,2019-08-27T01:11:37Z,{},other
457,no,"<p>Also, notice the weight that is applied to all of those infinite transitions.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fjzt63ns7obga%2Fweight.png"" alt="""" width=""726"" height=""339"" /></p>",2019-08-27T01:45:21Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzt64tne3om3qm,2019-08-27T01:45:21Z,{},other
458,no,<p>This visualization is very helpful! Thanks</p>,2019-08-27T02:06:37Z,64,Week 8/25 - 8/31,feedback,,jzj4205g7gd2fw,jzt6w66e9it7bk,2019-08-27T02:06:37Z,{},other
459,no,<p>Yw.</p>,2019-08-27T04:15:21Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jztbhptvqlz2z,2019-08-27T04:15:21Z,{},other
460,stud,"<p>Haha by notes I mean lecture notes, not the notes taken by ourselves... </p>
<p></p>
<p>I wonder the papers you mentioned are past exam papers or those papers (published conference/journal papers) in the reading list?</p>",2019-08-26T04:57:35Z,64,Week 8/25 - 8/31,followup,a_0,,jzrxk6gh5r73ox,2019-08-26T04:57:35Z,{},final_exam
461,no,<p>Reading list.</p>,2019-08-26T04:58:50Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzrxlslag0g4ch,2019-08-26T04:58:50Z,{},final_exam
462,no,In other words you aren’t being given past exams. You should also not seek them out. ,2019-08-27T15:35:40Z,64,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jztzsmmjx6fxo,2019-08-27T15:35:40Z,{},final_exam
463,no,"<p>&gt;What&#39;s the reason behind a closed books exam?</p><p><br /></p><p>Because it is one of the ways I choose to assess you. </p><br /><p>&gt;Proctortrack is great to check that no one is in the room to help us, but studying by heart takes a lot of time that could be used to better understand, do more exercises, no? </p><div><br /></div>No. <br /><p></p><br /><p>&gt;Books open or closed, if someone doesn&#39;t understand the theory, he won&#39;t make it anyway.  </p><div><br /></div>What about she?<br /><p></p><br /><p>&gt;What happens if we open a new tab by mistake? Will we be considered cheating?</p><div><br /></div>It will be checked for cheatIng, by annoyed human beings. I recommend not opening tabs. <br /><p></p><br /><p>&gt;Is it true we can&#39;t have a pad to write and calculate stuff?  </p><p><br /></p><p>You will be allowed what you need. Good luck on this thing happening several months from now. </p>",2019-08-27T22:53:48Z,64,Week 8/25 - 8/31,followup,,gph3si6rKEb,jzufg2ivcg934i,2019-08-27T22:53:48Z,{},final_exam
464,no,<p>Thanks for taking the time to clarify things.  </p>,2019-08-27T22:56:33Z,64,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzufjlk5jlr29,2019-08-27T22:56:33Z,{},final_exam
465,no,Happy to help!,2019-08-27T22:57:22Z,64,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jzufknb95tnym,2019-08-27T22:57:22Z,{},final_exam
466,no,"<p>i think the algorithm is trying to find a path to get the agent to a cycle with high value, so that it can then walk around the cycle indefinitely and collect the rewards. considering all cycles in the graph, the value for a state is the max [path cost to reach the cycle &#43; the infinite sum of rewards or costs from repeating the cycle again and again].</p>
<p></p>
<p>the thing i&#39;m not sure about is why the denominator there is $$1 &#43; \beta^l$$ instead of $$1 - \beta^{l}$$. once the cycle is reached at $$k&#43;1$$, it should repeat every $$l$$ steps since $$l$$ is the cycle length, forming an infinite geometric series with $$\beta^l$$ as the ratio and therefore $$1 / (1 - \beta^{l})$$ as the coefficient of the sum :( i&#39;m probably missing something.</p>
<p></p>
<p></p>",2019-08-26T18:11:08Z,37,Week 8/25 - 8/31,followup,,jzifg1e23c29s,jzspwoy1v767lp,2019-08-26T18:11:08Z,{},other
467,stud,"<p>Thanks, Steph! That&#39;s very helpful. </p>
<p></p>
<p>I don&#39;t know why it&#39;s $$1 &#43; \beta^{l}$$, either. </p>",2019-08-26T20:01:08Z,37,Week 8/25 - 8/31,feedback,a_0,,jzstu60lm371cb,2019-08-26T20:01:08Z,{},other
468,no,"<p>How much experience do you have to be able to understand a algo in pseudocode with 7 nested foreach loops with 2 paragraphs of explanations? </p>
<p>Have you done linear programming before? Even so, there&#39;s just not enough words to begin to explain how it works, except getting a very very rough idea.  </p>
<p>As you can see, I&#39;m still struggling with deciding what we are supposed to understand fully.  </p>
<p>I almost want to drop this chapter2 and invest time in learning Sutton which is laid down in orderly and step-by-step fashion.  </p>",2019-08-29T20:27:39Z,37,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzx53t17tjn6r0,2019-08-29T20:27:39Z,{},other
469,stud,"<p><em>&#34;How much experience do you have to be able to understand a algo in pseudocode with 7 nested foreach loops with 2 paragraphs of explanations?Have you done linear programming before? &#34;</em></p>
<p><em></em></p>
<p>I really don&#39;t know. I think it&#39;s different for everyone. I will say that this is my penultimate semester in the program, and I have seen linear programming before (only here at Tech in the GA class, though). I tried taking this course my first semseter, but decided not to stick with it. It&#39;s a lot if you don&#39;t have any prior background in ML or CS generally, I think. </p>
<p></p>
<p><em>&#34;Even so, there&#39;s just not enough words to begin to explain how it works, except getting a very very rough idea.&#34;</em></p>
<p><em></em></p>
<p>I mean, I think if you really slow down and work through the pseudocode and accompyaning commentary, you can understand most of what&#39;s going on in Littman&#39;s thesis. Most! I won&#39;t say all, since some of the more handwavy &#34;proofs&#34; just seem like filler (i.e., you&#39;d have to go looking for the papers he references and read those to really know what&#39;s going on. And ain&#39;t nobody got time for that!) </p>
<p></p>
<p>If something seems <em>really </em>technical (or obscure), you just move on (after maybe noting that some result was claimed to have been proven by so and so at such and such a date). And you can always ask about something here on Piazza if it&#39;s really bugging you. Every time I&#39;ve done so, I&#39;ve had my question answered fully by a TA or another student (or both). </p>
<p></p>
<p>I think skim the paper for ideas, and don&#39;t obsess over the minor details. Sutton&#39;s book is good! If you feel like you&#39;re getting more out of reading that than Littman&#39;s thesis, focus your efforts there. I can&#39;t imagine that we&#39;re going to be tested on any of the minor details, anyway. </p>
<p></p>
<p>I think a good metric for assessing whether you&#39;re taking in the material you need to be in this course is the lectures. If you can follow everything that&#39;s happening in the lectures, you&#39;re in pretty good shape. If you can&#39;t, keep reading (and thinking)!</p>
<p></p>",2019-08-29T20:46:19Z,37,Week 8/25 - 8/31,feedback,a_0,,jzx5rtvbfp26n,2019-08-29T20:46:19Z,{},other
470,no,(That’s an odd definition of filler),2019-08-29T20:59:58Z,37,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jzx69dr5575af,2019-08-29T20:59:58Z,{},other
471,no,"<p>Thanks &#39;Anonymous&#39; whoever you are for your thorough reply.</p>
<p>I kinda had the feeling that I had to tiptoe in this chapter, avoid digging when there&#39;s not enough explanations, and spend time on what seems to come with enough explanations, get an overall feeling and know it&#39;s there if I need sthg.</p>
<p>No criticism of Littman here, I understand it&#39;s a PhD thesis, not an (under)grad book.  It&#39;s not my style to study like this, but I&#39;ll adapt.  </p>
<p>I guess I&#39;ll have a much better idea after a few homeworks. and the midterm.  </p>
<p>I&#39;d hate to withdraw because I really like this type of learning and it can help my research.  </p>
<p>It&#39;s my first course so I don&#39;t know anything really, but I can quite easily follow the lectures on Udacity, so thanks for giving me that guideline.  </p>",2019-08-29T21:15:37Z,37,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzx6thyhbyj6ra,2019-08-29T21:15:37Z,{},other
472,stud,"<p><em>(That’s an odd definition of filler)</em></p>
<p><em></em></p>
<p>Maybe. But Maybe not?</p>
<p></p>
<p>The Cambridge English Dictionary defines &#34;filler&#34; as: something of lower quality included in a film, broadcast, speech, record, etc. in order to fill all the time or space.  </p>
<p></p>
<p>So I do think there&#39;s an argument to be made that proof &#34;sketches&#34; (generally, not just in the case of Littman&#39;s thesis) are often fillers, in the sense that they tend to be rather less helpful than the other portions of whatever work they are included in (and far less helpful than a proper proof would be, assuming one is in a position to follow it).</p>
<p></p>
<p>Nevertheless, as &#34;filler&#34; plausibly suggests a value judgement is here being made (and one which I had no desire or intention of making in the first place), I retract my statement. </p>
<p></p>
<p>Instead, I&#39;ll simply say that the proof sketches seem not terribly important on a first reading and not as worth hanging onto as the results the sketches are supposed to motivate. </p>",2019-08-29T21:31:16Z,37,Week 8/25 - 8/31,feedback,a_0,,jzx7dmcy6tx4d5,2019-08-29T21:31:16Z,{},other
473,stud,"<p><em>&#34;Thanks &#39;Anonymous&#39; whoever you are for your thorough reply.&#34;</em></p>
<p><em></em></p>
<p>Welcome! Good luck!</p>",2019-08-29T21:33:25Z,37,Week 8/25 - 8/31,feedback,a_0,,jzx7gdwg52q5ms,2019-08-29T21:33:25Z,{},other
474,no,I think the last paragraph is probably right. ,2019-08-29T21:34:16Z,37,Week 8/25 - 8/31,feedback,,gph3si6rKEb,jzx7hhajav6x5,2019-08-29T21:34:16Z,{},other
475,no,"<p>The $$1&#43;\beta^l$$ may be a typo.  Also, shouldn&#39;t a $$k$$-step path involve a max discount factor (on the final, $$k_\text{th}$$, transition) of $$\beta^{k-1}$$, in which case the first discount factor on the cycle would be $$\beta^k$$, not $$\beta^{k&#43;1}$$, and the entire term would be $$\beta^k\frac{A^l[s&#39;&#39;,s&#39;&#39;]}{1-\beta^l}.$$?</p>",2019-09-11T22:13:09Z,35,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ftlkluwko24e,2019-09-11T22:13:09Z,{},other
476,no,<p>Nice to meet you all! Thanks in advance for all the help we will need during the semester ;)</p>,2019-08-27T08:47:05Z,30,Week 8/25 - 8/31,followup,,jzjwcq2u8o7110,jztl76qf38pt,2019-08-27T08:47:05Z,{},logistics
477,no,<p>Absolutely!</p>,2019-08-27T12:04:58Z,30,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzts9nry3he6ha,2019-08-27T12:04:58Z,{},logistics
478,no,"<p>Great to hear everyone&#39;s background -- sounds like we&#39;re well equipped for great learning &amp; instruction this semester.  :-)</p>
<p></p>
<p></p>
<p>&#34;agents that can learn to solve the control problem&#34; (from Farrukh) -- interested in what problem this refers to. Is this a specific problem from control theory?  I&#39;ve of the <a href=""https://deepmind.com/research/publications/deepmind-control-suite"" target=""_blank"">DeepMind Control Suite</a> with benchmarks evaluating different control tasks, and heard AGI safety/corrigibility/control referred to by some as the &#34;AI control problem&#34; [<a href=""https://intelligence.org/2018/02/28/sam-harris-and-eliezer-yudkowsky/"" target=""_blank"">ref</a>], though it sounds like that&#39;s not the same problem referred to here.   Sounds like it might refer to the general class of control problems?</p>",2019-08-27T15:58:00Z,30,Week 8/25 - 8/31,followup,,isde34zracb1mz,jzu0lc0c5ft4js,2019-08-27T15:58:00Z,{},logistics
479,no,"<p>Hey Michael,</p>
<p></p>
<p>Yeah, I meant it in context of stochastic control problems (eg. finding the optimal way to behave (control) in a stochastic process (MDP)). However the AI control problem is very interesting as well.</p>",2019-08-27T21:47:51Z,30,Week 8/25 - 8/31,feedback,,jl1acpoc4HA9,jzud39c8wqf2sj,2019-08-27T21:47:51Z,{},logistics
480,no,"<p>Hey Farrukh, thanks for the clarification -- agreed stochastic control problems are among the most interesting to me as well. Looking forward to learning how to better approach them this semester.</p>",2019-08-27T23:14:29Z,30,Week 8/25 - 8/31,feedback,,isde34zracb1mz,jzug6nuoinb4cz,2019-08-27T23:14:29Z,{},logistics
481,no,"<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjqrr36mqfm8M%2Fjzuyf0gw5fi7%2F1.JPG"" target=""_blank"">1.JPG</a></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjqrr36mqfm8M%2Fjzuyfclpn6pt%2F2.JPG"" target=""_blank"">2.JPG</a></p>
<p></p>
<p>Lecture video Lesson 5, video 9, &#64; 00.56 says &#34;Unpack the definition of max norm&#34; and we are shown the 2,jpg. What does this max norm mean. It says we are maximizing between state, action pairs. But how come we get BQ1 - BQ2 = max <sub>a, s</sub> (BQ1(s,a),  BQ2(s,a) )</p>
<p></p>
<p>Previous few seconds to this, we were introduced to the concept of BQ1 - BQ2  &lt;= gamma (Q1 - Q2). This makes sense due to the contraction property.</p>",2019-08-28T08:02:46Z,64,Week 8/25 - 8/31,followup,,jqrr36mqfm8M,jzuz21l1h2n2j9,2019-08-28T08:02:46Z,{},other
482,no,"<p>When we start  &#64; 00.56 we use  max <sub>a, s</sub> ( BQ1, BQ2). Then in between we say a, s are not even there in expanded BQ1 and BQ2 and so we can remove it. So  then why introduce it in first place</p>",2019-08-28T08:17:44Z,64,Week 8/25 - 8/31,feedback,,jqrr36mqfm8M,jzuzla3j4sbt3,2019-08-28T08:17:44Z,{},other
483,no,"<p>Another thing is, well, you will read the same things in the book, other papers, lectures, etc. Just read and try to understand. Use office hours to ask questions, get more details, etc.</p>",2019-08-27T22:58:27Z,44,Week 8/25 - 8/31,followup,,hyx9thiqa6j4nn,jzufm1wdbgw1yh,2019-08-27T22:58:27Z,{},hw1
484,no,"<p>Sure, but I&#39;ve bumped quite a few times in sentences such as &#39;this thing can be done like that and this guy has shown it can be done [56]&#39;.  I can only make a mental note if it seems useful and then drop it, and not going to read [56].</p>
<p>It&#39;s not quite like me to accept things at face value but, here. I&#39;ll have to find the right trade-off I guess.</p>
<p>If the topics of ch2 are going to be repeated one way or another in the lectures, papers etc, then I should be fine..</p>
<p>Finding what&#39;s important and not, deciding where to invest time is reinforcement learning if you think about it.  </p>
<p>Thanks for your advices.  </p>",2019-08-27T23:13:50Z,44,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzug5u1wx93mu,2019-08-27T23:13:50Z,{},hw1
485,no,"<p>Whatever way works for you, everybody learns a different way.</p>
<p></p>
<p>I wouldn&#39;t worry about following [56] and just move on to the next task. Then ask questions during office hours and we&#39;ll talk about it (and tell you whether you should dig deeper on those topics.)</p>
<p></p>
<p>But, again... whatever works for you.</p>",2019-08-28T03:00:15Z,44,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzuo908ehtf2wq,2019-08-28T03:00:15Z,{},hw1
486,no,Thank yo so much for the information.<div><br /></div><div>I will rephrase my qtn. </div><div><br /></div><div>HW/ course work </div><div><br /></div><div>In the what form it would be ? </div><div><br /></div><div>You will send us a Question and i will write  an answer kind of typical paper based exam </div><div><br /></div><div>or </div><div><br /></div><div>there willbe an assignment to implement it through some programming language and submit the file somewhere? </div><div><br /></div><div>is that the github? where i submit my HW and Coure works? </div><div><br /></div><div>So I already missed two blue jeans sessions </div><div>because lack of information looks like. </div><div><br /></div><div>Thanks in advance. </div><div><br /></div><div>Regards</div><div>Pavan<br /><div><br /></div></div>,2019-08-28T01:42:06Z,44,Week 8/25 - 8/31,followup,,jzlwxqhkqj2e1,jzulgi3ncvx45r,2019-08-28T01:42:06Z,{},hw1
487,no,"<p><a href=""https://gatech.instructure.com/courses/45210/external_tools/717"" target=""_blank"">https://gatech.instructure.com/courses/45210/external_tools/717</a></p>
<p>If you go to the link above, scroll down, then you can watch the recording of last week office hours.  I think alot of your questions are answered in there.</p>
<p></p>
<p>The homework assignments require some programming.  Then you are given 10 sets of input and you have to enter the output into the website.</p>
<p></p>
<p>We make announcements on Piazza about all meetings so students are aware of them.  There are also weekly announcements as well as the calendar to help guide you through the course.</p>",2019-08-28T01:51:08Z,44,Week 8/25 - 8/31,feedback,,hz7meu55mi8sd,jzuls4dpbo9138,2019-08-28T01:51:08Z,{},hw1
488,no,"<p>Thanks So Much. Well, need to follow piazza forum to get updated time to time. probably it takes some time to get use to this environment looks like. As of now there is a lot of confusion about this course. Chapter 1 and chapter2 is not a typical thing where beginners can understand it. Though they explain in the class, But I have seen recording video of  chapter1 session, that is more like a casual things than a serious lecture. Anyhow course structure is awesome but the material is not understandable and this piazza, canvas, blue jeans and other course is using some other thing and all are mixed up , probably takes some time.</p>
<p></p>
<p></p>
<p></p>
<p>regards</p>
<p>pavan</p>",2019-08-28T02:03:45Z,44,Week 8/25 - 8/31,feedback,,jzlwxqhkqj2e1,jzum8bup3iq3hp,2019-08-28T02:03:45Z,{},hw1
489,no,<p>I&#39;ve wondered about this for quite a while.  Thank you for posting!</p>,2019-08-28T18:18:50Z,64,Week 8/25 - 8/31,followup,,jqmfuaidej9155,jzvl2b36rre146,2019-08-28T18:18:50Z,{},other
490,no,"<p>(responding to my own note, any other suggestions / feedback much appreciated, even casual intuitions are interesting IMO: there&#39;s a saying experts can struggle to find creative new ideas since their knowledge applies many constraints to what appears possible, whereas beginners less aware of apparent constraints may be able think of new creative directions more easily; on the beginner side of the spectrum myself, just trying to make the most of it)</p>
<p></p>
<p>Personally I&#39;m leaning towards exploring idea 1 first since it sounds significantly faster/easier to implement. My intuition is that Adam optimized learning rates can still be improved significantly, but it&#39;s unclear by how much: is a 10x improvement possible? (defined as a DNN converging in &lt;10% as many epochs due to more dynamic, near-optimal learning rates)  Seems like the superior learning rate hypothesis could be refuted (avoiding wasting time on it) if we have lower bound estimates on the # of epochs needed for a DNN to converge with theoretically optimal learning rates that show optimizers like Adam aren&#39;t too far off, but I haven&#39;t seen anything like that.  Learning rates optimization aside, it seems worth exploring action alternatives to standard back propagation an RL agent might take during training. My intuition is an RL agent in this role would naturally discover and apply <a href=""https://en.wikipedia.org/wiki/Batch_normalization"" target=""_blank"">batch normalization</a>, and may be able to carry gains much further.</p>",2019-08-29T16:42:35Z,46,Week 8/25 - 8/31,followup,,isde34zracb1mz,jzwx2djjog330l,2019-08-29T16:42:35Z,{},other
491,no,"<p>#1 - seems there is <em>some </em>research going in this direction - <a href=""https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_2H4E204/_pdf"">https://www.jstage.jst.go.jp/article/pjsai/JSAI2019/0/JSAI2019_2H4E204/_pdf</a> </p>
<p>#2 - after reading carefully, looks like it fits into AutoML initiative from my point of view.</p>
<div>
<div>
<div></div>
<div style=""font-size:13px;background-color:#ffffff"">
<div>
<p style=""color:#000000""></p>
<p style=""color:#737373""></p>
</div>
</div>
</div>
</div>",2019-08-29T18:58:52Z,46,Week 8/25 - 8/31,followup,,jzhwdil7ssn443,jzx1xmv06ea3hd,2019-08-29T18:58:52Z,{},other
492,no,"<p>Thanks, Evgenii. Interesting that paper&#39;s June&#39;19 -- checked for existing research this Spring (during ML) and didn&#39;t find any, though it&#39;s such an obvious idea I expect more probably exists &amp; I&#39;ve just been unable to locate it.</p>
<p></p>
<p>Reviewed the paper in depth and see several areas for improvement (elaborated below), but it was great to see their findings:</p>
<p></p>
<p>1) Learning rates that outperform Adam don&#39;t necessarily always decay. (suspected this, nice to see it confirmed)</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fjzy90d91iem5%2Fdqn_lr.png"" alt="""" /></p>
<p></p>
<p></p>
<p>2) Good news: DQN can achieve better loss reduction than Adam; Bad news: it&#39;s hardly a 2x-10x improvement. Good news: It seems possible to make a DQN that more consistently outperforms Adam with a few adjustments (elaborated below).</p>
<p> <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fjzy93dorp6kq%2Flossplot.png"" alt="""" /></p>
<p></p>
<p>3) It&#39;s interesting this DQN increases learning rate significantly in later iterations (left), and that there are periods of gains that are more efficient than Adam (right figure shows # of steps required for 0.02 loss improvement). Comparing those maximum efficiency periods (red circles) vs Adam, the best case efficiency gain only appears to be 10-12% better (1650 / 1850). There are reasons to believe this has more to do with problems in the methodology (elaborated below) as opposed to limits of theoretically optimal learning rates.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fjzy985qk3bz3%2Fplots.png"" alt="""" /></p>
<p></p>
<p>A few potential areas for improvement (descending by intuitive estimates of impact potential):</p>
<ul><li><strong>1 - Reward)</strong> The paper&#39;s reward function does not appear to encourage aggressive minimization of the loss function. It&#39;s described as having 3 priorities that determine whether reward is positive/negative: Priority 1) keeping learning rate (LR) within predefined min/max bounds (max: 0.5, min: 0.001 in their case). Priority 2) &#34;uses the range of fluctuation in the latest 20 losses to judge whether the LR is too small&#34;. Priority 3) if current loss is bigger than min of first half of loss data, big negative reward, and &#34;If the loss reaches a new minimum or the threshold loss (0.02 in this experiment), a positive reward will be given according to the total steps.&#34;
<ul><li><strong>Big problem here?</strong> If I understand correctly, the <em>only</em> positive reward is proportional to total steps taken, so reward can be <em>maximized </em>by making positive progress <em>as slowly as possible</em>.</li><li><strong>&amp; a smaller problem?</strong> The priority 2 negative reward discouraging LR being too low appears likely to be inappropriate at times, and could artificially explain why DQN LR in the left figure above re-increases at later steps regardless of whether that&#39;s actually beneficial.</li><li><strong>Suggested improvement: </strong>What if reward at each step was simply: (agent&#39;s reduction in loss) - (Adam&#39;s reduction in loss if it were to have taken action from the same state).  If your agent outperforms Adam at each step, it&#39;s rewarded positively proportional to how much it outperforms Adam; if it performs decently (equivalent to Adam) it gets no negative reward, and if it makes decisions much worse than Adam would have done, it gets a negative reward proportional to how much worse it did. This reward seeks to maximize loss reduction at each step, while avoiding unnecessary &amp; at times counter productive limitations imposed initially.</li></ul>
</li><li><strong>2 - State representation) </strong>The paper describes state representation as &#34;(1) Total step (2) Loss based: max(Loss), min(Loss), previous Loss, delta(Loss), (3) Parameter based: increase/decrease parameter number, parameter average change ratio (4) Gradient based: gradient in previous 3 epochs, absolute value of gradient, sum of positive/negative gradient (5) others: monotonically decrease number of loss, hyper-parameters&#34;.    I suspect there&#39;s significant room for improvement via better representation: what do the shapes of the weights and gradients look like (std deviation, potentially kurtosis), how well are desirable properties resembling <a href=""https://en.wikipedia.org/wiki/Batch_normalization"" target=""_blank"">batch normalization</a> being maintained, etc.</li><li><strong>3 - Single vs Multiple problems)</strong> Not necessarily a suggestion that would improve performance in their paper, but it seems like this RL agent should really be trained across multiple diverse DNN problems so it can focus its learning on general techniques that work well in DNN optimization, not problem-specific tricks found in the MNIST dataset.</li></ul>
<p></p>
<p>All things considered, really appreciated the work they published and your help finding it. Their approach and findings were helpful to review for subsequent iterations.</p>",2019-08-30T15:59:01Z,46,Week 8/25 - 8/31,feedback,,isde34zracb1mz,jzyay76hq2h2tg,2019-08-30T15:59:01Z,{},other
493,no,"<p>Disclaimer: This comment is probably 90% inaccurate!!!! Watch out</p>
<p></p>
<p>Thanks for the really interesting post! Some brief thoughts:</p>
<p></p>
<p>Idea 1: There&#39;s been some <a href=""https://arxiv.org/abs/1606.01885"" target=""_blank"">work in this direction</a> where optimization is structured as an RL problem, although it&#39;s a bit narrower in scope than your description. The state is composed of the current location &#43; a truncated history of gradients and changes in the objective. The action is the change in the location (i.e., the step). (In this approach it&#39;s modeled as a Gaussian, making the policy stochastic; the authors use a fixed variance and their model outputs the mean.) The reward is just the flipped value of the objective function -- this encourages the RL agent to try to minimize the objective as quickly as possible, since it&#39;s accruing these costs at every timestep.</p>
<p></p>
<p>It sounds like you&#39;re looking to expand the action space a great deal so that actions don&#39;t just encompass steps along the loss surface, but also include things like layer-by-layer operations on the inputs (= batchnorm). This sounds really useful, but also quite difficult / complex -- to me these two types of actions feel like inherently different aspects of the problem (plus you might want to use them both in sequence). In addition, it&#39;s not clear to me how you would structure your actions to allow the agent to discover batchnorm. If you focus on actions like &#34;normalize the minibatch&#34;, or split it up into &#34;get to zero-mean&#34;, &#34;get to unit-variance&#34; (&#43; optional reshifting and rescaling, making everything trainable, etc), an agent that tries these out in combination with a normal optimization step might be able to learn that batchnorm is useful, but you&#39;re injecting a lot of prior knowledge into the space (i.e., we know that batchnorm is good, so we&#39;re going to give the agent access to the operations it uses). If instead you allow a large set of generic operators, it seems like your action space would explode (especially if you allow for sequences of actions).</p>
<p></p>
<p>That was a lot of rambling, would be very interested to hear how you were thinking of approaching it :) I might be way off in my interpretation of your post (and also things generally)!</p>
<p></p>
<p>Idea 2: (Ok an actual brief thought this time) Sounds like hierarchical RL (subpolicies with a controller at the highest level). In general it does feel like there should be a lot of interesting uses of RL-based control &#43; NNs. Another existing example would be control nodes that direct inputs to different NNs based on difficulty (easier examples are fed to small NNs, difficult examples to larger ones --&gt; computational savings).</p>
<p></p>
<p>Anyway, I&#39;m a beginner in this area so I&#39;m not sure how useful this is, but happy to discuss further :)</p>",2019-08-29T19:14:45Z,46,Week 8/25 - 8/31,followup,,jzifg1e23c29s,jzx2i20ch3s3bp,2019-08-29T19:14:45Z,{},other
494,no,"<p>Steph, thanks for the great paper, summary, and feedback, and apologies it took me a while to process everything.</p>
<p></p>
<p>Your feedback on striking the right balance while not injecting too much prior knowledge seems key and reminds me of Sutton&#39;s <a href=""http://www.incompleteideas.net/IncIdeas/BitterLesson.html"" target=""_blank"">Bitter Lesson</a> from earlier this year. As humans training DNNs we know things like vanishing gradients are undesirable &amp; properties that resemble effects of batch normalization are desirable, and I have to admit a temptation to include metrics exposing those in the RL state representation. At the same time, I agree a superior longer term experiment may get even better results providing simpler access to the raw data. At this stage, I&#39;d be more confident in a positive result exposing some prior knowledge and would be okay with the trade off. If results are positive, toning down prior knowledge representations seems like a great next step.</p>
<p></p>
<p>Re - action space complexity &amp; how things like batch normalization may be naturally discovered: The design I had in mind doesn&#39;t include quite as many actions as you suggest (e.g. &#34;normalize the minibatch&#34;, or even the sub-steps), although I&#39;d definitely be interested in an an experiment that attempts including all of that. In my mind (which may be incorrect!), only a few simple actions are necessary for an RL agent to achieve results with beneficial properties resembling batch normalization, without actually having explicit actions for known techniques that are part of that process.</p>
<p></p>
<p>Example of a relatively simple action space where the agent makes two decisions at each step:</p>
<ul><li>1) Learning Rate: [same, increase, decrease]   (e.g. *= 1, *=1.1, or *=0.9)</li><li>2) Transform Strategy: [Transform1: standard back-propagation, Transform2: reduce large weight increases, Transform3: reduce small weight reductions, Transform4: both T2 and T3]
<ul><li>T2 rationale: all weights still shift in the correct direction, but maybe it&#39;s not necessary to apply the largest weight shifts quite as far as gradients suggest.</li><li>T3 rationale: all weights still shift in the correct direction, but maybe it&#39;s not necessary to reduce the size of small weights quite as far as gradients suggest.</li></ul>
</li></ul>
<p></p>
<p>So far the action space is only 12 (3 * 4). T2,T3,T4 could involve an additional parameter choice that determines how severely to apply each transform (e.g. lightly, moderately, strongly), increasing the action space to 30 (3 * (1 &#43; (3 * 3)). It&#39;d also be interesting to allow the agent to choose the transform strategy per layer, and wouldn&#39;t explode the action space too much for 2-3 layer networks.</p>
<p></p>
<p>My understanding is that T2 &amp; T3 make a trade-off between fully learning from each data point as much as gradients suggest vs giving the RL agent the option to regularize some adjustments on a case-by-case basis if it expects doing so retains desirable traits of the DNN&#39;s state. My expectation: given that we know vanishing gradients can be bad, and weights avoiding extremes are good, an RL agent may be able to positively influence these traits in addition to the learning rate. These transforms don&#39;t exactly allow it to replicate batch normalization, but give it some influence to show us (human reviewers) patterns of DNN training that work well, and my hunch is that some of those patterns may resemble things like batch normalization, in addition to whatever breakthroughs are next with active intelligence applied deeper in ML processes.</p>
<p></p>
<p>I&#39;m not set on T2 or T3 as the transforms to use, and expect if we could run hundreds of experiments with myriad transforms then travel back in time to tell ourselves which transforms were best, the top ones would not be T2 &amp; T3. These transforms were chosen mostly because they&#39;d be fast and easy to implement, they resemble existing forms of regularization known to be effective, and they&#39;d be easy to extend into new ideas we might come up with.</p>
<p></p>
<p>Hope that helps outline a simpler action space. Related food for thought: <a href=""https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii"" target=""_blank"">AlphaStar</a>&#39;s action space is 10**26, if we wanted to equip an RL agent with a few hundred potential actions (including sub-steps of batch normalization among many others) that does seem feasible and opens the space for potential breakthroughs considerably.  Thanks again for taking the time to provide feedback!</p>",2019-08-30T22:18:46Z,46,Week 8/25 - 8/31,feedback,,isde34zracb1mz,jzyoik2xdw50l,2019-08-30T22:18:46Z,{},other
495,no,"<p>Thanks for the detailed response!! I read your earlier comment in this post to mean that you wanted to see if your agent could independently discover the exact steps of batchnorm, which seemed like a difficult task even to structure -- it does seem more feasible to instead find a different process that leads to similar generalization or has some properties of batchnorm*, as you describe. :)</p>
<p></p>
<p>*whatever those might be, i.e. handling internal covariate shift / smoothing the loss landscape / ~*~alchemy~*~</p>
<p></p>
<p>Anyway, I think your setup makes a lot of intuitive sense! I guess my (really naive!) question would be: What are the benefits to splitting up the action into learning rate &#43; transform &#43; severity instead of having the action directly equal the step along the loss surface (as in the linked paper)? It seems like this could have the properties you&#39;re looking for, where by returning the step directly, the model has full control over the effective learning rate / reduction of large weight changes / magnitude of the transforms.</p>
<p></p>
<p>&gt;&gt;AlphaStar</p>
<p></p>
<p>we don&#39;t all have enough TPUs to fill a small nation :( but i was thinking more along the lines of: even with a reasonable set of simple operations, you&#39;d have to allow for sequences of these operations to get to something like batchnorm, so the action space is exponential in the sequence length, which blows up very easily (although there might be some symmetry breaking possible for certain sequences). BUT it doesn&#39;t seem like this was the approach you were going for anyway :)</p>",2019-08-31T00:42:11Z,46,Week 8/25 - 8/31,feedback,,jzifg1e23c29s,jzytmzynx3t63c,2019-08-31T00:42:11Z,{},other
496,no,"I was late in posting this this week, so just to make clear, this office hours will happen today, 8/29 9PM ET, which is in about 5 hours. <div><br /></div><div>You can post any questions you want addressed by TAs as a follow up on this post. I’ll kick it off just for an example.</div><div><br /></div><div>Do you have any general tips for approaching homeworks and projects?</div>",2019-08-29T19:40:10Z,37,Week 8/25 - 8/31,followup,,is6lq4mnzu02k5,jzx3eqoinjz6il,2019-08-29T19:40:10Z,{},logistics
497,no,"<p>From the TD learning lectures (lesson 4), its stated that TD is categorized as a value based method, and the lambda parameter would define how many steps in the past the value of the state should be measured of. </p>
<p>I would love to get a bit more intuition into how it can be used along the quality function Q to define which action to take, and how it relates to backpropagation as its shortly described in one of the lectures.</p>
<p>Thank you</p>",2019-08-29T21:18:17Z,37,Week 8/25 - 8/31,followup,,is6e83bsfvk,jzx6wxionqg6ra,2019-08-29T21:18:17Z,{},logistics
498,no,"<p>If in TD(lambda) lambda is optimal at around 0.8, why do we care about TD(0) and TD(1) ??</p>",2019-08-29T23:05:31Z,37,Week 8/25 - 8/31,followup,,jc554vxmyuy3pt,jzxaqtql5j63pn,2019-08-29T23:05:31Z,{},logistics
499,no,"<p>Wasn&#39;t able to attend OH, when do you guys plan to add the link to the recording? thanks</p>",2019-08-30T23:18:32Z,37,Week 8/25 - 8/31,followup,,is6e83bsfvk,jzyqnez38kc28s,2019-08-30T23:18:32Z,{},logistics
500,no,<p>You should be able to access them on the BlueJeans page on Canvas.</p>,2019-08-31T00:23:44Z,37,Week 8/25 - 8/31,feedback,,hz7meu55mi8sd,jzysz9nlgtm64f,2019-08-31T00:23:44Z,{},logistics
501,no,"<p>Great, appreciate info, thanks.</p>",2019-08-31T03:24:52Z,37,Week 8/25 - 8/31,feedback,,is6e83bsfvk,jzyzg796flx4xp,2019-08-31T03:24:52Z,{},logistics
502,no,"<p>Hi Tim</p>
<p>First class at GA Tech, so apologies for the newb questions.</p>
<p></p>
<p>My BlueJeans tab is empty - there is no link to the recording for OH-2 (or any other OH). How do I get a link to that recording?</p>
<p></p>
<p>Thanks,</p>
<p></p>
<p>binod</p>",2019-09-03T13:29:34Z,36,Week 9/1 - 9/7,feedback,,jzk2u4t3n5o5el,k03vdf9xfj466l,2019-09-03T13:29:34Z,{},logistics
503,no,"<p><strong attention=""hyxsfbkeit22m2"">&#64;Alec Feuerstein</strong> can you help us out with &#64;132?</p>",2019-09-04T12:11:36Z,36,Week 9/1 - 9/7,followup,,hyx9thiqa6j4nn,k05810go1um7h0,2019-09-04T12:11:36Z,{},logistics
504,no,"<p>Related question: why does this start only on the <em>second</em> iteration, and not the first? I get that the terminal state values are zero, but the rewards should be available immediately.</p>",2019-08-30T00:15:58Z,41,Week 8/25 - 8/31,followup,,jl3oi5v7qkSk,jzxd9f7xq2l15a,2019-08-30T00:15:58Z,{},other
505,no,"<p>The first iteration is defined to be the starting state of all zeros.  In this example, iteration 2 is the first application of the value update.</p>",2019-08-30T00:32:22Z,41,Week 8/25 - 8/31,feedback,,jzfsa4a37jf4aq,jzxduit8spm49t,2019-08-30T00:32:22Z,{},other
506,no,"<p>When you say this is defined, is this <em>always</em> the case, or this was defined in these specific slides?</p>",2019-08-30T03:10:43Z,41,Week 8/25 - 8/31,feedback,,jl3oi5v7qkSk,jzxji5hyhb021u,2019-08-30T03:10:43Z,{},other
507,no,"<p>It is not always the case, but in this grid world example, the initial values are zero. There is a lecture about setting the initial values to something other than zero. Some people consider this to be ok, others suggest that it&#39;s a bias in the solution that affects the convergence. In the infinite horizon where we iterate until there is no change in the values, it all washes out.</p>
<p></p>
<p>To start value iteration you pick a state, any state, but you pick it knowing that you MUST iterate over all of the states. That&#39;s the definition of value iteration, computing the stable/converged value of each state without any care to a policy (so we are doing a global survey of the state values).</p>
<p></p>
<p>Starting in 0,0 which is state 1, sample states 2 and 3, which are the neighbors (I defined this now, it does not need to be this exactly). The neighbors are 0, so the value is all 0, and we continue on. Only when we get to the states near the absorbing states (r=1, r=-1) do we start to get values. So we do that, and suddenly the values propagate backwards towards the start state (just a trick of iteration).</p>
<p></p>
<p>As you continue this iteration, the values start to &#34;smooth&#34; out, meaning they&#39;re not changing much. This happens because we&#39;ve applied uniform averaging on the neighbors (no transition model) and a contraction factor (gamma) to reduce the impact of the smoothed out neighbors. At some future time (t) the ||v(t)-v(t-1)|| &lt; epsilon, and we call it converged (v is the value over all states, each individually).</p>
<p></p>
<p>Miguel shared a section of his book last year with us that really explained this well, as I look over it again. Maybe he would share that again, or I can share it if he doesn&#39;t have it. It&#39;s a very good explanation (visually) of what is going on.</p>",2019-08-30T05:01:14Z,41,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzxngaop3gr53a,2019-08-30T05:01:14Z,{},other
508,no,"<p>&#64;Ben I was saying that the initial value for all non-terminat states is 0 by definition of the slides that are presenting this example, but I see now that that wasn&#39;t your point.</p>
<p></p>
<p>I agree that it&#39;s not obvious why the slides label the first value update as iteration 2 rather than iteration 1, especially considering that the all-zero state is defined in the slides as iteration 0.</p>
<p></p>
<p>Here&#39;s another set of slides that has the same example with the same exact convention: <a href=""http://www.cis.upenn.edu/~cis519/fall2015/lectures/14_ReinforcementLearning.pdf"">http://www.cis.upenn.edu/~cis519/fall2015/lectures/14_ReinforcementLearning.pdf</a><a href=""http://www.cis.upenn.edu/~cis519/fall2015/lectures/14_ReinforcementLearning.pdf""></a></p>
<p></p>
<p>If you figure out why the propagation of non-zero values is starting at iteration 2 rather than iteration 1, let me know!</p>",2019-08-30T13:34:51Z,41,Week 8/25 - 8/31,feedback,,jzfsa4a37jf4aq,jzy5sslkj8s2p3,2019-08-30T13:34:51Z,{},other
509,no,"<p><em>I agree that it&#39;s not obvious why the slides label the first value update as iteration 2 rather than iteration 1, especially considering that the all-zero state is defined in the slides as iteration 0</em></p>
<p><em></em></p>
<p>This starts at iteration 1 because we are using the prior iteration&#39;s values instead of the future values. Sutton discusses this nuance in his book and states that whether we choose the future or past values is irrelevant, the convergence is the same.</p>",2019-08-30T13:53:37Z,41,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzy6gxgw6222l9,2019-08-30T13:53:37Z,{},other
510,no,"<p></p>
<p>I got that. Thanks. I have further question. </p>
<p></p>
<p>So in the 3rd iteration, how did we reach the values of  0.52 (at cell 1,2), 0.43 (at cell 2,1).</p>
<p></p>
<p></p>",2019-08-30T00:41:53Z,41,Week 8/25 - 8/31,followup,,jc9nkspumds4ki,jzxe6rowo4e2k0,2019-08-30T00:41:53Z,{},other
511,no,"<p>It&#39;s the same exact procedure I outlined above.</p>
<p></p>
<p>0.52 = (0.8)*(0.9)*(0.72).</p>
<p></p>
<p>0.43 = (0.8)*(0.9)*(0.72) &#43; (0.1)*(0.9)*(-1.0)</p>",2019-08-30T00:48:31Z,41,Week 8/25 - 8/31,feedback,,jzfsa4a37jf4aq,jzxefa7tggp58f,2019-08-30T00:48:31Z,{},other
512,no,<p>It doesn&#39;t work for Chrome. Seems to work ok with Edge.</p>,2019-08-31T02:02:39Z,45,Week 8/25 - 8/31,followup,,jc73qxwxnYpJ,jzywihm7gvj6vl,2019-08-31T02:02:39Z,{},office_hours
513,no,"Ah, I use Firefox as my main browser, which seems to work fine with it, but thanks for that heads up.",2019-09-01T00:01:05Z,45,Week 8/25 - 8/31,feedback,,jl3oi5v7qkSk,k007lzrbtdh31x,2019-09-01T00:01:05Z,{},office_hours
514,no,<p>I’ll work on this on the weekend. It’s long overdue.</p>,2019-08-30T14:23:23Z,50,Week 8/25 - 8/31,followup,,hyx9thiqa6j4nn,jzy7j7q6wk01tl,2019-08-30T14:23:23Z,{},other
515,no,"<p>Thanks, appreciate it! If I had the time I would just read it all, but given that it&#39;s doubled in size since the last time I looked at it, that might not be feasible (along with all the videos and papers) ^_^</p>",2019-08-30T15:12:10Z,50,Week 8/25 - 8/31,feedback,,jl3oi5v7qkSk,jzy99yjrgpu329,2019-08-30T15:12:10Z,{},other
516,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  You the man!</p>
<p></p>",2019-08-30T22:34:52Z,50,Week 8/25 - 8/31,feedback,,jqmfuaidej9155,jzyp39t4l7yk7,2019-08-30T22:34:52Z,{},other
517,no,<p>That book has helped clear things up or reinforce (no pun intended) the content in lectures and papers.  Having a guide to point us to the specific sections or chapters would be very helpful.</p>,2019-08-31T03:06:04Z,50,Week 8/25 - 8/31,feedback,,is9so9huTMp,jzyys117yhj1h0,2019-08-31T03:06:04Z,{},other
518,stud,"<p>This will be incredibly helpful. Thanks, Miguel</p>",2019-09-01T00:08:02Z,49,Week 9/1 - 9/7,feedback,a_0,,k007uxuig454e1,2019-09-01T00:08:02Z,{},other
519,no,"<p>Looking forward to this! Can you please update on this so we know you haven&#39;t forgotten it?</p>
<p></p>",2019-09-05T12:38:48Z,49,Week 9/1 - 9/7,feedback,,i4jbttw9ru63ot,k06ofu15qrl6pr,2019-09-05T12:38:48Z,{},other
520,no,"<p>Miguel,</p>
<p></p>
<p>While you are working on that, could you advise if we should read the Chapter 5 (Monte Carlo Methods) to understand TD? Is Monte Carlo a pre-requisite for TD?</p>
<p></p>
<p>Thank you.</p>",2019-09-05T21:50:27Z,49,Week 9/1 - 9/7,feedback,,jc6xvgjncoey,k07859optvz3vf,2019-09-05T21:50:27Z,{},other
521,no,"<p>I mentioned this during office hours: read only the prediction portion of the Monte-Carlo Methods chapter, then move on to TD.</p>
<p></p>
<p>I&#39;m close to releasing the list!</p>",2019-09-06T11:51:20Z,49,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0826neuo614fu,2019-09-06T11:51:20Z,{},other
522,no,"<p>So excited to get this list, <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  !  Thanks so much for taking this on!</p>",2019-09-08T04:25:48Z,48,Week 9/8 - 9/14,feedback,,jqmfuaidej9155,k0ah5e9nzc54ec,2019-09-08T04:25:48Z,{},other
523,no,<p>Thanks. Super helpful. I&#39;ve found the lectures a useful first pass over the state space while the book a excellent resource to get the value of the states to converge in my head. </p>,2019-09-08T07:29:28Z,48,Week 9/8 - 9/14,feedback,,jzozvpx25to679,k0anpkprl5x5w2,2019-09-08T07:29:28Z,{},other
524,no,"<p>Nice...</p>
<p></p>
<p>Here! One thing, it is not perfect I <em>just</em> finished. I will add to the schedule later. <a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fk0azh4q57zft%2FSutton_reading_schedule.pdf"" target=""_blank"">Sutton_reading_schedule.pdf</a></p>
<p></p>
<p>Do let me know what your experience is with this list taking into account project/papers/hws/etc.</p>
<p></p>
<p></p>",2019-09-08T12:57:58Z,48,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0azg1nxrciy9,2019-09-08T12:57:58Z,{},other
525,no,"<p>Sorry, what do you mean by the &#39;implied action&#39;?</p>",2019-08-30T05:20:39Z,64,Week 8/25 - 8/31,followup,,jl3oi5v7qkSk,jzxo598el7b3j3,2019-08-30T05:20:39Z,{},other
526,no,"<p>Implied action in the grid world means that moving from grid(0,0) to grid(1,0) is an &#39;up&#39; action and moving to grid(0,1) is a &#39;right&#39; action because of how the states are organized relative to each other.</p>
<p></p>
<p>Say we have a simple 4 cell grid world with states S ={ s1, s2, s3, s4 }. If there is no implied topology to the states and their order, then there is no implied action. If we have a 4 cell grid of the same state labels, but that grid is a matrix [ [ 1, 2 ], [3, 4] ] then we have a topology that implies an action when we transition from s1 to s2 and likewise s1 to s3.</p>
<p></p>",2019-08-30T12:36:27Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzy3povm30g3ur,2019-08-30T12:36:27Z,{},other
527,no,"<p>This is not quite correct. Both VI and PI learn using the model of the environment; the reward and transition function, containing the same exact information. The main difference between the two is value iteration has a policy evaluation phase that is truncated after one iteration. In PI, the policy evaluation phase runs to convergence.</p>",2019-08-30T14:13:28Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzy76h1dsa3so,2019-08-30T14:13:28Z,{},other
528,no,"<p>I was under the impression that policy iteration has faster convergence where each step might be computationally expensive because of a matrix inversion.  Further, value iteration can be overkill, at the end of the day,  you&#39;re arg maxing over values and you only need to be &#34;close enough&#34; to get the final unchanging policy.</p>",2019-08-30T06:07:11Z,64,Week 8/25 - 8/31,followup,,ixpclzk97jg2fo,jzxpt3kmk0g6lq,2019-08-30T06:07:11Z,{},other
529,no,"<p>This came up on slack: &#34;<span style=""font-family:&#34;appleLogo&#34;,sans-serif;text-align:left;font-weight:400;font-size:15px;font-style:normal;background-color:#f8f8f8;color:#1d1c1d"">What you&#39;re saying is that (if I understand correctly) PI is a model-free approach, as it only uses the actual rewards it gets?</span>&#34;</p>
<p></p>
<p>PI is not model free. The model in RL is the T(s,a,s&#39;,r). The model dictates how the bias towards a specific path is encoded in the graph. In all of the most trivial examples we have explored, the T(s,a,s&#39;) is uniform (1/n) so we are just averaging out the values of the state neighbors (the (s,a) transitions).</p>
<p></p>
<p>Policy Iteration is the act of finding a policy (tuples of (s,a)) that satisfies some kind of rule. Mostly we use argmax as the rule, but Littman in chapter 3 suggests some 10 different kinds of rules for this. his generalized MDP theory is a good intuition about how this stuff works.</p>
<p></p>
<p>We don&#39;t iterate over a policy to find a zero change-in-value. We iterate in PI to find the &#34;best&#34; series of (s,a) tuples that satisfy &#34;some kind of rule.&#34; You can use the Vs in VI to compose a PI if you know the transition model (s,a,s&#39;), or if you can imply it from the topology of the states (grid world, for example).</p>
<p></p>
<p>Remember that we are looking at the most trivial of examples in RL, so the difference between VI and PI is not as obvious. If you look at a real world RL application of PI and VI you will see thousands of states and hundreds of thousands of (s,a,s&#39;) possibilities. That&#39;s when you will see that PI as an exploration versus exploitation is more interesting than computing the entirety of V using VI (which could take your entire lifetime).</p>",2019-08-30T13:10:47Z,64,Week 8/25 - 8/31,followup,,jc554vxmyuy3pt,jzy4xufqjs527z,2019-08-30T13:10:47Z,{},other
530,stud,"<p><em> &#34;In all of the most trivial examples we have explored, the T(s,a,s&#39;) is uniform (1/n) so we are just averaging out the values of the state neighbors (the (s,a) transitions).&#34;</em></p>
<p><em></em></p>
<p>Why would it matter that the transition probabilities are uniform? If they&#39;re not, then we&#39;ll see the more probable transitions more frequently and the less probable transitions less frequently. Shouldn&#39;t we still be able to use simple averaging to estimate the values of the state neighbors?</p>
<p><em></em></p>",2019-08-30T13:47:17Z,64,Week 8/25 - 8/31,feedback,a_0,,jzy68sr6a9y4p6,2019-08-30T13:47:17Z,{},other
531,no,"<p>Yes, that is correct.</p>",2019-08-30T13:52:03Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzy6ex4lo1l1sq,2019-08-30T13:52:03Z,{},other
532,no,"<p>What&#39;s the slack channel for this class? I looked sor 7642 or reinforcement, and nothing recent came up.</p>
<p>Thx</p>",2019-08-30T19:28:49Z,64,Week 8/25 - 8/31,feedback,,jzh6k6o994a6dh,jzyig0d89mh5ut,2019-08-30T19:28:49Z,{},other
533,no,"<p>&#64;Jean-Pierre  go to <a href=""https://omscs-study.slack.com"">https://omscs-study.slack.com</a> </p>
<p>There you can find cs7642</p>",2019-09-01T04:43:41Z,63,Week 9/1 - 9/7,feedback,,is9so9huTMp,k00hpfj2ynbrs,2019-09-01T04:43:41Z,{},other
534,no,<p>Go read that section and give me your intuition as to when would PI be a better fit than VI? When is policy evaluation needed to be run to convergence?</p>,2019-08-30T14:48:19Z,64,Week 8/25 - 8/31,followup,,hyx9thiqa6j4nn,jzy8fahdq138l,2019-08-30T14:48:19Z,{},other
535,no,"<p>Student response is misleading. In Reinforcement Learning the word &#34;model&#34; is used to refer to the transition and reward function. </p>
<p></p>
<p>And Chris failed to mention I would slightly disagree with his definition, even if it is correct.</p>
<p></p>
<p>The world would be a better place if we called algorithms the <em>require</em> an MDP something different than methods that <i>learn</i> an MDP.</p>
<p></p>
<p>Planning vs. model-free vs. model-based is what I prefer.</p>",2019-08-30T17:49:10Z,64,Week 8/25 - 8/31,followup,,hyx9thiqa6j4nn,jzyevulbu2w7hy,2019-08-30T17:49:10Z,{},other
536,no,"<p>So planning &lt;=&gt; a full model is provided and used</p>
<p>model-based &lt;=&gt; a(n estimate of the) model is learned and used</p>
<p>model-free &lt;=&gt; no model is used</p>
<p></p>
<p>but in practice the first two are often both called model-based.</p>
<p></p>
<p>Is this correct?</p>",2019-08-30T19:19:47Z,64,Week 8/25 - 8/31,feedback,,jzfsa4a37jf4aq,jzyi4dwr8b63ax,2019-08-30T19:19:47Z,{},other
537,no,<p>Exactly. </p>,2019-08-30T20:33:11Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzykqskhj3bk,2019-08-30T20:33:11Z,{},other
538,no,"That seems like a worthwhile and useful distinction, thanks :)",2019-08-30T23:18:36Z,64,Week 8/25 - 8/31,feedback,,jl3oi5v7qkSk,jzyqnifmqia3i4,2019-08-30T23:18:36Z,{},other
539,no,<p>Np!</p>,2019-08-31T22:20:04Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,k00402vxs9i4h5,2019-08-31T22:20:04Z,{},other
540,no,<p>what is the local environment setup - installations and dependent versions for running the programs in local?</p>,2019-08-30T17:27:40Z,64,Week 8/25 - 8/31,followup,,jc6s23mr74r45q,jzye47u7pky5qy,2019-08-30T17:27:40Z,{},hw1
541,no,<p>That is entirely up to you and how you want to approach it.  We do not require any certain language or programming environment.  You will simple enter the output of your program similar to the values given in the examples.</p>,2019-08-30T17:51:03Z,64,Week 8/25 - 8/31,feedback,,hz7meu55mi8sd,jzyey9pa67r3sa,2019-08-30T17:51:03Z,{},hw1
542,stud,"is RLDM website ready?
is it RLDM ready? Try to log in, but shows &#34;Can&#39;t find the user&#34;.",2019-08-31T01:50:36Z,64,Week 8/25 - 8/31,followup,a_0,,jzyw2zgzgdx79f,2019-08-31T01:50:36Z,{},hw1
543,no,,2019-08-31T01:50:36Z,64,Week 8/25 - 8/31,dupe,,hyx9thiqa6j4nn,jzyw2zhg9bw79g,2019-08-31T01:50:36Z,{},hw1
544,no,<p>&#64;73</p>,2019-08-31T01:50:36Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,jzyw2zhpv0w79h,2019-08-31T01:50:36Z,{},hw1
545,no,<p>I am still not able to login to herokuapp</p>,2019-09-01T06:25:52Z,63,Week 9/1 - 9/7,followup,,jzih70o61oy1af,k00lctudrwcdx,2019-09-01T06:25:52Z,{},hw1
546,no,<p>me either</p>,2019-09-01T13:35:07Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k010ou903j3ng,2019-09-01T13:35:07Z,{},hw1
547,no,<p>&#64;92</p>,2019-09-02T06:35:34Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02155dgx9yhf,2019-09-02T06:35:34Z,{},hw1
548,no,<p>I can login but dont see specific problem to submit the result. Am I missing something to complete the HW1 or rldm.herookuapp still not 100% up to complete the Hw1 submission? </p>,2019-09-01T18:53:29Z,63,Week 9/1 - 9/7,followup,,jl2gfssgneuU,k01c29lla9z3kq,2019-09-01T18:53:29Z,{},hw1
549,no,<p>Got it. I can see it now. Thank you </p>,2019-09-02T00:55:13Z,63,Week 9/1 - 9/7,feedback,,jl2gfssgneuU,k01ozgn4wyb1c3,2019-09-02T00:55:13Z,{},hw1
550,no,"<p>In the isBadSide vector, does a value of 1 represent a Bad Side, or is it the zero value? I am assuming a value of 1 represents a Bad Side.</p>",2019-09-03T05:06:39Z,42,Week 9/1 - 9/7,followup,,gx3c8l7z7r72zl,k03densrfg6n3,2019-09-03T05:06:39Z,{},hw1
551,no,"<p>From the problem statement:</p>
<p></p>
<p>And you roll a number not in isBadSide, you receive that many dollars. (eg.<br />if you roll the number 2 and 2 is not active -- meaning the second element<br />of the vector is 0 -- in isBadSide, then you receive 2 dollars)</p>",2019-09-03T05:47:37Z,42,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03evcmzwyg2qg,2019-09-03T05:47:37Z,{},hw1
552,no,"<p>numpy is to the rescue<br /><br /><code>&gt;&gt;&gt; np.round(2.675, 2)</code><br /><code>2.68</code></p>",2019-08-30T15:51:12Z,64,Week 8/25 - 8/31,followup,,jqkxzdmmolGf,jzyao56k4p04h0,2019-08-30T15:51:12Z,{},hw2
553,no,"<p>Fun Trivia, this is known as Banker&#39;s Rounding.  <a href=""http://wiki.c2.com/?BankersRounding"">http://wiki.c2.com/?BankersRounding</a></p>
<p></p>
<p>In this specific example, it rounds **toward** the nearest even **integer**, which means down.</p>
<p></p>
<p>On the off-chance any of you know any engineers that went to the University of Arkansas, learning this was required to receive full credit in the statics course.  :-)</p>",2019-08-30T22:31:41Z,64,Week 8/25 - 8/31,followup,,jqmfuaidej9155,jzyoz6go68u61z,2019-08-30T22:31:41Z,{},hw2
554,no,"<p>the idea being that round function, injects a bias (upward); alternating the round on 0.50 up or down restores balance (in the limit).  there are other alternatives.</p>
<p></p>
<p>interestingly, rounding to the nearest odd integer, is disclaimed because it will effectively, never round to zero.  </p>
<p></p>
<p>https://www.eetimes.com/document.asp?doc_id=1274515</p>",2019-08-31T02:10:25Z,64,Week 8/25 - 8/31,feedback,,jzivtxcbl6964n,jzywsh0zecs1o9,2019-08-31T02:10:25Z,{},hw2
555,no,"<p>Is <a href=""https://rldm.herokuapp.com/student/"">https://rldm.herokuapp.com/student/</a> for submission? When I tried to log in, I get an error of &#34;User does not exist. Try again&#34;. Is the site not ready yet?</p>",2019-08-30T22:46:12Z,43,Week 8/25 - 8/31,followup,,jzj4205g7gd2fw,jzyphu8uf9a4ak,2019-08-30T22:46:12Z,{},hw1
556,no,<p>Correct. See &#64;73</p>,2019-08-30T22:51:42Z,43,Week 8/25 - 8/31,feedback,,jl1acpoc4HA9,jzypowp29836vt,2019-08-30T22:51:42Z,{},hw1
557,no,<p>A follow up question. Can we submit multiple times? How is the grade calculated?</p>,2019-08-31T08:55:34Z,43,Week 8/25 - 8/31,followup,,jzjwcq2u8o7110,jzzb9huw8mn6b4,2019-08-31T08:55:34Z,{},hw1
558,no,<p>It was answered in OH 2. Up to 10 times (not confirmed) with speed bump between attempts (with possibility of being locked out)</p>,2019-08-31T11:33:52Z,43,Week 8/25 - 8/31,feedback,,jqkxzdmmolGf,jzzgx2anoaw4gd,2019-08-31T11:33:52Z,{},hw1
559,no,<p>Correct. Submit only every ~1 minute.</p>,2019-08-31T22:19:42Z,43,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,k003zm93yu04ax,2019-08-31T22:19:42Z,{},hw1
560,no,<p>Thanks!</p>,2019-09-01T09:58:25Z,43,Week 8/25 - 8/31,feedback,,jzjwcq2u8o7110,k00sy5t8eai54h,2019-09-01T09:58:25Z,{},hw1
561,no,"<p>How do we model the bankruptcy? Maybe that returns us to state 0 which is the bankroll $0?</p>
<p></p>
<p>That means our bankroll is really the states then, and we only have roll/cash-out as the two actions we can take. I believe that concurs with the description you provided. An then the (s,a,s&#39;,r) for s-&gt;roll-&gt;s0 = 0, which would be the only transition with a zero value. All of the other s-&gt;roll-&gt;sn would have r equal to the die face value.</p>
<p></p>
<p>For instance, (10,roll,13)=3 at p=1/N</p>
<p></p>
<p>Then there is the end state - for that to work doesn&#39;t the end state only connect back to itself with p=1 and r=0?</p>
<p></p>
<p>If after the bankruptcy state is reached, then how do you ensure that you don&#39;t continue rolling rather than ending?</p>
<p></p>
<p>This has always been the hardest part of RL for me - understanding how to model the states and actions.</p>",2019-08-31T19:17:14Z,64,Week 8/25 - 8/31,followup,,jc554vxmyuy3pt,jzzxgyxkeko1s8,2019-08-31T19:17:14Z,{},hw1
562,stud,"<p><em>&#34;How do we model the bankruptcy? Maybe that returns us to state 0 which is the bankroll $0?&#34;</em></p>
<p><em></em></p>
<p>I&#39;m not certain that we&#39;d go back to state $$s_{0}$$. When we&#39;re at state $$s_{0}$$, other states are in principle accessible to us (say, if we decide to roll again <em>and</em> we happen to role a value that isn&#39;t in the set $$isBadValue$$). But the game is supposed to terminate when we go bankrupt. That suggests that we should be in a terminal state post-bankruptcy. But $$s_{0}$$ isn&#39;t terminal, right? So my suspicion is that going bankrupt must take us to a terminal state distinct from $$s_{0}$$, where our &#34;reward&#34; for that transition is minus our entire bankroll. I think this is different from what I wrote above, but if we don&#39;t do it this way I have a hard time seeing how we&#39;re supposed to communicate the &#34;badness&#34; of going bankrupt. It doesn&#39;t seem enough that I don&#39;t get a reward when I roll a bad die value. I think we need to take into account that I&#39;m also loosing all of my accumulated bankroll. </p>
<p></p>
<p><em>&#34;Then there is the end state - for that to work doesn&#39;t the end state only connect back to itself with p=1 and r=0&#34;</em></p>
<p><em></em></p>
<p>Yes, that&#39;s my understanding of how a terminal state is supposed to work. </p>",2019-08-31T19:42:14Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzyd48d6mr7lf,2019-08-31T19:42:14Z,{},hw1
563,no,"<p>So, for the bankruptcy state to work properly we have to update the (s,a,s,t,r) model each time we transition from s-&gt;s&#39;(roll)?</p>",2019-08-31T19:44:00Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzzyfdsdb4t5yb,2019-08-31T19:44:00Z,{},hw1
564,no,"I think when you go bankruptcy you shouldn&#39;t go back to s0, instead you should create some final state/absorbing state for that. There should be multiple ways to model the states, what I did is using a tuple to represent the bankroll you have now and if you are in a final/absorbing state already. So (0,0) means that your are in the initial state, (0,1) means game ended with bankruptcy, (5,1) means you have earned $5 and have decided to stop rolling. Once you are in a final state, you can design your transition matrix to not allow it to move to the other states.",2019-08-31T19:47:39Z,64,Week 8/25 - 8/31,feedback,,jzj4205g7gd2fw,jzzyk337cft122,2019-08-31T19:47:39Z,{},hw1
565,stud,"<p>No, I don&#39;t think that follows. The rewards can be viewed as a function of triples of the form $$(s, a, s&#39;)$$. It seems like, for any nonterminal state $$s$$, if I decide to roll and the roll goes badly for me and puts me in a terminal state (so, $$s&#39; = s_{T}$$), then $$r(s, roll, s_{T}) = -s$$. I&#39;m not clear on why that would require the model to change over time, as the rewards can evidently be specified in advance. </p>",2019-08-31T19:49:51Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzymwg8tnr2pk,2019-08-31T19:49:51Z,{},hw1
566,stud,"<p><em>&#34;I think you can set a limit on T(number of episode to look ahead/ how many times do you roll the dies at max),  so there is a limit amount of s and Transaction matrix to look at. In fact I used T=5 in my code and achieved the same result as the examples given.&#34;</em></p>
<p><em></em></p>
<p>Isn&#39;t this just equivalent to truncating the state space? In other words, isn&#39;t this the same as simply leaving certain high (and therfore unlikely) bankrolls out of the model?</p>",2019-08-31T19:52:36Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzyqgbturn4t2,2019-08-31T19:52:36Z,{},hw1
567,no,"<p>Ah, yeah, I understand now. (s=5,a=roll,s=bankrupt): p=m/N r=-5 ..... that makes sense. Thanks!</p>",2019-08-31T19:54:50Z,64,Week 8/25 - 8/31,feedback,,jc554vxmyuy3pt,jzzytbdkaco12c,2019-08-31T19:54:50Z,{},hw1
568,stud,<p>Welcome!</p>,2019-08-31T19:56:40Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzyvo24ogm2d3,2019-08-31T19:56:40Z,{},hw1
569,stud,"<p>Ava, how did you propogate information about the &#34;badness&#34; of bankruptcy through the model? Did you stipulate that the transition to a terminal state from a bad die roll was the negative of the bakroll you had beforehand? </p>",2019-08-31T19:59:22Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzyz5fcg095hh,2019-08-31T19:59:22Z,{},hw1
570,stud,<p>I should have said: Did you stipulate that the <em>reward</em> for a transition to a terminal state from a bad die roll was the negative of the bakroll you had beforehand? </p>,2019-08-31T20:00:08Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzz04fqqyi5ve,2019-08-31T20:00:08Z,{},hw1
571,no,"I actually did it in a different way, I only look at the reward when we get to a terminal state, ie when we decided to end game. If the agent is not in a terminal state, the reward in each step is always 0. If you landed in the bankruptcy state then your final value/utility is 0, otherwise you end the game with a positive bankroll then you get that value/utility in the end. This way the reward function is only based on the state not the action and next state, so it&#39;s simpler. I actually think this is not the proper way to do it, but I get the result matching with the examples given. I also think what you mentioned, using reward = number the die rolled if not isBad ; reward = negative current bankroll if isBad is a better way to approach it. But this way you have to model the reward function as a function of current state, action, and next state so it&#39;s more complicated.",2019-08-31T20:07:55Z,64,Week 8/25 - 8/31,feedback,,jzj4205g7gd2fw,jzzza4qkxu010s,2019-08-31T20:07:55Z,{},hw1
572,stud,"<p>Got it! </p>
<p></p>
<p>I think either approach can work. I don&#39;t think there&#39;s just one way to model it (as you say). </p>
<p></p>
<p>Thanks!</p>",2019-08-31T20:09:56Z,64,Week 8/25 - 8/31,feedback,a_0,,jzzzcqfat3i32q,2019-08-31T20:09:56Z,{},hw1
573,no,You are welcome:),2019-08-31T22:01:50Z,64,Week 8/25 - 8/31,feedback,,jzj4205g7gd2fw,k003cn6jtue2mm,2019-08-31T22:01:50Z,{},hw1
574,no,"<p>&#64;Ava Jiang  Was this a typo?  I follow what you&#39;re saying in the post just above but this statement is misleading to me.</p>
<p></p>
<p><em>&#34;If the agent is not in a terminal state, the reward in each step is always 0.&#34;</em></p>
<p></p>
<p>Did you mean to say if the agent<strong> is</strong> in a terminal state the reward is always zero?</p>",2019-09-02T04:15:06Z,63,Week 9/1 - 9/7,feedback,,is9so9huTMp,k01w4imv1b4rn,2019-09-02T04:15:06Z,{},hw1
575,no,"&#64;Nikki Pappas No it&#39;s not a typo, I mean I modeled it in a way that I assume all the intermediate steps all have reward 0, only the terminal state has the true value. Its like during the rolls, you accumulate the bankroll number but you don&#39;t actually get that money until you terminate the game. This way, the reward function can be simpilify to a function of state and action only. Its the same as Rick mentioned in the followup question below. I&#39;m not saying it&#39;s the most proper way to model it, but it certainly works with some of the test cases. You can also model it with all intermediate step has the reward equal to the die rolled, but because it&#39;s not deterministic, so the reward function cannot be represent by a function of only state and action(roll action could end up in either bankruptcy or increasing bankroll), you will have to represent it by a function of (state, action, next state).",2019-09-02T14:58:58Z,63,Week 9/1 - 9/7,feedback,,jzj4205g7gd2fw,k02j4j78db348p,2019-09-02T14:58:58Z,{},hw1
576,no,"<p>&#64;Ava Jiang... I hope you see this message.  I understand what you mean with the reward =0 and getting the money only when we reach the terminal state, BUT the state-value BACKpropagates.  </p>
<p>Bellman&#39;s equations calculate v using the values of the states it can reach, not the other way around, so I don&#39;t even see how the final state could be anything but zero.</p>
<p></p>
<p>Can you explain how you managed to get the value in the terminal state?</p>
<p>And not in the bankruptcy state?</p>
<p>And also, what&#39;s your using of a tuple?? For what? </p>
<p>Thx</p>",2019-09-03T05:35:28Z,63,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k03efq1hxx8w,2019-09-03T05:35:28Z,{},hw1
577,no,"<p>It&#39;s not clear to me what the output is.  if we have an unlimited number of states V($).  Is output V(max dollar amount)?  I&#39;m assuming this is some type of value iteration?  V vs Q?    I&#39;m thinking so far:</p>
<p></p>
<p>states are: V($) - potentially infinite, V(done), V(bankrupt),</p>
<p>actions = { stay, roll},</p>
<p>T(s, stay, done) = 100%,</p>
<p>T(s, roll, s &#43; dice_value | good roll) = ???</p>
<p>T(s, roll, bankupt | bad roll) = ???</p>
<p></p>
<p>Questions:</p>
<p>1. rewards:  I&#39;m almost thinking that rewards are generally zero until you enter a terminal state.  R[$, quit] = $, R[$, roll] = 0, but I&#39;m not clear how to show R[$, roll - but bankrupt] = -$.</p>
<p>2. Solving: It would seem that we wouldn&#39;t have to simulate anything (rolling dice w/ uniform probability and checking isBadSide) and value iteration should be able to get us to a solution.  If it&#39;s value iteration, how do we bound V($)?</p>
<p>3. Formuation: v(s) = max_a R[s, a] &#43; sum_s&#39; T(s, a, s&#39;) v[s&#39;]</p>
<p>4. More to come..</p>
<p></p>
<p>Crossing my fingers OH will clear this up, have a feeling, I&#39;m going to flounder a bit over the weekend.</p>
<p></p>",2019-08-31T23:10:09Z,64,Week 8/25 - 8/31,followup,,ixpclzk97jg2fo,k005shq5dbt36x,2019-08-31T23:10:09Z,{},hw1
578,no,"<p>The homework is asking for the state-value function of the initial state (when you have 0 in the bankroll)</p>
<p></p>",2019-08-31T23:11:24Z,64,Week 8/25 - 8/31,feedback,,hyx9thiqa6j4nn,k005u408rbx6s,2019-08-31T23:11:24Z,{},hw1
579,no,"I&#39;m thinking about also representing it this way. I would point out that for Question 1 R[$, roll - but bankrupt] = 0, not -$. I think this way about it because you&#39;ve represented it as everything giving zero until you get to the end.",2019-09-01T03:57:31Z,63,Week 9/1 - 9/7,feedback,,ixpwxv7xdgi1u6,k00g2208wkd6pe,2019-09-01T03:57:31Z,{},hw1
580,stud,"<p>&#64;Mike Peacock, you may be right re 1 above. That was my first thought as well. Only, I&#39;m having a hard time figuring out how to solve this with just value iteration if that&#39;s the case. </p>",2019-09-01T14:21:45Z,63,Week 9/1 - 9/7,feedback,a_0,,k012ctk3svv2m2,2019-09-01T14:21:45Z,{},hw1
581,no,"<p>&#64;Mike Peacock.  Perfect, I was just jumbling absolute and relative rewards.  If you structure things in a particular way, your reward is simply a pot amount or 0 for bankruptcy.  I was &#34;punishing&#34; bankruptcies by saying the reward is -s.  My values were always slighly less than the HW examples, as soon as I changed to &#34;0&#34;, my answer match perfectly.</p>",2019-09-01T19:26:44Z,63,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k01d91girol18r,2019-09-01T19:26:44Z,{},hw1
582,stud,"<p>I&#39;m also curious about how to bound V($). We&#39;ll start at the initial state and there will be n&#43;1 states (roll 1, roll 2, ...roll n, stay/terminate). But then once you move to one of those states if we&#39;re using the bankroll as the new states, there are infinitely more states to consider. </p>",2019-09-01T20:05:43Z,63,Week 9/1 - 9/7,feedback,a_1,,k01en5omc5w2df,2019-09-01T20:05:43Z,{},hw1
583,no,"<p>You can compute the non-bankruptcy limit using some simple math:</p>
<p></p>
<p>E(&#39;cash-out&#39;|X) = E(&#39;roll&#39;|X) where X is the bank roll</p>
<p></p>
<p>The formula to solve for X is mostly trivial (it&#39;s part of the Bellman equations). Once you have X, you can use n*X to get a reasonable bound. You can use X alone too and that would be enough. That&#39;s what I&#39;ve been using.</p>
<p></p>
<p>I also tried to apply the same logic to formulate the bankruptcy value X, but that never worked for me.</p>
<p></p>
<p>For instance, n=6 bad=0,0,0,1,1,1, the non-bankruptcy value is $2. That can be your reasonable cap to determine the bankruptcy value. For my MDP, I compute that to be start choose roll with expected value $1.2453703703703702, but that value is wrong. I am over predicting the values right now (don&#39;t quite know why yet).</p>
<p></p>
<p>Here what I get for this test case. Note that I implemented VI according to Sutton page 83 and applied his policy iteration tweak to capture the action (policy) in the VI. This made more sense to me on this problem than the U(s) formulas in the smoov lectures. In smoov, the R(s) didn&#39;t make sense to me in terms of the (s,a,s&#39;,t)=r relationship.</p>
<p></p>
<pre>start choose roll with expected value $1.0833333333333333
$1 choose roll with expected value $0.4999999999999999
$2 choose cash-out with expected value $0.0
$3 choose cash-out with expected value $0.0
$4 choose cash-out with expected value $0.0
$5 choose cash-out with expected value $0.0
quit choose roll with expected value $0.0</pre>
<p></p>",2019-09-01T20:13:55Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01expf9atyp5,2019-09-01T20:13:55Z,{},hw1
584,no,"<p>Another fun example is my test 2 which is not exactly correct. I get test 1 and 3 correct. But test 2 is fun because the policy tells you some interesting information:</p>
<p></p>
<pre>Roll until your bankroll reaches $ 10.462 without bankruptcy
Roll until your bankroll reaches $ 0.238 with bankruptcy
MDP: N= 22 ,max bankroll=$ 30
start choose roll with expected value $6.274793388429751 ($6.314 is the answer)
$1 choose roll with expected value $0.0
$2 choose roll with expected value $0.0
$3 choose roll with expected value $0.0
$4 choose roll with expected value $0.0
$5 choose roll with expected value $0.0
$6 choose roll with expected value $0.0
$7 choose roll with expected value $2.045454545454543
$8 choose roll with expected value $0.0
$9 choose cash-out with expected value $0.0
$10 choose roll with expected value $0.0
$11 choose roll with expected value $0.0
$12 choose cash-out with expected value $0.0
$13 choose roll with expected value $0.0
$14 choose cash-out with expected value $0.0
$15 choose roll with expected value $0.0
$16 choose cash-out with expected value $0.0 !! If you reach here, always cash out.
$17 choose cash-out with expected value $0.0
$18 choose cash-out with expected value $0.0
$19 choose cash-out with expected value $0.0
$20 choose cash-out with expected value $0.0
$21 choose cash-out with expected value $0.0
$22 choose cash-out with expected value $0.0
$23 choose cash-out with expected value $0.0
$24 choose cash-out with expected value $0.0
$25 choose cash-out with expected value $0.0
$26 choose cash-out with expected value $0.0
$27 choose cash-out with expected value $0.0
$28 choose cash-out with expected value $0.0
$29 choose cash-out with expected value $0.0
$30 choose cash-out with expected value $0.0
quit choose roll with expected value $0.0</pre>
<p></p>",2019-09-01T20:31:15Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01fjzx4pgp5mf,2019-09-01T20:31:15Z,{},hw1
585,no,"<p>So, when I set my bankroll state limit to 4 * non-bankrupt-expectation, ($40 in test case 2), I can get the expected answer. (correction, the expected answer is the correct answer). I ran 1 .. 10 * bankroll to see where it converges:</p>
<p></p>
<pre>[0.7272727272727273, x1 
4.2727272727272725, x2
6.274793388429751, x3
6.314049586776859, x4
6.314049586776859, x5 .....
6.314049586776859, 
6.314049586776859, 
6.314049586776859, 
6.314049586776859]</pre>
<p></p>",2019-09-01T23:58:02Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01mxxibzy254a,2019-09-01T23:58:02Z,{},hw1
586,stud,"<p>&#64;Jacob When you say &#34;The formula to solve for X is mostly trivial (it&#39;s part of the Bellman equations),&#34; which Bellman equation are you referring to? V(s) or Q(s,a)? OR the third one? </p>",2019-09-01T23:59:03Z,63,Week 9/1 - 9/7,feedback,a_1,,k01mz825i2x2ft,2019-09-01T23:59:03Z,{},hw1
587,no,"<p>I&#39;m referring to the sum of the p * R in the equations, which is the p * V(s) in the equations from the books.</p>
<p></p>
<p>The E(&#34;roll&#34; | x) = sum of p*r[i] for those faces that are not bankruptcy</p>",2019-09-02T00:03:24Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01n4trgo6b431,2019-09-02T00:03:24Z,{},hw1
588,stud,"<p>&#64;Jacob Why in the  n=6 bad=0,0,0,1,1,1, the non-bankruptcy value is $2 and not $2.5. It seems like the values of non bankruptcy rolls would be 4,5,6 where a 1,2,3 would result in bankruptcy. </p>",2019-09-02T00:31:21Z,63,Week 9/1 - 9/7,feedback,a_1,,k01o4ro126sqx,2019-09-02T00:31:21Z,{},hw1
589,no,"<p>You&#39;re going to make me write out my dicey math!</p>
<p></p>
<p>$$X = \sum_i{p_i(r_i&#43;X)}$$</p>
<p></p>
<p>When that equation is solved for X you find the non-bankruptcy value. The key is the $$(r_i&#43;X)$$. We know that for us, $$p_i$$ is constant, which is 1/N, so that factors out. Then we also know that the X is constant in this formula, so that can rearrange nicely:</p>
<p></p>
<p>$$\sum_i{(r_i&#43;X)} = \sum_i{r_i} &#43; iX$$ ∴ $$X = p(iX &#43; \sum_i{r_i})$$</p>
<p></p>
<p>rearrange the terms and solve to isolate X:</p>
<p></p>
<p>$$X = \frac {\sum_i{r_i}} {1 - ip} = \frac {\sum_i{r_i}} {1 - \frac {i} {N}}$$</p>
<p>i is the number of good faces, not bankruptcy faces.</p>
<p></p>
<p>To compute the bankruptcy value, I tried adding $$-X\frac{m}{N}$$ to the RHS of the formula above, and the solving to isolate X. Unfortunately, none of my math has panned out. For $$\frac{m}{N}$$ the m is the number of bad faces. I don&#39;t think this is the correct approach for including bankruptcy.</p>",2019-09-02T01:00:32Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01p6avdni83o8,2019-09-02T01:00:32Z,{},hw1
590,no,"<p>There&#39;s a formula for the number of states in this problem given the non-bankruptcy horizon. I&#39;m running a scan of the 6-sided dice problem to begin my search. Then again, someone has probably already solved it ...</p>",2019-09-02T01:14:13Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01pnwmq8g329,2019-09-02T01:14:13Z,{},hw1
591,stud,"<p>You&#39;re using $$i$$ above as both the summation index and the upper index of summation. That&#39;s the only dicey thing I see above.</p>
<p></p>
<p>I&#39;m thinking that what you really meant is that $$X = \sum_{i \in B^{c}}p_{i}(r_{i} &#43; X)$$ where $$B$$ is the set of bad faces, $$B^{c}$$ is its complement (i.e., the set of good faces) and $$r_{i}$$ is the immediate reward for rolling side $$i \in B^{c}$$. So you&#39;re taking a sum over only those $$i$$ that aren&#39;t in the set of bad rolls, $$B$$. Then the result above becomes</p>
<p></p>
<p>$$X = \sum_{i \in B^{c}}p_{i}(r_{i} &#43; X) = \sum_{i \in B^{c}}\frac{1}{N}(r_{i} &#43; X) = \frac{1}{N}(\sum_{i \in B^{c}}r_{i} &#43; X) $$</p>
<p></p>
<p>$$= \frac{1}{N}(\sum_{i \in B^{c}}r_{i} &#43; |B^{c}|X)$$, where $$|B^{c}|$$ is the number of good rolls.</p>
<p></p>
<p>Then,  $$NX - |B^{c}|X = X(N - |B^{c}|) = \sum_{i \in B^{c}}r_{i} \implies X = \frac{\sum_{i \in B^{c}}r_{i}}{N - |B^{c}|} = \frac{\sum_{i \in B^{c}}r_{i}}{1 - \frac{{|B^{c}|}}{N}}$$</p>
<p></p>
<p>which is what you have above, without abusing the summation index : )</p>
<p></p>",2019-09-02T01:42:28Z,63,Week 9/1 - 9/7,feedback,a_0,,k01qo8cbq5sg2,2019-09-02T01:42:28Z,{},hw1
592,no,<p>Yes. I am very rusty with the notations. Thanks for cleaning that up!</p>,2019-09-02T01:49:34Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k01qxcvym784y1,2019-09-02T01:49:34Z,{},hw1
593,stud,<p>Welcome! Thanks for figuring it out! : )</p>,2019-09-02T02:05:40Z,63,Week 9/1 - 9/7,feedback,a_0,,k01ri2hqley1cy,2019-09-02T02:05:40Z,{},hw1
594,stud,"<p>I misread your example, I saw the three 0&#39;s and three 1&#39;s and assumed it was the example given in the homework when in fact you inverted isBadSide. </p>
<p></p>
<p>But now I am more confused, because if we go back to that example the sum would total 6 and the bottom of that equation would equal 1 - (3/6). So then X would equal 6/0.5 or 12. </p>
<p></p>
<p>I really appreciate your explanations, I am sorry I am struggling to understand. </p>",2019-09-02T02:40:13Z,63,Week 9/1 - 9/7,feedback,a_1,,k01sqhh1wt05qk,2019-09-02T02:40:13Z,{},hw1
595,no,"<p>&#64;Rick Sprague, your post about &#34;punishing&#34; bankruptcies really helped.  I didn&#39;t want to code any solutions until I could hand-calculate a couple of simple examples.  Once I stopped punishing bankruptcies I was able to get accurate results by-hand, even for the examples in the homework sheet.  Thanks!</p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=82""></a></p>",2019-09-02T23:02:44Z,63,Week 9/1 - 9/7,feedback,,jam28oymee84jg,k030eng3rpl48l,2019-09-02T23:02:44Z,{},hw1
596,no,"<p>sorry for the late replay &#34;anonymous.&#34; so are you referring to &#34;For instance, n=6 bad=0,0,0,1,1,1, the non-bankruptcy value is $2?&#34; let&#39;s take a look at test 3 instead so I don&#39;t annoy Miguel.</p>
<p></p>
<pre>Roll until your bankroll reaches $ 5.0 without bankruptcy
bad= [1, 1, 1, 0, 0, 0]</pre>
<p>N = 6</p>
<p>i = 3</p>
<p>$$\sum_j{rj} = 4 &#43; 5 &#43; 6 = 15$$ where j is in $$B^c$$ like you wrote above.</p>
<p></p>
<p>Plug and chug into the formula and you get:</p>
<p>$$X = \frac{15}{1 - \frac{3}{6}} = 30$$ which is correct. My calculation in my code was wrong. I was applying the probability to the face values in the $$\sum_j{r_j}$$ term. oops.</p>
<p></p>
<pre>Roll until your bankroll reaches $ 30.0 without bankruptcy
Roll until your bankroll reaches $ 2.5 with bankruptcy</pre>
<p>Check that out. So the modification to calculate the bankruptcy is also working, with a modest margin of error:</p>
<p></p>
<pre>test3 : Answer is: 2.583333333333333</pre>
<p>Thanks for calling me out on my dicey math!</p>
<p></p>",2019-09-06T02:38:03Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k07if40qiyl239,2019-09-06T02:38:03Z,{},hw1
597,no,"<p>&#64;Rick Sprague and others,</p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=82""></a></p>
<p>I am not sure where to start my value iteration as I see most examples start at a discrete terminal state, but for our case, the terminal state can be infinitely far away with many differ final values? How did you choose this and how did you choose the sequence of states to get the terminal state as there can be many options? After you chose your terminal state, did you start from the initial state or did you start from the terminal state and go backwards?</p>
<p>Also, I am not sure if I should give instant rewards from state to state or just wait until the end?</p>
<p> </p>
<p>Thank you for your help.</p>
<p></p>",2019-09-06T16:34:56Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08cbd01b553ky,2019-09-06T16:34:56Z,{},hw1
598,no,<p>&#64;157 shares some ideas.</p>,2019-09-06T20:25:43Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08kk5ivebd19d,2019-09-06T20:25:43Z,{},hw1
599,no,<p>I was struggling until I read Sutton &amp; Barto Example 4.2 &#34;Gambler&#39;s Problem&#34;.  I quickly found my mistaken assumptions after reading it.</p>,2019-09-01T02:46:02Z,63,Week 9/1 - 9/7,followup,,jzhl7qlwrpagr,k00di4ckxfy21x,2019-09-01T02:46:02Z,{},hw1
600,no,"<p>For reference, the gambler&#39;s problem is page 84 of the textbook (pg 106 of the pdf).</p>",2019-09-01T03:18:07Z,63,Week 9/1 - 9/7,feedback,,ixty1midfufhd,k00endgoe3b14r,2019-09-01T03:18:07Z,{},hw1
601,no,<p>Can you share the pdf? I only found the 2nd edition online. Not sure if I find the correct examples. Still feel lost on defining the MDP in this hw</p>,2019-09-01T06:39:53Z,63,Week 9/1 - 9/7,feedback,,j6ln9puq99s5uv,k00luuqmyi32so,2019-09-01T06:39:53Z,{},hw1
602,no,"<p>The pdf I was referring to was the one provided from the canvas course resources section of the syllabus: <a href=""http://www.incompleteideas.net/book/the-book-2nd.html"">http://www.incompleteideas.net/book/the-book-2nd.html</a></p>",2019-09-01T19:09:50Z,63,Week 9/1 - 9/7,feedback,,ixty1midfufhd,k01cnae43pq1b4,2019-09-01T19:09:50Z,{},hw1
603,stud,<p>where do we access this? I don&#39;t see it in the files. </p>,2019-09-08T21:52:46Z,62,Week 9/8 - 9/14,feedback,a_2,,k0bijsj9taj50j,2019-09-08T21:52:46Z,{},hw1
604,no,"<p>The page on the link above has the book available for free. You have &#34;Full Pdf&#34; or &#34;Without Margins&#34;, just click one and download it.</p>",2019-09-09T03:02:34Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0btm7c3id848p,2019-09-09T03:02:34Z,{},hw1
605,no,"<p>I would suggest, climbing up a ladder, as a good way to think of it.   if you slip, you fall to the ground (0); if you are on the ladder, you can climb up to get to a window where you can exit (on floor x); or you can keep trying to climb (to go higher) - provided you haven&#39;t fallen, or gotten off already (on floor x).</p>",2019-09-01T19:40:49Z,63,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k01dr50apnc118,2019-09-01T19:40:49Z,{},hw1
606,no,<p>Nice.</p>,2019-09-01T20:10:09Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k01esva79xo2sn,2019-09-01T20:10:09Z,{},hw1
607,no,"<p>thank you.  metaphorically speaking, its not far from the first structure I built; but I slimmed it down to a direct walk once I got the alg running.   The &#34;George Jefferson&#34; MDP :)</p>",2019-09-02T21:03:30Z,63,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k02w5bxmji83ns,2019-09-02T21:03:30Z,{},hw1
608,no,<p>You actually made me google that!</p>,2019-09-03T02:12:04Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k03764y4m195w7,2019-09-03T02:12:04Z,{},hw1
609,no,"<p>LOL :)  &#34;moving on up, (moving on up), to a deluxe apartment, in the sky-y-y&#34; :)</p>
<p></p>
<p>wink. :)</p>
<p></p>
<p></p>",2019-09-03T15:47:24Z,63,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k040anz5nig30h,2019-09-03T15:47:24Z,{},hw1
610,no,"<p><a href=""https://artint.info/html/ArtInt_227.html"">https://artint.info/html/ArtInt_227.html</a></p>
<p></p>
<p>The above explanation was linked during office hours, and it made me feel super confident that I understood what was going on, but apparently not, since I cannot seem to get the correct answers. I traced through the problem in excel before trying to do it in python, so I&#39;m certain its an issue with my understanding and not my code (for now anyways). </p>
<p></p>
<p>For the N=6 test case, V0 is initialized to all zeros. For V1[0] there are two actions, roll or quit. Roll is then subdivided into four cases, (1/6)(4&#43;V0[4]), (1/6)(5&#43;V0[5]), (1/6)(6&#43;V0[6]), (1/2)(0&#43;V0[0]) and taking their sum gives me 2.5 (since all of the V0 values are 0). For quitting, (1)(0&#43;V0[0]) gives 0, which is less than 2.5 so V1[0] would be 2.5. </p>
<p></p>
<p>2.5 is obviously not 2.5833, so I&#39;m definitely missing something. There also has to be some error in my above logic, since finding V2[0] is giving me 6.25 which is even worse off. Any suggestions as to what I am missing?</p>",2019-09-02T15:46:06Z,63,Week 9/1 - 9/7,followup,,jl2bq5rf8b67pq,k02kt50imu868f,2019-09-02T15:46:06Z,{},hw1
611,no,"<p>You are in the right track, but remember you have to complete that operation for all states S, and then continue with iteration k&#43;1, k&#43;2, and so on until there is no change (or minimal change) in the value function.</p>
<p></p>
<p>Also, are you taking into account the probability of losing everything from the roll action? I see your (1/2)(0&#43;V0[0]), but that is not saying the player <em>will lose it all</em>. If I was you I would create a terminal state, say state 100, and make the &#34;game over&#34; transitions take all the money away you have given (which you can get with the state, so -state?). BTW, you could also design the reward function to only pay the player at the end of the game when they quit (&#43;state?).</p>
<p></p>
<p>For context, here is the problem:</p>
<p></p>
<p>Input: N = 6, isBadSide = {1,1,1,0,0,0}, Output: 2.5833</p>
<p></p>
<p dir=""ltr"">$$V_{k&#43;1}(s) = max_a \sum_{s&#39;}{ T(s,a,s&#39;) [ R(s,a,s&#39;) &#43; \gamma V_{k}(s&#39;)]}$$</p>
<p> </p>
<p>Keep going, you&#39;re getting there.</p>
<p></p>",2019-09-02T15:58:39Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02l9a785wt5d9,2019-09-02T15:58:39Z,{},hw1
612,no,"<p>Thanks so much! I went through and added a terminal state with -s as the reward for when we roll a bad side, as well as a while loop till V converges, but it doesn&#39;t seem to have made much of a difference in my results. I also added a separate terminal state for quitting, since it seemed wrong to go back to state 0 for cashing out. I do have two questions that might help:</p>
<p></p>
<p>(1) Assume I&#39;m finding V2, and I get that at state three we should cash out. At that point, should I continue to do the calculations for state four and on, or should I quit and move on to calculating V3 and so forth until I converge?</p>
<p></p>
<p>(2) I&#39;m not sure when I should be updating V for the two terminal states. It would make sense that whenever I get that I should quit, I should update V[quit] with the exit value, but there doesn&#39;t seem to be a definitive time for updating V[lose], assuming I should be updating these at all?</p>",2019-09-02T17:00:53Z,63,Week 9/1 - 9/7,feedback,,jl2bq5rf8b67pq,k02nhb8h71z4m4,2019-09-02T17:00:53Z,{},hw1
613,no,"<p>Good thinking!</p>
<p></p>
<p>First, realize you (or your agent) are not interacting with this environment. Value Iteration (and other Dynamic Programming methods) plan/search/compute an optimal value function and optimal policy. So, you must continue to do all the calculations until the value function stops changing. That is, you stop the iteration only when &#96;max(np.abs(V_t - V_tplus1)) &lt; theta&#96;... that is the maximum change on any state is less than theta, a small value, say 0.000000001. So, remember, you will update all states on every k, and keep going for a many k&#39;s until convergence.</p>
<p></p>
<p>Second, by definition, the value function of a terminal state is 0. How much reward should you expect after death?! (sorry, no heavens here) There is no expected cumulative future discounted reward <em>after</em> a terminal state, so zero!</p>
<p></p>
<p>Remember, V is the &#34;expected cumulative future discounted reward.&#34; So, you shouldn&#39;t expect much on a terminal state; conversely, you should expect more from earlier states. </p>",2019-09-02T21:58:01Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02y3fidoey3hh,2019-09-02T21:58:01Z,{},hw1
614,no,"<p>I&#39;m still really stuck here because as i write out my transitions i am having trouble figuring out how they are supposed to be changing:</p>
<p></p>
<p>v0 is all initialized to zero</p>
<p>v1 you get 2.5 like Caroline describes above</p>
<p>For v2, I must be confused because none of the other boxes cause a change:</p>
<p>4/5/6 are all going to have the cash out option which will lead to a 100% chance of receiving the exact reward</p>
<p>conversely,1/2/3 are all automatic losses which lead to a 100% loss before reaching there.</p>
<p></p>
<p>This continue to leave me at a max of 2.5. It will even drop me to v3[0] to 1.25 if i set my v1[0] to 2.5 since the expected loss of 1/2/3 is now higher. I am clearly doing something wrong</p>
<p></p>
<p>I have read in Sutton&#39;s book the gambler&#39;s problem where he sets terminal state to 1 and it is now based on probability of reaching his goal of 100$. I can&#39;t figure out if there is a way to make that fit here since we don&#39;t have a &#34;goal&#34; to reach, simply trying to maximize and avoid bankruptcy. <br /><br /></p>
<p>In the course examples, when we choose an action (like a RIGHT into the terminating state) we work backwards from that &#43;1 as it propagates out to the rest of the matrix that was filled with 0s. Without trying to use the gambler&#39;s problem analogy and assigning a random value to the terminating state, i don&#39;t know how we work backwards for this model.</p>",2019-09-02T23:25:06Z,63,Week 9/1 - 9/7,feedback,,jzttp1ojahj6ju,k0317f488q75xt,2019-09-02T23:25:06Z,{},hw1
615,no,"<p>&#34;4/5/6 are all going to have the cash out option which will lead to a 100% chance of receiving the exact reward&#34;</p>
<p></p>
<p>Double check this.</p>",2019-09-02T23:36:55Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k031mmlkfbs3vd,2019-09-02T23:36:55Z,{},hw1
616,no,"<p>So i am double checking this and feel like maybe it is my error point because it keeps coming back consistent. Since the chance of making money is lower than the current value, it keeps pointing to choosing the cash out option indicated in the code below as accepting v[i] where i can be 4/5/6. Transition is 1/N or .16 in this case.</p>
<p></p>
<p>max (v[i], sum((transition*(4&#43;v[i]),transition*(5&#43;v[i]),transition*(6&#43;v[i]),(.5*(0-v[i]))))))</p>
<p></p>
<p>The above code logically follows for me because we are taking both actions:</p>
<p>      cash out as representated by v[i]</p>
<p>      roll the dice represented by the summation of each possible outcome&#39;s reward and multiplied by it&#39;s likelihood (.16)</p>
<p></p>
<p>I recognize there is probably an error here because i haven&#39;t worked backwards from a terminating state but i can&#39;t for the life of me figure out how to fix it.</p>",2019-09-03T00:01:17Z,63,Week 9/1 - 9/7,feedback,,jzttp1ojahj6ju,k032hyog2nr6o7,2019-09-03T00:01:17Z,{},hw1
617,no,"<p>I see your problem.  I&#39;m not sure how appropriate it is to post code and/or address bugs since this is a homework assignment, but here&#39;s a hint: think carefully about what the rewards/state-values should be under each action.</p>
<p></p>
<p>Edit: Actually, a couple of people in this thread had the same exact issue you&#39;re having.</p>",2019-09-03T00:12:04Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k032vu062757k1,2019-09-03T00:12:04Z,{},hw1
618,no,"<p>Yeah, no code, please. Equations, discussions, and even hand-worked <em>partial</em> solutions of a single example problem are fair game. </p>",2019-09-03T01:26:27Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k035jhczdpw5x7,2019-09-03T01:26:27Z,{},hw1
619,no,"<p>Definitely interesting, my V1, V2 and so on actually increase instead of decreasing, though mine increase way too much. Make sure, each V0, V1, etc. should be another iteration where you look at all of the states again, but look at the value you&#39;re adding to the reward: transition*(4&#43;v[i]), make sure that v[i] is actually looking at v(k-1)[4]. In that, the value you add in is both from the previous iteration and is also based on the new state (s&#39;). So if you&#39;re looking at V1, in state 0 with the new state 4, your added in value should be V0[4], not V1[0]. </p>
<p></p>
<p>Feel free to take with a grain of salt, my stuff still doesn&#39;t work, but I think I *might* see where your problem may be.</p>",2019-09-03T02:19:24Z,63,Week 9/1 - 9/7,feedback,,jl2bq5rf8b67pq,k037fl34z084mm,2019-09-03T02:19:24Z,{},hw1
620,stud,"<p>I feel like such an idiot...</p>
<p></p>
<p>If you&#39;re programming vi in python, remember that range(1, N) is only going to iterate through values 1...N-1. You need to take range(1, N&#43;1) to get values from 1...N. I overlooked this tiny detail and it screwed me up for days. I was iterating though the possible outcomes of a roll of the die but <em>truncating</em> by one die face value (i.e., N) each time. Fixing this, my original implementation of vi (very much like what Caroline describes above) calculates the correct value of $$s=0$$ to three decimal places. And it&#39;s only a few lines of code and uses nothing more than a list and some nested for loops.</p>
<p></p>
<p>I wish I had spotted this before turning to pymdptoolbox : \</p>
<p></p>",2019-09-03T18:40:11Z,63,Week 9/1 - 9/7,feedback,a_0,,k046gv7fo33zd,2019-09-03T18:40:11Z,{},hw1
621,no,<p>On the positive side that&#39;s one mistake you&#39;ll probably never make again.</p>,2019-09-03T19:42:59Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k048pmmcko32in,2019-09-03T19:42:59Z,{},hw1
622,no,"<p>&#34;4/5/6 are all going to have the cash out option which will lead to a 100% chance of receiving the exact reward&#34;  ==&gt; &#34;Double check this.&#34;</p>
<p></p>
<p>Any additional hints available on this?</p>
<p></p>
<p>If a player&#39;s current bankroll is 4 (for example), and they choose to cash out, there is 100% chance they end the game with a value of 4 and no additional future value expected.</p>
<p></p>
<p>If we assign rewards only at terminal states with rewards equal to the balance at the terminal state, it seems accurate to say the rewards should be 4,5,6 respectively for cashing out with balances of 4,5,6.</p>",2019-09-03T19:45:06Z,63,Week 9/1 - 9/7,feedback,,isde34zracb1mz,k048sd49cjj35l,2019-09-03T19:45:06Z,{},hw1
623,no,"<p>So the expected value of the decision, CASH OUT, for the triple $$(4,5,6)$$ is $$(4,5,6)$$, as you said.  What is the expected value of <em>not</em> cashing out for those three values?</p>",2019-09-03T19:53:28Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k049344gwhs64f,2019-09-03T19:53:28Z,{},hw1
624,no,"<p>I can&#39;t make sense of this either. My value for rolling should be 3/6 * -V0[s] &#43; 1/6 * (4 &#43; V0[s&#39;]) &#43; 1/6 * (5 &#43; V0[s&#39;]) &#43; 1/6 * (6 &#43; V0[s&#39;]) and my value for quitting should be V0[s]. Therefore, my updated value for the next iteration should be the greater of those two values. However, I converge to 2.5.</p>",2019-09-03T22:09:27Z,63,Week 9/1 - 9/7,feedback,,jqst1pv5Ozx5,k04dxzl2f8x50b,2019-09-03T22:09:27Z,{},hw1
625,no,"<p>Matthew,</p>
<p></p>
<p>Try to calculate your values using the formula that Miguel first posted on this thread.</p>
<p></p>
<p>$$V_{k&#43;1}(s)=max_{a}∑_{s′}T(s,a,s′)[R(s,a,s′)&#43;γ V_{k}(s′)]$$</p>
<p></p>
<p></p>
<p>Also, I think you may have misunderstood what V0[s] means.</p>
<p></p>
<p>V0[s] is the value at k=0 in the state s in the value iteration algorithm. You are assigning your value for quitting to be V0[s], but it should actually be only s (since the reward is s and the value of a terminal state is 0).</p>
<p></p>
<p>Check if you understood what I said and review how you are updating the value for rolling and quitting.</p>
<p></p>
<p>I hope it helps.</p>
<p></p>
<p></p>",2019-09-05T21:26:55Z,63,Week 9/1 - 9/7,feedback,,jc6xvgjncoey,k077azs9ir57jp,2019-09-05T21:26:55Z,{},hw1
626,no,"To clarify, I&#39;m using s to represent 1 time step (#4 in PDF). Since the game has an infinite horizon, I limit the time steps calculated. In my implementation, R(s, a, s&#39;) is different from s.",2019-09-06T02:57:59Z,63,Week 9/1 - 9/7,feedback,,jqst1pv5Ozx5,k07j4qv2n6q76,2019-09-06T02:57:59Z,{},hw1
627,no,"<p>Has someone got the value iteration algorithm working but with the results multiplied by 2?</p>
<p></p>
<p>My algorithm passes all cases if I divide the result by 2 (I know, it is funny!), but I can&#39;t see where it is wrong.</p>",2019-09-02T23:52:13Z,63,Week 9/1 - 9/7,followup,,jc6xvgjncoey,k0326atwq1r5oj,2019-09-02T23:52:13Z,{},hw1
628,no,"<p>Maybe check your probabilities. It seems like a weird issue, nothing I can recall as a common pitfall.</p>",2019-09-03T02:11:00Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0374rwkhu536x,2019-09-03T02:11:00Z,{},hw1
629,no,<p>Haha. My algorithm does the same thing. Have you found the root cause in your case?</p>,2019-09-03T13:57:29Z,63,Week 9/1 - 9/7,feedback,,jl1b27fpaYkv,k03wdbgd6c872q,2019-09-03T13:57:29Z,{},hw1
630,no,<p>when mine did this I was adding the reward twice. where could you be doing that?? hmmmmm</p>,2019-09-03T13:59:16Z,63,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k03wflwwzmn6ny,2019-09-03T13:59:16Z,{},hw1
631,no,"<p>Daniel and Jacob,</p>
<p></p>
<p>I still can&#39;t find what I&#39;m doing wrong. I don&#39;t see any place where I am adding the reward twice.</p>
<p></p>
<p>It may be the way I compute the probabilities, I will check that later today.</p>",2019-09-03T14:49:46Z,63,Week 9/1 - 9/7,feedback,,jc6xvgjncoey,k03y8k8fpr11ks,2019-09-03T14:49:46Z,{},hw1
632,no,<p>One thing to try is to do the simple (N = 6) example by hand.  That may help you pinpoint the bug in your code.</p>,2019-09-03T14:57:44Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03yisjkzsa3ar,2019-09-03T14:57:44Z,{},hw1
633,no,"<p>A question that just came up to my mind: </p>
<p>When processing the value iteration, should I consider the states that are unlikely to happen?</p>
<p>For example, in a N=6 die, the states S=1, 2, 3 are not likely to happen. Therefore, how should I threat these unlikely-to-happen states in the for loop of every state in value iteration algorithm?</p>
<p></p>",2019-09-03T20:29:54Z,63,Week 9/1 - 9/7,feedback,,jc6xvgjncoey,k04adyytdjl3mk,2019-09-03T20:29:54Z,{},hw1
634,no,"<p>All states should be updated during each sweep of value iteration, since you don&#39;t know ahead of time what the input isBadSide will look like.  If you&#39;re doing value iteration by hand for a particular input, and a state is impossible to reach, then that state won&#39;t be on the RHS of any of your assignments.  If it&#39;s not on the RHS of any assignment, then you really don&#39;t need to compute it.</p>",2019-09-03T21:21:00Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04c7oi7v09s4,2019-09-03T21:21:00Z,{},hw1
635,no,"<p>While I am working on this hw, I&#39;d like to clarify some questions in my head about value iteration.</p>
<p></p>
<p>Here is the general core algorithm you can find anywhere on the internet. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9puq99s5uv%2Fk0365029tlu6%2Fpseudotmp2.png"" alt="""" /></p>
<p></p>
<p><strong>My first question is: </strong></p>
<p>In the first loop, we need to update the V(s) for all s.This should be called one &#96;episode&#96;, is that correct?</p>
<p><strong>My second question is:</strong></p>
<p>Within each episode, we loop through all states. However, when we are at a certain state, for example, state 0 (in this case, with 0 bankroll), do we need to continue to next states after updating a state? For example, at state 0, we rolled 4 (which is not a bad size), then we are at state 4, and continue. In this case, should we <strong>update state 0, 1, 2, ..., </strong>or <strong>update state 1, 4, ...?</strong> Hope my explanation is clear. Basically, the question is if we need to  <strong>go to </strong>the next state after updating the current state. </p>
<p>Thanks</p>
<p></p>",2019-09-03T01:42:05Z,63,Week 9/1 - 9/7,followup,,j6ln9puq99s5uv,k0363krzm326cg,2019-09-03T01:42:05Z,{},hw1
636,no,"<p>First question:  I believe &#39;episode&#39; is reserved for a completion of a task in an MDP setup.  In this problem, it would be playing the game once.  I think you should call each run through the loop an <strong>iteration</strong>.</p>
<p></p>
<p>Second question: When you &#34;roll&#34; as part of your calculation for updating a state, it has no impact on the older version of the state values (the previous iteration&#39;s values).  So you can update the states in any order you wish.  You just have to update them all before going on to the next iteration.</p>",2019-09-03T02:03:29Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k036v3unhnu27i,2019-09-03T02:03:29Z,{},hw1
637,no,"<p>Awesome, Vahe!</p>
<p></p>
<p>Iteration is the right word here. Just to clarify, while running value iteration there are no interactions with the environment whatsoever!! After VI completes and returns an optimal value function and optimal policy, then the agent can just run the policy on the environment. But while running value iteration there are no interactions, no time steps, no episodes.</p>
<p></p>
<p>The first loop (the &#96;repeat&#96;) is called an iteration, the second loop (the &#96;for each s in S&#96;) is called a sweep. Sorry, I didn&#39;t name these things.</p>
<p></p>
<p>Remember, there are no interactions here! You are just planning, computing, calculating what would be the best thing to do given an MDP.</p>
<p></p>
<p>Hope that is clear, it really is the key for enjoying this homework.</p>",2019-09-03T02:10:20Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0373wnrdv426n,2019-09-03T02:10:20Z,{},hw1
638,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>    is the state-value function the same as the utility function or list of utility?</p>",2019-09-06T03:01:42Z,63,Week 9/1 - 9/7,followup,,gx3c8l7z7r72zl,k07j9iwxsp3bh,2019-09-06T03:01:42Z,{},hw1
639,no,"<p>Same thing.</p>
<p></p>
<p>A state-value function $$V$$ and the utility function $$U$$ are functions (in our case, simply tables) indicating the expected return from a given state $$s$$. Intuitively, they tell you how much reward you should expect from a state.</p>
<p></p>
<p>Capital $$V$$ is commonly used for <em>estimates</em> of the true state-value function lowercase $$v$$.</p>
<p></p>
<p>You will also see $$*$$ on them when the true state-value function is the optimal state-value function, so $$v^*$$. But sometimes you are just estimating an arbitrary non-optimal policy $$\pi$$. In that case the true state-value function of policy $$\pi$$ is $$v_{\pi}$$.</p>
<p></p>
<p>Hopefully that starts painting the whole picture.</p>",2019-09-06T04:01:32Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07lehblm9316,2019-09-06T04:01:32Z,{},hw1
640,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6lnhjhnzr3y5%2Fjzzv7p9n352p%2FScreen_Shot_20190831_at_11.13.18_AM.png"" alt="""" /></p>
<p></p>
<p></p>
<p><strong>Got &#34;This page isn&#39;t working&#34; for page &#34;<a href=""http://rldm.herokuapp.com/"">https://rldm.herokuapp.com/</a>&#34;.   Have tried for many times under different browsers.</strong></p>
<p><strong></strong></p>
<p></p>
<p></p>",2019-08-31T18:19:17Z,40,Week 8/25 - 8/31,followup,,j6lnhjhnzr3y5,jzzveg5pekp14s,2019-08-31T18:19:17Z,{},hw1
641,no,<p>FWIW https://status.heroku.com/</p>,2019-08-31T18:42:39Z,40,Week 8/25 - 8/31,feedback,,jc6utnpzmnw1xt,jzzw8hvxzgh3gh,2019-08-31T18:42:39Z,{},hw1
642,no,<p>Thanks.  The page works now.</p>,2019-09-01T03:29:19Z,40,Week 8/25 - 8/31,feedback,,j6lnhjhnzr3y5,k00f1sc4dx6vj,2019-09-01T03:29:19Z,{},hw1
643,no,"<p>thanks, this was a fun test case!</p>
<p></p>
<p>i found that it helped to calculate the number of useful states first (so value iteration wasn&#39;t updating a bunch of unnecessary states along the way).</p>",2019-08-31T20:16:04Z,33,Week 8/25 - 8/31,followup,,jzifg1e23c29s,jzzzkmsatol113,2019-08-31T20:16:04Z,{},hw1
644,no,<p>-deleted-</p>,2019-09-01T19:58:17Z,32,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k01edleygao5n3,2019-09-01T19:58:17Z,{},hw1
645,no,"<p>Make sure not to share any example outside the ones we provide. Once the website opens, these examples can be assigned problems.</p>",2019-09-01T20:09:25Z,32,Week 9/1 - 9/7,followup,,hyx9thiqa6j4nn,k01erxikawd265,2019-09-01T20:09:25Z,{},hw1
646,no,<p>a fair point ;) I shall delete it. :)</p>,2019-09-02T21:07:06Z,32,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k02w9yd74znhc,2019-09-02T21:07:06Z,{},hw1
647,no,<p>This particular example is safe since $$N = 36 &gt; 30$$. :P</p>,2019-09-02T21:35:38Z,32,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k02xancysg94mf,2019-09-02T21:35:38Z,{},hw1
648,no,<p>True... still! :)</p>,2019-09-02T21:46:02Z,32,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02xo14v5es6y7,2019-09-02T21:46:02Z,{},hw1
649,no,"<p>Yes, thanks, I saw that, but there are also docs on the net with solutions, like the one I linked to.  </p>
<p></p>
<p>I admire Sutton&#39;s effort to reply to everyone individually, but I&#39;m  not going to try to solve all the problems when all I want, is, for instance, the answer to one prob 3.5... </p>",2019-09-01T00:24:41Z,40,Week 8/25 - 8/31,followup,,jzh6k6o994a6dh,k008gcgc4br31m,2019-09-01T00:24:41Z,{},hw1
650,no,<p>Have you tried solving prob 3.5?</p>,2019-09-01T15:53:05Z,39,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k015m9vr37g2li,2019-09-01T15:53:05Z,{},hw1
651,no,"<p>yes actually, but I&#39;m more focused on 3.9 because it shows how to mathematically solve Bellman&#39;s equations for a 2 state, 2 action robot, which is actually what (I think) we have in hw1.</p>
<p>Two states: &#39;what do I do next?&#39; and &#39;it&#39;s over&#39;.  I use eq  3.19 like in the exercise, but the formula I get does not produce the numbers you&#39;ve given us... </p>
<p>I won&#39;t have time to try to solve all exercises in the book, TD is around the corner, so I have to focus and having a cheat sheet for the exercises would have been good.  </p>
<p>But if not, that&#39;s ok.  I have to stop being exhaustive :-)</p>
<p></p>
<p>I &#39;wasted&#39; too much time on Littman ch2 while I should have started reading Sutton sooner, so now I&#39;m trying to catch up.  </p>
<p></p>
<p>In fact, when I see the exercise and the theory it involves, I would have been good with watching the videos of lesson2 and reading ch3 about markov.  </p>
<p>I feel it&#39;s a good strategy from now on, <span style=""text-decoration:underline"">before</span> doing any additional reading.</p>
<p>Can you confirm this?</p>
<p></p>",2019-09-01T20:36:26Z,39,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k01fqnt4jhc3ey,2019-09-01T20:36:26Z,{},hw1
652,no,"<p>From my experience active learning is always better than passive learning.  This means constantly trying to solve problems, whether it&#39;s an exercise in Sutton or something from the homework.  When working on a problem you will often realize that your understanding of a concept is much worse than you thought it was while you were reading the text or listening to the lecture.  You will also gain motivation for why the theory is important, which makes understanding the theory much easier.</p>
<p></p>
<p>Like Miguel said in one of his office hours, (paraphrasing) &#34;Don&#39;t stop just because you think you understand something -- keep going.&#34;</p>",2019-09-01T23:18:05Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01lijrqj7i47n,2019-09-01T23:18:05Z,{},hw1
653,no,"<p>Incidentally, I did both 3.5 and 3.9 (for the 2nd edition) so if you have questions about them just ask here.  Your description of Exercise 3.9 doesn&#39;t match what it looks like in the 2nd edition.</p>",2019-09-02T00:16:07Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01nl6mpi24li,2019-09-02T00:16:07Z,{},hw1
654,no,"<p>Ha, ha, that&#39;s so true.  I&#39;m realizing that now, by trying to do the homework.  That&#39;s why I&#39;m reading ch 3 on MDP.  Everytime I re-read sthg the next day, I still find things I didn&#39;t understand fully.  </p>
<p>Thanks for the advice.  </p>",2019-09-02T00:16:43Z,39,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k01nlxxpm4z644,2019-09-02T00:16:43Z,{},hw1
655,no,"<p>&#64;Vahe... I checked and it IS the second edition.  but  it&#39;s <span style=""text-decoration:underline"">example</span> 3.9, not ex 3.9, my mistake.</p>
<p>thanks for the offer, I may take you up on it</p>",2019-09-02T00:19:31Z,39,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k01npk3dpom2vq,2019-09-02T00:19:31Z,{},hw1
656,no,"<p>Yeah, Exercise 3.23 is the continuation of Example 3.9.  I did that too - it was pretty brutal...</p>",2019-09-02T01:36:48Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01qgxim11s2h1,2019-09-02T01:36:48Z,{},hw1
657,no,"<p>Thanks for the answer Miguel,</p>
<p></p>
<p>I thought I needed to follow the TD(lamda) rule as defined in the lectures and readings with several different values of lamda to find the correct one. Your statement makes me think that this isn&#39;t the case, since I don&#39;t need to calculate different value estimates now, and just need to find the lamdas that match? I guess we are not needing to use the TD(lamda) Episode T update rule then? I&#39;ll need to reevaluate my approach to the problem.</p>
<p></p>
<p>A second question, based on the plots from the lecture and reading, I don&#39;t really see how a lamda of 0.6226 would be the same as a lamda of 1... it seems like they formed a downward curve for the error, but I guess we are not actually calculating out the error at certain lamda, but instead just doing TD(lamda) equation at the top of the homework. I think I talked myself into that I was doing the wrong thing...</p>",2019-09-01T19:05:20Z,44,Week 9/1 - 9/7,followup,,ixty1midfufhd,k01chifikp052o,2019-09-01T19:05:20Z,{},hw2
658,no,"<p>I think you are getting close to it.</p>
<p></p>
<p>Do me a favor, calculate the TD(1) for one of the sample problems and share here. Then calculate the k-step estimators for 1-step, 2-step, ..., infinity-step. Put them down below.</p>",2019-09-02T22:01:00Z,44,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02y79xjre23o,2019-09-02T22:01:00Z,{},hw2
659,no,"<p>Hi Miguel,</p>
<p></p>
<p>I think what I&#39;m getting caught up on based on other information I&#39;ve seen. Based on the homework, the TD(lamda) is the sum of all the k-step estimates; and based on the lecture, we know that the only estimator that matters is the infinite step, as follows following:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixty1midfufhd%2Fk035grklbd6%2FTDPic.PNG"" alt="""" /></p>
<p></p>
<p>From this, I&#39;m confused, since it&#39;s been stated that since we are not learning something, the alpha isn&#39;t needed, but it&#39;s in the equation we have.... But if I took alpha<sub>T</sub> to be 1 (to ignore it), then it&#39;s really just the sum of the the discounted rewards minus the state; and since the discount is 1, then I would get the sum of the rewards.</p>
<p></p>
<p>so, TD(1) = 0.0 &#43; (0.81(7.9&#43;2.5) &#43; 0.19(-5.1&#43;-7.2) &#43; 9 &#43; 0.0 &#43; 1.6 - 0.0 ==&gt; 16.687?</p>
<p>But this is mostly doing the same type of operation that Charles did for the Value Computation Example, which is partially why I&#39;m confused, since we don&#39;t need to update the values...</p>
<p></p>
<p>Calculating out the k step estimators using the following:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixty1midfufhd%2Fk036b4slj7wt%2FTD2.PNG"" alt="""" /></p>
<p>I get:</p>
<p>1 step) 0 &#43; 0.81 (7.9&#43;4) &#43; 0.19(-5.1&#43;25.7) -0 ==&gt; 13.553</p>
<p>2 step) 0 &#43; 0.81(7.9&#43;2.5) &#43; 0.19(-5.1&#43;-7.2) &#43; 0 - 0 ==&gt; 6.087</p>
<p>3 step) 0 &#43; 0.81(7.9&#43;2.5) &#43; 0.19(-5.1&#43;-7.2) &#43; 9 &#43; 20.1 - 0 ==&gt; 35.187</p>
<p>4 step) 0 &#43; 0.81(7.9&#43;2.5) &#43; 0.19(-5.1&#43;-7.2) &#43; 9 &#43; 0 &#43; 12.2 - 0 ==&gt; 27.287</p>
<p>5 step) 0 &#43; 0.81(7.9&#43;2.5) &#43; 0.19(-5.1&#43;-7.2) &#43; 9 &#43; 0 &#43; 1.6 &#43; 0 - 0 ==&gt; 16.687</p>
<p>then adding in the lamdas</p>
<p>1 step) (1-.6226).622^1-1 = 0.377 * 13.553 = 5.1144</p>
<p>2 step) (1-.6226).622^2-1 = 0.234 * 6.087 = 1.430209</p>
<p>3 step) (1-.6226).622^3-1 = 0.146 * 35.187 = 5.147</p>
<p>4 step) (1-.6226).622^4-1 = 0.091 * 27.287 = 2.458</p>
<p>5 step) (1-.6226).622^5-1 = 0.056 * 16.687 = 0.946</p>
<p>which adds to 15.095 for the sum of value. Since I ran out of states, I stopped calculating, since the last part is the step we end up in.... but if it still needed to keep going over and over, it should just be that last value getting smaller and smaller but added in.</p>
<p></p>
<p>Thoughts on where I went wrong?</p>",2019-09-03T02:13:50Z,44,Week 9/1 - 9/7,feedback,,ixty1midfufhd,k0378exaxra5et,2019-09-03T02:13:50Z,{},hw2
660,no,"<p><em>&#34;</em><em>we know that the only estimator that matters is the infinite step&#34;</em> &lt;-- I&#39;m not sure this statement is correct.</p>
<p></p>
<p><em>&#34;From this, I&#39;m confused, since it&#39;s been stated that since we are not learning something, the alpha isn&#39;t needed, but it&#39;s in the equation we have.... But if I took alphaT</em><em> to be 1 (to ignore it), then it&#39;s really just the sum of the the discounted rewards minus the state; and since the discount is 1, then I would get the sum of the rewards.&#34;</em> &lt;--- almost correct, but either way it will work. Because we are not learning you don&#39;t need the &#96;-V(St)&#96;, and you don&#39;t need the &#96;V(St)&#96;... the good thing is those two will cancel out.</p>
<p></p>
<p>You&#39;re having a good start. I wouldn&#39;t stop at 5 steps, also, can you calculate an infinite-step?</p>
<p></p>
<p>Also, you are not normalizing the weights. Check out this image:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fk0388fa54yyk%2Fweights.png"" alt="""" width=""581"" height=""391"" /></p>
<p></p>
<p></p>",2019-09-03T02:42:23Z,44,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k03894y8n4g5n0,2019-09-03T02:42:23Z,{},hw2
661,no,"<p>I get exactly what George gets, however, how do you go past step 5 (16.687)? Are the remaining K step estimators just 16.687?  I suppose after step 5, it&#39;s a zero reward transition back into 6?  Not sure how that changes anything.  For me TD(1) goes to 0 -- because (1-l) = 0 -- perhaps it doesn&#39;t analytically.</p>",2019-09-03T21:45:25Z,44,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k04d33d4hoo561,2019-09-03T21:45:25Z,{},hw2
662,no,<p>I&#39;m also confused how to resolve this issue. How to we have an estimator after step 5?</p>,2019-09-04T19:47:19Z,44,Week 9/1 - 9/7,feedback,,jzjr6qef6i06tp,k05ob1vcn6d3zv,2019-09-04T19:47:19Z,{},hw2
663,stud,<p>I would too like some clarity on this. I am basically stuck after finding $$G_{t}$$ and setting it to my solve for $$\lambda$$</p>,2019-09-05T01:15:27Z,44,Week 9/1 - 9/7,feedback,a_1,,k06011s2k0q2x4,2019-09-05T01:15:27Z,{},hw2
664,no,<p>&#34; it&#39;s a zero reward transition back into 6? &#34; You getting there... what is the definition of a terminal state?!?! Anyone?!</p>,2019-09-05T01:50:42Z,44,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k061ad6hils15p,2019-09-05T01:50:42Z,{},hw2
665,no,"<p>Hi Miguel,</p>
<p></p>
<p>Sorry, I wasn&#39;t able to do anything the last few days due to prior commitments, but thanks for getting back to me. As you suggested, I didn&#39;t stop at 5 and continued on for 1000 steps and see that the value get&#39;s very close to the expected TD(1) value.</p>
<p></p>
<p>As far as calculating the infinite step, I understand that I can&#39;t manually do this and will need to apply some additional math to calculate the true sum for the series, but I like doing pen and paper to work through the steps to get a conceptual understanding of the algorithm and how it updates numbers.</p>
<p></p>
<p>In any case, I&#39;m not sure exactly what you mean by I&#39;m not normalizing the weights... I was applying the $$(1-\lambda )\lambda^{k-1}$$ equation (adjusting k to the step number) to the start of my values (was the second step 1-5 numbers). Am I missing something else?</p>
<p></p>
<p>Thanks,<br />George</p>",2019-09-05T03:42:00Z,44,Week 9/1 - 9/7,feedback,,ixty1midfufhd,k0659hv1efl7d7,2019-09-05T03:42:00Z,{},hw2
666,no,"<p>I&#39;m also struggling with the infinite part, something is not quite right (perhaps in my code) but I´ll add my thinking and perhaps we can figure it out together. So we have:</p>
<p>$$TD(\lambda ) =\sum_{k=1}^{\inf} (1-\lambda )\lambda ^{k-1}*E_{k}$$ with $$E_{k} = (\sum_{i}^{k} R_{i})&#43;V(S_{k})$$ (since gamma is 1).</p>
<p>Since we have a terminal state, every reward after that is going to be 0, so past a certain point $$E_{k}$$ is always going to be the same (16.687 in example 1) lets call that $$E_{\inf}$$. That means we have in this case:</p>
<p> $$\sum_{k=1}^{5}(1-\lambda)\lambda^{k-1}E_{k} &#43; \sum_{k=6}^{\inf} (1-\lambda)\lambda^{k-1}E_{\inf}$$.</p>
<p>Is this on the right track? It should be possible to simplify the second sum right, but I´m not quite sure how to do it? I tried doing it like in the lecture but the answer does not turn out right.</p>
<p>Edit:</p>
<p>Also I suppose TD(1)=$$E_{\inf}$$?</p>",2019-09-05T08:20:58Z,44,Week 9/1 - 9/7,feedback,,jzjwcq2u8o7110,k06f89nt2zf6d,2019-09-05T08:20:58Z,{},hw2
667,no,"<p>This is what  I am doing as well.  Using $$TD(1)=E_{inf}$$, I go out up to k = 10,000 for the given $$\lambda$$= 0.6226326309908364, and the closest I get to $$G_{t}$$ (which I have as 16.687) is 16.685720572103648, which is close not correct.  I suppose there is something wrong with either how I am calculating $$G_{t}$$, or the way I am calculating the estimators after step 5.  Since after about k =30,  the termination states are be computing so small they will never get me to by   $$G_{t}$$ = 16.687.  </p>",2019-09-05T14:22:49Z,44,Week 9/1 - 9/7,feedback,,jzjr6iihb286bm,k06s5ln3oit3oc,2019-09-05T14:22:49Z,{},hw2
668,stud,"<p>&#64;OscarThörn, are you sure about the expression for $$E_{k}$$ above?</p>
<p></p>
<p>From lecture,</p>
<p></p>
<p>$$E_{k} = V(s_{t}) &#43; \alpha_{T}(r_{t&#43;1} &#43; ... &#43; \gamma^{k-1}r_{t&#43;k} &#43; \gamma^{k}V(s_{t&#43;k}) - V(s_{t}))$$</p>
<p></p>
<p>If we take $$\gamma = \alpha_{T} == 1$$, then the above becomes</p>
<p></p>
<p>$$E_{k} = V(s_{t}) &#43; (r_{t&#43;1} &#43; ... &#43; \gamma^{k-1}r_{t&#43;k} &#43; V(s_{t&#43;k}) - V(s_{t}))$$</p>
<p>$$ = (V(s_{t}) - V(s_{t})) &#43; \sum_{i=1}^{k}r_{t&#43;i} &#43; V(s_{t&#43;k}) = \sum_{i=1}^{k}r_{t&#43;i} &#43; V(s_{t&#43;k})$$.</p>
<p></p>
<p>Unless you&#39;re assuming that $$t=0$$?</p>
<p></p>
<p>Then we&#39;d have $$E_{k} \rvert_{t=0} =  \sum_{i=1}^{k}r_{i} &#43; V(s_{k})$$, which is what you have above. </p>
<p></p>
<p>If you assume that $$E_{\infty} = constant$$ then the second sum above becomes the product of a constant ($$E_{\infty}$$) and a terminal segment of a geometric series: $$\sum_{k=6}^{\infty}(1-\lambda)\lambda^{k-1}$$. The ratio between successive terms looks like it&#39;s going to be $$\frac{(1-\lambda)\lambda^{k}}{(1-\lambda)\lambda^{k-1}} = \lambda$$.</p>
<p></p>
<p>So as long as $$|\lambda| &lt; 1$$, the corresponding (complete) geometric series will converge (good news for us, since we&#39;re told in the homework that this is true of $$\lambda$$!)</p>
<p></p>
<p>So I think that the second term above can be written as the differrence of two terms: the fisrt will be the sum of the convergent geometric series given by $$\sum_{k=1}^{\infty}(1-\lambda)\lambda^{k-1}$$, and the second will be the sum of the (finite) geometric series corresponding to the initial segment of <em>that</em> series, i.e. $$\sum_{k=1}^{5}(1-\lambda)\lambda^{k-1}$$. </p>
<p></p>
<p>We can rewrite the first of these so that the lower limit of summation is $$0$$ rather than $$1$$: $$\sum_{k=1}^{\infty}(1-\lambda)\lambda^{k-1} = \sum_{k=0}^{\infty}(1-\lambda)\lambda^{k}$$.</p>
<p></p>
<p>The second can be rewritten as a sum over $$0$$ to $$4$$ rather than $$1$$ to $$5$$: $$\sum_{k=1}^{5}(1-\lambda)\lambda^{k-1} = \sum_{k=0}^{4}(1-\lambda)\lambda^{k}$$</p>
<p></p>
<p>We can then use the standard formulas for the sums of infinite and finite geometric series respectively to work out these values in terms of $$\lambda$$: </p>
<p></p>
<p>$$\sum_{k=0}^{\infty}(1-\lambda)\lambda^{k} = \frac{(1-\lambda)}{(1-\lambda)} = 1$$.</p>
<p></p>
<p>$$\sum_{k=0}^{4}(1-\lambda)\lambda^{k} = (1 - \lambda)\frac{1-\lambda^{5}}{(1 - \lambda)} = 1 - \lambda^{5}$$</p>
<p></p>
<p>This implies that,</p>
<p></p>
<p>$$\sum_{k=6}^{\infty}(1-\lambda)\lambda^{k-1} = \sum_{k=1}^{\infty}(1-\lambda)\lambda^{k-1} - \sum_{k=1}^{5}(1-\lambda)\lambda^{k-1}$$</p>
<p>$$= \sum_{k=0}^{\infty}(1-\lambda)\lambda^{k} - \sum_{k=0}^{4}(1-\lambda)\lambda^{k}$$</p>
<p>$$ = 1 - (1 - \lambda^{5}) = \lambda^{5}$$</p>
<p></p>
<p>That would make your second sum in the post above work out to $$E_{\infty}\times\lambda^{5}$$, it seems. </p>
<p></p>
<p>I&#39;m not sure how (or even if) any of this helps, but I think the math is right. </p>",2019-09-05T16:42:50Z,44,Week 9/1 - 9/7,feedback,a_2,,k06x5o738mc1bl,2019-09-05T16:42:50Z,{},hw2
669,stud,"<p><em>&#34; it&#39;s a zero reward transition back into 6? &#34; You getting there... what is the definition of a terminal state?!?! Anyone?!&#34;</em></p>
<p><em></em></p>
<p>We&#39;ve talked about this in Piazza and during OH. By defintion, a terminal state has value zero (argument: the value of a state is the expected return when starting from that state. But there are no sucessor states for an absorbing state other than the absorbing state itself, for which we get precisely <em>nothing</em> for a reward, since (again by defintion) a terminal state transitions to itself with probability $$1$$ and reward $$0$$). </p>",2019-09-05T16:46:50Z,44,Week 9/1 - 9/7,feedback,a_2,,k06xasut56a5e2,2019-09-05T16:46:50Z,{},hw2
670,no,<p>I&#39;m getting the same issue as Cameron. I&#39;m not sure what&#39;s going wrong.</p>,2019-09-05T17:07:18Z,44,Week 9/1 - 9/7,feedback,,jzjr6qef6i06tp,k06y14pmtbo6ry,2019-09-05T17:07:18Z,{},hw2
671,no,"Nice write up of the math. I arrived at the same conclusion but have not been able to verifiy. And yes I assume t=0 since it is the weight at state 0 that should match up, but perhaps that is a false assumption.<div><div><br /></div><div>Anyhow I managed to solve the homework by approximation but I still would like to make sure I really got the math down. But Miguel posted an example today so I’ll try to solve that one!</div></div>",2019-09-06T07:02:47Z,44,Week 9/1 - 9/7,feedback,,jzjwcq2u8o7110,k07rvke09ce6io,2019-09-06T07:02:47Z,{},hw2
672,no,"<p>For the &#34;actual, final term&#34; (called Gt in the image above), can we just plug in our calculated values for TD(1)? From the lectures, we know that E-infinity is the same as TD(1).</p>",2019-09-06T17:54:37Z,44,Week 9/1 - 9/7,feedback,,jzjr6qef6i06tp,k08f5thvvk45oj,2019-09-06T17:54:37Z,{},hw2
673,no,"<p>Sounds like a wise thing to do. Also, notice the weight given to the final return...</p>",2019-09-06T20:17:19Z,44,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08k9cim35g64m,2019-09-06T20:17:19Z,{},hw2
674,no,"I&#39;ve followed this thread closely, and thought I had it figured out, but end up with only imaginary numbers returned from numpy.roots when solving the problems where the final value is non-zero. Any advice? I&#39;ve posted a very detailed private post because I was nervous about giving too much direct information on my solution.",2019-09-11T01:57:31Z,43,Week 9/8 - 9/14,feedback,,ixpwxv7xdgi1u6,k0em68x3wen4tu,2019-09-11T01:57:31Z,{},hw2
675,no,I think I may have solved it. I spent a bit of extra time thinking about how to represent a terminal state.,2019-09-11T02:45:08Z,43,Week 9/8 - 9/14,feedback,,ixpwxv7xdgi1u6,k0envhos6z22bu,2019-09-11T02:45:08Z,{},hw2
676,no,<p>Anyone else getting the best lambda as 0.6227...? Changing the loop length (on state 6) and/or the size of lamba (i.e. 1e-10) doesn&#39;t effect the first 4 digits.</p>,2019-09-12T04:31:52Z,43,Week 9/8 - 9/14,feedback,,jzkke6iz3cl28t,k0g74lghf3c61b,2019-09-12T04:31:52Z,{},hw2
677,no,"I think I might have gotten that result, but u don&#39;t remember for sure. It only have to be accurate to 3 digits. Are the other examples right?",2019-09-12T04:48:07Z,43,Week 9/8 - 9/14,feedback,,ixpwxv7xdgi1u6,k0g7pi3dwy2py,2019-09-12T04:48:07Z,{},hw2
678,no,<p>Also in are we finding λ value such that &#34;TD(λ) estimate for S0 equals TD(1) estimate for S0&#34; or &#34;TD(λ) estimate for all states equals TD(1) estimate for all states&#34;?</p>,2019-09-01T21:15:08Z,44,Week 9/1 - 9/7,followup,,jzj4205g7gd2fw,k01h4fsqmca22i,2019-09-01T21:15:08Z,{},hw2
679,no,"<p>Correct, $$S_0$$.</p>",2019-09-02T22:01:21Z,44,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k02y7q10dr4nm,2019-09-02T22:01:21Z,{},hw2
680,no,<p></p>,2019-09-05T20:03:25Z,44,Week 9/1 - 9/7,followup,,isde34zracb1mz,k074bm0vd0w5sa,2019-09-05T20:03:25Z,{},hw2
681,no,"<p>In the example from the lecture, we used a simple $$\alpha $$ of $$\frac{1}{T}$$. If this is an arbitrary time step t, how do we calculate $$\alpha _{t}$$? Or maybe $$\alpha $$ can be chosen ramdonly because it&#39;s irrelevant (maybe will eventually be cancelled out)?</p>",2019-09-01T19:34:39Z,32,Week 9/1 - 9/7,followup,,jzj4205g7gd2fw,k01dj7zn2x0md,2019-09-01T19:34:39Z,{},hw2
682,no,<p>You are not learning anything so there is no need for alpha.</p>,2019-09-01T20:07:11Z,32,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k01ep1ije4e7hb,2019-09-01T20:07:11Z,{},hw2
683,no,<p></p>,2019-09-02T18:44:49Z,32,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k02r6z4adtv4y9,2019-09-02T18:44:49Z,{},hw2
684,no,"<p>It&#39;s not clear to me because the Ek&#39;s (which are used by TD(l)) are of this form which have alpha in the video:</p>
<p> </p>
<p>E_k: v[s_t] = v[s_t] &#43; alpha(r_1 &#43; ... &#43; v[s_t &#43; k] - v[s_t])</p>
<p> </p>
<p>Some thoughts/questions:</p>
<p> </p>
<p>* Is there a TD for each state?  For this HW we&#39;re computing TD(lambda) for state 0?</p>
<p>* I was almost thinking that a lambda that makes TD(lambda) = TD(1) would be 1, but you said *less* than 1.  So I&#39;m thinking this estimate is parabolic (or something else) and that I&#39;ll hit the same value of TD(1) for some lambda less than 1.</p>
<p>* It&#39;s not clear how to handle the stochastic reward of state 0, I&#39;m sure it&#39;ll involve some weighted average of rewards between states 1 and 2.</p>
<p>* couldn&#39;t find the TD equation in sutton in barto.</p>
<p>* I&#39;m thinking the look ahead is finite from state 0, so k in TD(lambda) would be maybe 5, but then the TD equation would give me a lambda to the 4th power.  are we to numerically solve for lambda? I&#39;m thinking probably not, but because it&#39;s not clear what the equation to solve looks like - I&#39;m asking..</p>
<p>* I&#39;m thinking I&#39;m making this problem harder than need be.</p>
<p>* so valueEsimtate are state values at time t?  That is, I know the estimated value for each state &#64; time t?</p>
<p>* how to compute TD(1)?  Isn&#39;t that just E_infinity?  but in the video is just a string of rewards to the terminal state (starting at state 0), however, we have a non deterministic transition &#64; state 0 and a recombine at  state 3</p>",2019-09-02T18:46:25Z,32,Week 9/1 - 9/7,followup,,ixpclzk97jg2fo,k02r917hslv69y,2019-09-02T18:46:25Z,{},hw2
685,no,"<p>Right! In the video, they are teaching you how to learn the different k-step estimates. In this case, you don&#39;t need to learn anything because you have the MDP. You can calculate them directly (not learn from experience).</p>
<p></p>
<p><strong>* Is there a TD for each state?  For this HW we&#39;re computing TD(lambda) for state 0?</strong></p>
<p>Yes. There is a TD estimate for each state. TD is a way of calculating the state-value function V. TD(lambda) is another way of doing that. For this HW, you are asked to find a value of lambda (less than 1) such that the TD(lambda) estimate of state 0, equals the TD(1) estimate of state 0.</p>
<p></p>
<p><strong>* I was almost thinking that a lambda that makes TD(lambda) = TD(1) would be 1, but you said *less* than 1.  So I&#39;m thinking this estimate is parabolic (or something else) and that I&#39;ll hit the same value of TD(1) for some lambda less than 1.</strong></p>
<p>You can plot these values and gain some insight. I invite you to do so.</p>
<p><strong></strong></p>
<p><strong>* It&#39;s not clear how to handle the stochastic reward of state 0, I&#39;m sure it&#39;ll involve some weighted average of rewards between states 1 and 2.</strong></p>
<p>You have the MDP, the probability of transitioning to state 1 vs. 2, I&#39;m sure you can calculate it that way.</p>
<p><strong></strong></p>
<p><strong>* couldn&#39;t find the TD equation in sutton in barto.</strong></p>
<p>Chapter 6 is all about Temporal-Difference Learning, which is TD. I would recommend you read about k-step estimators, too. Sutton calls these n-step estimators, there is a whole chapter on this topic. Chapter 7: n-step Bootstrapping. </p>
<p><strong></strong></p>
<p><strong>* I&#39;m thinking the look ahead is finite from state 0, so k in TD(lambda) would be maybe 5, but then the TD equation would give me a lambda to the 4th power.  are we to numerically solve for lambda? I&#39;m thinking probably not, but because it&#39;s not clear what the equation to solve looks like - I&#39;m asking..</strong></p>
<p>I would disagree with it being finite. You also learn this in the lecture. How can you make it infinite? Yes, you will have to solve a polynomial.</p>
<p><strong></strong></p>
<p><strong>* I&#39;m thinking I&#39;m making this problem harder than need be.</strong></p>
<p>That&#39;s good, that&#39;s how we all learn. Keep thinking, you&#39;re going through the normal process for this homework.</p>
<p></p>
<p><strong>* so valueEsimtate are state values at time t?  That is, I know the estimated value for each state &#64; time t?</strong></p>
<p>Right, but these are estimates, they are [likely] wrong. The point is that it doesn&#39;t matter. The only reason for those estimates is that you find a lambda that we set for you.</p>
<p><strong></strong></p>
<p><strong>* how to compute TD(1)?  Isn&#39;t that just E_infinity?  but in the video is just a string of rewards to the terminal state (starting at state 0), however, we have a non deterministic transition &#64; state 0 and a recombine at  state 3</strong></p>
<p>But you got the probabilities! I&#39;m sure you can calculate these values exactly (again, no learning, no alpha). And yes, E_infinity is TD(1), which BTW is Monte-Carlo estimate (MC).</p>
<p></p>
<p>Keep working it, you are asking the right questions!</p>",2019-09-03T02:29:28Z,32,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k037sio9lx75mu,2019-09-03T02:29:28Z,{},hw2
686,no,"<p>You are trying to calculate these:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fk0389hy6jfxg%2Fnstep.png"" alt="""" /></p>",2019-09-03T02:42:56Z,32,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0389u3keda60i,2019-09-03T02:42:56Z,{},hw2
687,no,"<p></p>
<p>&gt;&gt; I was almost thinking that a lambda that makes TD(lambda) = TD(1) would be 1, but you said *less* than 1.  So I&#39;m thinking this estimate is parabolic (or something else) and that I&#39;ll hit the same value of TD(1) for some lambda less than 1.</p>
<p>&gt; You can plot these values and gain some insight. I invite you to do so.</p>
<p></p>
<p>It seems to me the goal is to find the smallest lambda that still produces the same estimate as TD(1). Otherwise I&#39;m pretty sure 0.999 would work and satisfy the requirement of being less than 1. The homework description mentions &#34;return the best lambda&#34;, but it doesn&#39;t mention what &#34;best&#34; means. &#34;Smallest&#34; is my best guess.</p>
<p></p>",2019-09-09T02:51:55Z,31,Week 9/8 - 9/14,feedback,,jtel7temdxy3lq,k0bt8hrz4ld39n,2019-09-09T02:51:55Z,{},hw2
688,no,"<p>We are obviously not asking for 0.999, or 0.99999999999 for that matter. So, the best lambda is the lambda that best fits the homework requirements.</p>",2019-09-09T03:00:15Z,31,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0btj8bt6796t8,2019-09-09T03:00:15Z,{},hw2
689,no,<p></p>,2019-09-02T05:00:47Z,49,Week 9/1 - 9/7,followup,,jzh6k6o994a6dh,k01xr9h7skt145,2019-09-02T05:00:47Z,{},office_hours
690,no,"<p>sorry, error, disregard this</p>",2019-09-02T05:01:58Z,49,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k01xss0xj9t5qd,2019-09-02T05:01:58Z,{},office_hours
691,stud,<p>Is there any way we could aggregate these sessions so we could see all recordings in 1 tab/view? For those who are not able to view during the working week it would help vs. having to hunt down links from threads. If not... a master list of sessions might help as well. Though it does look like BlueJeans has the capability to add multiple chapters (OH sessions) into 1 recording tab so we could have 1 source of truth for previous OH videos.</p>,2019-09-03T04:12:04Z,49,Week 9/1 - 9/7,followup,a_0,,k03bggtww2g2jj,2019-09-03T04:12:04Z,{},office_hours
692,no,<p>We have done this meta-post in the past.</p>,2019-09-03T12:25:32Z,49,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k03t32kp4k64cm,2019-09-03T12:25:32Z,{},office_hours
693,no,"<p>Go ahead, create it with the initial 4 sessions and we will keep adding.</p>",2019-09-03T12:26:21Z,49,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k03t44aoncj4t2,2019-09-03T12:26:21Z,{},office_hours
694,stud,<p>&#64;132</p>,2019-09-04T06:42:22Z,49,Week 9/1 - 9/7,feedback,a_0,,k04w9luosn13e3,2019-09-04T06:42:22Z,{},office_hours
695,stud,<p>&#64;73</p>,2019-09-02T00:09:16Z,63,Week 9/1 - 9/7,followup,a_0,,k01ncdq2fsv3ev,2019-09-02T00:09:16Z,{},hw1
696,no,"<p></p><ul><li>If you try to enter answers very very quickly, the site will flag it and ban you</li></ul>
<p>Does it mean &#34;enter answers very very quickly&#34; to the same question or to any question? E.g, if I have to provide 5 answers (or whatever number specified) for a Homework, does it mean that each separate answer out of these 5 I need to enter with a delay?</p>",2019-09-02T00:22:47Z,39,Week 9/1 - 9/7,followup,,jqkxzdmmolGf,k01ntrek1zn5iw,2019-09-02T00:22:47Z,{},hw1
697,no,<p>I waited about 10 seconds between each time I clicked a button (to submit a new answer) and didn&#39;t have a problem.</p>,2019-09-02T01:56:51Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01r6ps5frq1mh,2019-09-02T01:56:51Z,{},hw1
698,no,Both to the same question and to all the questions.,2019-09-02T02:11:56Z,39,Week 9/1 - 9/7,feedback,,i4op5p9vfbq5yz,k01rq4jfq4p5av,2019-09-02T02:11:56Z,{},hw1
699,no,Thank you guys for your input,2019-09-02T04:40:16Z,39,Week 9/1 - 9/7,feedback,,jqkxzdmmolGf,k01x0vxepk9263,2019-09-02T04:40:16Z,{},hw1
700,no,Is there anything we have to do to &#34;Submit&#34; our solution or just hit &#34;Check&#34; for each problem?,2019-09-02T02:25:08Z,39,Week 9/1 - 9/7,followup,,ixpwxv7xdgi1u6,k01s738q3j32jz,2019-09-02T02:25:08Z,{},hw1
701,no,<p>Just hit check.</p>,2019-09-02T02:27:30Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01sa4w4vn57h4,2019-09-02T02:27:30Z,{},hw1
702,no,Awesome. Thanks!,2019-09-02T02:28:11Z,39,Week 9/1 - 9/7,feedback,,ixpwxv7xdgi1u6,k01sb13275m4s2,2019-09-02T02:28:11Z,{},hw1
703,no,"<p>For some reason, there are 2 out of 10 cases keep saying my output is wrong but the other 8 are fine. I used the same set of code. Has anyone encountered a similar issue? Any hint for this? I tried to play with stoping certiera and max-iter for mdptoolbox but nothing changes...</p>",2019-09-02T23:20:24Z,39,Week 9/1 - 9/7,followup,,j6lb135ieso2nq,k0311e0daas29o,2019-09-02T23:20:24Z,{},hw1
704,no,"<p>I&#39;m not sure anyone can help you without seeing your code, but assuming you have no bugs, off the top of my head, maybe try entering a higher precision answer?  I entered four digits after the decimal point for all the answers where I would have had to round up on the third digit.</p>
<p></p>
<p>Assuming that suggestion is not helpful, I would look and see if there&#39;s something special about those 2 cases that may reveal a bug in your implementation.</p>",2019-09-02T23:30:27Z,39,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k031ebcdefw2ze,2019-09-02T23:30:27Z,{},hw1
705,no,<p>I found out why. It is not something wrong with stopping criteria nor max-iter. It&#39;s the assumption I had for the states. Thanks.</p>,2019-09-03T01:51:41Z,39,Week 9/1 - 9/7,feedback,,j6lb135ieso2nq,k036fxm218129v,2019-09-03T01:51:41Z,{},hw1
706,no,"<p>To clarify, are we free to run our solution against the problems in RLDM, then take time to make modifications before making another attempt later? Or do all 10 attempts need to occur in the same &#34;session&#34;?</p>",2019-09-04T01:02:20Z,39,Week 9/1 - 9/7,followup,,jl282yfimdd2k9,k04k4be4z495ta,2019-09-04T01:02:20Z,{},hw1
707,no,<p>They do not need to occur in the same session.  You can try and then comeback later and try some more.  You are just limited 10 attempts total for each problem.</p>,2019-09-04T01:18:30Z,39,Week 9/1 - 9/7,feedback,,hz7meu55mi8sd,k04kp3r6hzk6fp,2019-09-04T01:18:30Z,{},hw1
708,no,<p>I submitted my answers on RLDM and got full credit. When will the grade for HW1 be updated in Canvas?</p>,2019-09-04T18:41:14Z,39,Week 9/1 - 9/7,followup,,jl2egn5k4zo4lp,k05ly2updeq2q,2019-09-04T18:41:14Z,{},hw1
709,no,"<p>From &#64;8, &#34;Homework assignments will be auto-graded in an external tool and moved to Canvas a few weeks after the due dates.&#34;</p>",2019-09-05T01:00:14Z,39,Week 9/1 - 9/7,feedback,,ixty1midfufhd,k05zhh3n5x272x,2019-09-05T01:00:14Z,{},hw1
710,no,<p>Yeah typically we do them 2-3 at a time. So expect them to show up around the time we put Project 1 grades in.</p>,2019-09-05T01:28:50Z,39,Week 9/1 - 9/7,feedback,,hz7meu55mi8sd,k060i9dl2h6v5,2019-09-05T01:28:50Z,{},hw1
711,no,"<p><img src=""https://pbs.twimg.com/media/DywRpEiVYAAgpMc.jpg"" alt=""Related image"" /></p>",2019-09-05T02:43:50Z,39,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0636pmt3dq3e6,2019-09-05T02:43:50Z,{},hw1
712,no,<p>Good one :)</p>,2019-09-05T14:55:19Z,39,Week 9/1 - 9/7,feedback,,jl2egn5k4zo4lp,k06tbdvxmx51nt,2019-09-05T14:55:19Z,{},hw1
713,stud,<p>what if I told you memes are rude.</p>,2019-09-06T16:21:35Z,39,Week 9/1 - 9/7,feedback,a_0,,k08bu6nc6c45qq,2019-09-06T16:21:35Z,{},hw1
714,no,<p>What if I told you are not anonymous to the instructors?</p>,2019-09-06T18:37:46Z,39,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08gpbvqqq35lw,2019-09-06T18:37:46Z,{},hw1
715,no,<p>It&#39;s disappointing to see instructors engaging in this kind of behavior. Responses like these create a hostile environment and discourage students from asking questions.</p>,2019-10-03T20:46:10Z,35,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1b66gc0rm67kv,2019-10-03T20:46:10Z,{},hw1
716,no,<p>Take it easy. I am sure it was just a joke.</p>,2019-10-04T10:57:31Z,35,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k1c0la9eakw2de,2019-10-04T10:57:31Z,{},hw1
717,no,"<p>Geez...</p>
<p></p>
<p>:) At least you got it, Mayank.</p>",2019-10-04T14:37:23Z,35,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c8g17rngb3ty,2019-10-04T14:37:23Z,{},hw1
718,no,"<p>if one can model the environment for HW1, one can use the pympdtoolbox to  find the optimal state-value function for the initial state of the game using value iteration? correct?</p>",2019-09-03T00:43:22Z,29,Week 9/1 - 9/7,followup,,j6ll2xkiDJf,k034033mpz612u,2019-09-03T00:43:22Z,{},hw1
719,no,<p>Correct.</p>,2019-09-03T00:43:56Z,29,Week 9/1 - 9/7,feedback,,jl1acpoc4HA9,k0340sr0mng67g,2019-09-03T00:43:56Z,{},hw1
720,no,<p>I want a Reddit upvote option. Or the stack overflow like... so we can tell what is good.</p>,2019-09-03T03:48:50Z,29,Week 9/1 - 9/7,feedback,,jzj6p4ln7yi5de,k03amld1h2i5og,2019-09-03T03:48:50Z,{},hw1
721,no,"<p>Some people use Chrome plugins for this E.g.: <a href=""https://chrome.google.com/webstore/detail/piazza-likes/eonfnigfcmackjgojglbgknickndelcp?hl=en"">https://chrome.google.com/webstore/detail/piazza-likes/eonfnigfcmackjgojglbgknickndelcp?hl=en</a>.</p>
<p></p>
<p>Can&#39;t recommend.</p>",2019-09-03T03:51:53Z,29,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k03aqios1ng6vg,2019-09-03T03:51:53Z,{},hw1
722,no,<p>value iteration values returned by this tool does not match what is in some reputable lectures (but I may be wrong) &#64;142</p>,2019-09-05T04:08:41Z,29,Week 9/1 - 9/7,followup,,j6ll2xkiDJf,k0667t9bmz617y,2019-09-05T04:08:41Z,{},hw1
723,no,"<p>I&#39;m familiar with a very common issue that even experts fall into... The issue is that the value function of the terminal state is not being set to 0. Move the 100 to the <strong>reward</strong> for landing of the terminal state, and not the <strong>value</strong> of the terminal state.</p>
<p></p>
<p>This is not the only lecture by reputable professors that swap these two.</p>",2019-09-06T03:21:20Z,29,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07jyrxmr542p,2019-09-06T03:21:20Z,{},hw1
724,no,"<p>BTW, I replicate these environments and results here: <a href=""https://github.com/mimoralea/gym-aima"">https://github.com/mimoralea/gym-aima</a></p>
<p></p>",2019-09-06T03:25:22Z,29,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07k3z1kjlk65,2019-09-06T03:25:22Z,{},hw1
725,no,<p>Thanks miguel. I am not familiar with gym but I will start learning it. I managed to implement value iteration myself and used it for HW1. </p>,2019-09-06T04:15:59Z,29,Week 9/1 - 9/7,feedback,,j6ll2xkiDJf,k07lx2d6yoqn3,2019-09-06T04:15:59Z,{},hw1
726,no,"<p>Great... You will find implementing Q-Learning is pretty much the same thing, and DQN very similar, too.</p>
<p></p>",2019-09-06T04:21:23Z,29,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07m404lwn925g,2019-09-06T04:21:23Z,{},hw1
727,no,<p>I have never heard of pymdptoolbox before. Do we have to use it for hw1?</p>,2019-09-08T15:47:30Z,28,Week 9/8 - 9/14,followup,,jqmjg3txqw6ji,k0b5i1qzc8p4rr,2019-09-08T15:47:30Z,{},hw1
728,no,<p>Have is a strong word...</p>,2019-09-08T16:35:08Z,28,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0b77b5pxkn16q,2019-09-08T16:35:08Z,{},hw1
729,no,"<p>Now that HW1 is done, can someone show an example of how to use the pymdptoolbox  to compute the N=6 example for HW1. Or any made up example.</p>
<p></p>
<p>Thank you for your help.</p>",2019-09-20T19:18:38Z,27,Week 9/15 - 9/21,followup,,gx3c8l7z7r72zl,k0sibskqon7lk,2019-09-20T19:18:38Z,{},hw1
730,no,<p>No we do not release code solutions.</p>,2019-09-22T00:18:18Z,27,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0u8h12ucg86sp,2019-09-22T00:18:18Z,{},hw1
731,no,"<p>Michael,</p>
<p></p>
<p>I think maybe your issue is that the reward in the Bellman equations should be associated with <em>exiting</em> the state, not entering it.  In Sutton and Barto they emphasize that the reward and the next state form a kind of pair - when you choose your action, you get (potentially stochastically) assigned both a reward and a next state.  Also, in the Bellman equations where you&#39;re maximizing over actions, it wouldn&#39;t make sense to maximize a reward that you have already gotten, and hence doesn&#39;t depend on the action.</p>",2019-09-02T02:04:13Z,45,Week 9/1 - 9/7,followup,,jzfsa4a37jf4aq,k01rg6zfoo84tt,2019-09-02T02:04:13Z,{},hw1
732,stud,"<p><em>&#34;Also, in the Bellman equations where you&#39;re maximizing over actions, it wouldn&#39;t make sense to maximize a reward that you have already gotten, and hence doesn&#39;t depend on the action.&#34;</em></p>
<p><em></em></p>
<p>I sort of see your point...but isn&#39;t the view on how the MDP works suggested by, say, value iteration <em>atemporal</em>. </p>
<p></p>
<p>I think lectures also explicitly talk of getting a reward for <em>entering</em> a state (but I&#39;ll double check on this now that you&#39;ve pointed this out).</p>
<p></p>
<p></p>",2019-09-02T02:19:59Z,45,Week 9/1 - 9/7,feedback,a_0,,k01s0h96eyi1a0,2019-09-02T02:19:59Z,{},hw1
733,no,"<p>Well you definitely get a reward when you enter a state, since entering a state is just another way of looking at leaving a (different) state.  The question is when does the reward in the Bellman equation get assigned - before or after the action is chosen.  <strong>That</strong> reward is a random variable, which means you don&#39;t have it yet.</p>",2019-09-02T02:26:13Z,45,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k01s8hnufi46g9,2019-09-02T02:26:13Z,{},hw1
734,stud,"<p>I think I see what you&#39;re saying...</p>
<p></p>
<p>In the bellman equation, we&#39;ve got </p>
<p></p>
<p>$$V_{t}(s) = R(s) &#43; \max_{a}(\sum_{s&#39;}T(s,a,s&#39;)V_{t-1}(s&#39;)$$)</p>
<p></p>
<p>The argument of the reward function is $$s$$, but we&#39;re adding it to a max with respect actions over <em>successor</em> states. It&#39;s maybe natural to think of that as suggesting that we&#39;re getting the reward for <em>leaving</em> $$s$$ as a result of taking the action $$a$$ that lands us in state $$s&#39;$$. </p>
<p></p>
<p>Still, if $$R(s)$$ is supposed to mean that $$R$$ is a function of state, $$s$$, then whether we see $$R(s)$$ as the reward for <em>leaving $$s$$, </em>or the reward for <em>entering</em> $$s$$, then it looks like it can only have one value. So either all possible ways of leaving $$s$$ lead to the same reward, or else all possible ways of entering into $$s$$ lead to the same reward. Otherwise, $$R$$ looks to be not single valued (and thus not a function). Right?</p>",2019-09-02T02:43:42Z,45,Week 9/1 - 9/7,feedback,a_0,,k01suzgsw3cgq,2019-09-02T02:43:42Z,{},hw1
735,no,"<p>Addressing your point about functions, $$R$$ is a random variable, not a deterministic function of $$s$$.  It&#39;s technically a stochastic process -- for any value of $$s, R(s)$$ is a random variable.  In any case, this means it can have many different output values, according to some probability mass (or density) function, for a single $$s$$.</p>
<p></p>
<p>Back to the Bellman equation.  What you have above is the Bellman Optimality Equation, which means that there is an implied optimal policy being used to choose actions.  This means that once you have an $$R(s)$$, there should be an implied $$a$$ that goes along with it -- the $$a$$ that maximizes expected returns.  For this reason I prefer using $$R(s,a)$$ as it makes that explicit.</p>
<p></p>
<p>Having said all that, this stuff is still settling inside my head so if I&#39;m wrong I would appreciate someone correcting me.</p>",2019-09-02T06:12:32Z,45,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k020bjcnnok40i,2019-09-02T06:12:32Z,{},hw1
736,no,"<p>Reward should be a reward that we receive <em><strong>after</strong></em><strong> </strong>leaving the state and taking a particular action since we are calculating <strong>value of the state</strong>. Why should <strong>value of the state</strong> be dependent on the rewards that we collected before entering the state? It doesn&#39;t make sense. The <strong>value of the state (or &#34;<em>cost</em></strong>&#34; of that state) equals to how many rewards we collect on average if we start from this state and move to the goal.</p>
<p></p>
<p>I draw below the function V(s) how it was presented in the Sutton&#39;s book. I would assume that R(s) is the reward that you would get after taking an action that leads to the optimal V(s). Since after taking an action, we are not 100% guaranteed to end up in the state we want (like in my example, there are 2 different states that you might end up), so R(s) is a random variable after taking an action &#34;a&#34;. Let&#39;s say a_2 is the action that we need to take in order to get the optimal value, then in that case R(s) can be either r_3 or r_4 with some probabilities. Is that correct? Honestly, now I&#39;m confused too lol  </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzih0fdt4sn1cq%2Fk02tl6xhp1f9%2FScreen_Shot_20190902_at_3.51.15_PM.png"" alt="""" /></p>",2019-09-02T20:05:08Z,45,Week 9/1 - 9/7,feedback,,jzih0fdt4sn1cq,k02u29w2vzysx,2019-09-02T20:05:08Z,{},hw1
737,no,"<p>I watched the lecture and see where the confusion came from. So the professor said R(s) is a scaler value for being in a state. Also he said that the reward of entering to the state is usefulness of entering to this state. This usefulness of entering to the state doesn&#39;t mean we calculated rewards before entering to the state. Usefulness of the state can be calculated based on all rewards that we accumulate after leaving the state. That&#39;s how they can calculate the usefulness of each state. Also this is Markov process, so we don&#39;t remember our past, only the current state. Based on the current state, we make our decisions. So that means R(S) should be calculated based on rewards after leaving the state.</p>
<p></p>
<p>Although I&#39;m confused why R(s) = R(s,a,s&#39;). I understand why R(s) = R(s,a), but since this is stochastic process that means we don&#39;t always ended up in the state s&#39; even if we take action a. So in my opinion it should be R(s) = R(s,a) = Sum over all s&#39; that we might end up after taking the action a of R(s,a,s&#39;) </p>",2019-09-03T13:04:58Z,45,Week 9/1 - 9/7,feedback,,jzih0fdt4sn1cq,k03uhsfw8pm6f9,2019-09-03T13:04:58Z,{},hw1
738,no,"<p>R(s) = R(s,a,s&#39;) because we are not mapping a 1:1 relationship between state and action transitions. Instead, we have a set of possible states that derive from a common action, as shown in the diagram you shared.</p>
<p></p>
<p>There is a 1/N chance that you roll 5, therefore you have a 1/N chance of moving into state 6 from state 1 when your action is roll. You also have a 1/N chance of moving into state 3 from state 1 when you roll a 2. Now, you can say that your reward for this movement is the face value, or it can be zero. I don&#39;t think there is much difference between the two <em>so long as</em> you are consistent.</p>
<p></p>
<p>There is really no R(s) here. I think you are referring to U(s) which is the utility of state &#39;s&#39;. This is the V(s), so to speak loosely, of the value iteration we are doing. The V(s) is the weighted average reward of the future path taking from state &#39;s&#39; plus my current value in this state. So if I am 6 steps away from a diamond mine, my V(s) is lower than the one where I am one step away.</p>",2019-09-03T13:55:45Z,45,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k03wb33kc2j64a,2019-09-03T13:55:45Z,{},hw1
739,no,"<p>Careful guys.  $$R(s) \neq R(s,a,s&#39;).$$</p>
<p></p>
<p>You can use $$R(s,a,s&#39;)$$ instead of using $$R(s)$$ or $$R(s,a)$$ but the three variables mean three different things. Depending on which one you use, the rest of your Bellman equation will look different.</p>
<p></p>
<p>$$R(s,a,s&#39;)$$ is the reward you get if $$S(t) = s, A(t) = a, S(t&#43;1) = s&#39;$$ whereas $$R(s,a)$$ is the reward you get if $$S(t) = s \text{ and } A(t) = a$$.  In the latter case, you still have to average over all possible next states, depending on the probability transition function of the environment.</p>",2019-09-03T14:14:55Z,45,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03wzq9mfcd7nd,2019-09-03T14:14:55Z,{},hw1
740,no,"<p>The confusion came from the lecture. The professor Isbell said that R(s) = R(s,a) = R(s,a,s&#39;) in the lecture 2. In the Sutton&#39;s book there is no R(s), just values or scalars for rewards. So we are trying to understand what is R(s) and why it equals to R(s,a,s&#39;).</p>
<p></p>
<p>So in my example above (or below I added more info to my diagram), let&#39;s say V(s) is optimal if we follow the action a1. Then, in my understanding, R(s) = Pr(a1|s)*[Pr(s3,r3|s,a1)*r3 &#43; Pr(s4,r4|s,a1)*r4] which is not equal to R(s,a1,s3) = r3 or R(s,a1,s4) = r4</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzih0fdt4sn1cq%2Fk04cuzowk911%2FScreen_Shot_20190903_at_5.38.39_PM.png"" alt="""" /></p>",2019-09-03T21:39:14Z,45,Week 9/1 - 9/7,feedback,,jzih0fdt4sn1cq,k04cv4mpg45mn,2019-09-03T21:39:14Z,{},hw1
741,no,"<p>Yes, I think that $$R(s) = R(s,a) = R(s,a,s&#39;)$$ should have been clarified as meaning something like &#34;All three can be used in equivalent forms of the Bellman equation.&#34;</p>
<p></p>
<p>With regards to the (scalar) reward for leaving state $$s$$, which we&#39;ll call $$R(s),$$ it&#39;s just</p>
<p></p>
<p>$$R(s) = \mathbb{E}[R_t|S_{t-1} = s] = \sum\limits_r\sum\limits_a\sum\limits_{s&#39;}r\cdot p(s&#39;,r,a|s).$$</p>
<p></p>
<p>If you are stipulating that we are following an optimal policy, and that that policy chooses branch $$a_1$$, then the above simplifies to:</p>
<p></p>
<p>$$R(s) = \sum\limits_r\sum\limits_{s&#39;}r\cdot p(s&#39;,r|s, a_1).$$</p>
<p></p>
<p>In the case of your diagram, only reward $$r_3$$ can be attained in state $$s_{3}$$ and only reward $$r_4$$ can be attained in state $$s_{4}$$, so we have</p>
<p></p>
<p>$$R(s) = r_3p(s_3|s, a_1) &#43; r_4p(s_4|s, a_1).$$</p>
<p></p>
<p>Notice that we don&#39;t need $$p(a_1|s)$$ since this optimal policy will always be choosing the branch leading to $$a_1$$, so $$p(a_1|s) = 1$$..</p>
<p></p>
<p>If we weren&#39;t following the optimal policy but some general policy, then we would need $$p(a_1|s)$$ as well as $$p(a_2|s)$$.</p>",2019-09-03T23:06:11Z,45,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04fyy99tm35c7,2019-09-03T23:06:11Z,{},hw1
742,no,"<p><span style=""font-size:120%"">that is very irritating; the editor ate a very nice, technical post on the topic due to the inclusion of copy and pasted mathematical symbols.  as I am pressed for time and disinclined to rewrite it.   if someone could post a complete set of &#34;codes&#34; for the mathematical symbols, it would help me tremendously in discussing topics in the future.  or link to a cheat sheet for whatever piazza uses for its mathematical notations.<br /></span></p>
<p><span style=""font-size:120%""></span></p>
<p><span style=""font-size:120%""></span></p>
<p><span style=""font-size:120%"">the gist of it is:</span></p>
<p></p>
<p><span style=""font-size:120%"">you are correct; but there are subtler issues at play; that the equations that you have above are stated all with respect to V(s); that they are all equal; that the argmax limits grouping across the terms; that symbolic substitution allows you to rewrite portions of the equations without have refer to functions difficult to define; that the model in this case is known a priori, so any of the three forms would work; that the equations contain a subtly with respect to the credit assignment problem (that may be due to numerical or mathematical conditions for convergence, or subtler issues) -- that it posted a question on slack regarding the conditioning of V(s&#39; | a,s*) vs V(s&#39; | a, s) -- and that it is in fact, conditioned on V(s&#39; | a, s) due to the additional functions that act to portion out the utility to proportionate to how they are additively summed and max&#39;d.    and that your preference for any form of the above in practice is determined by what functional grouping you are using; and what information you are able to gather from the environment.   as R is a function of S&#39; (implicit to RL i.e. a succession of states); than observing s,a,s&#39; itself is a form of computation of R</span></p>
<p></p>
<p><span style=""font-size:120%"">... and I used form 3 for the homework; easier due to the way I set up the model (how I programmed it).  but I could have used any (since it is just a question of where the argmax goes).</span></p>
<p></p>
<p><span style=""font-size:120%"">sorry for the clipped repost.  also sorry for the large font.<br /></span></p>",2019-09-02T13:30:00Z,45,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k02fy45q9fj6jj,2019-09-02T13:30:00Z,{},hw1
743,no,<p>thanks for this. this will help everyone through out this course</p>,2019-09-09T03:23:54Z,36,Week 9/8 - 9/14,followup,,jqtti9gfnagH,k0budmrss905rm,2019-09-09T03:23:54Z,{},hw1
744,no,<p>you welcome</p>,2019-09-09T13:43:16Z,36,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0cgi56zos962k,2019-09-09T13:43:16Z,{},hw1
745,no,<p>Agreed. Thanks JPB.</p>,2019-09-11T17:02:23Z,36,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0fihx0drfz1uz,2019-09-11T17:02:23Z,{},hw1
746,no,<p>Unable to view the document. Did it get stripped out ?</p>,2019-09-12T20:35:49Z,36,Week 9/8 - 9/14,followup,,jc9nkspumds4ki,k0h5k90jrp24nx,2019-09-12T20:35:49Z,{},hw1
747,no,"<p>Nope, still there for me.</p>",2019-09-12T21:03:12Z,36,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0h6jgwnmqw2gk,2019-09-12T21:03:12Z,{},hw1
748,no,<p>Strange. I still cannot view it. Is it possible to repost or you can send them to me at nsinha&#64;gatech.edu</p>,2019-09-13T16:40:13Z,36,Week 9/8 - 9/14,feedback,,jc9nkspumds4ki,k0icl49vxow6wg,2019-09-13T16:40:13Z,{},hw1
749,no,"<p><a href=""https://www.cs.upc.edu/~mmartin/Ag4-4x.pdf"">https://www.cs.upc.edu/~mmartin/Ag4-4x.pdf</a></p>",2019-09-13T17:51:24Z,36,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0if4nhy10238v,2019-09-13T17:51:24Z,{},hw1
750,no,"<p>thx</p>
<p></p>",2019-09-14T23:19:27Z,36,Week 9/8 - 9/14,feedback,,jc9nkspumds4ki,k0k6adgug2658e,2019-09-14T23:19:27Z,{},hw1
751,no,"<p>Ah, thanks, that makes sense!</p>",2019-09-03T05:40:18Z,63,Week 9/1 - 9/7,followup,,jzjwcq2u8o7110,k03elxkzrj54iq,2019-09-03T05:40:18Z,{},hw1
752,no,"<p>here is the example and explanation also </p>
<p></p>
<p><a href=""https://github.com/rldm/rldm_tutorials/blob/master/pymdptoolbox_example/pymdptoolbox_example.ipynb"">https://github.com/rldm/rldm_tutorials/blob/master/pymdptoolbox_example/pymdptoolbox_example.ipynb</a></p>
<p></p>
<p><a href=""https://www.youtube.com/watch?v=dOa_CA2JdLE"">https://www.youtube.com/watch?v=dOa_CA2JdLE</a></p>
<p></p>
<p></p>",2019-09-04T20:00:29Z,63,Week 9/1 - 9/7,followup,,jc9nkspumds4ki,k05orzcrpnw6ia,2019-09-04T20:00:29Z,{},hw1
753,no,<p>very helpful</p>,2019-09-07T19:21:58Z,63,Week 9/1 - 9/7,feedback,,ijb0gakjJnn,k09xq0uq4kx20o,2019-09-07T19:21:58Z,{},hw1
754,no,Not doing value iteration,2019-09-03T00:56:58Z,63,Week 9/1 - 9/7,followup,,jzj7y1ofgsro1,k034hkolkz23pa,2019-09-03T00:56:58Z,{},hw1
755,no,"<p>What algorithm are you using to solve this MDP?  Solving the regular (linear) Bellman equations requires you to have a policy, which is something we don&#39;t have at the start of this problem.</p>",2019-09-03T01:11:22Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03503e4du13bu,2019-09-03T01:11:22Z,{},hw1
756,no,<p>ah thank you!</p>,2019-09-03T15:07:16Z,63,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k03yv2h44my79t,2019-09-03T15:07:16Z,{},hw1
757,no,"<p>Vahe, I thought we do have a policy of : $$\prod = argmax_action(\gamma *T*U))$$</p>",2019-09-04T04:37:30Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k04rt143dvf6e5,2019-09-04T04:37:30Z,{},hw1
758,no,"<p>There <em>is</em> an optimal policy, but we&#39;re not told what it is.  Once we figure out what it is, we can use it to solve the Bellman Optimality equation.  But we need to find it first!</p>",2019-09-04T04:48:44Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04s7h7lcak55v,2019-09-04T04:48:44Z,{},hw1
759,no,"<p>Honestly, if you have a working MDP, ie good prob and reward matrices, I find it much easier, and prob closer to what pro&#39;s do, to plug it in the mdptoolbox, which will give you the v and policy vectors.  </p>",2019-09-04T17:28:29Z,63,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k05jcijpji2he,2019-09-04T17:28:29Z,{},hw1
760,no,"<p>Are you talking about solving the Bellman OPTIMALITY equation? Mathematically, like in the example in the book with the can collecting robot?</p>
<p></p>
<p>I tried that, but I couldn&#39;t figure out how to represent the &#39;bankruptcy&#39; state.  The probability p(s&#39;|s,a) to go to that state is easy, but what value do you use is beyond me, for now.  </p>",2019-09-03T04:04:58Z,63,Week 9/1 - 9/7,followup,,jzh6k6o994a6dh,k03b7cbb3fy1g6,2019-09-03T04:04:58Z,{},hw1
761,no,"<p>It depends on how you&#39;ve structured your rewards.  What&#39;s the Expected Sum of Future Rewards for going to the &#39;bankruptcy state&#39;? That&#39;s the state value of the bankruptcy state (by definition):</p>
<p></p>
<p>$$v(\text{Bankruptcy State}) = \mathbb{E}[G_t|S_t = \text{Bankruptcy State}]$$</p>",2019-09-03T05:55:05Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03f4y71ipo5og,2019-09-03T05:55:05Z,{},hw1
762,no,"<p>sure but the return G from that state is zero since there&#39;s nowhere to go from there, no?</p>",2019-09-03T07:56:58Z,63,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k03jhointlq4j,2019-09-03T07:56:58Z,{},hw1
763,no,<p>What&#39;s wrong with $$0$$?</p>,2019-09-03T15:32:31Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03zrjep70yn,2019-09-03T15:32:31Z,{},hw1
764,no,"<p>when you open up your wallet, you have a certain amount of cash in it; what happens if you put 2 more dollars in, close your wallet, and then look in it again?  do you have the same amount of cash before? or do you have more now?  what happens if you put another 3 dollars in, close it, and open it again?</p>",2019-09-02T23:49:07Z,63,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k0322b7rwhn2nw,2019-09-02T23:49:07Z,{},hw1
765,no,Why do we need a gatech github for this course?,2019-09-04T00:37:56Z,34,Week 9/1 - 9/7,followup,,jzq2pgabccw28t,k04j8y3o50j12u,2019-09-04T00:37:56Z,{},logistics
766,no,"<p>From &#64;8:<br /><br /></p>
<p dir=""ltr""><strong>Projects</strong></p>
<p dir=""ltr"">There are 3 projects in this class that require programming and analysis.  You will be required to submit your code along with a report for each project to Canvas.  We will release more details for each project.  </p>
<p dir=""ltr""></p>
<p dir=""ltr"">Your code will be submit to a repo <strong>that we will create for you </strong>on<strong> <a href=""https://github.gatech.edu/gt-omscs-rldm"" target=""_blank"">https://github.gatech.edu/gt-omscs-rldm</a>.  </strong>You will be able to find the repo at https://github.gatech.edu/gt-omscs-rldm/7642Fall2019&lt;your_gatech_username&gt;.  We will post an announcement when all the repos should be created.  You will then be responsible for verifying you have access to that repo BEFORE the due date of the first project.  If you do not submit to that repo, you will be deducted 50 points.</p>",2019-09-04T01:05:30Z,34,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04k8e5s7os4pl,2019-09-04T01:05:30Z,{},logistics
767,no,"<p>Hi &#64;Vahe, I am redirected to <a href=""https://github.gatech.edu/jbianchi3"">https://github.gatech.edu/</a>&lt;gatech name&gt;... is that normal? </p>",2019-09-14T04:57:34Z,33,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0j2xcr3oce4q4,2019-09-14T04:57:34Z,{},logistics
768,no,"<p>Maybe not.  I&#39;m not the best person to ask though.  Ask the TAs directly, or start a new post.</p>",2019-09-14T05:04:30Z,33,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0j369n1rx93b5,2019-09-14T05:04:30Z,{},logistics
769,no,"<p>The site (<a href=""https://github.gatech.edu/"">https://github.gatech.edu/</a>) is down for me right now.</p>",2019-09-04T01:33:51Z,34,Week 9/1 - 9/7,followup,,ixnwq0s4ozg,k04l8uv5fkc2m6,2019-09-04T01:33:51Z,{},logistics
770,no,"<p>Several students have reported this.  Make sure to submit a help ticket to <a href=""https://oit.gatech.edu/?ticket=ST-577545-vafLeAfPcTZr9DEflXdt-cas-prod5.gatech.edu"">https://oit.gatech.edu/?ticket=ST-577545-vafLeAfPcTZr9DEflXdt-cas-prod5.gatech.edu</a> so that they are aware and can fix it.</p>",2019-09-04T01:55:29Z,34,Week 9/1 - 9/7,feedback,,hz7meu55mi8sd,k04m0oijkmk5ld,2019-09-04T01:55:29Z,{},logistics
771,no,"<p>Note for everyone, if you use Git Credential Manager, your login/password will be stored in Windows Credentials Manager under &#34;Windows Credentials&#34; section - <a href=""https://support.microsoft.com/en-ca/help/4026814/windows-accessing-credential-manager"">https://support.microsoft.com/en-ca/help/4026814/windows-accessing-credential-manager</a> - do not forget to update them upon expiration.</p>
<p>You need to use Gatech VPN for SSH access to work: <a href=""https://faq.oit.gatech.edu/content/why-cant-i-establish-ssh-session-gt-github"">https://faq.oit.gatech.edu/content/why-cant-i-establish-ssh-session-gt-github</a></p>
<p></p>",2019-09-06T04:05:48Z,34,Week 9/1 - 9/7,followup,,jzhwdil7ssn443,k07ljymm3woki,2019-09-06T04:05:48Z,{},logistics
772,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  I am redirected to <a href=""https://github.gatech.edu/jbianchi3"" target=""_blank"" rel=""noopener noreferrer"">https://github.gatech.edu/</a>&lt;gatech name&gt;... is that normal? </p>",2019-09-14T15:10:08Z,33,Week 9/8 - 9/14,followup,,jzh6k6o994a6dh,k0jot45a4ts2dr,2019-09-14T15:10:08Z,{},logistics
773,no,"<p>How did you get your V equations?  Given the E_k equations are of the form in the videos and HW:  </p>
<p></p>
<p>E_1: v[0] = v[0] &#43; alpha(r &#43; v[1] - v[0])</p>
<p></p>
<p>and</p>
<p></p>
<p>TD[l] = (1-l) (l^0 E1 &#43; l^1 E2 &#43;...)</p>
<p></p>
<p>What I did for E_1 was (and it kinda sorta looks like yours):</p>
<p>E_1: v[0] &#43; alpha(probState1 (r0 &#43; v[1]) &#43; (1-probState1) (r1 &#43; v[2]) - v[0])</p>
<p>...</p>
<p>E_5: v[0] &#43; alpha(probState1 (r0 &#43; r2 &#43; r4 &#43; r5 &#43; r6 &#43; v[6]) &#43;  (1-probState1) (r1 &#43; r3 &#43; r4 &#43; r5 &#43; r6 &#43; v[6]) - v[0]);</p>
<p>...</p>
<p></p>
<p>NOTE: E_5 for me is 16.687</p>
<p></p>
<p>It seems E_inf only has 5 levels of lookahead that is, you can only look no further than state 6.  Also, I&#39;m starting to think that alpha&#39;s will cancel.  I end up having to solve for lambda to the 5th power.  Something is really wrong.</p>
<p></p>
<p></p>",2019-09-03T14:01:44Z,38,Week 9/1 - 9/7,followup,,ixpclzk97jg2fo,k03wisf31vt3vg,2019-09-03T14:01:44Z,{},hw2
774,no,"<p>Oh, yeah I messed up and confused $$\gamma$$ and $$\lambda$$.</p>",2019-09-03T17:18:05Z,38,Week 9/1 - 9/7,feedback,,jzjwcq2u8o7110,k043ja4rmjm2ri,2019-09-03T17:18:05Z,{},hw2
775,no,"<p>It looks like the update equation with alpha 1, and gamma for lambda, which is 1.  When I think about it, I don&#39;t know how to compute TD(lambda), it&#39;s an infinite weighted sum and I only know the E terms maybe up to 5, I&#39;m not sure how to go from 5 to infinity.</p>",2019-09-03T18:34:43Z,38,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k0469uiftos1ol,2019-09-03T18:34:43Z,{},hw2
776,no,"I get the same answer! I’ve done the following.<div>First, calculate Gt (Barto and Sutton notation) which is equal to the TD(1) estimate as the sum of all the rewards, weighted by the probabilities for the first step) which I get to be 16.687.</div><div>Then I calculate the expected values of each state in a k step estimate, I,e, the 1 step estimate which is the reward for the first step plus the estimated value of the next state (weighted by the probability for the first step). I then use the summation formula in the homework pdf to derive a formula for the TD(lambda estimate) as a function of lambda. Based on some of Miguel’s previous answers I add the post termination term from equation 12.3 of Sutton and Barto (figure is reproduced in one of Miguel’s previous answers) to complete the finite sum, making it an infinite sum (this involves Gt)</div><div>I then rearrange the terms to get a 5th order polynomial in lambda, which I equate to Gt.</div><div>I then solve the resulting polynomial using Numpy and get...the same wrong(but oh so close) answer as you!</div><div>I seem to get the right answers in the other two examples, but not in the first problem of the assignment.</div><div>Can anyone help please?</div>",2019-09-03T19:56:31Z,38,Week 9/1 - 9/7,feedback,,jzvoaw0pl6326,k049715eb0o2sy,2019-09-03T19:56:31Z,{},hw2
777,no,<p>I&#39;m under the impression that E_k&#39;s are all with respect to state 0.  That is each state get their own E_1...inf.  The TD we&#39;re looking for is for state 0.</p>,2019-09-03T21:24:18Z,38,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k04cbx4xnby21c,2019-09-03T21:24:18Z,{},hw2
778,no,"<p>I get exactly what geoge gets in this post.  <a href=""/class/jzh9tkzzxkd7ph?cid=87"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=87</a>.  Is E_inf = E_5, because we&#39;re getting a zero reward transition back into 6?  Further, TD(1) evaluates to 0 because of the (1-l) term - perhaps it doesn&#39;t analytically.</p>",2019-09-03T21:55:54Z,38,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k04dgkdd1cy7lj,2019-09-03T21:55:54Z,{},hw2
779,no,"<p>Yeah, after analytically creating a TD[l] that just keeps discounting E_5, because I&#39;m saying E_5 = E_inf, I get lambda to match the first example.  No luck on the second and third - opposite of Deborah Fish.  Basically lambda has to be equal to 1 in case 2 and 3.</p>",2019-09-03T23:22:14Z,38,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k04gjky8cp748p,2019-09-03T23:22:14Z,{},hw2
780,no,<p>The key is to compute E6 correctly.</p>,2019-09-04T20:13:01Z,38,Week 9/1 - 9/7,feedback,,ixpclzk97jg2fo,k05p83oglz32h7,2019-09-04T20:13:01Z,{},hw2
781,stud,<p>Can you elaborate on this? I get to my TD(I) = Gt by doing the prob(sum(r)) &#43; (1-prob)(sum(r)) which is 16.687 like stated above.  But when I try to set that equal to 5th order poly for lambda I do not get $$\lambda$$ = 0.622632.  I don&#39;t see how to compute E6 since we aren&#39;t given any estimated values.</p>,2019-09-05T00:44:29Z,38,Week 9/1 - 9/7,feedback,a_0,,k05yx85fbg724a,2019-09-05T00:44:29Z,{},hw2
782,no,"<p></p>
<pre>    #
    #     (1)
    #  r0/   \r2  r4     r5     r6
    # (0)     (3)----(4)----(5)----(6)----(inf)
    #  r1\   /r3
    #     (2)
    #      |   |      |      |      |       |
    #     E1  E2     E3     E4     E5      E6

</pre>
<p>For TD(1) we are computing using the full episode (V0 -&gt; V6) estimate, G_t. That&#39;s this equation for TD(1):</p>
<p></p>
<p>$$V(S_t) = V(S_t) &#43; \alpha(G_t - V(S_t))$$</p>
<p></p>
<p>Why wouldn&#39;t the $$G_t$$ be $$p(R_0&#43;R_2)&#43;(1-p)(R_1&#43;R_3)&#43;R_4&#43;R_5&#43;R_6$$ ?? (&#64;147) My reasoning is that you won&#39;t get rewards $$R_0$$ and $$R_2$$ 100% of the time, but only p% of the time. Right? You can only get to $$S_1$$ p% of the time, which means you can&#39;t get $$R_2$$ unless you are in $$S_1$$ which is going to be p% of the time.</p>
<p></p>
<p>I&#39;m just starting to think about this, so my own $$\alpha$$ is pretty low.</p>
<p></p>
<p>Thus, using my $$G_t$$, I get a value of 16.687</p>
<p></p>
<p>These are the $$E_k$$ values that I compute using my $$G_t$$</p>
<p></p>
<pre>G_t= 16.687
E_1= 13.553
E_2= 6.087000000000003
E_3= 35.187000000000005
E_4= 27.287
E_5= 16.687
E_6= 16.687</pre>
<p></p>",2019-09-04T00:14:41Z,38,Week 9/1 - 9/7,followup,,jc554vxmyuy3pt,k04if1b0foxuk,2019-09-04T00:14:41Z,{},hw2
783,no,"<p>TD(1) : $$V_t(S) = V_t(S) &#43; \alpha(r_t &#43; \gamma V_{t-1}(S_t) - V_{t-1}(S_{t-1}))w_t$$</p>
<p></p>
<p>$$w_0 = 0$$ by definition in Isbell-Littman videos</p>
<p>$$w_1 = \gamma(0&#43;1)$$</p>
<p>$$w_2 = \gamma(\gamma(0&#43;1)&#43;1) = \gamma(\gamma&#43;1)$$</p>
<p>$$w_3 = \gamma(\gamma(\gamma&#43;1)&#43;1) = \gamma^3&#43;\gamma^2&#43;\gamma$$</p>
<p></p>
<p>$$w =\sum_{i=1,t}{\gamma^i}$$ because the eligibility trace in TD(1) is just $$e(S_t) = \gamma (e(S_{t-1})&#43;1)$$</p>
<p>does that seem correct?</p>",2019-09-04T13:31:11Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k05avcf92s95cq,2019-09-04T13:31:11Z,{},hw2
784,no,"<p>If TD(1) is $$G_t$$, then I expand the finite sum for TD($$\lambda$$) as:</p>
<p></p>
<p><strong>Miguel EDIT (content removed):</strong> <em>Let them figure this part out by themselves. At least for now, there is still time...</em></p>
<p></p>
<p>equating that to $$G_t$$ gives a formula to be solved?</p>",2019-09-05T13:42:28Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k06qpps8ua76r6,2019-09-05T13:42:28Z,{},hw2
785,no,"<p>the root finder I used results in:</p>
<p></p>
<p>[-2.14692153  1.          0.6227695  -0.22113099]</p>
<p></p>
<p>tests 2 and tests 3 are imaginary, so no solution there.</p>",2019-09-05T13:54:57Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k06r5rukmvc3pg,2019-09-05T13:54:57Z,{},hw2
786,no,"<p>This is the right approach.</p>
<p></p>
<p>One thing I will say is you have a different number of E&#39;s than you had in the equation.</p>",2019-09-06T11:47:44Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08220pqo6g6zs,2019-09-06T11:47:44Z,{},hw2
787,no,<p>My E6 is the E-inf term. sorry about the algebra expansion.</p>,2019-09-06T12:54:29Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k084fv3sh2z2ma,2019-09-06T12:54:29Z,{},hw2
788,no,"<p>How/why does your E_6 term increase from 16.687 to 33.374?  (doubling)</p>
<p></p>
<p>My understanding is terminal states have value of 0 (since no later rewards propagate back into them), and reward of 0 (for the same reason, no action can be taken).</p>
<p></p>
<p>Related excerpt from lecture: <a href=""https://youtu.be/TbncXOZdcwo?t=485"">https://youtu.be/TbncXOZdcwo?t=485</a></p>
<p></p>
<p>My E_inf value equals TD(1), which also == E_5 for test case 1 since the final valueEstimate is 0.0. Not sure this is correct, but I notice it&#39;s different from your entry so am pointing it out as an area to take a closer look.</p>",2019-09-06T16:56:42Z,38,Week 9/1 - 9/7,feedback,,isde34zracb1mz,k08d3ceptdg5pn,2019-09-06T16:56:42Z,{},hw2
789,no,<p>That was an old value that was incorrect. On &#64;146 I posted my final values that passed all of the heroku tests.</p>,2019-09-06T17:29:16Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k08e98grrjq42w,2019-09-06T17:29:16Z,{},hw2
790,no,"<p>Ah, thanks!</p>",2019-09-06T17:30:11Z,38,Week 9/1 - 9/7,feedback,,isde34zracb1mz,k08eaexntg1d0,2019-09-06T17:30:11Z,{},hw2
791,no,"<p>Please, keep sharing! Just a drop at a time...</p>",2019-09-06T20:14:41Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08k5yab7px3dm,2019-09-06T20:14:41Z,{},hw2
792,no,"<p>See &#64;82. You can compute the non-bankruptcy optimal value. In the case of the no-bad-faces this comes out to infinity, which means there is no policy of interest.</p>",2019-09-03T13:26:14Z,63,Week 9/1 - 9/7,followup,,jc554vxmyuy3pt,k03v94rt2du194,2019-09-03T13:26:14Z,{},hw1
793,no,<p>Do you have any advice for working it out by hand? There are so many examples and versions of the Bellman Equation that I&#39;m having trouble identifying exactly how to plug the numbers in.</p>,2019-09-03T14:36:29Z,63,Week 9/1 - 9/7,followup,,jl284xdcifz44g,k03xrgoioka6ry,2019-09-03T14:36:29Z,{},hw1
794,no,"<p>Have you tried using one of the dynamic programming algorithms to find V(0), e.g. value iteration?</p>",2019-09-03T14:52:58Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03ycob7pua4uz,2019-09-03T14:52:58Z,{},hw1
795,no,"<p>Like this equation?<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk03yjeqx71bk%2FScreen_Shot_20190903_at_10.57.54_AM.png"" alt="""" /></p>
<p>I&#39;m trying, but I&#39;m having trouble fitting the numbers from the problem into the equation. Is there a more useful version of the Bellman equation that I should be using?</p>
<p></p>",2019-09-03T14:59:32Z,63,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k03yl48244u3d1,2019-09-03T14:59:32Z,{},hw1
796,no,"<p>The equation you posted is part of the algorithm, but you should look at the entire algorithm so you know exactly how to start and what the iterative procedure is.  Take a look, for example, at Sutton and Barto, 2nd ed, page 83.</p>",2019-09-03T15:30:48Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k03zpc0lz6653s,2019-09-03T15:30:48Z,{},hw1
797,no,<p>Can anyone link their by hand solution of this problem?</p>,2019-09-03T17:19:25Z,63,Week 9/1 - 9/7,followup,,jzj7y1ofgsro1,k043l05u5e711w,2019-09-03T17:19:25Z,{},hw1
798,no,"<p>I think it may be more instructive for you to post an attempt at the hand solution, then others can comment on it/add to it.</p>",2019-09-03T17:26:13Z,63,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k043trdftxy6j4,2019-09-03T17:26:13Z,{},hw1
799,no,"<p>States: &#39;stay&#39;, &#39;quit&#39; and &#39;end&#39;    -- i.e &#39;end&#39; state happens when isBad die face shows up</p>
<p></p>
<p>Transition probability of stay to quit = 1</p>
<p>Transition probability of stay to stay = 3/6</p>
<p>Transition probability of stay to end = 3/6</p>
<p>V<sub>π</sub> end(t) =  -  V<sub>π</sub> stay(t-1)     -- i.e  if you reach end, you lose  everything you have earned in prev state</p>
<p>reward (stay, end) = 0</p>
<p>reward (stay, stay) = (4 &#43; 5&#43; 6 )/3 = 5</p>
<p></p>
<p>V<sub>π</sub> stay(0) = 1/2 * 5 = 2.5</p>
<p></p>
<p>V<sub>π</sub> stay(1) = 1/2  * (5 &#43;  V<sub>π</sub> stay(0))  &#43; 1/2 * (reward (stay, end) &#43; V<sub>π</sub> end(0) )  </p>
<p>                  = 3.75  &#43; 0 = 3.75</p>
<p></p>
<p>V<sub>π</sub> stay(2) = 1/2 * (5 &#43; 3.75) &#43; 1/2 * ( 0   &#43; V<sub>π</sub> end(1)  )</p>
<p>                  = 4.375  - 1.25   = 3.125</p>
<p></p>
<p>V<sub>π</sub> stay(3) = 1/2 * (5 &#43; 3.125) &#43; 1/2 * ( 0   &#43; V<sub>π</sub> end(2) )</p>
<p>                  = 4.0625  -  1.875   = 2.1875</p>
<p></p>
<p>...</p>
<p>V<sub>π</sub> stay(4) =  2.03125</p>
<p>V<sub>π</sub> stay(5) =  2.421875</p>
<p>V<sub>π</sub> stay(6) =  2.6953125</p>
<p>V<sub>π</sub> stay(7) =  2.63671875</p>
<p>..</p>
<p>..</p>
<p>V<sub>π</sub> stay(11)  = 2.528076171875</p>
<p>...</p>
<p>...</p>
<p>converges to 2.500000112390497  at around t = 100</p>
<p></p>
<p>what is wrong with the picture</p>
<p></p>
<p></p>
<p></p>",2019-09-04T00:37:25Z,63,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k04j89qmr1l42d,2019-09-04T00:37:25Z,{},hw1
800,no,"<p>Start with your terminology; don&#39;t writeV(0) - write V.0(s) and V.1(s); of the form V.t(s).  Your state is not 0 - it is bank0; the V.t -- is an estimate FOR a state, at time t.  Don&#39;t use numbers for your state, this is where you are getting confused.</p>
<p></p>
<p>second, your equation above is not accurate -- it should be:</p>
<p>V.t(s) = max a ( R(s,a,) &#43; gamma * sigma.s&#39;  T(s,a,s&#39;) * V.t-1(s&#39;)</p>
<p>that is:  the current estimate for each s (for all s at time t), is determined by ... taking the max of the reward for taking action (s,a,) &#43; gamma * sigma of all expected utilities of transitioning into s&#39; (using the previous timestep&#39;s estimate for arriving in s&#39; e.g. V.t-1(s&#39;).</p>
<p></p>
<p>third, the bellman update is applied across all states, at time 0..,t,..t&#43;1.  it is an iterative procedure. i.e.</p>
<p>you calculate out V.0(for all s)...  then you use those values in the next step to calculate out V.1(s)::for all s.</p>
<p></p>
<p>So you can image that if you a table, with n entries for n states (V.current(s))..and a second column for V.new(s))</p>
<p>and on each time step, you calculate out new estimates using V.current(s)... and place those values into V.new(s).</p>
<p>... and when you are done calculate all of the V.new values for all of the states; you copy them over into V.current(s).</p>
<p>... that way all of your V.new(s) calculations, use the same older estimates in the second part of your equation.</p>
<p></p>
<p>.. and while you&#39;re at it; include a delta.v(s) column; before you update your V.current with V.new; take the difference of the two.  This lets you track which values have changed (and by how much) between successive iterations of V.t(s)::for all s.   Then you can detect when you have &#34;converged&#34; enough,  when the difference between your old estimate, and your new estimate for s, has not changed substantially.   of course, you would want to sum all of the delta.v(s)&#39;s.... and when you are close enough, you quit.</p>
<p></p>
<p>with regard to the max a; consider how you are doing your book-keeping.  you have a table T = (s,a,s&#39;).</p>
<p></p>
<p>so for the calculations you need:</p>
<p>R(s,a)... this is simply the reward for taking action a, from s  - in practice your table for T should be T(s,a,s&#39;,r)</p>
<p>T(s,a,s&#39;) .. the probability of going to s&#39;, by action a, for s.</p>
<p>V.t-1(s&#39;) ... the &#34;value function&#34; for s&#39;, from the previous time-step.  when you are doing your calculations, this should be a cached value i.e. this is why there are two columns in your policy table..  you use the old values for all of your calculations; make the V.new...s and then move them to replace the V.current.</p>
<p></p>
<p>once you have all of those values, the next step is to start summing.</p>
<p>so your sigma is written as sigma s&#39;; but in reality it is sigma a, sigma s&#39;.</p>
<p>that is first you sum all the V.t-1(s&#39;) * T(s,a,s&#39;)</p>
<p>and then you sum all of those, by action.</p>
<p></p>
<p>so by way of example:  V.t(s)  should have two things:<br />a set of actions associated with s;<br />a set of s&#39;s reachable by taking a from s;</p>
<p></p>
<p>and what you are doing by summing is:</p>
<p>first summing all of the transitions(s,a,s&#39;) V.t-1(s&#39;) to get a sum for each state s&#39; reachable by taking a from s ... <br />note: an action may lead to more than one s&#39;!  and and an s&#39;, may be reachable by more than one a from s -- consider the example of two actions from s, a1 and a2, each with a 50/50 probability of going to s&#39;.1 s&#39;2...    so s&#39;1 has two &#34;in-actions&#34; ... s,a.1,s&#39;1 and s,a.2,s&#39;1 ... and s&#39;.2 has two &#34;in actions&#34; s,a.2,s&#39;2 and s,a.2, s&#39;2...</p>
<p>.. so this is the sum you are calculating with sigma.s&#39;;  you are simply summing all of the actions leading into state s&#39;.</p>
<p>now.</p>
<p>this pops out two actions - a.1, and a.2 from s going to two different states (s&#39;.1, s&#39;.2)  ; so now the next step is to sum over a&#39;s</p>
<p>in effect you are collapsing it down to two simple entries.   s,a.1,(some expected utility) and s,a.2 (some expected utility).  <br />...you don&#39;t care where you end up; you ONLY care, which action has the highest sum.<br />... and this is why R(s,a) is R(s,a) -- you add in the rewards for taking an action, for each action (s,a.1) (s,a.2)</p>
<p>... and then you take the max of those two, to spit out &#34;which action is best&#34;.</p>
<p>... and THEN, you update V.t(s) so that its utility, is the utility of that best action.</p>
<p></p>
<p>To recap:</p>
<p>-- it is V.t(s) = ... V.t-1(s&#39;)</p>
<p>- you calculate V.t for all s, using cached values of V.t-1(s&#39;)</p>
<p>- you do some sum magic to get the expected utility for all actions (s,a)</p>
<p>- you take the best action .. and then use ONLY that utility to update V.t(s)</p>
<p>- and you keep doing those tabular updates for all s, for all t.  until your delta&#39;s are less than some value, epsilon.</p>
<p></p>
<p>Hope this helps.</p>
<p></p>
<p>addendum: i made some small corrections.   you should try following this procedure, using your V.0(s) ...  it should become clear once you do it once on paper.  also I would change your states to strings not numbers until you get the procedure clear in your head i.e. state(0) =&gt; b0, 1=&gt; b1 etc.</p>
<p></p>
<p>one last note, and this is subtle:</p>
<p>terminal states, don&#39;t have T(terminal, a, s&#39;)...  they only show up as T(s,a,terminal)...  so in reality, their V.t(s) never changes - it is always the reward state &#43; gamma * sigma (0) ...  so you don&#39;t really need to include them for calculating out V.t(s) for all s; since they never actually change.   You could, for programming (but not computational!) efficiency include them.  there value will never change.</p>
<p></p>
<p>one caveat:  this is a length post, I may have made an error in explaining it.  If something I said does not make sense, fix it, until it is right.</p>
<p></p>
<p>FINI.</p>",2019-09-03T16:26:41Z,40,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k041p6lo53q5zb,2019-09-03T16:26:41Z,{},hw1
801,no,"<p>WOW thank you so much for this response. I&#39;m trying to work through the problem now, I&#39;ll post any more questions as follow ups so it&#39;s easier to follow</p>",2019-09-03T20:23:10Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k04a5b7ma06mq,2019-09-03T20:23:10Z,{},hw1
802,no,"<p>np.  I hope, that it helps, and not hurts.  best of luck.</p>",2019-09-04T02:15:44Z,40,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k04mqpy5mf04j1,2019-09-04T02:15:44Z,{},hw1
803,no,"<p>Given the following tables:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk04bqb53ruyn%2F20190903_170203.jpg"" alt="""" width=""497"" height=""662"" /></p>
<p></p>
<p>I was able to plug the values in to the following equation:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk04btbjiwlq3%2F20190903_170224.jpg"" alt="""" /></p>
<p></p>
<p>But I&#39;m unsure how to get an actual value for V<sub>t</sub>(b0) because I don&#39;t have any values for V<sub>t-1</sub>(s) to use in the summation. I have this table ready to be filled in, but where do our numbers come from on the first iteration?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk04bx2ps2fxo%2F20190903_170218.jpg"" alt="""" /></p>",2019-09-03T21:12:55Z,40,Week 9/1 - 9/7,followup,,jl284xdcifz44g,k04bxa8rc6865a,2019-09-03T21:12:55Z,{},hw1
804,no,"<p>Nice Table!</p>
<p></p>
<p>You need to <em>initialize</em> $$V$$.  There should be a $$V_0$$ with the initial values.  See Sutton and Barto page 83, for example, for a precise description of the Value Iteration Algorithm.</p>
<p></p>
<p>In any event, it turns out you are free to choose any initial values for $$V_0$$, except that terminal states must be initialized to $$0$$.</p>",2019-09-03T21:26:58Z,40,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04cfcntfb241b,2019-09-03T21:26:58Z,{},hw1
805,no,<p>Thank you for all the help! I&#39;m going to break into this tomorrow. </p>,2019-09-04T03:03:22Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k04ofyl8mml2fo,2019-09-04T03:03:22Z,{},hw1
806,no,"<p>As mentioned:  V0.(s) for all s, is initialized to zero.<br /><br />I have two suggestions up front, before you read any further:<br /><br />* simplify your model to d2 where isbad(0,1)  -- it will make your calculations easier<br />* calculate out not just V.1; but V.2, V.3 <br /><br />---<br /><br />With regard to your last page (prospective calculation of V1(b0):<br /><br />V1.(b0) = max.a [ R(b0,a) &#43; 1 sigma.s&#39; T(s,a,s&#39;) * V.t-1(BR) <br /><br />it should be noted, that different actions lead you to different rewards;<br /><br />that is, your R(b0, roll), should be:  R(b0, [roll or quit or bankrupt])..<br />... since you have not taken the max.a R(s,a) yet, you have not determined<br />how to break up the  T(s,a,s&#39;) * V.t-1(BR) under the sigma.<br /><br />it might help you to consider this alternate form (for the stuff under sigma):<br /><br />sigma s&#39; V.t-1(Br | bankrupt) * T(s,bankrupt,Br)<br />or<br />sigma s&#39; V.t-1(B4 | roll) * T(s,roll,B4)<br />or<br />sigma s&#39; V.t-1(B5 | roll) * T(s,roll,B5)<br />or<br />sigma s&#39; V.t-1(B6 | roll) * T(s,roll,B6)<br />or<br />sigma s&#39; V.t-1(Out | quit) * T(s,quit,Out)<br /><br />that is; once you have the sums, from the probability of transition to state s&#39; * the old utility of s&#39; (and have grouped by s&#39;);<br />you need to think about how you handle R(s,a) *which depends on the actions you took to get to s&#39;*.<br /><br />so from above:<br /><br />R(b0,bankrupt) = 0<br />R(b0,roll) = 0 -- doesn&#39;t matter where you went, since R(s,a) should be a weighted combination of those s&#39;.<br />R(b0,quit) = ?<br /><br />because once you have calculated out all the utilities using just s&#39; (under sigma s&#39;),<br />you still have to sum once more, for action a.<br /><br />that is:<br /><br />max.a = take the max of the possible actions.  you are trying to decide, which action to do.  <br /><br />it is NOT just based on the probability of transition * expected utility for any s&#39; reachable from s.<br /><br />it also strong depends, on the reward you get...  <br /><br />so here:<br /><br />roll -&gt; b4, b5, b6 ... which means you have multiple final states, reachable by a from s, that need to be summed, to determine the *actions* total utility.<br />similarly, you have &#39;quit&#39; or &#39;bankrupted&#39;.<br /><br />whichever one of these is the max.a (maximum utility) determines &#39;which action you took&#39;, and hence which of the total utilities to use in your update.<br /><br />this also implies, that R(s,a) is some sort of weighted combination...  see my note below.<br /><br />________________<br /><br />one last note:  there is another form of the equation which I used which makes the intuition easier.<br /><br />the whole point of V.t(s); is to give you a ranking *at time t*, of which actions you want to might want to take.<br /><br />and this is, from an intuitive standpoint, simple:<br /><br />* I am at state x.<br />* I want to determine WHICH actions I can take from x.<br />* I want to determine WHICH s.prime&#39;s those actions take me to.<br />* I want to lookup, WHAT my old estimates are for those s.primes<br />* I want to determine, WHAT my reward is for reaching that an s&#39;, by action a.<br />... that gives me the ideal state:  what can I expect to gain (if T(s,a,s&#39;) = 1)...(if I took a, to get to s&#39;)<br />* and then I want to determine, HOW likely am I to get that &#34;expected total utility&#34; (using the actual T(s,a,s&#39;).<br />... and then I want to add up all of those &#34;expected total utilities&#34; for each action (doesn&#39;t matter if its stochastic and I might end up in other states); the total utility at that point, has to include the award, which is a function of which action I took, not which state I ended up in!<br />... and then I want to select the best one.<br /><br />so the form for this is:<br /><br />max.a T(s,a,s&#39;) * sigma.s&#39; R(s,a,s&#39;) &#43; gamma * V.t-1(S)<br /><br />this is easier to handle, because in this case you know that the reward for reaching state s&#39;, is heavily influenced by the action you take to get to it.<br /><br />functionally the form of the equation you are doing handles the same stuff.  but its difficult to envision how to calculate R(s,a) in a clean fashion...<br /><br />and remember:<br />* you are looking to use old estimates V.t-1(s&#39;) for all transitions from s, (which may include multiple a&#39;s)<br />* but at the end of the day, the only thing you really care about, is the total expected utility of the action itself.<br />... so you can make a decision.<br /><br />it helps, if you put yourself mentally, in the game.  if I were at b0; given two or three actions, which ones would I think were most useful given R, T, and V.t-1.<br /><br />this will guide you in working out the math.   I should reiterate. DO NOT, just do V.1.  this is almost a pathological case.  Keep going through V.2, and V.3 ;) .. and try the simplest model you can first.  I suggest 2 dice, and 1 good face, 1 bad faces.  then 2 good faces.   It keeps you from doing a lot of useless calculations; and you will get the gist of it quick.<br /><br />good luck!<br /><br /><br />addendum:  in the case of a deterministic mdp, or where you always know where the action will take you; the R(s,a) is easy to calculate ;)  the third equation referenced above, handles those cases with some extra calculations ;) but its not strictly necessary :)</p>",2019-09-04T02:10:56Z,40,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k04mkj62hwd2gt,2019-09-04T02:10:56Z,{},hw1
807,no,<p>Thank you so much for this response! I&#39;m going to go through this and all my other notes again tomorrow</p>,2019-09-04T03:11:43Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k04oqpnng0q2lr,2019-09-04T03:11:43Z,{},hw1
808,no,<p>np :)</p>,2019-09-04T14:53:20Z,40,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k05dszi51vs4kk,2019-09-04T14:53:20Z,{},hw1
809,no,"<p>I thought I had it, but I&#39;m still not getting 2.5833. I have similar results to &#64;119, but on their third iteration they end up with 2.5833 as the expected value, and I get 2.5 again. </p>
<p></p>
<p>Here&#39;s my value iteration table:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk05m07ryokv8%2F20190904_143536.jpg"" alt="""" width=""501"" height=""669"" /></p>
<p></p>
<p>And here&#39;s my attempt at solving for V<sub>t</sub>(b0) for t=2:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk05m1nungdsz%2F20190904_143551.jpg"" alt="""" width=""495"" height=""660"" /></p>
<p></p>
<p>My answer essentially comes down to (1/6) * 4.5 &#43; (1/6) * 5 &#43; (1/6) * 5.5 = 2.5. I&#39;m sure I&#39;m missing something in the equation, but I&#39;ve gone through it again and I can&#39;t tell where the issue is. Am I using value iteration properly?</p>",2019-09-04T18:45:45Z,40,Week 9/1 - 9/7,followup,,jl284xdcifz44g,k05m3vmtvt110t,2019-09-04T18:45:45Z,{},hw1
810,no,"<p>When you do your calculation for the &#34;ROLL&#34; action, you&#39;re assuming that you must always roll, even in future states.  Remember that the value of a future state assumes that, once again, you&#39;re taking the maximum EV action at that future state.  It might be to roll, it might be to quit...</p>",2019-09-04T19:57:48Z,40,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k05oojeax2rag,2019-09-04T19:57:48Z,{},hw1
811,no,"<p>I got it! Thank you! That took a lot of paper, but I think I understand it now</p>",2019-09-04T20:25:29Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k05po4wi1qm7iy,2019-09-04T20:25:29Z,{},hw1
812,no,<p>Now I just have to figure out pymdptoolbox</p>,2019-09-04T20:25:43Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k05pofkhwbav,2019-09-04T20:25:43Z,{},hw1
813,no,<p>congrats :)</p>,2019-09-04T21:03:09Z,40,Week 9/1 - 9/7,feedback,,jzivtxcbl6964n,k05r0l2vksg5l2,2019-09-04T21:03:09Z,{},hw1
814,no,"<p>&#64;Dalton Bassett could you explain what you changed above to get it to work? While I understand what Vahe said intuitively, I&#39;m not sure how it changes the calculations. When rolling from B0, there are still only four possible outcomes. Do the probabilities change to incorporate the possibility of quitting? Do we need to do something with gamma? This has been my sticking point so far, and pouring over piazza, calculating by hand, and modifying my code have done nothing to change the fact that I have no idea what I&#39;m doing.</p>",2019-09-05T00:44:05Z,40,Week 9/1 - 9/7,feedback,,jl2bq5rf8b67pq,k05ywoxs9487j9,2019-09-05T00:44:05Z,{},hw1
815,no,"<p>&#64;Caroline</p>
<p></p>
<p>1. We know rolling is better than quitting at B0, so we roll (that&#39;s our maximum EV action).</p>
<p></p>
<p>2. Some fraction of the time we end up in B4, some fraction B5, and another fraction B6.</p>
<p></p>
<p>3. We need to now add the value of these three states, multiplied by the probability of getting there, to our starting value of B0.</p>
<p></p>
<p>4. Go back to Step 1 for each of the three states above (this is what Dalton wasn&#39;t quite doing initially).</p>",2019-09-05T02:09:57Z,40,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k061z4ce2ek2ow,2019-09-05T02:09:57Z,{},hw1
816,no,<p>That&#39;s the best part of this course... using lots of paper and pencil and figuring these algorithms by hand before implementing your solution... good times!</p>,2019-09-05T02:47:13Z,40,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k063b1jgpi33ld,2019-09-05T02:47:13Z,{},hw1
817,no,"<p>&#64;Caroline, it clicked when I remembered that V<sub>t-1</sub>(s) at the end of the equation is shorthand for the entire equation written over again for the previous timestep. If you write the whole thing out (instead of just V<sub>t-1</sub>(s), use the whole equation) you&#39;ll realize that you need to choose the max of an expected value of 6 vs expected value of 5.5. My issue is that I was just using the 5.5. </p>",2019-09-05T16:29:14Z,40,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k06wo60xhlr2sk,2019-09-05T16:29:14Z,{},hw1
818,no,"<p>Ah ha! Ten sheets of paper and many curses later, I have discovered my issue! With my (incorrect) understanding, I was doing ΣP(a, s&#39;)*(V(s&#39;)&#43;Vk-1(s&#39;)) when in fact it is more intuitively just (to me at least) ΣP(a, s&#39;)*(Vk-1(s&#39;)). I&#39;ve included the before/after solutions below for reference. I was wondering why my values kept going up and up, it was because I was adding way more reward than I needed to!</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2bq5rf8b67pq%2Fk07e7f2ehbnl%2Fbad.jpg"" alt="""" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2bq5rf8b67pq%2Fk07e9nzxcrjv%2Fgood.jpg"" alt="""" /></p>",2019-09-06T00:43:17Z,40,Week 9/1 - 9/7,feedback,,jl2bq5rf8b67pq,k07ebil3dt96z2,2019-09-06T00:43:17Z,{},hw1
819,no,"<p>Hey Caroline,</p>
<p></p>
<p>My math was a little similar to your initial calculations. I notice you had no negative reward when going bankrupt. That may explain why your values were increasing so much. For example when you go from $0 to $4 you are now in state 4, but when you go bankrupt in state 4 you are not taking that $4 away from the player. </p>",2019-09-06T02:41:19Z,40,Week 9/1 - 9/7,feedback,,j6m1jeidndu6wq,k07ijbhwxld4vj,2019-09-06T02:41:19Z,{},hw1
820,no,<p>I think we should go though value iteration for a simple problem this weekend.</p>,2019-09-06T03:11:00Z,40,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07jlhek7lsdg,2019-09-06T03:11:00Z,{},hw1
821,no,"<p>One thing to pay attention to is to move the reward inside the expectation if your reward depends on the state you land on... So, if you represent your MDP for this homework as getting paid $s&#39;, after landing on a state s&#39; but you use the equation at the top of this post, you will have problems! There the reward only depends on the state and action!!!</p>
<p></p>
<p>Simply move the reward as being part of the expectation so that the probability weights the full: $$r &#43; \gamma v_{k}(s&#39;)$$</p>
<p></p>
<p>My advise is use equation 4.10 from Sutton:</p>
<p></p>
<p>$$v_{k&#43;1}(s) = max_a \sum_{s&#39;,r}p(s&#39;, r | s, a)[r &#43; \gamma v_{k}(s&#39;)]$$</p>",2019-09-06T03:50:06Z,40,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07kzs7iy294us,2019-09-06T03:50:06Z,{},hw1
822,no,"<p>I think what I am not getting is - how do we know when it is time to stop iterating? <br /><br />Because in this case, I would think that moving forward would be better than quitting in all except the 6 case (since 6&gt;5.5) <br /><br /></p>
<p>Like why do we not go to the next for states 8,9,10,11,12 ? </p>",2019-09-08T20:46:58Z,39,Week 9/8 - 9/14,feedback,,jqr9leosvw8P,k0bg766fnca1fr,2019-09-08T20:46:58Z,{},hw1
823,no,"<p>I think, you stop when it converges. Meaning, when the Vt&#43;1 (s) = Vt(s), for all s.</p>",2019-09-09T00:59:04Z,39,Week 9/8 - 9/14,feedback,,jl8j7vzvUNs2,k0bp7dmhycb34d,2019-09-09T00:59:04Z,{},hw1
824,no,"<p>Thank you &#64;Sriram - I thiink I am getting way closer now, but I have to rewrite some things. </p>",2019-09-09T01:02:47Z,39,Week 9/8 - 9/14,feedback,,jqr9leosvw8P,k0bpc5qbtka58o,2019-09-09T01:02:47Z,{},hw1
825,no,"Hi Dalton, in the value iteration table, how/why do you start with states 0, 4, 5, 6, 7, 8, 9, 10, 11, 12?",2019-09-08T22:13:07Z,39,Week 9/8 - 9/14,followup,,jqu95q68ljj1pn,k0bj9yjqw7j4q4,2019-09-08T22:13:07Z,{},hw1
826,no,"<p>So I ignore states 1, 2, 3 because there&#39;s no way to reach them (you&#39;ll always end up bankrupt if you roll those numbers). I guess I shouldn&#39;t have used 7, because there&#39;s also no way to roll a 7. But I used 4-12 because that simulates 2 rolls (the max earnings from two rolls is 6). I think that&#39;s the wrong way of approaching it, but that was my logic at the time. If I did this problem by hand again, I would start with 0, 4, 5, and 6 and iterate over those states. </p>",2019-09-08T23:40:09Z,39,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0bmdvscq0f7kj,2019-09-08T23:40:09Z,{},hw1
827,no,"<p>Thanks Vahe. The probability for each of these corners would be 0.25 ?</p>
<p></p>
<p>Two corners have V1 = 0. Should we multiply V with 0.5 here to take probability into account ?</p>
<p></p>",2019-09-05T07:32:59Z,40,Week 9/1 - 9/7,followup,,jfzaqnqvtQ1m,k06dijl2kjx4pa,2019-09-05T07:32:59Z,{},hw1
828,no,"<p>No, all four corners have a value of -0.2 after the first iteration.  I think you are misunderstanding the problem.</p>
<p></p>
<p>The page you linked is not the original problem - it shows a sub-grid of the full grid.</p>
<p></p>
<p>Scroll down to Example 9.25: <a href=""https://artint.info/html/ArtInt_224.html#gridworld-ex"">https://artint.info/html/ArtInt_224.html#gridworld-ex</a></p>",2019-09-05T07:47:23Z,40,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k06e12dbiy24g,2019-09-05T07:47:23Z,{},hw1
829,no,"<p>Another approach is to add states as the transition function produces them, with an initial value of 0, and rely on the stopping condition of value iteration (delta &lt; epsilon) to break the loop. This way there is no need to predefine the horizon length.</p>",2019-09-03T18:50:31Z,43,Week 9/1 - 9/7,followup,,is8ald0uljj3u4,k046u5u8h8u5g7,2019-09-03T18:50:31Z,{},hw1
830,no,"<p>So lets add this in the following is then the states:</p>
<p>0, 4, 5, 6, 8, 9, 10, 11, 12, 0(terminal bad side roll), 4 - 12 terminal</p>
<p>?</p>",2019-09-03T17:55:36Z,43,Week 9/1 - 9/7,followup,,jzj7y1ofgsro1,k044vjmbt713xp,2019-09-03T17:55:36Z,{},hw1
831,no,"<p>Yeah, now you have all possible states for two rolls.</p>",2019-09-03T18:05:50Z,43,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k0458p0c11d1kt,2019-09-03T18:05:50Z,{},hw1
832,no,"<p>you should notice that the number of states to track really depends on the number of rolls you want to track. (assuming state = bankrolls) For starter, i did N = 2 and track 2 rolls. Then I expand the concept from there. (since it easier to visualize with a small N ) </p>",2019-09-05T07:47:22Z,43,Week 9/1 - 9/7,followup,,jvfpllmsggt7p4,k06e11i28c57ek,2019-09-05T07:47:22Z,{},hw1
833,no,Correction as per Vahe(thank you btw) 2nd iteration vest0= 2.5 right?,2019-09-03T19:14:22Z,43,Week 9/1 - 9/7,followup,,jzj7y1ofgsro1,k047otvjwt5aa,2019-09-03T19:14:22Z,{},hw1
834,no,<p>Right!  Any luck with coding it?</p>,2019-09-03T19:41:13Z,43,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k048ncsh1xjbb,2019-09-03T19:41:13Z,{},hw1
835,no,<p>with mdptoolbox for this case I did. the trick for me was to realize a terminal state had a probability = 1 for itself.</p>,2019-09-04T18:31:21Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k05lldgwe1i20t,2019-09-04T18:31:21Z,{},hw1
836,no,"<p>I have a state for just &#39;stay&#39;, &#39;end&#39; and &#39;quit&#39; </p>
<p></p>
<p>So for 6 side die example, the transition probability from &#39;stay&#39; to &#39;end&#39; will be 3/6</p>
<p></p>
<p> </p>",2019-09-04T00:22:00Z,43,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k04iog8b6uqwd,2019-09-04T00:22:00Z,{},hw1
837,no,"<p>You might want to rethink your state and actions.  At each point in time, what value or &#34;state&#34; do you use to make your decision?  What decision do you make at each time step?</p>
<p></p>
<p>Also check out <a href=""/class/jzh9tkzzxkd7ph?cid=89"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=89</a> for more hints.</p>",2019-09-04T01:59:38Z,43,Week 9/1 - 9/7,feedback,,hz7meu55mi8sd,k04m609umxd1we,2019-09-04T01:59:38Z,{},hw1
838,no,<p>Can someone explain why the calculation for Vest(0) doesn&#39;t equal 0? I keep getting 2.5 for my optimal policy</p>,2019-09-04T03:28:05Z,43,Week 9/1 - 9/7,followup,,hl6t3szewpti8,k04pbrgpf1314l,2019-09-04T03:28:05Z,{},hw1
839,no,I presume you mean for the third iteration. This is because the value is 1/6*V4est&#43; 1/6*V5estq&#43; 1/6*V6est. As the best action is to roll and those are the highest values for those outcomes.,2019-09-04T03:51:29Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k04q5ukojvw10z,2019-09-04T03:51:29Z,{},hw1
840,no,"<p>I have 2 questions:</p>
<ol><li>In case of 2 rolls, why there will be states of 4, 5, 6? Does that mean the first roll is on the bad side and the second roll on the good side? (But the game ends when you roll on the bad side, based on Description 4.a.ii)</li><li>Why the Vest4=4.5 and Vest5=5.5 in 2nd iteration? And why they are not changing in third iteration? I just cannot figure it out.</li></ol>",2019-09-04T04:11:48Z,43,Week 9/1 - 9/7,followup,,jl2bqebwzel2s,k04qvzhor5s35r,2019-09-04T04:11:48Z,{},hw1
841,no,"<p>1. In the two-roll mini-game they&#39;ve posted above, you don&#39;t <em>have</em> to roll twice.  You can quit after one roll (or no rolls).  Two rolls is just the maximum.</p>
<p></p>
<p>2. I think you misread Vest5.</p>
<p></p>
<p>Remember that in this game, you don&#39;t <em>have</em> to roll.  Sometimes the optimal action for a given state is to quit.  What must be true about $$\mathbb{E}[\text{Quitting}]$$ vs $$\mathbb{E}[\text{Rolling}]$$ for quitting to be better than rolling?</p>",2019-09-04T04:41:29Z,43,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04ry5lymmm3oi,2019-09-04T04:41:29Z,{},hw1
842,no,"<p>Sorry, my question 2 is about Vest4=4.5 and Vest6=5.5 respectively.</p>
<p></p>
<p>I think I got it. My understanding is, taking state of bankroll=4 as example, there are 2 possible actions: roll or quit</p>
<ul><li>$$Q(4, roll) = R(4, roll) &#43; \gamma \sum_{s&#39;}^{}[p(s&#39;)*V_{t-1}(s&#39;)]=0&#43;1*[\frac{1}{2}*0&#43;\frac{1}{6}*(8&#43;9&#43;10)]=4.5$$</li><li>$$Q(4, quit) = R(4, quit) &#43; \gamma \sum_{s&#39;}^{}[p(s&#39;)*V_{t-1}(s&#39;)]=0&#43;1*[1*4]=4$$</li></ul>
<p>So taking the $$argmax_{a}(Q(4, roll), Q(4, quit))$$ would be $$Q(4, roll) = 4.5$$</p>
<p></p>
<p>For Vest5, no difference between roll and quit.</p>
<p>For Vest6, $$argmax_{a}(Q(6, roll), Q(6, quit))$$ would be $$Q(6, quit) = 6$$ rather than $$Q(6, roll) = 5.5$$, right?</p>
<p>So the 2nd and 3rd iteration in the top post should be refined into Vest6=6. Is that correct?</p>
<p></p>
<p>And then, for 3rd iteration of Vest0, it would be 1/6*(Vest4 &#43; Vest5 &#43; Vest6) = 1/6*(4.5&#43;5&#43;6) = 2.5833</p>
<p></p>
<p>Am I right?</p>",2019-09-04T05:35:13Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k04tv8xdxf216p,2019-09-04T05:35:13Z,{},hw1
843,no,<p>You got it!</p>,2019-09-04T08:18:05Z,43,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k04zoou8wyu1h6,2019-09-04T08:18:05Z,{},hw1
844,no,"<p>hmm I get the γ∑s′[p(s′)∗Vt−1(s′)] , but why does R(4,roll) = 0?</p>",2019-09-04T09:55:39Z,43,Week 9/1 - 9/7,feedback,,jl2842xr2jt3fe,k05366gqip508,2019-09-04T09:55:39Z,{},hw1
845,no,"<p>In my opinion, all the R(n, roll)=0 and R(n, quit)=n. Therefore, Vest4q, for example, Vest4q=R(4, quit)&#43;0=4.</p>
<p>The summation part would be all 0 because there is no next state (said s&#39;) for a=quit.</p>",2019-09-04T10:32:54Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k054i30aloo64c,2019-09-04T10:32:54Z,{},hw1
846,no,"<p>Oh I see, that makes sense. If you are not quitting, then the reward would be zero since you are leaving all your money on the bet. But if you quit, then you leave with the reward.</p>",2019-09-04T17:31:49Z,43,Week 9/1 - 9/7,feedback,,jl2842xr2jt3fe,k05jgsv2rel6os,2019-09-04T17:31:49Z,{},hw1
847,no,"<p>In Q(4, roll), why does p(s&#39;) = 1/2 * 0? EDIT: Misread it. Ignore my comment :D</p>",2019-09-05T02:48:07Z,43,Week 9/1 - 9/7,feedback,,jzj3l9wevx4734,k063c77ir3x127,2019-09-05T02:48:07Z,{},hw1
848,no,"<p>Actually, why is the probability of rolling 1,2 or 3 = 1/2? Isn&#39;t it 1/6? EDIT: Lol nvm, got it cause it has to be stochastic.</p>",2019-09-05T02:52:05Z,43,Week 9/1 - 9/7,feedback,,jzj3l9wevx4734,k063hb2nagn3g4,2019-09-05T02:52:05Z,{},hw1
849,no,"<p>It&#39;s because I simplify the (1/6*0 &#43; 1/6*0 &#43; 1/6*0) into (1/6&#43;1/6&#43;1/6)*0=1/2*0.</p>
<p>You can also think that the probability of rolling into the bad side is 1/2 and getting 0 reward.</p>",2019-09-05T05:12:49Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k068iau0k6s12p,2019-09-05T05:12:49Z,{},hw1
850,no,"<p>BIG thanks to this thread!  I had a reasonable yet murky idea of how to proceed, but was making some similar mistakes to other students.  This helped me work through the simple example by hand and understand what was going on. Writing the code to handle any case was pretty straightforward after that!!</p>",2019-09-07T22:26:52Z,43,Week 9/1 - 9/7,feedback,,jzhnj3cj4pl2uy,k0a4bskzf5s16t,2019-09-07T22:26:52Z,{},hw1
851,no,"<p></p>
<blockquote>
<p>In my opinion, all the R(n, roll)=0 and R(n, quit)=n</p>
</blockquote>
<p></p>
<p>I think that is right. However, we are not required to have initial value of a state to be 0. Some other values would also work: -inf, &#43;inf. Actually, I think any value would work, since we will eventually stabilize toward optimal value. I choose to initialize with $$ R(n, roll) = n, R(n, quit) = n $$ the rational being if we have $$n$$ in our bank roll, we will estimate and likely end up with $$ $n $$ in the pocket than in other values. That would likely speed up the convergent.</p>",2019-09-09T07:05:17Z,42,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0c2ac0oqbj5db,2019-09-09T07:05:17Z,{},hw1
852,no,"<p>I think I&#39;m doing most of the value-iteration process right. I&#39;m just stuck on how to pull the converging value.</p>
<p></p>
<p><span style=""text-decoration:underline"">Here&#39;s my though process for the value-iteration process:</span></p>
<p></p>
<p>Value(s) at 0: (0, 0, 0)</p>
<p>=&gt;Value(4) at 1 = max between [(1/6)*(4 &#43; Value(4) at 0)) &#43; ((1/6)*(5 &#43; Values(5) at 0)) &#43; ((1/6)*(6 &#43; Values(6) at 0))] and [1*R(S)] = 2.5</p>
<p><strong>*Which is the same for the other states</strong></p>
<p>=&gt; Value(s) at 1: (2.5, 2.5, 2.5)</p>
<p>=&gt; Value(4) at 1 = max between [(1/6)*(4 &#43; Value(4) at 1)) &#43; ((1/6)*(5 &#43; Values(5) at 1)) &#43; ((1/6)*(6 &#43; Values(6) at 1))] and [1*R(S)] = max between 3.75 and 4.</p>
<p>=&gt; Value(s) at 2: (4, 5, 6)</p>
<p></p>
<p>So, if I&#39;m doing the value-iteration process right, how do I pull the converging value out of this? Thanks in advance!</p>",2019-09-04T06:48:50Z,43,Week 9/1 - 9/7,followup,,jzkke6iz3cl28t,k04whxhqxzz53f,2019-09-04T06:48:50Z,{},hw1
853,no,"<p>I think the problem is Value(4) at 1 should be max between Q(4, roll) and Q(4, quit), which should be [(1/6)*(Value(4) at 0)) &#43; ((1/6)*(Values(5) at 0)) &#43; ((1/6)*(Values(6) at 0))] (<strong>No dollar earned here</strong>) and [1*Vest4q] (<strong>Not reward function</strong>), said 0 and 4 respectively.</p>
<p></p>",2019-09-04T08:38:29Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k050exsa7lv1c6,2019-09-04T08:38:29Z,{},hw1
854,no,"<p>Thanks for the replay! I adjusted my approach to follow the one you posted above. I got the same answer for the 3rd iteration but after that it kept growing. Were you able to solve the problem? If so could you expand on what you did during the third (and beyond) iteration?</p>
<p></p>
<p>For example, if you had a bankroll of 8 would you have Q(4, roll) as (1/6)(12 &#43; 13 &#43; 14)? If so, that would just blow up over time. How would this approach continue to converge?</p>",2019-09-06T05:33:42Z,43,Week 9/1 - 9/7,feedback,,jzkke6iz3cl28t,k07op0eaz103x1,2019-09-06T05:33:42Z,{},hw1
855,no,"<p>Actually the post expand all the calculation and there is little problem on 2nd iteration. For more information, you can check my previous reply. I had expanded for how to calculate Vest4 for 2nd iteration and the following calculation on Vest5 and Vest6 based on the same logic. And the calculation will converge at 3rd iteration.</p>
<p>If still problem on this, how about expanding your calculation procedure like the post did and we can discuss based on that? Clarifying your on thought is easier than absorbing idea from others. :)</p>",2019-09-08T03:56:51Z,42,Week 9/8 - 9/14,feedback,,jl2bqebwzel2s,k0ag45ou6814cr,2019-09-08T03:56:51Z,{},hw1
856,no,"<p>In the above example, what does Vest0 represent?. If it represents the bankroll, how is it possible that you can have a second or third  iteration for Vest0</p>
<p></p>
<p>Also I dont see the state that represents the agent getting kicked out of game because of isBad dice (ie 1,2,3). Is that someway Vest0</p>",2019-09-04T17:07:06Z,43,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k05il0b7nf03z4,2019-09-04T17:07:06Z,{},hw1
857,no,<p>Vest0 is the estimate of the value of the bankroll = 0 state. In value iteration all your states are set to zero. Then after iterations they get update to their true value.</p>,2019-09-04T18:29:55Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k05ljil35uh7eg,2019-09-04T18:29:55Z,{},hw1
858,no,"<p>if Vest0  -&gt; bankroll = 0, its weird that it can have a value higher than 0, at any iteration.. no? Let me think about it more..</p>",2019-09-04T20:32:05Z,43,Week 9/1 - 9/7,feedback,,jqrr36mqfm8M,k05pwmu5n4k130,2019-09-04T20:32:05Z,{},hw1
859,no,Remember value is not only reward but future possible rewards,2019-09-04T22:27:02Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k05u0g6r15915,2019-09-04T22:27:02Z,{},hw1
860,no,"<p>yeah got it. Vest0 is not the state of bankcruptcy, rather its the state to represent the starting bankroll</p>",2019-09-04T22:40:33Z,43,Week 9/1 - 9/7,feedback,,jqrr36mqfm8M,k05uhuhpz5548e,2019-09-04T22:40:33Z,{},hw1
861,no,"<p>If I define the probabilities and rewards from each state to possible states like below</p>
<p>then what will be the probability for quiting in between?</p>
<p>For example, after first roll at state 4 if I quit how we define that probability?</p>
<p></p>
<table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""413"">     <tbody><tr height=""20""><td height=""20"" width=""64"">State</td><td width=""89"">Action</td><td width=""68"">State&#39;</td><td width=""87"">Probability</td><td width=""105"">Reward</td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>1</td><td>1/6</td><td>0</td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>2</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>3</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>4</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>5</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">State_s0</td><td>roll</td><td>6</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20"">1</td><td>quit</td><td>bad_state</td><td>1</td><td> </td></tr><tr height=""20""><td height=""20"">2</td><td>quit</td><td>bad_state</td><td>1</td><td> </td></tr><tr height=""20""><td height=""20"">3</td><td>quit</td><td>bad_state</td><td>1</td><td> </td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>1</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>2</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>3</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>8</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>9</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">4</td><td>roll</td><td>10</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20""><strong>4</strong></td><td><strong>quit</strong></td><td><strong>quit_4</strong></td><td> </td><td>4</td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>1</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>2</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>3</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>9</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>10</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>roll</td><td>11</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">5</td><td>quit</td><td>quit_5</td><td> </td><td>5</td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>1</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>2</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>3</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>10</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>11</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>roll</td><td>12</td><td>1/6</td><td> </td></tr><tr height=""20""><td height=""20"">6</td><td>quit</td><td>quit_6</td><td> </td><td>6</td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20""> </td><td> </td><td> </td><td> </td><td></td></tr><tr height=""20""><td height=""20"">8</td><td>quit</td><td>quit_8</td><td>1</td><td>8</td></tr><tr height=""20""><td height=""20"">9</td><td>quit</td><td>quit_9</td><td>1</td><td>9</td></tr><tr height=""20""><td height=""20"">10</td><td>quit</td><td>quit_10</td><td>1</td><td>10</td></tr><tr height=""20""><td height=""20"">11</td><td>quit</td><td>quit_11</td><td>1</td><td>11</td></tr><tr height=""20""><td height=""20"">12</td><td>quit</td><td>quit_12</td><td>1</td><td>12</td></tr></tbody></table>",2019-09-04T19:58:54Z,43,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k05opyh9zpq6zj,2019-09-04T19:58:54Z,{},hw1
862,no,<p>What is the probability that you head over to the &#39;quit_4&#39; state if your action is quit?</p>,2019-09-04T20:08:30Z,43,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k05p2am4dbhw2,2019-09-04T20:08:30Z,{},hw1
863,no,"<p>yes quitting is not stochastic at all. So if you policy/action is for quit, then quit at probability = 1. All the time</p>",2019-09-04T20:17:45Z,43,Week 9/1 - 9/7,feedback,,jqrr36mqfm8M,k05pe6y5sud347,2019-09-04T20:17:45Z,{},hw1
864,no,"<p>That is why we are doing value iteration so we can compare two policies - stay /quit. At every iteration, the agent has a conscious choice to pick one policy based on which policy generates more value</p>",2019-09-04T20:27:44Z,43,Week 9/1 - 9/7,feedback,,jqrr36mqfm8M,k05pr15mh5g7hh,2019-09-04T20:27:44Z,{},hw1
865,no,"<p>Why isn&#39;t there a negative reward when the next state could be a bad state. For example, I would have expected to see the following for state with bankroll=4</p>
<p></p>
<p dir=""ltr"">Vest4  = ⅙(4&#43; 4) &#43; ⅙(5 &#43; 4) &#43; ⅙(6 &#43; 4) &#43; ½(-4) = 4.5 -2 = 2.5</p>
<p dir=""ltr""></p>
<p dir=""ltr"">where 1/2 (-4) is accounting for the situation where at state4, I have $4 but I have now rolled a bad state dice and I will lose the $4</p>
<p> </p>
<p></p>",2019-09-05T00:32:34Z,43,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k05yhw16qp61wr,2019-09-05T00:32:34Z,{},hw1
866,no,"<p>Think you have rolled a 4 in first roll, and your bankroll is 4 but you still don&#39;t have any reward. You have to QUIT to get that 4 dollar.</p>
<p>On the other hand, if you decide to roll, when the next roll is on the bad side, you will get nothing and leave the game (reward is 0) rather than a negative reward. The bankroll is virtual doesn&#39;t mean you have it until you realize it into reward (said, QUIT the game).</p>
<p>Above description is &#34;pay the amount when you proactively quit the game and realize the bankroll&#34;.</p>
<p>$$Vest4 = R(4,roll)&#43;γ∑_{s′}[p(s′)∗V_{t−1}(s′)]=0&#43;1∗[\frac{1}{2}∗0&#43;\frac{1}{6}∗(8&#43;9&#43;10)]=4.5$$</p>
<p>where $$R(4, roll)=0, \gamma =1$$</p>
<p></p>
<p>If you solve the problem in another situation, said &#34;the reward will realize at the time on the good side&#34;, the Vest4 will be</p>
<p>$$Vest4 = R(4)&#43;γ∑_{s′}[p(s′)∗V_{t−1}(s′)]=4&#43;1∗[\frac{1}{2}∗(-4)&#43;\frac{1}{6}∗(4&#43;5&#43;6)]=4&#43;(-2&#43;2.5)=4.5$$, still the same result as above.</p>
<p>where $$R(4)=4, \gamma =1,V(BadSide)=-4, Vest(4&#43;n)q=n$$ if N is good.</p>
<p></p>
<p>The different approach will lead to the same result but you cannot mix the reward situation.</p>",2019-09-05T05:28:36Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k0692lcln004m,2019-09-05T05:28:36Z,{},hw1
867,no,"<p>You can think the above analysis using different reward function: $$R(s, a)$$ vs. $$R(s)$$.</p>",2019-09-05T05:30:02Z,43,Week 9/1 - 9/7,feedback,,jl2bqebwzel2s,k0694fmuvyo18u,2019-09-05T05:30:02Z,{},hw1
868,no,"<p>thanks makes sense. I was not using bankroll in my state and I was using the formula below. But the basic problem is just on not using the bankroll in my state. Its clear now</p>
<p></p>
<p>Policy evaluation</p>
<p>-----------------------</p>
<p>for every iteration t</p>
<p>     for every state s</p>
<p>           Vπ s(t)  =  ∑ T (s, π (s, s&#39;))  [reward(s, π, s&#39;)  &#43; γ  * Vπ s&#39; (t-1) ]</p>
<p></p>
<p></p>",2019-09-05T16:52:25Z,43,Week 9/1 - 9/7,feedback,,jqrr36mqfm8M,k06xhzqgi7j386,2019-09-05T16:52:25Z,{},hw1
869,no,"<p>How do I generalize the number of states for N sided die for which say m sides are bad and T rolls are done ? </p>
<p></p>
<p>Like for N = 6 and T = 2  and m = 3 (3 are bad out of 6)</p>
<p>Then for example the total states are 0, 4, 5, 6, 8, 9, 10, 11, 12, bad_state, end_state -&gt; 11 states in total here</p>
<p>bad state -&gt; where no reward (when game stops because of bad state)</p>
<p>end_state -&gt; has some reward(when you quit the game)</p>
<p></p>
<p>So if I need to do the calculation for different values of N, and m is there some way to generalize it ?</p>
<p>What approach did u take to validate the N=30 and similar higher values scenarios ?</p>",2019-09-05T16:11:39Z,43,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k06w1kh5zrz1nk,2019-09-05T16:11:39Z,{},hw1
870,no,<p>I used pymdptoolbox and the rest was playing with arrays.</p>,2019-09-05T16:25:34Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k06wjgmnhs31wc,2019-09-05T16:25:34Z,{},hw1
871,no,"<p>Does anyone have a doubling issue? After my first iteration, I get the correct values (V0 = 2.5, V4 = 4.5, etc)... but then after my second I&#39;m getting V0 = 5.0, V4 = 9.0..</p>
<p></p>
<p>here&#39;s the sigma part of bellman&#39;s eq that I&#39;m using.. I think my mis-logic is somewhere here</p>
<p>gamma = 1</p>
<p>V_prev is V(s) from the last iteration</p>
<pre>sigma[i] &#43;= P * (reward &#43; gamma * V_prev[s_prime])</pre>
<p>Then I take the max and add is to V(s)</p>
<p></p>",2019-09-05T19:08:55Z,43,Week 9/1 - 9/7,followup,,jzhdgaq99za3um,k072dil68t3146,2019-09-05T19:08:55Z,{},hw1
872,no,"<p>I figured it out!! My terminal state and initial state were the same (both 0) - I changed my terminal state to -1 (the end of my V(s) array in python) and it worked.</p>
<p></p>
<p>Miguel pointed it out for someone else here &#64;135</p>",2019-09-05T20:46:51Z,43,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k075vgkf1hq6b4,2019-09-05T20:46:51Z,{},hw1
873,no,"Can someone please explain the value iteration table? I really don&#39;t understand what iteration means here or what values are being referred to

From my understanding at the first iteration the state is 0, and possible future states are 4, 5 and 6. So in the first iteration I would expect s = 0 and s&#39; = {4, 5, 6}",2019-09-08T21:57:42Z,42,Week 9/8 - 9/14,followup,,jqu95q68ljj1pn,k0biq4ncnpa50u,2019-09-08T21:57:42Z,{},hw1
874,no,<p>I have talked about this extensively during office hours. &#34;Iteration&#34; in Value Iteration refers to the iterative approximation of the estimates. It has nothing to do with moving from one state to another.</p>,2019-09-08T22:15:19Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bjcsqvpxn22f,2019-09-08T22:15:19Z,{},hw1
875,no,"Thanks for your response. I hope you understand that it&#39;s extremely challenging to absorb and consolidate all of the information between lectures, piazza, slack, the recommended reading, the optional reading, office hours, and all of the other resources that get recommended, especially as a beginner to this subject.",2019-09-08T22:31:56Z,42,Week 9/8 - 9/14,feedback,,jqu95q68ljj1pn,k0bjy5kxux06v9,2019-09-08T22:31:56Z,{},hw1
876,no,"<p>Of course, maybe starting early would help, too.</p>
<p>Anyway, the semester is just getting started. Hoping you enjoy.</p>",2019-09-09T02:39:12Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bss56hnps3ty,2019-09-09T02:39:12Z,{},hw1
877,no,<p>Has anyone been able to implement this strategy with 3 rolls? With only 2 rolls I am only getting 3/10 of the hw questions correct.</p>,2019-09-08T23:34:15Z,42,Week 9/8 - 9/14,followup,,jqmjg3txqw6ji,k0bm6b6ob6t1ys,2019-09-08T23:34:15Z,{},hw1
878,no,"<p>I used pymdptoolbox, which isn&#39;t so much about the &#34;number of rolls&#34; as it is the probabilities of transitioning from one state to another. I know this isn&#39;t exactly answering your question, but I&#39;d recommend pymdptoolbox if you can understand the example here:</p>
<p></p>
<p><a href=""https://github.com/rldm/rldm_tutorials"">https://github.com/rldm/rldm_tutorials</a></p>",2019-09-08T23:45:06Z,42,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0bmk9ja8dbpb,2019-09-08T23:45:06Z,{},hw1
879,no,"<p>I dont really understand how we get to the &#34;end state&#34; like - why should we not keep going after 3rd iteration? How do we know that the 2nd was the best? It seems like at some point the probability of losing overcomes, but I don&#39;t understand how we know. I have been reading up but I am having a hard time with this one. I definitely don&#39;t have it right now. </p>",2019-09-09T00:36:02Z,42,Week 9/8 - 9/14,followup,,jqr9leosvw8P,k0bodr5k9d76re,2019-09-09T00:36:02Z,{},hw1
880,no,"<p>Imagine I drop you in the middle of one of this game that your agent was playing.</p>
<p></p>
<p>Your agent had made for you $100. The die has 100 sides (yeah, crazy).</p>
<p></p>
<p>99 sides are bad, 1 side is good. And, the side that is good is... 1, so 2-100 are bad sides!!!</p>
<p></p>
<p>So, do you roll or quit?!</p>
<p></p>
<p>I don&#39;t know you, but I quit right away! I won&#39;t even ask how in the world my agent got those $100 rolling 1 x 100 times consecutively. I just quit and take $100 home.</p>
<p></p>
<p>So, there is a point, a number, a state, in which rolling is no longer an optimal strategy.</p>
<p></p>
<p></p>",2019-09-09T02:45:25Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bt057cb5g1z6,2019-09-09T02:45:25Z,{},hw1
881,no,"<p>In this case, you can go through the same process to 4 or more iteration and you will find that the Vests had already converged at 3rd iteration (it&#39;s a little bit misleading to say that is <strong>3rd</strong> iteration because you only update Vest <strong>twice</strong>).</p>
<p>You can refer to other discussion for how many rolls you should take or what&#39;s the maximum bound on states (<a href=""/class/jzh9tkzzxkd7ph?cid=177"" target=""_blank"">&#64;177</a>) should be.</p>
<p>Otherwise, you can simply loop it until the expected value converges. (Brutal-force, you know)</p>",2019-09-09T02:48:29Z,42,Week 9/8 - 9/14,feedback,,jl2bqebwzel2s,k0bt4395fzd6yq,2019-09-09T02:48:29Z,{},hw1
882,stud,"<p>Maybe I don&#39;t follow this submission process very well, but it is little bit strange that there is no need to submit any proof of assignment solution, for example, either manual calculation on the paper or code solution.</p>",2019-09-06T03:43:44Z,63,Week 9/1 - 9/7,followup,a_0,,k07krku7bsf5yk,2019-09-06T03:43:44Z,{},hw1
883,no,"<p>We&#39;ll evaluate your understanding in projects 1, 2, 3 and the final. Trust me, it is of your interest to work these homework assignments and try to understand as much as you can. You don&#39;t need to submit proof of work for them, but we will certainly be able to gauge your understanding of the concepts later on.</p>",2019-09-06T04:27:01Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07mb998pam79w,2019-09-06T04:27:01Z,{},hw1
884,no,<p>Thank you for asking! I was weirded out by not submitting anything on canvas. lol</p>,2019-09-07T22:22:57Z,63,Week 9/1 - 9/7,followup,,jqr9leosvw8P,k0a46r3e8me3ae,2019-09-07T22:22:57Z,{},hw1
885,no,"<p>Hey jean, Thanks for posting this even i am stuck on the work and totally clueless where to head. The chapter which you suggested is it from udacity or litmann reading. Thanks</p>",2019-09-03T23:16:20Z,63,Week 9/1 - 9/7,followup,,jzu2o3mdsd9198,k04gc0bsxc92xj,2019-09-03T23:16:20Z,{},hw1
886,no,"<p>I find Littman hard to read but it&#39;s because it&#39;s a PhD thesis.</p>
<p>That&#39;s why I switched to reading Sutton ch3.  </p>
<p></p>
<p>But in the end, you just need to buld a model and give it to mdptoolbox to solve.  If you do it that way, you don&#39;t even need to understand Bellman&#39;s equation.  Not a good idea but here, the hardest part is to build the MDP model, ie the transition matrix and the reward matrix.  </p>
<p></p>
<p>Sutton&#39;s examples are really helpful in that regard: look for the gridworld problem and the can removal robot p65.</p>
<p></p>
<p>The hard part for me was to thouroughly understand what is a state, what is a reward and when it is given.  I thought I understood when I was reading, but it was a different story when I had to make it work.  </p>
<p></p>
<p>The TA&#39;s provided mdptoolbox material.  I found the file pymdptoolbox_example.ipynb quite helpful to build the &#39;mdp engine&#39;.  </p>
<p></p>
<p>Good luck.</p>",2019-09-03T23:31:07Z,63,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k04gv0oh8z35gv,2019-09-03T23:31:07Z,{},hw1
887,no,"<p>Thanks Jean, can you please share the link to TA&#39;s provided mdptoolbox material</p>",2019-09-03T23:40:06Z,63,Week 9/1 - 9/7,feedback,,jzu2o3mdsd9198,k04h6k4w7d26yt,2019-09-03T23:40:06Z,{},hw1
888,no,<p>see &#64;94</p>,2019-09-03T23:53:55Z,63,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k04hoc2xcxs2sf,2019-09-03T23:53:55Z,{},hw1
889,no,"<p>&#64;Jean-Pierre, super impressed with your helpfulness here!  Keep it up!</p>",2019-09-05T01:42:27Z,63,Week 9/1 - 9/7,feedback,,jqmfuaidej9155,k060zrjw3z43v8,2019-09-05T01:42:27Z,{},hw1
890,no,<p>Follow up on this - do we need to watermark the graphs we use in our write up?</p>,2019-09-06T04:05:24Z,63,Week 9/1 - 9/7,followup,,jl3oi5v7qkSk,k07ljfw04ku1ga,2019-09-06T04:05:24Z,{},project1
891,no,"<p>Nope, only the ones you share on Slack/Piazza (if you do).</p>",2019-09-06T04:07:28Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07lm3u4q2h51r,2019-09-06T04:07:28Z,{},project1
892,no,"<p>Here are my graphs for the three figures. Has anyone got graphs that are similar. I believe I am implementing the experiments per Sutton&#39;s paper but I see some subtle differences between mine and his. Any comments on why appreciated...</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ll2xkiDJf%2Fk11wk7x6mz7z%2FFigure_3.png"" alt=""Figure 3"" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ll2xkiDJf%2Fk11wkykwlrvh%2FFigure_4.png"" alt=""Figure 4"" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ll2xkiDJf%2Fk11wlrpsyvdq%2FFigure_5.png"" alt=""Figure 5"" /></p>",2019-09-27T09:08:40Z,60,Week 9/22 - 9/28,followup,,j6ll2xkiDJf,k11wmc0kag05ee,2019-09-27T09:08:40Z,{},project1
893,no,"<p>I don&#39;t know how much I am allowed to say here, but let&#39;s say they look familiar to me :) But you have to figure yourself why.</p>
<p></p>",2019-09-27T14:09:06Z,60,Week 9/22 - 9/28,feedback,,jzlmql2eisd2ts,k127cpgg8wn76a,2019-09-27T14:09:06Z,{},project1
894,no,"<p>yes, but a little bit of discussion should still be within the rules...I am hoping.</p>",2019-09-27T19:02:49Z,60,Week 9/22 - 9/28,feedback,,j6ll2xkiDJf,k12huf7kyag3rp,2019-09-27T19:02:49Z,{},project1
895,no,"<p>Hey Kamran, I have nearly identical graphs to yours:<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjh0tpwh0qjr2t0%2Fk15hjdpikzqr%2Ffigure_4_add_dw_False.png"" alt="""" /></p>
<p>Note that I get this only when I&#39;m updating with ONLY the last dW though (i.e., if the sequence is length 7, then I only update with dW_7). I think this might be wrong but it&#39;s not clear to me how it could work otherwise... because then I think you&#39;d actually be weighting the FIRST observation this highest. I.e.:</p>
<p>$$\bigtriangleup W_{total} = \bigtriangleup W_1 &#43; ... &#43; \bigtriangleup W_n$$<br />$$\bigtriangleup W_{total} = \alpha (P_{2} - P_1) \sum_{k=1}^{t} \lambda^{t-k}\bigtriangledown x_t) &#43; ... &#43; \alpha (P_{n} - P_{n-1}) \sum_{k=1}^{t} \lambda^{t-k}\bigtriangledown x_t$$</p>
<p>In that case the oldest observation has the largest &#34;total effect&#34; on the weight change, since in iteration one you have</p>
<p>$$lambda^0 x_1$$</p>
<p>and in iteration n you have</p>
<p>$$lambda^{n-1} x_1$$</p>
<p>So x_1 is represented in all of them.</p>
<p></p>
<p>However, when I do do it as a sum, I get something even weirder:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjh0tpwh0qjr2t0%2Fk15hjvq4i7x4%2Ffigure_4_add_dw_True.png"" alt="""" /></p>
<p>I think that one might be because with such small batch sizes, even one really long sequence can throw it off big time.</p>
<p></p>
<p>Anyway, let me know if you have any other thoughts, I&#39;m still thinking through it.</p>",2019-09-29T21:18:46Z,59,Week 9/29 - 10/5,feedback,,jh0tpwh0qjr2t0,k15hkyfbje053r,2019-09-29T21:18:46Z,{},project1
896,no,<p>DONE!</p>,2019-09-04T16:21:03Z,63,Week 9/1 - 9/7,followup,,jzj7y1ofgsro1,k05gxse5kv66hc,2019-09-04T16:21:03Z,{},project1
897,no,"<p>When I try to call &#34;git clone https://github.gatch.edu/...&#34; in cmd.exe,  it fails in authentication (windows 10 system):</p>
<p></p>
<p>Cloning into &#39;7642Fall2019myid&#39;...<br />remote: Invalid username or password.<br />fatal: Authentication failed for &#39;https://github.gatech.edu/gt-omscs-rldm/7642Fall2019ryuan37.git/&#39;</p>
<p></p>
<p>Could anyone advise how to resolve this? </p>
<p></p>
<p></p>",2019-09-05T13:32:08Z,63,Week 9/1 - 9/7,followup,,jzg6jh2hn6f43c,k06qcfijihv33q,2019-09-05T13:32:08Z,{},project1
898,no,<p>Do you have a global .gitconfig or .git-credentials that use a login that is different from your gatech GitHub account or are your credentials stale from a password change? That first case was my issue because my ~/.git* rc files refer to my non-gatech account. I just made local copies of those files in my local repo and updated them to refer to my gatech account.</p>,2019-09-05T16:33:55Z,63,Week 9/1 - 9/7,feedback,,jc9hybafy446pq,k06wu70afzi5d8,2019-09-05T16:33:55Z,{},project1
899,no,"<p>Check if you can go to the repo&#39;s URL directly, (in your case: <a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019ryuan37.git/%29"">https://github.gatech.edu/gt-omscs-rldm/7642Fall2019ryuan37/)</a></p>
<p>This is just to make sure you have correct credential setup. Last time I heard, you may need to setup 2-factor authentication in order to access resources from GaTech.</p>",2019-09-05T19:06:34Z,63,Week 9/1 - 9/7,feedback,,jl5wq8mca7o0,k072aib1u8a2pk,2019-09-05T19:06:34Z,{},project1
900,no,"<p>I am able to log in the repository <a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid.git,"">https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid.git,</a> but cannot use &#34;git clone&#34;.</p>
<p></p>
<p>Could you advise how to set up the configure file? I have other git accounts which may have conflict. </p>
<p></p>
<p>In Credential Manager,  I have updated password for &#34;git:https://github.gatech.edu&#34;, but not helpful. </p>
<p></p>
<p></p>",2019-09-05T19:15:44Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k072maw28vt4kn,2019-09-05T19:15:44Z,{},project1
901,no,"<p>If you are having trouble with terminal git command workflow, I recommend you just use Github Desktop.It&#39;s way simpler and easy to use. </p>",2019-09-06T03:33:33Z,63,Week 9/1 - 9/7,feedback,,jqy7j43dlz4di,k07kehvly654rp,2019-09-06T03:33:33Z,{},project1
902,no,"<p>Thank you, everyone. Cannot log in through Github Desktop, so I am wondering if the GTech Github is not set up well. But I find a workaround to upload files through the website directly, though it is not easy to commit changes and maintain the repository. </p>",2019-09-06T13:37:56Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k085zqaua5k78j,2019-09-06T13:37:56Z,{},project1
903,no,"<p>I don&#39;t have my personal laptop with me, but remembered you install the GitHub Desktop from the official Github website, and go to File--&gt;Options--&gt;Accounts --&gt; sign in with Enterprise Account. Copy and paste the your assigned repo, i,e.</p>
<p><a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid.git,"">https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid</a>/</p>
<p></p>
<p>Sign in with your Gatech username and password. Create a readme.md in your local repo, and click push&amp;commit to master, refresh your github page, the Readme.md will appear on the repo, you should then be done with the set up. </p>
<p></p>
<p>Here is the guide:</p>
<p><a href=""https://help.github.com/en/desktop/getting-started-with-github-desktop/authenticating-to-github"">https://help.github.com/en/desktop/getting-started-with-github-desktop/authenticating-to-github</a></p>",2019-09-06T16:07:31Z,63,Week 9/1 - 9/7,feedback,,jqy7j43dlz4di,k08bc3hm81f6re,2019-09-06T16:07:31Z,{},project1
904,no,"<p>Thank you, Alison. The GitHub Desktop works now. I should choose Enterprise account as the way to sign in. Thanks for the guidance. </p>",2019-09-06T17:20:09Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k08dxi491ad2c,2019-09-06T17:20:09Z,{},project1
905,no,<p>thanks Alison</p>,2019-09-07T19:26:19Z,63,Week 9/1 - 9/7,feedback,,ijb0gakjJnn,k09xvloh99ffl,2019-09-07T19:26:19Z,{},project1
906,no,"<p>Where should I send the message with my username ? </p>
<p></p>",2019-09-07T17:15:30Z,63,Week 9/1 - 9/7,followup,,jziefhpcejc3lr,k09t7dg3g455ps,2019-09-07T17:15:30Z,{},project1
907,no,<p>You can create private posts in Piazza that only the instructors can see.</p>,2019-09-07T17:30:12Z,63,Week 9/1 - 9/7,feedback,,ixnwq0s4ozg,k09tq9wyega6tk,2019-09-07T17:30:12Z,{},project1
908,no,"<p>thanks :)</p>
<p></p>",2019-09-07T17:34:16Z,63,Week 9/1 - 9/7,feedback,,jziefhpcejc3lr,k09tvib5ysmem,2019-09-07T17:34:16Z,{},project1
909,no,"<p><strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  Hi, I&#39;m a total noob with github, so I&#39;m afraid to do sthg stupid.</p>
<p>I have access to my account and I started creating the 3 projects, but then I have to decide if I wan it &#39;private&#39; (maybe good so no one can see it) or &#39;public&#39; (maybe good so the TA&#39;s can see it, but everyone else too).</p>
<p>But maybe the TA&#39;s have access by default??</p>
<p>Do I have to create a read.me file as some suggest?</p>
<p>What should I choose?</p>
<p>Thx</p>",2019-09-14T04:46:46Z,62,Week 9/8 - 9/14,followup,,jzh6k6o994a6dh,k0j2jgwib402nd,2019-09-14T04:46:46Z,{},project1
910,no,Are you in the repo that we created for you?,2019-09-14T15:12:44Z,62,Week 9/8 - 9/14,feedback,,hz7meu55mi8sd,k0jowgdbomn4rc,2019-09-14T15:12:44Z,{},project1
911,no,"<p>It must be because I didn&#39;t create any by myself.</p>
<p>But, when I type <a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid.git,"" target=""_blank"" rel=""noopener noreferrer"">https://github.gatech.edu/gt-omscs-rldm/7642Fall2019myid</a>/, I get a 404 page and then I am redirected to <a href=""https://github.gatech.edu/jbianchi3"" target=""_blank"" rel=""noopener noreferrer"">https://github.gatech.edu/</a>&lt;gatech name&gt;... is that normal? </p>",2019-09-14T15:16:32Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0jp1c5agb63ny,2019-09-14T15:16:32Z,{},project1
912,no,Send us a private message with the link you are trying and we can help. ,2019-09-14T15:17:35Z,62,Week 9/8 - 9/14,feedback,,hz7meu55mi8sd,k0jp2p559tw7on,2019-09-14T15:17:35Z,{},project1
913,no,"<p>The &#96;https://github.gatech.edu/&lt;gatech_name&gt;/&#96; is your GeorgiaTech github account. Sometime you get redirect there.</p>
<p>The &#96;https://github.gatech.edu/gt-omscs-rldm/&#96; is our class account. It is not related to your account. Under the class account, you will see the your own repo, which TA has created for you. By default, your repo is private, so that if you go to the class account, you won&#39;t see my repo. And I don&#39;t think we are able to change the setting of that repo, even if we (you and I) want to. Only the TA can change the setting.</p>",2019-09-15T19:49:52Z,61,Week 9/15 - 9/21,feedback,,jl5wq8mca7o0,k0le8pdme1x6k4,2019-09-15T19:49:52Z,{},project1
914,no,"<p>I can log into my gatech account in Github but I keep getting the 404 page.</p>
<p></p>
<p>Paths I&#39;ve tried:</p>
<ul><li>.../7642Fall2019&lt;gatech username&gt;</li><li>.../7642Fall2019&lt;gatech student id&gt;</li><li>.../7642/Fall2019/&lt;gatech username&gt;</li><li>.../7642/Fall2019/&lt;gatech student id&gt;</li></ul>
<p></p>
<p>Can someone help me figure this out?</p>
<p>Thanks!</p>
<p></p>",2019-09-15T17:43:49Z,61,Week 9/15 - 9/21,followup,,jzkke6iz3cl28t,k0l9qlvgmdw5bs,2019-09-15T17:43:49Z,{},project1
915,no,"<p><strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  I would send a private message but I can&#39;t seem to find where... the message icon in the top right doesn&#39;t give me an option to send any messages.</p>",2019-09-15T17:48:38Z,61,Week 9/15 - 9/21,feedback,,jzkke6iz3cl28t,k0l9wszr38235n,2019-09-15T17:48:38Z,{},project1
916,no,"<p>By private message, they mean make a new post and set the visibility to instructors only. This will create a private posting.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixty1midfufhd%2Fk0ldkmb010rj%2FprivateMost.PNG"" alt="""" /></p>",2019-09-15T19:34:17Z,61,Week 9/15 - 9/21,feedback,,ixty1midfufhd,k0ldonyij7y1g6,2019-09-15T19:34:17Z,{},project1
917,no,"<p>If you go to the class&#39; root link, you will be able to see the repo that TA has created for you:</p>
<p><a href=""https://github.gatech.edu/gt-omscs-rldm/"">https://github.gatech.edu/gt-omscs-rldm/</a></p>
<p>If you don&#39;t see any, then let the TA know. Maybe their script didn&#39;t work properly.</p>",2019-09-15T19:41:03Z,61,Week 9/15 - 9/21,feedback,,jl5wq8mca7o0,k0ldxdnxjdr2d6,2019-09-15T19:41:03Z,{},project1
918,no,"<p>Thanks George! </p>
<p></p>
<p>&#64;Quang Vu - I went there but I see nothing... everything is empty for that link.</p>",2019-09-16T00:18:11Z,61,Week 9/15 - 9/21,feedback,,jzkke6iz3cl28t,k0lntrj73c73bm,2019-09-16T00:18:11Z,{},project1
919,no,"<p>I am having the same issue. </p>
<p></p>
<p>&#64;John Mckinstry: Were you able to resolve the issue and get access to your repo?</p>",2019-09-19T17:03:54Z,61,Week 9/15 - 9/21,feedback,,jzkpm3i0pxp2sg,k0qy2o7mt9lod,2019-09-19T17:03:54Z,{},project1
920,no,"<p>Hi All,</p>
<p></p>
<p>How do we create our three folders?</p>
<p></p>
<p>Do we add projects under the &#34;Projects&#34; tab like so:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk0q5mkjosdqf%2FCapture.PNG"" alt="""" width=""683"" height=""418"" /></p>
<p></p>
<p>If not, can you please explain.</p>",2019-09-19T03:48:22Z,61,Week 9/15 - 9/21,followup,,gx3c8l7z7r72zl,k0q5nlye54i5d2,2019-09-19T03:48:22Z,{},project1
921,no,"<p>You can ignore the &#34;Projects&#34; tab, that is for something else, like planning, set task, deadline... We are not using those.</p>
<p>Just create 3 folders, then add them to your repo, then push your commit. By default git will not track empty folder, so you may want to have a dummy file in each folder to &#34;stage&#34; that folder into your commit.</p>",2019-09-19T07:11:38Z,61,Week 9/15 - 9/21,feedback,,jl5wq8mca7o0,k0qcx0ndztu67m,2019-09-19T07:11:38Z,{},project1
922,no,"<p>Hi Aaron, try this - </p>
<p></p>
<p>https://github.com/KirstieJane/STEMMRoleModels/wiki/Creating-new-folders-in-GitHub-repository-via-the-browser</p>",2019-09-21T13:15:44Z,61,Week 9/15 - 9/21,feedback,,j6ilcw6hxoc77h,k0tksyrmtab1r5,2019-09-21T13:15:44Z,{},project1
923,no,<p>Thank you everyone.</p>,2019-09-22T18:58:13Z,60,Week 9/22 - 9/28,feedback,,gx3c8l7z7r72zl,k0vch93iy6231u,2019-09-22T18:58:13Z,{},project1
924,no,"<p>Hey Instructors, I&#39;ve posted a private message about can&#39;t access my repo more than 24 hours ago, any updates?</p>",2019-09-25T20:58:25Z,60,Week 9/22 - 9/28,followup,,jziegutgfx2455,k0zr3dmuz0b5up,2019-09-25T20:58:25Z,{},project1
925,no,<p>I&#39;m in the same boat. I would just like a confirmation that my post is being seen and looked into.</p>,2019-09-25T21:32:57Z,60,Week 9/22 - 9/28,feedback,,k0i2gsf6my96ge,k0zsbs8x2rq3px,2019-09-25T21:32:57Z,{},project1
926,no,<p>I&#39;m all set!</p>,2019-09-26T18:52:05Z,60,Week 9/22 - 9/28,feedback,,k0i2gsf6my96ge,k1120rl4aud2kx,2019-09-26T18:52:05Z,{},project1
927,no,"<p>If we change the directory structure and make a new commit, would that change the Git hash for Project 1? If so would that affect our grades since the instructions required Git Hash to be provided?</p>",2019-10-15T21:36:45Z,57,Week 10/13 - 10/19,followup,,is69soxc2782fw,k1sd9pgc37b6zz,2019-10-15T21:36:45Z,{},project1
928,no,"<p>Any change you make to the repo will change its hash. But TAs will only use the hash provided in our paper. In another word, any code change made after commit which was in our paper would be ignored.</p>",2019-10-16T02:39:27Z,57,Week 10/13 - 10/19,feedback,,jl5wq8mca7o0,k1so2zxlcbj7c7,2019-10-16T02:39:27Z,{},project1
929,no,<p>Thank you Quang</p>,2019-10-17T04:01:42Z,57,Week 10/13 - 10/19,feedback,,is69soxc2782fw,k1u6gm3dowe5gp,2019-10-17T04:01:42Z,{},project1
930,no,"<p>I did copy the code from the file the TA&#39;s gave us: pymdptoolbox_example.ipynb.  see&#64;94</p>
<p>It&#39;s pretty straightforward once you have defined the prob and reward matrices.</p>",2019-09-04T17:15:31Z,43,Week 9/1 - 9/7,followup,,jzh6k6o994a6dh,k05ivuh1wch5sa,2019-09-04T17:15:31Z,{},hw1
931,no,<p>This. It is pretty straightforward &#34;once you have defined the prob and reward matrices.&#34; Make sure to study the example to understand how the matrices are being set up. Look at other piazza posts for great discussions on N=6 example cases.</p>,2019-09-05T03:02:22Z,43,Week 9/1 - 9/7,feedback,,hzoi2qsuCAd,k063ujd9af2308,2019-09-05T03:02:22Z,{},hw1
932,no,"<p>Thanks Kim, but yeah, I worked on the matrices for 4 days like a maniac.  </p>
<p>I really enjoyed it in the end, especially when I tried to implement Miguel&#39;s suggestion, to pay the player only on exit to allow zero rewards before that, to gently evacuate the problem of the bankrupt state.  </p>
<p>I wanted to find the solution and submit the hw to relieve the stress (this is my first class first term).  Now, I have a couple of days to keep working on it, exploring other ways etc before starting hw2.  </p>",2019-09-05T04:55:31Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k067w1hyhz38u,2019-09-05T04:55:31Z,{},hw1
933,no,<p>I am not sure if mdptoolbox is bugfree! &#64;142</p>,2019-09-05T04:05:04Z,43,Week 9/1 - 9/7,followup,,j6ll2xkiDJf,k06635zu48j7dr,2019-09-05T04:05:04Z,{},hw1
934,no,"<p>Seriously? What do professionals use?</p>
<p>Maybe I should see if the solution fits into the Bellman equation.  Good idea!</p>",2019-09-05T04:56:41Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k067xjty2w11mu,2019-09-05T04:56:41Z,{},hw1
935,no,"<p>I do not know. There are a number of open source implementations out there. As we know most software has bugs. This, if a bug,could be serious and there may be ways to setup the T and R to get the values in the lecture....I do not know but I thought to present my findings and hope for an explanation....</p>
<p></p>",2019-09-05T05:01:38Z,43,Week 9/1 - 9/7,feedback,,j6ll2xkiDJf,k0683x2l9ic7po,2019-09-05T05:01:38Z,{},hw1
936,stud,<p>How do you actually watch these? I can&#39;t find instructions on signing up for blue jeans. </p>,2019-09-08T01:08:14Z,30,Week 9/8 - 9/14,followup,a_0,,k0aa3bn7gfs2u8,2019-09-08T01:08:14Z,{},office_hours
937,no,"<p>Can you access <a href=""http://gatech.bluejeans.com/"">http://gatech.bluejeans.com/</a>?</p>",2019-09-08T02:21:42Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0acpsjf1ry6rg,2019-09-08T02:21:42Z,{},office_hours
938,stud,"<p>ohh thank you! Once I log in there, then I can click the above links. :) </p>",2019-09-08T14:17:10Z,30,Week 9/8 - 9/14,feedback,a_0,,k0b29w93ta96yw,2019-09-08T14:17:10Z,{},office_hours
939,no,<p>Good to know!</p>,2019-09-08T15:18:42Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0b4h1153a51b4,2019-09-08T15:18:42Z,{},office_hours
940,no,<p>Looks like their plugin doesn&#39;t work in Chromium for Linux.</p>,2019-09-08T23:57:43Z,30,Week 9/8 - 9/14,followup,,jh8epcdrbh6l,k0bn0hep3uj3u,2019-09-08T23:57:43Z,{},office_hours
941,no,<p>is there a way to look at the chat log for the recordings? It is sometimes hard to follow what the question was that prompted a response by one of the TAs.</p>,2019-09-09T17:36:34Z,30,Week 9/8 - 9/14,followup,,j6ll2xkiDJf,k0cou6bk3v851g,2019-09-09T17:36:34Z,{},office_hours
942,no,<p>I don&#39;t think there is. We try to read the chat out loud before answering. I will endeavor to stick to that more firmly.</p>,2019-09-09T18:12:02Z,30,Week 9/8 - 9/14,feedback,,hyxsfbkeit22m2,k0cq3se2shm43s,2019-09-09T18:12:02Z,{},office_hours
943,no,"<p>I think in one of these, going through a whiteboard of experiment 1 was mentioned - was that ever recorded? I couldn&#39;t seem to find it. </p>",2019-09-24T01:17:43Z,28,Week 9/22 - 9/28,followup,,jqr9leosvw8P,k0x5h4nzaod3ye,2019-09-24T01:17:43Z,{},office_hours
944,no,<p>Are these still being held Thursday at ___ pm consistently? I have tried joining the last few and do not see anyone in the meeting.</p>,2019-10-11T00:41:20Z,26,Week 10/6 - 10/12,followup,,jcg0nzvdk8272b,k1lenu8qtfg7n,2019-10-11T00:41:20Z,{},office_hours
945,no,"<p>One of them is Thursdays &#64; 9pm, the other Saturdays &#64; 1pm. You should have it in Canvas.</p>",2019-10-14T01:14:16Z,25,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1pq5qa73qlan,2019-10-14T01:14:16Z,{},office_hours
946,no,"<p>For the week 12 recording above, we are encouraged to explain/show the policy that was learned for project 3. Can someone give an example of how to explain the policy that was learned? Would an example be if the agent always hits its head against a wall but gets all the points, note this is a random example, we just state: &#34;The agent learned a policy of banging its head against the wall which was good for x,y,z reasons.&#34;?</p>",2019-11-15T20:13:06Z,21,Week 11/10 - 11/16,followup,,gx3c8l7z7r72zl,k30kxjoaegk47r,2019-11-15T20:13:06Z,{},office_hours
947,no,<p>&#64;874</p>,2019-11-16T19:46:29Z,21,Week 11/10 - 11/16,feedback,,i4i9bi8rFqk,k31zf60kwhq34i,2019-11-16T19:46:29Z,{},office_hours
948,no,<p>Is there a link for the November 14th office hours recording?</p>,2019-11-21T03:13:06Z,20,Week 11/17 - 11/23,followup,,idghbt86wqe,k3854xw2lvq7ef,2019-11-21T03:13:06Z,{},office_hours
949,no,"Thanks, Alec!",2019-11-22T19:20:00Z,20,Week 11/17 - 11/23,feedback,,idghbt86wqe,k3aj47u2rj743t,2019-11-22T19:20:00Z,{},office_hours
950,no,"<p>actions are : stay, quit</p>
<p></p>
<p>If agent decides to stay, then stochastically it might end up in &#39;stay&#39; state again or in &#39;end state</p>
<p></p>
<p>If agent decides to quit, then it goes to quit state</p>
<p></p>
<p>So are you saying I should split the &#39;stay&#39; state into individual die face states that are not isBad. So it will be -- S4, S5, S6 in case of the 6 die example</p>
<p></p>
<p>Everything else I have above is in terms of rewards makes sense?</p>",2019-09-04T14:14:57Z,43,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k05cfmhqc597ee,2019-09-04T14:14:57Z,{},hw1
951,no,"<p>I reviewed the bluejeans recording where Miguel mentions that states could be modeled as bankrolls. So you roll a dice and get 2, so you are state2, then dice gives you 4, so you are in state6. But that would mean a unbounded set of states because the horizon is infinite and we keep iterating.. Is this right ?</p>
<p></p>
<p></p>",2019-09-04T16:18:34Z,43,Week 9/1 - 9/7,followup,,jqrr36mqfm8M,k05guldwvt96pf,2019-09-04T16:18:34Z,{},hw1
952,no,"<p>Yes, that&#39;s correct. There&#39;s been much discussion about this on the forum.</p>",2019-09-04T17:06:00Z,43,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k05ijljjua13co,2019-09-04T17:06:00Z,{},hw1
953,no,"<p>&#64;anand sure, but it converges pretty quickly</p>
<p>I think it was a good exercise because it showed why one ends up with trillions of states, since a state has to carry the information of all past events sometimes, and the only way to do that seems to be to create one state per possible past combinations</p>
<p></p>
<p>For this exercise, statistically, you can&#39;t go very far in the future since you end up blowing up on a BadSide, even if there&#39;s only 1.  That&#39;s why I think Miguel said &#39;a smart agent will stop pretty quickly&#39;.  For instance, if you manage to get a high number, the best strategy is to quit immediately.  </p>",2019-09-04T17:11:29Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k05iqnpjdfd4wx,2019-09-04T17:11:29Z,{},hw1
954,no,"<p>&#64;instructors:</p>
<p></p>
<p>thats an interesting point; re: terminal states have (by definition) an expected value value of zero;  is this strictly speaking, necessary for the convergence?  would you be able to run the version above, say with lambda &lt; 1 ?</p>",2019-09-04T14:57:51Z,42,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k05dyshfjqb4to,2019-09-04T14:57:51Z,{},hw1
955,no,"<p>Can you explain to me what is so great with this system? I can&#39;t relate with the problem at all.</p>
<p>What&#39;s those dictionaries for?</p>
<p>I did the exercise by defining the MDP and running mdptoolbox.</p>
<p>Is this another way to solve it?</p>",2019-09-04T17:13:47Z,42,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k05itmb8l5mvb,2019-09-04T17:13:47Z,{},hw1
956,no,"<p>Correct, uno. This is one of the reasons why OpenAI Gym returns a &#34;terminal&#34; flag so that you can set up your value function appropriately.</p>
<p></p>
<p>It is also intuitively correct... like what is the expected future reward if there is not future?</p>",2019-09-05T01:37:47Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k060trmnyj54j5,2019-09-05T01:37:47Z,{},hw1
957,no,I am showing initial state as 1 then next state as 0 where do you see me showing my terminal state as zero?,2019-09-04T18:57:18Z,42,Week 9/1 - 9/7,followup,,jc6mqevhagl262,k05miqdmuoz13w,2019-09-04T18:57:18Z,{},hw1
958,no,"<p>I see.</p>
<p></p>
<p>Check the transitions of the &#39;quit&#39; actions.</p>
<p></p>
<p>By definition: all transitions from all actions of a terminal state must loop back to itself providing a 0 reward.</p>
<p></p>
<p>Think about it as a sink state.</p>",2019-09-05T01:46:52Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0615g0ayrm5j1,2019-09-05T01:46:52Z,{},hw1
959,no,"<p>I think I figured it out I changed my states to (0,0) and (0,1) for quit. I tried putting this form into you ch3 notebook and I get really weird values...</p>",2019-09-05T01:59:46Z,42,Week 9/1 - 9/7,feedback,,jc6mqevhagl262,k061m191emq3jd,2019-09-05T01:59:46Z,{},hw1
960,no,"<p>Make sure states and actions are integers from 0-nS, 0-nA.</p>
<p></p>
<p>I see you are using -1, which in Python it would take the last index of the array. I recommend you use the explicit number. Also, using an index of (0, 0) and/or (0, 1), will definitely give you weird results.</p>
<p></p>
<p>Look at the other examples and follow the pattern. There are plenty in there for you! Also, I would change actions to integers, too.</p>",2019-09-05T02:40:36Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0632jsny0aj5,2019-09-05T02:40:36Z,{},hw1
961,no,"<p>Can a terminal state be the same number? for example,</p>
<p>if I am at state 0 and roll and get bad side and say I put the &#34;last state&#34; as terminal. Would this also be the same S&#39; that I point to when I lets say am sitting currently at 4 and then quit?<br /></p>
<p></p>
<p>State in a range of say [1-100]<br />S --&gt; A --&gt; S&#39; </p>
<p>0 --&gt;roll badDie --&gt; maxState or lastState</p>
<p>4 --&gt; walk --&gt; maxState or lastState</p>
<p></p>
<p>are these the same and this is ok?</p>
<p></p>
<p></p>",2019-09-05T03:00:02Z,42,Week 9/1 - 9/7,feedback,,jc6mqevhagl262,k063rj4wpvm77h,2019-09-05T03:00:02Z,{},hw1
962,no,"<p>I think I got your question...</p>
<p></p>
<p>If you are asking whether you can have a single terminal state, then, yes. This is, in fact, the proper way of building an MDP, but it would work fine even if you have more final states as long as you build them as defined above.</p>",2019-09-05T03:13:07Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0648d1wq4m1x8,2019-09-05T03:13:07Z,{},hw1
963,no,"<p>So the below is fine? First key is state and second is action (0 roll,1 quit). And my highest state in this roll is 12 so I put 13 as terminal state. Is this what you mean?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6mqevhagl262%2Fk068811rwgl7%2FScreen_Shot_20190904_at_10.04.32_PM.png"" alt="""" /></p>",2019-09-05T04:28:37Z,42,Week 9/1 - 9/7,feedback,,jc6mqevhagl262,k066xg8a1l22sf,2019-09-05T04:28:37Z,{},hw1
964,no,<p>I can&#39;t see how state 13 is defined... The rest looks good.</p>,2019-09-05T12:45:32Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k06oohuk2yt5fy,2019-09-05T12:45:32Z,{},hw1
965,no,"<p>In state 8, action: roll, you also reach state 13. Does this 13 collide with you choosing 13 as terminal state ?</p>
<p></p>
<p>Should you choose something like 19 (last state for N=6 is 12, and you can reach 12&#43; 6 = 18 as last state in this game) ?</p>",2019-09-05T23:40:06Z,42,Week 9/1 - 9/7,feedback,,jfzaqnqvtQ1m,k07c2a2lxi45bg,2019-09-05T23:40:06Z,{},hw1
966,no,"<p>Good point, Anurag... If this is for the N=6, then these are not enough states.</p>",2019-09-06T01:39:27Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07gbrc6mz9nf,2019-09-06T01:39:27Z,{},hw1
967,no,"<p>It is unlikely I will be able to make tonight&#39;s office hours, but I wanted to ask the following question.</p>
<p></p>
<p>Can we please do a short written example of what we are trying to find for HW1 I am still not quite getting it.</p>",2019-09-05T23:55:52Z,63,Week 9/1 - 9/7,followup,,gx3c8l7z7r72zl,k07cmjo0fcvi5,2019-09-05T23:55:52Z,{},office_hours
968,no,"<p>Miguel and others answer this pretty well in &#64;82</p>
<p></p>
<md><br />&gt; The homework is asking for the state-value function <br />&gt; of the initial state (when you have 0 in the bankroll)<br /></md>
<p></p>
<p>I was pretty stuck until I started doing the N = 6, isBadSide = {1,1,1,0,0,0} example by hand</p>",2019-09-06T00:20:38Z,63,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k07didzmmjg5zq,2019-09-06T00:20:38Z,{},office_hours
969,no,"<p>Thank you for the response David. I an trying to do it by hand now, and creating a transition matrix seems straight forward</p>
<p>But I am not sure where to start my value iteration as I see most examples start at a discrete terminal state, but for our case, the terminal state can be infinitely far away with many differ final values?</p>
<p></p>
<p>Also, I am not sure if I should give instant rewards from state to state or just wait until the end?</p>
<p></p>
<p>Thank you for your help.</p>",2019-09-06T16:11:22Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08bh1jj4wybf,2019-09-06T16:11:22Z,{},office_hours
970,no,"<p>To answer your first question, you can set a horizon. Think about how many possible states your agent can be in. For N = 6, it&#39;s highly unlikely to get to bank roll 60 right? ( (1/6)^10 chance). Therefore, I chose 10 * N for states and 100 * N for V(s) (don&#39;t forget to initialize V(s) to zeros).</p>
<p></p>
<p>As for the second, you can do either! I set my model up to reward only at the cash out stage since it&#39;s the simplest ( using R(s) instead of R(s, a, s&#39;) )</p>
<p></p>
<p>Lmk if you have anymore questions.</p>",2019-09-06T16:26:31Z,63,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k08c0jcqvj63rn,2019-09-06T16:26:31Z,{},office_hours
971,no,"<p>Thanks again David.</p>
<p>After you chose your terminal state, did you start from the initial state or did you start from the terminal state and go backwards?</p>
<p></p>",2019-09-06T17:23:04Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08e19icuyx2ah,2019-09-06T17:23:04Z,{},office_hours
972,no,"<p>(This is not the only way to do it)</p>
<p><a href=""https://artint.info/html/ArtInt_227.html"" target=""_blank"">Following this algorithm </a></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhdgaq99za3um%2Fk08e7hmmcm90%2FScreen_Shot_20190906_at_1.27.15_PM.png"" alt="""" width=""449"" height=""379"" /></p>
<p></p>
<p>&#34;For each state&#34; so you want to start with the initial state each iteration (k). If you hit a terminal state, make sure to go to the end of your horizon (V(s)) to differentiate btwn initial (most people choose 0) and the terminal. I was hung up on that mistake for a while</p>",2019-09-06T17:29:56Z,63,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k08ea2ur2cc3st,2019-09-06T17:29:56Z,{},office_hours
973,no,"<p>Again though, most of this is discussed in &#64;82 - I really recommend checking that and other posts out!</p>",2019-09-06T17:37:01Z,63,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k08ej707zmo6k7,2019-09-06T17:37:01Z,{},office_hours
974,no,"<p>Thanks David.</p>
<p></p>
<p>Can someone tell me where I am going wrong with the above value iteration scheme:</p>
<p></p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk08flolzx74g%2F20190906_1100481.jpg"" target=""_blank"">20190906_1100481.jpg</a></p>",2019-09-06T18:08:04Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08fn46odqe4xw,2019-09-06T18:08:04Z,{},office_hours
975,no,"<p>You added the terminal state (quit) to the initial state - don&#39;t do that! Cause then you&#39;ll get 2.5, 5, 7.5... You need to add quit or bankrupt to the last V(s)</p>",2019-09-06T19:03:39Z,63,Week 9/1 - 9/7,feedback,,jzhdgaq99za3um,k08hmlxy2re2rz,2019-09-06T19:03:39Z,{},office_hours
976,no,"<p>Are you talking about the part that I circled in red?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk08k5xd4ain5%2F20190906_110048.jpg"" alt="""" width=""638"" height=""851"" /></p>",2019-09-06T20:17:17Z,63,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08k9b1nmln4qp,2019-09-06T20:17:17Z,{},office_hours
977,no,"<p>The MDP is wrong.</p>
<p></p>
<p>You don&#39;t transition to $$V(0)$$ from $$V(4)$$. You transition to a terminal state, call it $$V(T)$$, which is not to be confused with $$V(0)$$.</p>
<p></p>
<p>$$V(0)$$ is the initial state, you should expect the value of that state to be very high, in fact, the highest. $$V(T)$$ is a terminal state, there is no future in a terminal state, you should expect its value to be zero, which is the definition of a terminal state.</p>",2019-09-06T20:22:05Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08kfgo0eyt437,2019-09-06T20:22:05Z,{},office_hours
978,no,"<p>Am keeping start and terminal state separately. Still on iteration the value is doubling. </p>
<p>[&#39;start&#39;, &#39;bad&#39;, 4, 5, 6, 8, 9, 10, 11, 12, &#39;end&#39;]</p>
<p>Actions: Roll, Quit</p>
<p></p>
<p>From start state on roll am going to bad state with 1/2 probability and good states 4,5,6 with 1/6 probability. </p>
<p></p>
<p>[[[0. 0.5 0.16666667 0.16666667 0.16666667 0. 0. 0. 0. 0. 0. ] . -&gt; Start state for roll<br /> [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] . -&gt; bad state for roll<br /> [0. 0. 0.5 0. 0. 0.16666667 0.16666667 0.16666667 0. 0. 0. ]<br /> [0. 0. 0.5 0. 0. 0. 0.16666667 0.16666667 0.16666667 0. 0. ]<br /> [0. 0. 0.5 0. 0. 0. 0. 0.16666667 0.16666667 0.16666667 0. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]] -&gt; end state for roll</p>
<p></p>
<p>From start state for quit with probability 1</p>
<p>[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ] -&gt; start state for quit <br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]  -&gt; bad state for quit<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]<br /> [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. ]]] -&gt; end state for quit</p>",2019-09-07T17:55:13Z,63,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k09umfw0p7w4nw,2019-09-07T17:55:13Z,{},office_hours
979,no,<p>Have these office hours been posted?</p>,2019-09-10T02:32:54Z,62,Week 9/8 - 9/14,followup,,gx3c8l7z7r72zl,k0d7zws7ed95be,2019-09-10T02:32:54Z,{},office_hours
980,no,"<p>Yes, office hour for week 1, 2, 3 have been posted. Check on &#64;132</p>
<p></p>
<p></p>",2019-09-10T02:44:18Z,62,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0d8ejy6x053tm,2019-09-10T02:44:18Z,{},office_hours
981,no,<p>Thank you Quang! :)</p>,2019-09-10T02:46:23Z,62,Week 9/8 - 9/14,feedback,,gx3c8l7z7r72zl,k0d8h8r09jr1km,2019-09-10T02:46:23Z,{},office_hours
982,no,"<p>&#64;Barranco, how did you manage to account for incomplete stochasticity in the Roll action portion of the T matrix? <br /><br /></p>
<p>A potential T matrix. &#39;E&#39; is Vquit, L is the lose state where you end up if you hit a bad die face. As you can see the bottom portion of the matrix is incomplete and each row needs to sum to 1. Should it just be completed with 1&#39;s on the diagonal?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2FjhqqvwpyvXsQ%2Fk05r0qjerzg7%2FTMatrix.PNG"" alt="""" /></p>
<p></p>
<p></p>",2019-09-04T21:00:07Z,43,Week 9/1 - 9/7,followup,,jhqqvwpyvXsQ,k05qwo9tctw6e,2019-09-04T21:00:07Z,{},hw1
983,no,8-12 should quit since max 2 rolls.,2019-09-04T22:19:56Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k05trbu992oj,2019-09-04T22:19:56Z,{},hw1
984,no,"<p>Guys, this is as close as giving away the solution as it can get.  Are we allowed to go that far?</p>",2019-09-05T04:59:33Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k06818b1oxn3p3,2019-09-05T04:59:33Z,{},hw1
985,no,<p>its not code and the sample problem (rather than a personal problem) so I think we are allowed.</p>,2019-09-05T13:55:11Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k06r626yo9y38r,2019-09-05T13:55:11Z,{},hw1
986,no,"<p>According the the response to &#64;104, I think this is ok</p>",2019-09-05T15:29:52Z,43,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k06uju2vbhz19y,2019-09-05T15:29:52Z,{},hw1
987,no,"<p>If they ask me to delete this (which they haven&#39;t) then I will, but Angel is right that it&#39;s not really against any of the rules they&#39;ve put forward.</p>",2019-09-05T16:48:42Z,43,Week 9/1 - 9/7,feedback,,jhqqvwpyvXsQ,k06xd7nlrgj7l0,2019-09-05T16:48:42Z,{},hw1
988,no,"<p>Don&#39;t we need a &#34;quit&#34; state and a &#34;bankrupt&#34; state, where both states never transition to any other states, &#34;quit&#34; gives a reward of our current earnings, and &#34;bankrupt&#34; gives a reward of 0?</p>",2019-09-05T16:15:14Z,43,Week 9/1 - 9/7,followup,,jl284xdcifz44g,k06w663l8z81rc,2019-09-05T16:15:14Z,{},hw1
989,no,<p>I did both quit and bankrupt states.</p>,2019-09-05T16:23:57Z,43,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k06whdi28h14x4,2019-09-05T16:23:57Z,{},hw1
990,no,<p>Hi Angel! I&#39;m stuck in the reward value for the &#34;quit &#34; state (not as an action). How was your approach to tackle it?</p>,2019-09-08T07:32:29Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0antghdjp915j,2019-09-08T07:32:29Z,{},hw1
991,no,"Hi Juan, my approach was to set the reward for taking the quit action from a non-terminal state as the value of the bankroll, but the reward for the &#34;quit&#34; state is itself 0. There are other ways to model it, but this worked for me.",2019-09-08T19:37:38Z,42,Week 9/8 - 9/14,feedback,,jnjgrn6usm9x,k0bdq02uy263v,2019-09-08T19:37:38Z,{},hw1
992,no,<p>That&#39;s the way I took. Thanks Michael!</p>,2019-09-08T23:55:02Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0bmx10vxxj48e,2019-09-08T23:55:02Z,{},hw1
993,no,"My values are initialized to 0, but after several iterations, Vk[s0] converges to 2.5.",2019-09-05T21:34:47Z,63,Week 9/1 - 9/7,followup,,jqst1pv5Ozx5,k077l4bo5gq4im,2019-09-05T21:34:47Z,{},hw1
994,no,What about the next roll?,2019-09-05T21:43:40Z,63,Week 9/1 - 9/7,feedback,,jzj7y1ofgsro1,k077wiyfvy1q9,2019-09-05T21:43:40Z,{},hw1
995,no,"<p>4 &#43; V0[s&#39;]???</p>
<p></p>
<p>Did you mean 4 &#43; V0[s]? I guess your states are defined such that V0[s&#39;] is 4 &#43; V0[s] when dice rolls 4.</p>
<p></p>
<p>So you either use V[s&#39;] or add 4 to V0[s], you shouldn&#39;t do both.</p>",2019-09-05T22:41:48Z,63,Week 9/1 - 9/7,feedback,,jl2egn5k4zo4lp,k079zb08kfi6hj,2019-09-05T22:41:48Z,{},hw1
996,no,"<p>Matthew,</p>
<p></p>
<p>Have you seen my reply &#64;82 to your question?</p>",2019-09-05T23:45:29Z,63,Week 9/1 - 9/7,feedback,,jc6xvgjncoey,k07c96xsbp33o0,2019-09-05T23:45:29Z,{},hw1
997,no,"To clarify, I&#39;m using s to represent 1 time step (#4 in PDF). Since the game has an infinite horizon, I limit the time steps calculated.",2019-09-06T02:54:18Z,63,Week 9/1 - 9/7,followup,,jqst1pv5Ozx5,k07j00v4m2b479,2019-09-06T02:54:18Z,{},hw1
998,no,"<p>Also, if we are asked to solve for a large number like N=30 how do we automate it, am thinking there has to be a way to generate the transition prob matrix P and reward matrix R dynamically? Any hints?</p>",2019-09-05T16:50:58Z,42,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k06xg4kslm93ml,2019-09-05T16:50:58Z,{},hw1
999,no,<p>Have you computed the transition probabilities at all by hand?  Why can&#39;t you then write that same calculation in code?</p>,2019-09-05T17:33:11Z,42,Week 9/1 - 9/7,feedback,,jzfsa4a37jf4aq,k06yyei2uh35vl,2019-09-05T17:33:11Z,{},hw1
1000,no,<p>I think what we&#39;re stuck on (at least what I&#39;m stuck on) is this process relies on large tables of information that would be difficult and error prone to enter by hand. Is there a simple way to generate these tables for pymdptoolbox that we might not know about so we can avoid manually inputting 100&#43; states?</p>,2019-09-05T18:28:53Z,42,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k070y1o5e4y35b,2019-09-05T18:28:53Z,{},hw1
1001,no,"<p>I agree that its doable for smaller states like N=6, but for large ones like N = 30 what is the approach everyone suggest? come up with prob/rewards matrix manually? </p>",2019-09-05T19:20:22Z,42,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k072s8prqcq48m,2019-09-05T19:20:22Z,{},hw1
1002,no,"<p>I was able to do it with a very small piece of code - about 15 lines in python.</p>
<p></p>
<p>The key is to define the states. Rewards are closely related to the state value, if you have defined the states correctly. Update function needs to look ahead only at next N states in the reward table, since the dice roll can be only 1-N.</p>
<p></p>
<p>The code I wrote can handle any N and any <em>isBadState</em> array.</p>",2019-09-05T22:47:13Z,42,Week 9/1 - 9/7,feedback,,jl2egn5k4zo4lp,k07a692ljc83g8,2019-09-05T22:47:13Z,{},hw1
1003,no,"<p>It took me around 50 lines (of very inefficient code), but I finally solved it. Here are the steps I&#39;d recommend:</p>
<p></p>
<ol><li>Write up, by hand, the mapping from state to state as described in this tutorial: <a href=""https://github.com/rldm/rldm_tutorials"">https://github.com/rldm/rldm_tutorials</a>. So you&#39;ll need prob[0], prob[1], and a reward matrix.</li><li>Plug that in to pymdptoolbox using the simple test case <strong>Input: N = 6, isBadSide = {1,1,1,0,0,0}, Output: 2.5833. </strong>Make sure you&#39;re getting the correct output</li><li>Using a case from the homework problems themselves where N is small, write up the matrices by hand and plug them in to pymdptoolbox. Hopefully you get the correct answer</li><li>Write<strong> </strong>a script that can generate the probability and reward matrices for one example, then pass them to the ValueIteration function. Don&#39;t worry if it isn&#39;t robust, just get something that generates them properly</li><li>Test it between your two samples. This is where you&#39;ll need to make it more robust, and will likely involve some (hours of) debugging</li><li>Include more and more test cases until your code can handle any input of N and isBadSide and it gives the correct answer!</li></ol>
<p></p>
<p>Between asking this question and finishing the homework, I worked for about 3.5 hours on it (and I&#39;m not an algorithm whiz) so if you&#39;re already at this point you&#39;re probably close</p>",2019-09-05T23:11:53Z,42,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k07b1zh3p016f2,2019-09-05T23:11:53Z,{},hw1
1004,no,"<p>test2 Ek&#39;s:</p>
<p></p>
<pre>G_t= 11.225999999999999
E_1= -1.048
E_2= 28.326
E_3= 22.125999999999998
E_4= 14.325999999999999
E_5= 23.526
E_6= 11.225999999999999</pre>
<p></p>
<p>test3 Ek&#39;s:</p>
<p></p>
<pre>G_t= 6.164
E_1= 7.864
E_2= -5.336
E_3= 25.864
E_4= -11.936
E_5= -0.3360000000000003
E_6= 6.164</pre>
<p></p>",2019-09-06T03:35:06Z,38,Week 9/1 - 9/7,followup,,jc554vxmyuy3pt,k07kghkk91w62m,2019-09-06T03:35:06Z,{},hw2
1005,no,"Thanks Miguel. However, when I put &#64;155 into my code I get exactly the right answer, whilst it’s still wrong by a small amount for the first example, and the first problem in the test, so I am still perplexed...<div><br /><div>And thanks Jacob, those are the values of Gt and E I get for tests 2 &amp; 3 for which I get pretty much the right answer for lambda, except I don’t have an E6 - I stop at E5, unless we are using different nomenclature! I use Gt in calculating the post termination term.</div><div>Jacob - what do you get for the first test problem? I get:</div><div>Gt = 0.687</div><div>E1 = 13.553</div><div>E2 = 6.087</div><div>E3 = 35.187</div><div>E4 = 27.287</div><div>E5 = 16.687</div><div>I get lambda = 0.6227695, whereas it should be 0.6226326.</div><div><br /></div><div>The equation I am solving is</div><div>(Gt-E5) lambda**5 &#43; (E5-E4) lambda**4 &#43; (E4-E3) lambda**3 &#43; ... &#43; (E1-Gt) = 0.</div><div><br /></div><div>Arrgghhhh!</div></div>",2019-09-06T06:57:34Z,38,Week 9/1 - 9/7,feedback,,jzvoaw0pl6326,k07rov1oueg3kd,2019-09-06T06:57:34Z,{},hw2
1006,no,"<p>What is different on 1 that 2, 3 and <em>your</em> 10 problems do not have?</p>",2019-09-06T11:35:48Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k081modgkbj69z,2019-09-06T11:35:48Z,{},hw2
1007,no,"<p>Also, your Gt looks off.</p>",2019-09-06T11:40:32Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k081srejhv6351,2019-09-06T11:40:32Z,{},hw2
1008,no,"<p>Here&#39;s my test1:</p>
<p></p>
<pre>G_t= 16.687
E_1= 13.552999999999999
E_2= 6.087000000000002
E_3= 35.187000000000005
E_4= 27.287000000000003
E_5= 16.687000000000005
E_6= 16.687000000000005</pre>
<p>Final answer 0.623 (Root 0.6227695030954604)</p>",2019-09-06T12:50:42Z,38,Week 9/1 - 9/7,feedback,,jc554vxmyuy3pt,k084azq33r0k,2019-09-06T12:50:42Z,{},hw2
1009,no,<p>Nice!</p>,2019-09-06T14:31:33Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k087wok7tse3va,2019-09-06T14:31:33Z,{},hw2
1010,no,"<p>&#64;Deb, where did you get the equation <strong>(Gt-E5) lambda**5 &#43; (E5-E4) lambda**4 &#43; (E4-E3) lambda**3 &#43; ... &#43; (E1-Gt) = 0</strong> from?</p>",2019-09-08T16:20:03Z,37,Week 9/8 - 9/14,feedback,,jzhdgaq99za3um,k0b6nwrquhh2e8,2019-09-08T16:20:03Z,{},hw2
1011,no,"<p>Sutton 12.2?</p>
<p></p>
<p>$$G_t^\lambda = (1 - \lambda) \sum_{n=1}^{\infty}{\lambda^{n-1}G_{t:t&#43;n}}$$</p>",2019-09-08T18:10:35Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bam2dqb3g251,2019-09-08T18:10:35Z,{},hw2
1012,no,<p>Check 12.3 as well.</p>,2019-09-08T18:11:06Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bamqjgck137q,2019-09-08T18:11:06Z,{},hw2
1013,no,<p>Ohh I see it now - thanks!</p>,2019-09-08T19:47:15Z,37,Week 9/8 - 9/14,feedback,,jzhdgaq99za3um,k0be2dgoih956b,2019-09-08T19:47:15Z,{},hw2
1014,no,"<p>In case anyone else wanted to see it</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhdgaq99za3um%2Fk0bemmj0r78z%2FScreen_Shot_20190908_at_4.02.39_PM.png"" alt="""" /></p>",2019-09-08T20:04:49Z,37,Week 9/8 - 9/14,feedback,,jzhdgaq99za3um,k0beoyzk7le606,2019-09-08T20:04:49Z,{},hw2
1015,no,"<p>mate, you&#39;re giving away the solution ... </p>",2019-09-11T10:54:13Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0f5cfxyy4142w,2019-09-11T10:54:13Z,{},hw2
1016,no,"<p>How is it a 5th degree, there is a term in solving TD($$\lambda $$)= TD(1)</p>
<p>Where did the 6th degree E6 term go?</p>
<p>(1-$$\lambda $$)$$\lambda $$$$^{5}$$E$$^{6}$$</p>",2019-09-14T18:58:26Z,37,Week 9/8 - 9/14,feedback,,jc6s23mr74r45q,k0jwyppjq7w58x,2019-09-14T18:58:26Z,{},hw2
1017,no,"<p>Oh i see, i got it , &#64;David thanks for giving the steps to solve this equation, really helped</p>",2019-09-14T19:01:13Z,37,Week 9/8 - 9/14,feedback,,jc6s23mr74r45q,k0jx2a7bhta1n1,2019-09-14T19:01:13Z,{},hw2
1018,no,"<p>&#64;anju, no problem! </p>
<p></p>
<p>&#64;jean, it&#39;s my understanding that we&#39;re allowed to share &amp; discuss equations, theory, etc, but not explicit code. If I&#39;m mistaken, I genuinely welcome any correction, however, the instructors have obviously seen this and not pinged me / taken it down. </p>",2019-09-23T17:32:41Z,35,Week 9/22 - 9/28,feedback,,jzhdgaq99za3um,k0wov3ixzr272p,2019-09-23T17:32:41Z,{},hw2
1019,no,"Correction - I am now sorted, 10/10. I still don’t quite get the same answer for test case 1, but I’ve now correctly answered all the homework - I had the Vs from problem 1 and the rewards from problem 2 - doh! Still no idea why the small error on test case 1!<div>Thanks Miguel &amp; Jacob.</div>",2019-09-06T07:27:38Z,38,Week 9/1 - 9/7,followup,,jzvoaw0pl6326,k07srj26l8m2vr,2019-09-06T07:27:38Z,{},hw2
1020,no,"<p>Hi Deborah,</p>
<p></p>
<p>Are you still getting a value of  0.6227695 for test case 1?</p>",2019-09-06T15:43:00Z,38,Week 9/1 - 9/7,feedback,,jzjr6qef6i06tp,k08agks0pmj503,2019-09-06T15:43:00Z,{},hw2
1021,no,<p>I also got 0.6227695 though I am calculating it in a pretty similar way. I wonder if there might be a slight error on the example posted on the hw.</p>,2019-09-07T08:39:04Z,38,Week 9/1 - 9/7,feedback,,jccyemecUB8q,k09ar8jszu54t3,2019-09-07T08:39:04Z,{},hw2
1022,no,<p>Not the first time a student thinks this.</p>,2019-09-07T13:24:08Z,38,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k09kxu2uws3j5,2019-09-07T13:24:08Z,{},hw2
1023,no,<p>I also get 0.6227695.  Not sure if there is something I&#39;m missing.</p>,2019-09-11T03:45:17Z,37,Week 9/8 - 9/14,feedback,,jl8j7vzvUNs2,k0eq0tt31b965a,2019-09-11T03:45:17Z,{},hw2
1024,no,<p>Rounding. That&#39;s the issue.</p>,2019-09-12T01:54:46Z,37,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0g1ik1r1nh642,2019-09-12T01:54:46Z,{},hw2
1025,no,<p>But who has the rounding error?  The students or the official solution?</p>,2019-09-12T02:30:13Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g2s5ff4rc60r,2019-09-12T02:30:13Z,{},hw2
1026,no,"<p>Student, check Deborah&#39;s and Jacob&#39;s Es estimates. It is a minimal difference, but it explains the discrepancies.</p>",2019-09-12T03:07:25Z,37,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0g43zwa9ck4i9,2019-09-12T03:07:25Z,{},hw2
1027,no,"<p>Deborah gets the same $$0.6227695$$, as does Jacob A.  In fact, the only place a different number is posted is on the official solution.  </p>",2019-09-12T04:27:43Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g6z988gol7am,2019-09-12T04:27:43Z,{},hw2
1028,no,<p>I actually got all of the homework right with just having passed test2 and test3. I get the same  0.6227695 for test1 </p>,2019-09-14T04:44:40Z,37,Week 9/8 - 9/14,feedback,,jqr9leosvw8P,k0j2grj44594kr,2019-09-14T04:44:40Z,{},hw2
1029,no,"<p>Yeah, it&#39;s starting to look like we&#39;re the ones who are right :D</p>",2019-09-14T05:08:41Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0j3bnhougm588,2019-09-14T05:08:41Z,{},hw2
1030,no,<p>I&#39;m getting 0.6227695 as well. Not a warm fuzzy at midnight :)</p>,2019-09-15T04:03:54Z,36,Week 9/15 - 9/21,feedback,,jzk2u4t3n5o5el,k0kgg6wfgp12x7,2019-09-15T04:03:54Z,{},hw2
1031,no,Try some of the homework examples in the assessment - I got them all right but my code still gives the wrong number like yours.,2019-09-15T10:41:20Z,36,Week 9/15 - 9/21,feedback,,jzvoaw0pl6326,k0kunan2y6oju,2019-09-15T10:41:20Z,{},hw2
1032,no,"<p>How to calculate the E6 above? I got all the E1 - E5 correct. I thought E6 should be equal to E5 because once reaching the final state, reward is 0 if it keeps looping at the final state. The value estimate at E6 should be equal to V(6). Clearly, it doesn&#39;t work that way. </p>",2019-09-14T05:38:06Z,37,Week 9/8 - 9/14,followup,,j6ln9puq99s5uv,k0j4dh9yqy03up,2019-09-14T05:38:06Z,{},hw2
1033,no,"<p>Estimates of state values should look forward, not backward, since the value of a state is defined as the sum of discounted <strong>future</strong> rewards.</p>",2019-09-14T07:12:46Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0j7r7xx4v37z,2019-09-14T07:12:46Z,{},hw2
1034,no,<p>So is Gt always same as E5 since you have no more steps beyond 5 in this example?</p>,2019-09-14T19:53:40Z,37,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0jyxqo1tyk46f,2019-09-14T19:53:40Z,{},hw2
1035,no,"<p>So if E6 is calculated upto 6 steps ahead which means it has to land on virtual &#34;state 7&#34; which has no reward and VS(7) = 0</p>
<p>then it is always that E6 =E5?</p>",2019-09-14T20:00:40Z,37,Week 9/8 - 9/14,feedback,,jc6s23mr74r45q,k0jz6qpgq5d48j,2019-09-14T20:00:40Z,{},hw2
1036,no,"<p>Hi Anju,</p>
<p>You are on the right track to calculate VS(7) and R(7).</p>
<p></p>
<p>Just plug this in your formula for E(k) and you will see the difference between E5 and E6 yourself.</p>
<p></p>
<p>They are not always same.</p>
<p></p>
<p></p>",2019-09-14T20:25:59Z,37,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0k03arglje4n9,2019-09-14T20:25:59Z,{},hw2
1037,no,"<p>Hi All,<br />I am confused how to calculate for E6 as we don&#39;t have enough data points for this as we have finished all states as mentioned above. Does E6=Gt always?</p>
<p></p>
<p>I see that Anurag said they are not always the same, but again how do we calculate this, we don&#39;t have enough data? If I assume VS(7) and R(7) are both zero, then E6=E5 as all we are doing is adding zeros to the equation.</p>",2019-09-14T21:42:10Z,37,Week 9/8 - 9/14,feedback,,gx3c8l7z7r72zl,k0k2t9jchx2pr,2019-09-14T21:42:10Z,{},hw2
1038,no,"Hi Aaron,<div>Check the V of last state you transitioned  in E5 vs E6. That would be the difference.</div>",2019-09-14T21:44:39Z,37,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0k2wgztnnr6bu,2019-09-14T21:44:39Z,{},hw2
1039,no,"<p>Thanks, Anurag! This tip is very helpful! For all, just write the formulas for E5 and E6 and you will see the difference :-)</p>",2019-09-15T17:39:24Z,36,Week 9/15 - 9/21,feedback,,jzih0fdt4sn1cq,k0l9kxkgzaq4qa,2019-09-15T17:39:24Z,{},hw2
1040,no,"<p>Thanks, Anurag and Aida! This was a very helpful. I second Aida&#39;s point, write the formulas for E5 and E6, and the difference will present itself.</p>",2019-09-16T03:16:07Z,36,Week 9/15 - 9/21,feedback,,jl32178hpgpS,k0lu6l9gbvc5ro,2019-09-16T03:16:07Z,{},hw2
1041,no,<p>Isn&#39;t this the Maximum Likelihood Estimate?</p>,2019-09-07T21:15:20Z,63,Week 9/1 - 9/7,followup,,jl2egn5k4zo4lp,k0a1rt526ro3bp,2019-09-07T21:15:20Z,{},hw2
1042,no,"<p>TD(0) is the maximum likelihood estimate, TD(1) is the outcome-based estimate.</p>
<p></p>
<p>TD(0) converges to the max likelihood Markov model, TD(1) converges to the minimum mean-squared error.</p>
<p></p>
<p>TD(0) converges to the solution to the MDP that best fits the data, TD(1) converges to the best fit of the observed returns.</p>
<p></p>",2019-09-07T23:16:22Z,63,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0a63g4gtvm5c,2019-09-07T23:16:22Z,{},hw2
1043,no,<p>So is the above way of calculation TD(1)?</p>,2019-09-13T21:06:21Z,62,Week 9/8 - 9/14,feedback,,j6ln9puq99s5uv,k0im3d05f2c1zj,2019-09-13T21:06:21Z,{},hw2
1044,no,"<p>Yes,</p>
<p></p>
<p>$$E_{\infty} = TD(1)$$.</p>",2019-09-13T23:34:37Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ire14ufdf4p6,2019-09-13T23:34:37Z,{},hw2
1045,no,"<p>Hi Folks,</p>
<p>Could someone explain, how did we get E1 here as:</p>
<p>81∗(7.9&#43;2.5)&#43;.19∗(−5.1−7.2)&#43;9&#43;1.6=16.687.</p>
<p></p>
<p>does it translate to:<br />E2 = P(S1) * (R0 &#43; R2 &#43; V1) &#43; (1-P(S)) * (R1 &#43; R3 &#43; V3) &#43; R4 &#43; R5 &#43; R6 </p>",2019-09-14T08:04:41Z,62,Week 9/8 - 9/14,followup,,jfzaqnqvtQ1m,k0j9lz4v70c7ip,2019-09-14T08:04:41Z,{},hw2
1046,no,<p>Can you write out the generic formula for $$E_2$$ and compare it to what you have written above?  Remember that the $$k$$ in $$E_k$$ refers to how many state transitions you&#39;re going to make before you stop accumulating rewards and instead use the state value of the state you&#39;ve reached as a proxy for all future rewards.</p>,2019-09-14T08:34:51Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jaos18b5g35q,2019-09-14T08:34:51Z,{},hw2
1047,no,"<p>Thanks Vahe. I think I got it.</p>
<p></p>
<p>E(2) = P(S1) ( R0 &#43; R2) &#43; (1- P(S1) ( R1 &#43; R3) &#43; V3.</p>
<p></p>",2019-09-14T08:41:17Z,62,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0jax1kx3fl5mm,2019-09-14T08:41:17Z,{},hw2
1048,no,"<p>Do you mean installing git on Windows, or using git on Windows?</p>
<p>I will assume you mean the later. </p>
<p></p>
<p>This could be one basic entry guide if you just start with git:</p>
<p><a href=""https://rogerdudler.github.io/git-guide/"">https://rogerdudler.github.io/git-guide/</a></p>
<p>It would get you familiar with basic git commands: clone, add, status, commit, pull, push...</p>
<p>If you want to get more fluent with git, I would recommend this:</p>
<p><a href=""https://www.atlassian.com/git/tutorials/learn-git-with-bitbucket-cloud"">https://www.atlassian.com/git/tutorials/learn-git-with-bitbucket-cloud</a></p>
<p>It has lot more than what you would need for this course, but very good if you plan to get more involve as daily workflow in a team who uses git.</p>
<p></p>
<p>For this class, it would be helpful to look into setting up git cache/store credential as well, so you don&#39;t have to type your credential everytime.</p>
<p><a href=""https://git-scm.com/docs/git-credential-cache"">https://git-scm.com/docs/git-credential-cache</a></p>",2019-09-05T19:49:00Z,63,Week 9/1 - 9/7,followup,,jl5wq8mca7o0,k073t2rnqou1ht,2019-09-05T19:49:00Z,{},project1
1049,no,"<p>I am familiar with the basic usages. But still cannot pass the git configuration. </p>
<p></p>
<p>The git remote works fine </p>
<pre>git remote add origin https://github.gatech.edu/gt-omscs-rldm/7642Fall2019xxxx.git</pre>
<p></p>
<p>But still fail in pushing or cloning</p>
<p>    remote: Invalid username or password.<br />    fatal: Authentication failed</p>
<p></p>
<p>Tried to copy a local .gitconfig file and set up the accout:</p>
<p>[user]<br /> email = namexx&#64;gatech.edu<br /> name = namexx</p>
<p></p>
<p>Tried to update the password in the Credential Manager:</p>
<p>    git:http://github.gatech.edu</p>
<p></p>
<p>Tried to set up the two factor verification. </p>
<p></p>
<p>Not sure what else I could try... Do not know it is caused by wrong user name or wrong password...</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-05T20:06:11Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k074f5uhl1g269,2019-09-05T20:06:11Z,{},project1
1050,no,"<p></p>
<p>I can log in <a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019ryuan37"">https://github.gatech.edu/</a>   with my Gatech ID and password, but cannot log in through GitHub Desktop using the same information. </p>
<p></p>
<p>Is there a way to check the root issue? Many thanks.</p>",2019-09-05T20:12:08Z,63,Week 9/1 - 9/7,feedback,,jzg6jh2hn6f43c,k074mtkhari19p,2019-09-05T20:12:08Z,{},project1
1051,no,"<p>I don&#39;t have access to any Windows machine, to offer further help about Git Credential Manager on Window. But on the look of your setup:</p>
<md><br />&#96;git:http://github.gatech.edu&#96;<br /></md>
<p>probably should be:</p>
<md><br />&#96;https://github.gatech.edu&#96;<br /></md>
<p>username and password are the same as what you use on <a href=""https://github.gatech.edu"">https://github.gatech.edu</a> , and are unrelated to what you have under [user] config.</p>",2019-09-05T22:53:47Z,63,Week 9/1 - 9/7,feedback,,jl5wq8mca7o0,k07aepi88172ry,2019-09-05T22:53:47Z,{},project1
1052,no,"<p>No SSH support? I have added my SSH key, but connection simply times out.</p>
<pre>$ git clone git&#64;github.gatech.edu:gt-omscs-rldm/7642Fall2019eignatev3.git
Cloning into &#39;7642Fall2019eignatev3&#39;...
ssh: connect to host github.gatech.edu port 22: Connection timed out
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.</pre>",2019-09-06T04:01:49Z,63,Week 9/1 - 9/7,feedback,,jzhwdil7ssn443,k07leu5wcs69a,2019-09-06T04:01:49Z,{},project1
1053,no,"<p>Found the answer - <a href=""https://faq.oit.gatech.edu/content/why-cant-i-establish-ssh-session-gt-github"">https://faq.oit.gatech.edu/content/why-cant-i-establish-ssh-session-gt-github</a></p>
<p>I guess I will request Gatech VPN.</p>",2019-09-06T04:06:36Z,63,Week 9/1 - 9/7,feedback,,jzhwdil7ssn443,k07lkzdovyr1l2,2019-09-06T04:06:36Z,{},project1
1054,no,"<p>Using GitHub Desktop, make sure to login / clone with &#34;GitHub Enterprise Server&#34;, not &#34;GitHub.com&#34;. That has always worked for me, without using the VPN.</p>",2019-09-08T05:03:35Z,62,Week 9/8 - 9/14,feedback,,jzlyi4e55bz5kj,k0aihyyo8up39n,2019-09-08T05:03:35Z,{},project1
1055,no,<p>I connected to VPN and still see this error :(</p>,2019-09-15T23:57:37Z,61,Week 9/15 - 9/21,feedback,,jqr9leosvw8P,k0ln3b5y2c1y,2019-09-15T23:57:37Z,{},project1
1056,no,"<p>actually this works on vpn: <br /><br /></p>
<p>it clone https://github.gatech.edu/gt-omscs-rldm/7642Fall2019kkohfeldt3</p>",2019-09-15T23:58:34Z,61,Week 9/15 - 9/21,feedback,,jqr9leosvw8P,k0ln4jq5bk618c,2019-09-15T23:58:34Z,{},project1
1057,no,"<p>dalton there is an extensive discussion on the slack ;)</p>
<p></p>
<p>:)</p>
<p></p>
<p></p>",2019-09-05T20:53:04Z,42,Week 9/1 - 9/7,followup,,jzivtxcbl6964n,k0763h454584em,2019-09-05T20:53:04Z,{},hw1
1058,no,<p>I finally solved it! The whole thing! Thanks so much for all the help along the way</p>,2019-09-05T22:57:19Z,42,Week 9/1 - 9/7,feedback,,jl284xdcifz44g,k07aj8t1dcn2i3,2019-09-05T22:57:19Z,{},hw1
1059,stud,"<p>^ Any update on when this will happen, ballpark? (Just anticipating there will be incorrect account issues etc. so want to verify if I am in that boat :p)</p>",2019-09-14T22:18:59Z,62,Week 9/8 - 9/14,followup,a_0,,k0k44mkuanq5db,2019-09-14T22:18:59Z,{},hw1
1060,no,"<p>A fun prank would be for the TAs to give $$0$$ grades to all submitted homeworks, in Canvas, and when the deluge of posts come in, to say there&#39;s no way to validate anyone&#39;s claim that they submitted the correct solutions on time.</p>",2019-09-14T22:31:16Z,62,Week 9/8 - 9/14,followup,,jzfsa4a37jf4aq,k0k4kfeam094rp,2019-09-14T22:31:16Z,{},hw1
1061,no,"<p>Back in my undergrad, I used Julia primarily in one of my ml classes, and I personally loved it. (: If you have ample time to kill dealing with the intricacies of the language, I&#39;d highly recommend it.</p>",2019-09-06T05:04:31Z,63,Week 9/1 - 9/7,followup,,jzjetsja3at5h9,k07nngxqk562p7,2019-09-06T05:04:31Z,{},logistics
1062,no,<p>I&#39;ve done all the work so far in Julia! I love it! I also think the Bellman equations play well with the unicode support in Julia. I&#39;m also doing the Bayesian Stats class right now in Julia. </p>,2019-09-06T06:02:05Z,63,Week 9/1 - 9/7,followup,,jzkifxpxau95an,k07pphv8rkq1iv,2019-09-06T06:02:05Z,{},logistics
1063,no,<p>I know a few people when I took the class (this past summer) that used Julia for P1 and P3. Unsure about P2.</p>,2019-09-06T13:04:14Z,63,Week 9/1 - 9/7,followup,,jl1acpoc4HA9,k084se7z25q6cf,2019-09-06T13:04:14Z,{},logistics
1064,no,<p>Very cool!  I may try to get a Julia version of Project 1 working after I finish the python implementation.  We&#39;ll see what time allows for.</p>,2019-09-07T03:46:57Z,63,Week 9/1 - 9/7,feedback,,jqmfuaidej9155,k090bkj16bm1ww,2019-09-07T03:46:57Z,{},logistics
1065,no,"<p>I also really liked it, but it is more work if you&#39;re learning it for the first time.</p>",2019-09-06T19:06:17Z,63,Week 9/1 - 9/7,followup,,jzhdgaq99za3um,k08hpzy537j68p,2019-09-06T19:06:17Z,{},logistics
1066,no,<p>I&#39;m able to do the example by hand now. Gonna write the code tomorrow. I still have that little itch to figure out what I was doing wrong in my initial approach because I know it&#39;s possible haha Thanks for the response John!</p>,2019-09-06T04:09:49Z,42,Week 9/1 - 9/7,followup,,j6m1jeidndu6wq,k07lp4nfgl41z4,2019-09-06T04:09:49Z,{},hw1
1067,no,"<p>I&#39;m not sure if I&#39;m reading it correctly David, but I think you are giving more reward than you should at state 4, for instance.</p>",2019-09-06T04:13:06Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07ltcovfe52sk,2019-09-06T04:13:06Z,{},hw1
1068,no,"<p>If you have $4 and you roll a 4, you only get $4 more and move to state 8.</p>",2019-09-06T04:13:50Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07lua9mphn66p,2019-09-06T04:13:50Z,{},hw1
1069,no,"<p>Ok I just had a &#39;duh&#39; moment. Yeah if you reward as you go then ending up in state 8 from 4 should be a reward of 4, not 8. Because that would be a total of 12 which is incorrect! Thanks Miguel! </p>
<p></p>
<p>Right! My reward with that case would be what I roll, not the state I&#39;m in!</p>",2019-09-06T04:14:14Z,42,Week 9/1 - 9/7,feedback,,j6m1jeidndu6wq,k07lusrukfi3gq,2019-09-06T04:14:14Z,{},hw1
1070,no,"<p>Very nice setup you have in there with the whiteboard!</p>
<p></p>
<p>Glad it makes sense.</p>",2019-09-06T04:14:53Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07lvn2o26i75s,2019-09-06T04:14:53Z,{},hw1
1071,no,<p>Thanks! It&#39;s the new Coda building they built on campus at Tech. I&#39;m a research engineer for GTRI so I usually just work in my office</p>,2019-09-06T04:15:46Z,42,Week 9/1 - 9/7,feedback,,j6m1jeidndu6wq,k07lwsh4wje4va,2019-09-06T04:15:46Z,{},hw1
1072,no,"<p>Oh, nice. GTRI is pretty big on a simulation engine I use for training deep RL agents at work, AFSIM. In fact, I think GTRI is hosting the AFSIM User Group later this year in Marietta, I might be going.</p>",2019-09-06T04:23:46Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k07m7295ese4kg,2019-09-06T04:23:46Z,{},hw1
1073,no,"<p>Oh that&#39;s awesome. I&#39;d definitely like to learn more about simulation, I&#39;ll be sure to pick your brain about it sometime this semester </p>",2019-09-06T04:51:51Z,42,Week 9/1 - 9/7,feedback,,j6m1jeidndu6wq,k07n76bc9up7it,2019-09-06T04:51:51Z,{},hw1
1074,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjla4c8d6bj94ou%2Fk08g9hby8a6o%2FIMG_20190906_212311.jpg"" alt="""" width=""802"" height=""601"" /></p>
<p>Hey guys, I was wondering if you can say that I&#39;m going in the right direction</p>
<p>So in the first iteration we make the calculation and get these numbers for action &#34;roll&#34; V(0,r)=2.5005, V(4,r)=0.5005 V(5, r) = -.005, V(6, r)=-0.49, and for &#34;quit&#34; wet get V(0,q)=0, V(4, q)=4, V(5,q)=5, V(6,q)=6</p>
<p></p>
<p>For the next iteration we choose the max which is (V0,r)=2.5005 and plug it in the equation right back, e.g V(0, r)=.1667 * (4 &#43; 2.5005) &#43; .1667 * (5 &#43; 2.5005) &#43; .1667 * (6 &#43; 2.5005)...</p>
<p>Continue calculating V(k, r) and choosing the max across all the states.</p>
<p></p>
<p>What should I do with V(k, q) I&#39;m not sure tho. Maybe I should chose V(6,q) instead V(0,r)?</p>",2019-09-06T18:38:23Z,42,Week 9/1 - 9/7,followup,,jla4c8d6bj94ou,k08gq3paxdzb2,2019-09-06T18:38:23Z,{},hw1
1075,no,"<p>First, a notation issue: what you are calculating is the Q-function, the value of state-action pair, you want the V, which is the max over the actions of the Q-function. I see you are saying you are using the V(s), but I would recommend you rearrange your calculations that way.</p>
<p></p>
<p>Here is Value Iteration in words:</p>
<p></p>
<p>Initialize $$V_k(s)$$ to 0 for all s in S. Calculate $$Q(0, r)$$, then $$Q(0, q)$$ (with the equation you have above), and set $$V(0) = max_a Q(0, a)$$. Then move to calculate <strong><em>states</em></strong> 1, then 2, and so on until you reach <strong>~</strong>12 states. That&#39;s your $$V_{k&#43;1}(s)$$, you have to move now to calculating $$V_{k&#43;2}(s)$$. Keep doing that until $$V_{k&#43;i}(s) \approx V_{k&#43;i-1}(s)$$ for all s in S.  </p>
<p></p>
<p>The homework is asking you for $$V_{k&#43;i}(0)$$.</p>",2019-09-06T20:02:34Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08jqd27pk266v,2019-09-06T20:02:34Z,{},hw1
1076,no,"<p>Hi all,</p>
<p></p>
<p>John points out that: &#34;<em>I switched from “reward-as-you-go and a bankruptcy penalty” to “cumulative winnings (zero reward per transition along the way) if you decide to quit, zero reward if you lose (no need for negative reward since you didn’t accumulate anything along the way)</em>&#34;</p>
<p></p>
<p>So would that make David&#39;s first calculation of Q1(0, roll) = 0.1667(0 &#43; 0) &#43; 0.1667(0 &#43; 0) &#43; 0.1667(0 &#43; 0)  = 0   (This does NOT make sense to me)</p>
<p></p>
<p></p>
<p>Additionally, based on this:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk08ho9okme0k%2FCapture2.PNG"" alt="""" /></p>
<p>When we get to <i>|V<sub>k</sub>[s]-V<sub>k-1</sub>[s]| &lt; θ</i>, is our answer <i>V<sub>k</sub>[s]<sub> ?</sub></i></p>
<p></p>",2019-09-06T19:06:23Z,42,Week 9/1 - 9/7,followup,,gx3c8l7z7r72zl,k08hq46ec0f2cj,2019-09-06T19:06:23Z,{},hw1
1077,no,"<p>When you get to $$|V_k(s)-V_{k-1}(s)| &lt; θ$$, the answer the homework is asking for is $$V_k(0)$$.</p>
<p></p>
<p></p>",2019-09-06T20:10:10Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08k05ljhe05e5,2019-09-06T20:10:10Z,{},hw1
1078,no,"<p>Thanks <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  . Below you can see my attempt so far of value iteration, but it looks like with each successive K iteration that the value function values keep growing? I am not sure where I am going wrong, does anyone see something egregious?</p>
<p></p>
<p>Do with each new K iteration, do the probabilities change? for EX: K=1, 1/6 to get 4-6, K = 2, (1/6)*(1/6) to get 4-6</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk08kq3gx5gzk%2F20190906_132705.jpg"" alt="""" /></p>",2019-09-06T20:30:48Z,42,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08kqo5ztbd63,2019-09-06T20:30:48Z,{},hw1
1079,no,"<p>Yes, I mentioned somewhere else $$V(0) \neq V(T)$$.</p>
<p></p>
<p>The initial state and the terminal state cannot be the same state.</p>",2019-09-06T20:40:10Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08l2q896uz56v,2019-09-06T20:40:10Z,{},hw1
1080,no,<p>&#64;137</p>,2019-09-06T20:41:30Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08l4fnlf136hm,2019-09-06T20:41:30Z,{},hw1
1081,no,"<p>Hey <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p style=""text-align:left"" dir=""ltr"">You stated in the other post:</p>
<p style=""text-align:left"" dir=""ltr""></p>
<p style=""text-align:left"" dir=""ltr""><span style=""text-decoration:underline""><em>You don&#39;t transition to </em></span><span style=""font-size:120%"">V</span>(0) from <span style=""font-size:120%"">V</span>(4). You transition to a terminal state, call it <span style=""font-size:120%"">V</span>(T), which is not to be confused with <span style=""font-size:120%"">V</span>(0).</p>
<p style=""text-align:left"" dir=""ltr""><span style=""text-decoration:underline""><em> </em></span></p>
<p style=""text-align:left"" dir=""ltr""><span style=""text-decoration:underline""><em></em></span><span style=""font-size:120%"">V</span>(0) is the initial state, you should expect the value of that state to be very high, in fact, the highest. <span style=""font-size:120%"">V</span>(T) is a terminal state, there is no future in a terminal state, you should expect its value to be zero, which is the definition of a terminal state.</p>
<p style=""text-align:left"" dir=""ltr""><span style=""text-decoration:underline""><em></em></span></p>
<p style=""text-align:left""></p>
<p style=""text-align:left"">Where am I transitioning to the V(0) from V(4)? Are you talking about what I have circled below? If so, I thought we still need to account for rolling a 1-3?</p>
<p style=""text-align:left""></p>
<p style=""text-align:left""><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk08lftc9j8ru%2FCapture2.PNG"" alt="""" /></p>
<p></p>",2019-09-06T20:51:04Z,42,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k08lgqkkso06to,2019-09-06T20:51:04Z,{},hw1
1082,no,"<p>But you don&#39;t transition back to V(0), you transition to a terminal state V(T). You do lose money, and you perhaps start a new episode, which we will start at V(0), but that is most certainly not equivalent to saying that from V(4) you go to V(0) with 3/6 prob.</p>
<p></p>
<p>This whole column is what is complicating your calculations:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fk08lz4dmetig%2FCS_7642__1_unread_.png"" alt="""" /></p>
<p></p>
<p>The rules of the game are: </p>
<p></p>
<ul><li>You start with 0 in the bankroll (your initial state is s=0).</li><li>If you decide to roll: 
<ul><li>and roll a 1, 2, or 3 you die you are taken away all of your current money &#96;-s&#96; (go to a terminal state, state T).</li><li>or roll a 4, 5, or 6 you get paid that number s&#39; and move to state s&#39;.</li></ul>
</li><li>At any time you may choose to quit, instead, in which case you move straight to T.</li></ul>
<p></p>
<p>0 is the expected value of the game when you enter fresh, but if you also add 0 as the state when you die, it&#39;s like giving your agent a free pass. Just kill the agent, terminate the episode by sending it to a terminal state.</p>
<p></p>",2019-09-06T21:10:52Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08m675xp0lgn,2019-09-06T21:10:52Z,{},hw1
1083,no,"<p>Remember this, state numbers do not equal reward received, they mean current bankroll, except the terminal state, which you can make state 15. Just don&#39;t pay the agent equivalent to 15 to get there. </p>
<p></p>
<p>Also, rewards received does not equal the value of a state. </p>
<p></p>
<p>Keep at it, you are getting there!!!</p>",2019-09-06T21:13:24Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08m9gtqro82w6,2019-09-06T21:13:24Z,{},hw1
1084,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>OK, so I did a more complete set of iterations and my final value for the n=6 example is 2.5 not 2.5833. Is this still OK? Can anyone see an issue with my work?</p>
<p>Thank you so much for your help!</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk09wbqjh1y2q%2F20190907_113144.jpg"" alt="""" /></p>",2019-09-07T18:43:15Z,42,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k09wc8b9ce1160,2019-09-07T18:43:15Z,{},hw1
1085,no,"<p>You are getting closer, but still not correct if you are getting 2.5.</p>
<p></p>
<p>Your calculations seem much better now, but I&#39;m missing the calculations for $$Q_1(0, quit)$$, $$Q_1(4, quit)$$, $$Q_1(5, quit)$$, $$Q_1(6, quit)$$. Do those and make sure to take the MAX over the actions as the Value of the state. In specific, your $$Q_1(6, roll)$$ may not be the max!!! What is $$Q_1(6, quit)$$????? Therefore, your $$Q_2(0, roll)$$ is incorrect.</p>",2019-09-07T18:50:25Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k09wlfqhlpi6r8,2019-09-07T18:50:25Z,{},hw1
1086,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>OK, so I added in Q1(0,quit), Q1(4,quit), Q1(5,quit), Q1(6,quit) which made me take the quit terminal state for all but s=0. Now my Q2(0,roll) seems way too high?</p>
<p></p>
<p>Have I veered off track again?</p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk0a1eebu5bcm%2F20190907_135837.jpg"" alt="""" /></p>",2019-09-07T21:05:42Z,42,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k0a1fem43nv5k0,2019-09-07T21:05:42Z,{},hw1
1087,no,"<p>Haha. Yes, because you are paying off your reward twice. You don&#39;t have to pay again when the agent quits, you already paid on landing on a state.</p>",2019-09-07T21:40:25Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0a2o1wg30y6rx,2019-09-07T21:40:25Z,{},hw1
1088,no,"<p>Wooow, I finally found it.</p>
<p> </p>
<p>Thank you <strong><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </strong>  !</p>
<p></p>",2019-09-07T22:18:01Z,42,Week 9/1 - 9/7,feedback,,gx3c8l7z7r72zl,k0a40fac407hk,2019-09-07T22:18:01Z,{},hw1
1089,no,<p>There you go!!! Congrats!</p>,2019-09-07T23:09:39Z,42,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k0a5ut6ijc428p,2019-09-07T23:09:39Z,{},hw1
1090,no,<p>This is a bit beyond my ability but if you can come up with a sketch of a proof I&#39;d be super curious to see it.</p>,2019-09-07T03:52:00Z,43,Week 9/1 - 9/7,followup,,jqmfuaidej9155,k090i24alg35f3,2019-09-07T03:52:00Z,{},hw1
1091,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj4sh1p7pf5af%2Fk093shg8t5yx%2FIMG_1805.jpeg"" alt="""" /></p>
<p></p>
<p>This is using the example for N = 6.</p>
<p></p>
<p>If this provides any insight, calculating the infinite series of just rolling, following the value iteration method, leaves you with the number 3. I don&#39;t think this has true value because there is never a terminal state. But it seems to validate the idea that rolling forever is worse than just rolling either a 4,5,6 and then cashing out.</p>
<p></p>
<p>(Wolfram alpha spit out 3, I hope it&#39;s right)</p>",2019-09-07T05:23:10Z,43,Week 9/1 - 9/7,feedback,,jzj4sh1p7pf5af,k093rb1upjpqs,2019-09-07T05:23:10Z,{},hw1
1092,no,"<p>Is the definition of best lambda, the value of lambda that makes TD(lambda)=TD(1) ?</p>
<p>If so, for the above network:</p>
<p>E1=5.5, E2=4, E3=4, E4=6, E5=5</p>
<p>assuming Lamda = 0.403032</p>
<p></p>
<table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""473"" height=""158""><tbody><tr height=""20""><td height=""20"" width=""64"">K</td><td width=""64"">E(K)</td><td width=""64"">(1-lambda)*power(lambda,K-1)* E(K)</td></tr><tr height=""20""><td height=""20"">1</td><td>5.5</td><td>3.283324</td></tr><tr height=""20""><td height=""20"">2</td><td>4</td><td>0.962389</td></tr><tr height=""20""><td height=""20"">3</td><td>4</td><td>0.387873</td></tr><tr height=""20""><td height=""20"">4</td><td>6</td><td>0.234488</td></tr><tr height=""20""><td height=""20"">5</td><td>5</td><td>0.078755</td></tr></tbody></table>
<p></p>
<p>The the sum of the last column =4.94683 for lambda=0.403032</p>
<p></p>
<p>However if lambda = 0.365, then </p>
<p></p>
<table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""485"" height=""158""><tbody><tr height=""20""><td height=""20"" width=""64"">K</td><td width=""64"">E(K)</td><td width=""64"">(1-lambda)*power(lambda,K-1)* E(K)</td></tr><tr height=""20""><td height=""20"">1</td><td>5.5</td><td>3.4925</td></tr><tr height=""20""><td height=""20"">2</td><td>4</td><td>0.9271</td></tr><tr height=""20""><td height=""20"">3</td><td>4</td><td>0.338392</td></tr><tr height=""20""><td height=""20"">4</td><td>6</td><td>0.185269</td></tr><tr height=""20""><td height=""20"">5</td><td>5</td><td>0.056353</td></tr></tbody></table>
<p></p>
<p>and the sum of the last column would be 4.999614 which is closer to TD(1) (value of 5).</p>
<p></p>
<p>shouldn&#39;t the best lambda be 0.365 and not 0.403032 ? am I doing something wrong?</p>",2019-09-10T15:53:02Z,62,Week 9/8 - 9/14,followup,,j6ll2xkiDJf,k0e0kvx88fi6x2,2019-09-10T15:53:02Z,{},hw2
1093,no,"<p>Yes, you are.  TD((λ) definition does not stop at E5... k goes to infinity. </p>
<p></p>",2019-09-10T15:59:48Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0e0tkq0w9t7lg,2019-09-10T15:59:48Z,{},hw2
1094,no,<p>Thanks. I am wondering if there is some code out there that computes E(K) for a given network? or  should we write this from scratch?</p>,2019-09-10T16:45:27Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0e2gahdohu769,2019-09-10T16:45:27Z,{},hw2
1095,no,"<p>I don&#39;t know but I can imagine how to do that without too many programming skills.</p>
<p>It&#39;s &#39;just&#39; following the trajectory, going from one state to the state it leads to.  </p>
<p></p>
<p>But I am pretty sure TD is used when there is no/partial model of the environment, so I think, in real life, they use TD to compute the estimates from what the environment throws back at the learning engine.  </p>
<p>We may get a totally different trajectory, at every episode.  </p>
<p>This HW was just to make us acquainted with how TD is computed from one episode to the next.  </p>",2019-09-10T17:10:24Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0e3cdbykxv3kt,2019-09-10T17:10:24Z,{},hw2
1096,no,"<p>&#64;Kamran I think the point is to do it yourself (from scratch), not necessarily by code though - could be by hand.</p>",2019-09-10T21:30:49Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ecn9yvsj011a,2019-09-10T21:30:49Z,{},hw2
1097,no,"<p>&#64;vahe, yes I think it is mean&#39;t to be done by hand. I am wondering if the valueEstimates are derived from MDP of the given network or they are randomly selected?</p>",2019-09-10T21:58:28Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0edmu6rokn1ou,2019-09-10T21:58:28Z,{},hw2
1098,no,<p>randomly selected</p>,2019-09-10T23:08:17Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eg4m6k67n37c,2019-09-10T23:08:17Z,{},hw2
1099,no,<p>Vahe is correct.</p>,2019-09-11T01:38:49Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0eli72q4vn1ru,2019-09-11T01:38:49Z,{},hw2
1100,no,"<p>How do we calculate E_inf when we only have 6 states? To make E_inf a little more manageable, let&#39;s say it&#39;s E<sub>10</sub>. In order to calculate E<sub>10</sub>, wouldn&#39;t I need r<sub>0</sub> to r<sub>10</sub>, and V<sub>11</sub>? How do we calculate values higher than E<sub>5</sub>, since we only have 6 states?</p>",2019-09-11T21:11:05Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0frdqp35va3bt,2019-09-11T21:11:05Z,{},hw2
1101,no,<p>I wrote this a little clearer as its own question in &#64;248</p>,2019-09-11T21:57:32Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0ft1ho4uqzxz,2019-09-11T21:57:32Z,{},hw2
1102,no,"<p>I ran the same algo that Karman did above on the first example in the HW2 sheet:</p>
<p>1st example:</p>
<p>p=0.81</p>
<p>v = [0.0,4.0,25.7,0.0,20.1,12.2,0.0]</p>
<p>r = [7.9,-5.1,2.5,-7.2,9.0,0.0,1.6]</p>
<p>the lambda with closest value to TD(1) = 16.687 was 0.5876</p>
<p>am I doing something wrong?</p>",2019-09-13T18:44:00Z,62,Week 9/8 - 9/14,feedback,,jl2842xr2jt3fe,k0ih0at6vfl7gq,2019-09-13T18:44:00Z,{},hw2
1103,no,"<p>I&#39;m having trouble calculating E<sub>k</sub> for any k.</p>
<p></p>
<p> <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk0e4jp94b3wc%2FScreen_Shot_20190910_at_1.43.51_PM.png"" alt="""" width=""401"" height=""82"" /></p>
<p>In this example, is V(s<sub>0</sub>) = 0, V(s<sub>1</sub>) = 3, V(s<sub>2</sub>) = 8, etc? I&#39;m confused about the notation around s, T, t, and k. I&#39;m not even sure where to begin plugging in numbers to this equation. </p>
<p></p>
<p>Also, is E<sub>1 </sub>equal to V(s<sub>t</sub>) ? The lecture only solves for V(s<sub>t</sub>), but here it looks like we need a value for E<sub>1</sub></p>
<p></p>",2019-09-10T17:48:51Z,62,Week 9/8 - 9/14,followup,,jl284xdcifz44g,k0e4ptvow2c34b,2019-09-10T17:48:51Z,{},hw2
1104,no,"<p>$$E$$ refers to an estimate of the state value function of an MDP using the TD method, by &#34;looking ahead&#34; a certain number of time steps.  The $$k$$ in $$E_k$$ refers to how many time steps we&#39;re looking ahead.  It equivalently refers to how many rewards we accumulate before we stop adding rewards and instead add the current estimate of the value of the state we have reached after that many time steps.</p>
<p></p>
<p>Looking at the equation you pasted above for $$E_1$$ (which, by the way, is also known as TD(0)), we can see that, indeed, there is only one reward ($$r_{t&#43;1}$$) being accumulated, after which we abruptly stop adding rewards and instead add the value of the state we have reached: $$V(S_{t&#43;1})$$.</p>
<p></p>
<p>A couple of quick questions for you:</p>
<p></p>
<p>1. What value of $$\gamma$$ are we supposed to use?</p>
<p>2. What value(s) of $$\alpha_T$$ should we use?</p>
<p></p>
<p>The reason I ask is, you may be able to considerably simplify the above equation by answering those two questions.</p>",2019-09-10T21:28:29Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eck9mkmwf7gz,2019-09-10T21:28:29Z,{},hw2
1105,no,"<p>We should be derive the value of αT right? because the value estimates for the states are already given</p>
<p></p>
<p></p>",2019-09-11T03:06:51Z,62,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0eonf03d9o28e,2019-09-11T03:06:51Z,{},hw2
1106,no,"We&#39;re using 1 for gamma and I&#39;ve been told the learning rate doesn&#39;t matter, but I&#39;m still confused about what E equals. The equation I listed just has E1 written on the side. We can compute V(st), but is that equal to E1? What equation do I need to solve that gives me a value for E1?",2019-09-11T03:55:02Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0eqddlyoz14ei,2019-09-11T03:55:02Z,{},hw2
1107,no,"<p>Yes, $$V(S_t)$$ is $$E_1$$ in the equation you pasted above, but if $$\gamma = 1$$ and $$\alpha = 1$$, that equation simplifies to (for general $$k$$)</p>
<p></p>
<p>$$E_k(S_t) = \sum\limits_{i=1}^kr_i &#43; V(S_{t&#43;k}) $$</p>
<p></p>
<p>We&#39;re summing up the next $$k$$ rewards in the path of the mdp under the given policy, then stopping and using a state value estimate to approximate the remaining rewards.</p>
<p></p>
<p>For $$E_1$$, we just let $$k=1$$ above and get $$E_1(S_t) = r_1 &#43; V(S_{t&#43;1})$$.</p>
<p></p>
<p></p>",2019-09-11T04:28:46Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0erkrebd324ut,2019-09-11T04:28:46Z,{},hw2
1108,no,<p>&#64;218</p>,2019-09-11T05:14:37Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0et7pphkv01hy,2019-09-11T05:14:37Z,{},hw2
1109,no,"<p>&#64;Vahe, when you write E<sub>k</sub>, you write it in terms of a state, like E<sub>k</sub>(S<sub>t</sub>). When the above example writes E<sub>k</sub>, it is not written in terms of a state. What states are the above E1, E2, E3, etc attached to?</p>",2019-09-11T20:50:29Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0fqn93uzj35vs,2019-09-11T20:50:29Z,{},hw2
1110,no,"<p>$$E_k$$ is estimating something, right?  What it&#39;s estimating is (roughly) the difference between the current value of a state and the $$k$$-step lookahead value of that state, which we call the target value.  So, by definition $$E_k$$ is a function of the set of states of an MDP (you need to pick a state for which to compute that difference).</p>
<p></p>
<p>In the case of the current value of the state being zero, as in the three examples in the homework, that difference is just the $$k$$-step lookahead value of that state $$V_{target} - 0 = V_{target}$$.</p>
<p></p>
<p>In the homework, all the $$E$$s are referring to the values of state $$0$$ of the provided MDP, so you could think of it as $$E_k(\text{State 0})$$.</p>",2019-09-11T21:23:40Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0frtxjrrv363u,2019-09-11T21:23:40Z,{},hw2
1111,no,<p>You should consider TAing.</p>,2019-09-12T01:37:16Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0g0w1w1btn3nw,2019-09-12T01:37:16Z,{},hw2
1112,no,<p>I straight up thought Vahe was a TA. His comments are very helpful. </p>,2019-09-12T03:08:47Z,62,Week 9/8 - 9/14,feedback,,jzkifxpxau95an,k0g45rbixa1tn,2019-09-12T03:08:47Z,{},hw2
1113,no,<p>Agreed...  </p>,2019-09-12T03:53:59Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g5rvskk33pe,2019-09-12T03:53:59Z,{},hw2
1114,no,"<p>&#64;Vahe, in the simplified equation you shared above.  Is the <strong><em>t</em></strong> in the <em>S<strong><sub>t </sub></strong> </em>the &#34;state&#34; for which we are estimating the value of?  I understand that <em>k</em> is the number of steps we&#39;re looking ahead.  But I&#39;m not certain how to interpret <em>t</em>.  I&#39;m thinking they are the state that we are estimating the value of with the E().  Also, based on the notation in the paper and textbook, I&#39;m thinking that <strong>S<sub>t</sub></strong> is the state we&#39;re estimating, and <strong>S<sub>T</sub></strong> is a terminal state.</p>
<p></p>
<p>So if we consider t = 0 , then we are estimating the value of State(0), and if t = 1 then we estimating the value of State(1)?</p>",2019-09-12T04:59:11Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g83ptau29uw,2019-09-12T04:59:11Z,{},hw2
1115,no,"<p>No, the $$t$$ references the temporal location of that state, i.e. $$t=1$$ is the <em>first</em> state to be visited, $$t=2$$ is the <em>second</em> state to be visited, etc.  That is completely independent of the state labels in your MDP.</p>
<p></p>
<p>Edit:  Depending on your convention, $$t=0$$ may be your first state.</p>",2019-09-12T05:03:25Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g895xo92d4n6,2019-09-12T05:03:25Z,{},hw2
1116,no,"<p>OK, what about when they have an  uppercase T in the subscript of S?<br /><br />What you said just above makes sense.  We&#39;re looking at trajectories, and that is the index of the state with respect to the trajectory.. got it.</p>",2019-09-12T05:04:50Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g8b00bled3b0,2019-09-12T05:04:50Z,{},hw2
1117,no,<p>Can you point me to where you&#39;re seeing it so I can see the context?</p>,2019-09-12T05:06:03Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g8ckbldrl6yg,2019-09-12T05:06:03Z,{},hw2
1118,no,"<p>Copied from Miguel&#39;s post in &#64;218 </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fis9so9huTMp%2Fk0g8jmvulyx9%2FCapture.PNG"" alt="""" /></p>",2019-09-12T05:11:40Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g8js1akcq26h,2019-09-12T05:11:40Z,{},hw2
1119,no,"<p>I found one place:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk0g8g9a2n66%2FnStepAlgo.PNG"" alt="""" /></p>
<p></p>
<p>If you look in the bowels of the algorithm, you&#39;ll see that T is just a constant that is set to the time at which we will be entering the terminal state.  In other words, $$T$$ is still an element of the time axis.  $$S_T$$ happens to be the terminal state because $$T$$ was set that way.</p>
<p></p>
<p>Edit, to reference the post you just sniped me with.  It&#39;s the same issue there.  $$T$$ is on the time axis, representing the <em>time</em> at which the episode ends.</p>",2019-09-12T05:14:44Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g8nq4fn8l4ay,2019-09-12T05:14:44Z,{},hw2
1120,no,<p>Oh wait I see it now... EDITED (a few times):  It&#39;s a subscript representing the end of the episode... You&#39;re right I see it now.</p>,2019-09-12T05:16:04Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g8pflsvg11mj,2019-09-12T05:16:04Z,{},hw2
1121,no,"<p>Yeah, it&#39;s definitely a subscript of $$S$$.</p>
<p></p>
<p>These notations can get confusing, especially when variables are overloaded.</p>",2019-09-12T05:16:49Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g8qee7g7b7k6,2019-09-12T05:16:49Z,{},hw2
1122,no,<p>Thanks for the guidance here.  I fumbled around a bit on that t and T... I think I got it now.</p>,2019-09-12T05:23:09Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g8yjeb82d6eo,2019-09-12T05:23:09Z,{},hw2
1123,no,"<p>Yeah, I tried to be very explicit. Not always a good thing, but you guys are at a stage in which it should help. Otherwise, just feel free to remove all of the subscripts and you should still follow. But, yeah, $$T$$ is a pretty fundamental one: The final time step of an episode. $$t, t&#43;1, ..., T-1, T$$. It&#39;s everywhere so become familiar with it.</p>
<p></p>",2019-09-12T10:20:16Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0gjkn47yl6200,2019-09-12T10:20:16Z,{},hw2
1124,no,<p>I will etch this in my memory.  The neurons that were storing images of Mr. T have now been replaced by this subscript notation.</p>,2019-09-13T06:02:33Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0hpt24gbv86mz,2019-09-13T06:02:33Z,{},hw2
1125,no,"<p>So having been thoroughly overwhelmed by the hundreds of different equations all over piazza, I figured I would try to just start simple and calculate the E values in the example above using the equation Vahe mentioned. If Ek is truly calculated in this way, and with the given numbers for valueEstimates and rewards, how on earth does E1 ever equal 5.5? I definitely know adding integers will never give me a non-integer of 5.5. What am I missing? Do I need to be doing something to the valueEstimates above? I figured I&#39;d set t=0 for my subscripts here, but even then I&#39;m not sure why. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2bq5rf8b67pq%2Fk0inqml6e27i%2Fek.jpg"" alt="""" /></p>",2019-09-13T21:58:14Z,62,Week 9/8 - 9/14,feedback,,jl2bq5rf8b67pq,k0iny332ig2va,2019-09-13T21:58:14Z,{},hw2
1126,no,"<p>Keep in mind that $$E_k$$ is a random variable.  So if the MDP is stochastic, we need to average over all possible transitions...</p>",2019-09-13T22:07:14Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0io9nvroa4j8,2019-09-13T22:07:14Z,{},hw2
1127,no,"<p>So it&#39;s a k-step look ahead. So if k=1 and we&#39;re starting from state 0, we have .5 chance of getting 3 (in state 1) and a .5 chance of getting 8 (in state 2), so .5(3)&#43;.5(8)=5.5? But I&#39;m not convinced on that, if k=2, we either go 0-&gt;1-&gt;3 or 0-&gt;2-&gt;3, so .5(3&#43;2)&#43;.5(8&#43;2) which is certainly not 4.</p>",2019-09-13T22:13:44Z,62,Week 9/8 - 9/14,feedback,,jl2bq5rf8b67pq,k0ioi0o2qjs7fy,2019-09-13T22:13:44Z,{},hw2
1128,no,"<p>When you look ahead two steps, what are you actually summing in those two steps?</p>
<p></p>
<p>This might help (i.e. your hand-written equations may not be correct):</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk0ior3gn4uc6%2FCapture5.PNG"" alt="""" /></p>",2019-09-13T22:15:35Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0iokef6zryk6,2019-09-13T22:15:35Z,{},hw2
1129,no,"<p>The value of each state we visit was my initial thought? But that doesn&#39;t really factor in the rewards for the transitions themselves... I guess we get 4 for k=2 because if we go 0-&gt;1-&gt;3 there&#39;s a 100% chance of getting a 2, and if we go 0-&gt;2-&gt;3 there&#39;s a 100% chance of getting a 2, so 1(2)&#43;1(2)=4? I still am not sure I&#39;m thinking about this correctly. Because once we reach state 3 the reward is no longer 0, so there&#39;s got to be some extra steps I don&#39;t understand yet.</p>",2019-09-13T22:20:52Z,62,Week 9/8 - 9/14,feedback,,jl2bq5rf8b67pq,k0ior6x4x7953s,2019-09-13T22:20:52Z,{},hw2
1130,no,"<p>Look carefully at the equation I posted above for $$E_2$$ and remember that $$\gamma = 1$$ as per the homework instructions, and also that since we&#39;re not learning, we should be setting $$\alpha_T$$ to $$1$$.  Then simplify and compare to your handwritten equation.</p>",2019-09-13T22:24:30Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0iovv0jwsg139,2019-09-13T22:24:30Z,{},hw2
1131,no,<p>Ah!!! The summation only applies to r! I was reading your sigma notation to include the V(S) in the summation. I&#39;m getting the right values now!</p>,2019-09-13T22:35:50Z,62,Week 9/8 - 9/14,feedback,,jl2bq5rf8b67pq,k0ipafu3qos4nu,2019-09-13T22:35:50Z,{},hw2
1132,no,<p>Great!</p>,2019-09-13T22:38:25Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ipdrkc65t70p,2019-09-13T22:38:25Z,{},hw2
1133,no,<p>Nicely done.</p>,2019-09-14T02:03:23Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0iwpcu8isp132,2019-09-14T02:03:23Z,{},hw2
1134,no,"<p>is the lambda computed as part of HW2, the lambda that produces the least error as described in Sutton&#39;s paper? My guess is that the answer is &#34;yes&#34; but I am not sure. Does any one know?</p>",2019-09-10T23:39:58Z,62,Week 9/8 - 9/14,followup,,j6ll2xkiDJf,k0eh9cnqobl26k,2019-09-10T23:39:58Z,{},hw2
1135,no,"<p>Actually, I think the answer is &#34;no.&#34;</p>
<p></p>
<p>From the HW2 problem statement:</p>
<p></p>
<p>&#34;Find a value of $$\lambda$$, strictly less than $$1$$, such that the TD estimate for $$\lambda$$ equals that of the $$TD(1)$$ estimate.&#34;</p>
<p></p>
<p>There&#39;s nothing there about minimizing error.</p>",2019-09-10T23:51:27Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eho4k3jyi592,2019-09-10T23:51:27Z,{},hw2
1136,no,<p>Yeah you want to compute the $$\lambda $$ such that $$TD(\lambda) = TD(1)$$</p>,2019-09-11T00:03:11Z,62,Week 9/8 - 9/14,feedback,,hz7meu55mi8sd,k0ei37kh23u1b8,2019-09-11T00:03:11Z,{},hw2
1137,no,<p>OK. But what is the significance of this lambda computation? why should I care about it?</p>,2019-09-11T00:31:09Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0ej36q0ql71rz,2019-09-11T00:31:09Z,{},hw2
1138,no,"<p>Good question.  I found it interesting that there exists a non-trivial linear combination of $$k$$-step estimators that provides the same state value estimate as the TD(1) estimate.  But beyond that, I don&#39;t know either.</p>",2019-09-11T01:35:39Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ele4eegvh5la,2019-09-11T01:35:39Z,{},hw2
1139,no,"<p>I think people have already figured this out, but I haven&#39;t.  From the lectures it seems like:</p>
<p>$$E_{\infty} : TD(1) = r_{1} &#43; \gamma r_{2} &#43; \gamma^{2}r_{3} &#43; ... - V_{t-1}(s_{0}) $$</p>
<p></p>
<p>However, using the above MDP, is the below true?  Since I feel (intuition, rather) we need to somehow encapsulate both transitions to calculate the TD error.<br />$$r_{1} = prob(s_{1}) r_{0} &#43; (1-prob(s_{1})) r_{1} $$</p>",2019-09-11T00:52:09Z,62,Week 9/8 - 9/14,followup,,jl8j7vzvUNs2,k0eju6nd4eh7m,2019-09-11T00:52:09Z,{},hw2
1140,no,"<p>TD(1) <strong>does not</strong> bootstrap.</p>
<p></p>
<p>Your agent is <strong>not learning</strong>.</p>
<p></p>
<p>Your intuition is correct.</p>",2019-09-11T01:35:48Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0elebx7eg35qn,2019-09-11T01:35:48Z,{},hw2
1141,no,"<p>Sriram,</p>
<p></p>
<p>I think part of your confusion is that the $$E$$s in the homework are estimators, not errors, so there&#39;s no subtraction of state values at the end of the return summation.</p>",2019-09-11T01:51:17Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ely8rdhf6ia,2019-09-11T01:51:17Z,{},hw2
1142,no,"<p>Interesting and lucky I guess.  When I jotted the above on paper, I was getting $$E_{i}  \forall i $$ correct, since I had put a 0 for $$V_{t-1}(s_{0})$$</p>
<p>And yes, it makes sense when I treat $$E$$ are estimator instead of error.</p>
<p>Thanks &#64;Vahe, <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>",2019-09-11T02:20:47Z,62,Week 9/8 - 9/14,feedback,,jl8j7vzvUNs2,k0en065oksk3mu,2019-09-11T02:20:47Z,{},hw2
1143,no,"<p>Yeah, some weak TA made that change so that coincidentally it worked either way... I&#39;ll make sure to re-fix it next semester :)</p>",2019-09-11T02:47:28Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0enyhgm6hd13d,2019-09-11T02:47:28Z,{},hw2
1144,no,"<p>If I may also suggest to have a non-zero value for V(6).  I missed a term, considering E(6) due to that.</p>",2019-09-11T03:41:45Z,62,Week 9/8 - 9/14,feedback,,jl8j7vzvUNs2,k0epwasckvl10e,2019-09-11T03:41:45Z,{},hw2
1145,no,<p>Will do.</p>,2019-09-11T05:15:29Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0et8uf5jgi23p,2019-09-11T05:15:29Z,{},hw2
1146,no,"<p>I dropped some extra hints <a href=""/class/jzh9tkzzxkd7ph?cid=218"">&#64;218</a>.</p>",2019-09-11T05:17:00Z,62,Week 9/8 - 9/14,followup,,hyx9thiqa6j4nn,k0etas19gla430,2019-09-11T05:17:00Z,{},hw2
1147,no,"<p>How did we get this value for example above ?</p>
<p>TD(0)=5.5  and TD(1)=  5 ?</p>
<p>Which equation to use here?</p>
<p></p>",2019-09-12T02:16:20Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0g2aayv9426p,2019-09-12T02:16:20Z,{},hw2
1148,no,<p>How did we get E1 = 5.5 using 1 step estimator ?</p>,2019-09-12T02:46:04Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0g3cj2qb6n9j,2019-09-12T02:46:04Z,{},hw2
1149,no,"<p>Using below </p>
<p>E1(St)=r1&#43;V(St&#43;1)</p>
<p>Prob to state 1 = 0.5, prob state 2 = 0.5</p>
<p>Reward to state 1= 0, reward to state 2 = 0</p>
<p>Value of state 1(Vst&#43;1) = 1</p>
<p>$$\gamma $$ = 1</p>
<p>E1 should be .5*(0 &#43; 1*1) &#43; 0.5 (0&#43; 1*2)</p>
<p></p>
<p>Can some one explain how to calculate E1 here?</p>
<p></p>",2019-09-12T02:50:06Z,62,Week 9/8 - 9/14,feedback,,jc6s23mr74r45q,k0g3hpuw4kr3q0,2019-09-12T02:50:06Z,{},hw2
1150,no,<p>Where are you getting the values for State 1 and State 2?</p>,2019-09-12T02:51:28Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g3jh1u55h25o,2019-09-12T02:51:28Z,{},hw2
1151,no,"<p>I used the following first equation for E1,E2,E3,E4  and the last equation for E5</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk0fspzyljih0%2FScreen_Shot_20190911_at_5.46.22_PM.png"" alt="""" width=""460"" height=""186"" /></p>
<p>and the following last eqn for T(lambda)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fis9so9huTMp%2Fk0g8jmvulyx9%2FCapture.PNG"" alt="""" width=""424"" height=""153"" /></p>
<p>i could get T(1)= T(0.403032) for the example provided in piazza and close enough value for HW2, 1st example. and thought that i have got the right understanding</p>
<p>but for HW2, 2nd example, i cant get the same value  T(1) = 11.2259 and T(0.4956)=10.850</p>
<p>I am wondering if i have missed out something.</p>
<p></p>",2019-09-13T10:46:49Z,62,Week 9/8 - 9/14,followup,,k012haa329t4gp,k0hzymzkci4wk,2019-09-13T10:46:49Z,{},hw2
1152,no,<p>Is there anything different about example 2 compared to example 1 that may be causing this?</p>,2019-09-13T13:40:37Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0i665imfeu2qz,2019-09-13T13:40:37Z,{},hw2
1153,no,"<p>oh i realized that T should be 6 not 5, so the T-1 step estimator should be G(t:5) = R(t&#43;1)  &#43;R(t&#43;2) ... R(t&#43;5)&#43;V(S(t&#43;5))</p>
<p>and the final return G(t:6) should be R(t&#43;1)  &#43;R(t&#43;2) ... R(t&#43;5)!</p>
<p></p>
<p>managed to get the all the examples right after the correction.</p>
<p>Thanks Vahe!</p>",2019-09-13T16:21:00Z,62,Week 9/8 - 9/14,feedback,,k012haa329t4gp,k0ibwef8g1j790,2019-09-13T16:21:00Z,{},hw2
1154,no,"<p>Super nice, man... wow!</p>",2019-09-14T02:03:51Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0iwpye76ae1fp,2019-09-14T02:03:51Z,{},hw2
1155,no,"<p>I kind of don&#39;t understand one thing about this, which is E(inf). </p>
<p>Since the states stop at 5, do we just stop at 5? How would we keep going otherwise? I get the following fine. <br /><br /></p>
<p>td_1 is: 5.0</p>
<p>e1: 5.5</p>
<p>e2: 4.0</p>
<p>e3: 4.0</p>
<p>e4: 6.0</p>
<p>e5: 5.0</p>",2019-09-14T03:20:55Z,62,Week 9/8 - 9/14,followup,,jqr9leosvw8P,k0izh2b5407350,2019-09-14T03:20:55Z,{},hw2
1156,no,"<p>I found an easy way to look at this, this way: Imagine that there is an infinite amount of states after S6, all with a zero reward and a zero value.  </p>
<p>It would be the same system no? </p>
<p>What would happen to those states when you plug them in $$TD(\lambda)$$? </p>",2019-09-14T04:19:53Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0j1kwdq53s6dp,2019-09-14T04:19:53Z,{},hw2
1157,no,"<p>I am able to calculate E1 to E5</p>
<p>But how do I calculate TD(1) since I dont know the values up to infinity</p>
<p></p>",2019-09-14T03:44:21Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0j0b6sp415uh,2019-09-14T03:44:21Z,{},hw2
1158,no,"<p>Imagine that there is an infinite amount of states after S6, all with a zero reward and zero values.  </p>
<p>It would be the same system no?</p>",2019-09-14T04:17:33Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0j1hvmwopr2t4,2019-09-14T04:17:33Z,{},hw2
1159,no,<p>^^^^ This!</p>,2019-09-14T04:47:32Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0j2kftjdko1w9,2019-09-14T04:47:32Z,{},hw2
1160,no,"<p>What is E6? Since 5 step look ahead is already at final state 6. I think E6 should be just the same to E5 since the there&#39;s no reward repeating the final state and the value for final state is the same for all E5, E6,... E(inf). </p>",2019-09-14T05:45:06Z,62,Week 9/8 - 9/14,feedback,,j6ln9puq99s5uv,k0j4mh3q29rnd,2019-09-14T05:45:06Z,{},hw2
1161,no,"<p>Remember how the value of a state is defined....sum of future (discounted) rewards.</p>
<p></p>
<p>You have no control over what the given states of the MDP were initialized to.  But you have a lot of control over estimates of the states that weren&#39;t given to us, and lie in the future.  Would the best estimates of their values require you to look forward, or look backward to a previous state?</p>",2019-09-14T06:48:14Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0j6vnpx35f2i6,2019-09-14T06:48:14Z,{},hw2
1162,no,"<p>So if E6 is calculated up to 6 steps ahead which means it has to land on virtual &#34;state 7&#34; which has no reward and VS(7) = 0</p>
<p>then it is always that E6 =E5?</p>
<p></p>
<p>So a self loop State 6 with reward 0?, but how about the value function for that state ?</p>
<p></p>
<p>Still not clear how E6 is calculated</p>",2019-09-14T20:02:28Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0jz927mljw67e,2019-09-14T20:02:28Z,{},hw2
1163,no,"<p>Close but not quite, E5 has the value estimate for E6 built into it. So like you&#39;re saying, you have to use V(7). I made the same mistake at first thinking E6 = E5 always. It only works for the first example. </p>",2019-09-14T20:12:01Z,62,Week 9/8 - 9/14,feedback,,jzkifxpxau95an,k0jzlc0iyoe2xh,2019-09-14T20:12:01Z,{},hw2
1164,no,"<p>Is it okay to have other code in the one made for us? (i.e hw1, hw2, etc.) I was planning on having them in separate folders.</p>",2019-09-06T16:28:36Z,63,Week 9/1 - 9/7,followup,,jzhdgaq99za3um,k08c37j89n97kl,2019-09-06T16:28:36Z,{},project1
1165,no,<p>yeah that is perfectly fine.  You can keep it all in there</p>,2019-09-06T18:27:20Z,63,Week 9/1 - 9/7,feedback,,hz7meu55mi8sd,k08gbwsxje31cy,2019-09-06T18:27:20Z,{},project1
1166,no,"<p>There is a way to calculate the upper bound exactly, but this is a good approach, too.</p>
<p></p>",2019-09-06T14:30:13Z,43,Week 9/1 - 9/7,followup,,hyx9thiqa6j4nn,k087uyjj2oe3p8,2019-09-06T14:30:13Z,{},hw1
1167,stud,"<p>Hi Miguel,</p>
<p></p>
<p>Could you explain the approach for calculating the upper bound exactly?</p>
<p>Many thanks in advance</p>",2019-09-07T10:48:42Z,43,Week 9/1 - 9/7,feedback,a_0,,k09fdxyjhqm3s8,2019-09-07T10:48:42Z,{},hw1
1168,no,"<p>I think the approach for a exact upper bound would be to look at the expected return. Rolling the dice is a good decision if we expect an improvement in the total reward. So, we can calculate that via the chance we lose all of our current reward and then the chance of each different reward from rolling the dices. If we want a maximum number of rolls, then we can be conservative and consider the case where we got the smallest possible improvement from each of our successful die rolls. Given that assumption, we rearrange for the maximum number of rolls where it could still be a good idea to roll the dice.</p>",2019-09-08T04:17:41Z,42,Week 9/8 - 9/14,feedback,,jqstjkc1dh1602,k0aguyaznzd59k,2019-09-08T04:17:41Z,{},hw1
1169,no,<p>Correct.</p>,2019-09-08T20:22:06Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bfb6poqjj6z5,2019-09-08T20:22:06Z,{},hw1
1170,no,<p>there are issues depending on the state space where your program can go wonky.  (time increases nonlinearly).</p>,2019-09-09T16:58:19Z,42,Week 9/8 - 9/14,followup,,jzivtxcbl6964n,k0cngzoohgd3mf,2019-09-09T16:58:19Z,{},hw1
1171,no,<p>Thanks for the confirmation!</p>,2019-09-08T02:52:58Z,45,Week 9/1 - 9/7,followup,,jzj4205g7gd2fw,k0adtzyy4q94x2,2019-09-08T02:52:58Z,{},project1
1172,no,<p>I am not clear on where to get the training data and test data for this project? Do we simulate it by generating random left/right decisions?</p>,2019-09-12T19:51:20Z,44,Week 9/8 - 9/14,followup,,jl2egn5k4zo4lp,k0h3z1an6f3ao,2019-09-12T19:51:20Z,{},project1
1173,no,<p>Yes</p>,2019-09-12T21:02:19Z,44,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0h6ibmb20b1bu,2019-09-12T21:02:19Z,{},project1
1174,no,"<p>If we simulate randomly we could get a sequence like:<br />$$ D -&gt; C -&gt; D -&gt; E -&gt; F -&gt; G $$ - which I think is valid, since it terminates in 10 steps.  Please correct if wrong.</p>
<p></p>
<p>$$ D -&gt; C -&gt; D -&gt; E  -&gt; D -&gt; C -&gt; D -&gt; E -&gt; F -&gt; E -&gt;D ....$$  which may not end.  Do we just discard such training sequences</p>",2019-09-18T01:12:02Z,43,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0okmpysbzj4u,2019-09-18T01:12:02Z,{},project1
1175,no,<p>I am also confused about the above as well</p>,2019-09-18T06:03:47Z,43,Week 9/15 - 9/21,feedback,,jc6mqevhagl262,k0ov1wk4oux2w7,2019-09-18T06:03:47Z,{},project1
1176,no,"I was confused at the first but sorted that out now. No matter what the length of the sequence, it should enss in either A or G, then it&#39;s valid. The 10 limit is the number of sequence in a training set not the steps in a sequence. I training set should contain 10 sequences where each sequence could be DCBA, DEFG, DEFEDCBCDEFG, or any sequences that follows random walk rule.",2019-09-18T21:53:18Z,43,Week 9/15 - 9/21,feedback,,jzj4205g7gd2fw,k0psz02oxr2dg,2019-09-18T21:53:18Z,{},project1
1177,no,<p>Thanks for the confirmation Ava.</p>,2019-09-18T22:50:07Z,43,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0pv022r3se1ki,2019-09-18T22:50:07Z,{},project1
1178,no,"<p>Can we use some other code for this? Like, if we want to use things available to us, can we just cite them? I don&#39;t want to reinvent the wheel on a doubly linked list, for example. </p>",2019-09-22T15:45:49Z,42,Week 9/22 - 9/28,feedback,,jqr9leosvw8P,k0v5ltoy6mb7en,2019-09-22T15:45:49Z,{},project1
1179,stud,"yeah the office hours didn&#39;t help, it seemed like everyone already knew what was going on whereas i&#39;m still trying to understand how to get started",2019-09-07T16:04:47Z,43,Week 9/1 - 9/7,followup,a_0,,k09qofhp5fg6rn,2019-09-07T16:04:47Z,{},hw1
1180,no,<p>If you have specific questions feel free to ask them.</p>,2019-09-07T18:33:03Z,43,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k09vz41h6v42n3,2019-09-07T18:33:03Z,{},hw1
1181,no,"<p>I feel the same way. I think I currently do not understand how to calculate future states. </p>
<p><i></i></p>
<p><i>V<sub>k</sub>[s] = max<sub>a</sub> ∑<sub>s&#39;</sub> P(s&#39;|s,a) (R(s,a,s&#39;)&#43; γV<sub>k-1</sub>[s&#39;])</i></p>
<p><i></i></p>
<p>For the above function, how do you know what s&#39; to calculate for? Am I understanding that we always assume s&#39; is the value with the maximum return? So if we have the six sided die, we say that s&#39; is rolling 6 every time?</p>",2019-09-07T19:29:09Z,43,Week 9/1 - 9/7,followup,,jqmjg3txqw6ji,k09xz8qfvtr3fo,2019-09-07T19:29:09Z,{},hw1
1182,no,"<p>In theory, s&#39; could be any state in your space set. However, states unreachable from s have a zero probability of being reached, so you do not need to consider those. Each action a can take you to one of several states s&#39; with probability P(s&#39;|s, a).</p>
<p></p>
<p>The value of state s is not the value of the successor s&#39; with maximum return, because you do not always get there. Instead, the value of s is that of the action that gives you the maximum combined value over its possible successor states, weighted by the probability of reaching each of them.</p>
<p></p>
<p>To answer your question, let me use a simpler example of a 4-sided die. I get the choice of getting 2 dollars if I don&#39;t roll it, or an amount equal to each side if I do. What should I do? If I don&#39;t roll, I get 2 dollars with probability 1. If I do roll, there is .25 chance I get 1, .25 I get 2, .25 I get 3 and .25 I get 4. Weighting each reward by its probability tells me that rolling has a value of .25*1&#43;.25*2&#43;.25*3&#43;.25*4 = .25(1&#43;2&#43;3&#43;4) = 2.5. The action that gives me the highest expected reward is rolling, and I do so. In the initial state before rolling, I can expect to get a reward of 2.5 from playing this little game, and my policy is to roll the die.</p>",2019-09-07T21:32:31Z,43,Week 9/1 - 9/7,feedback,,is8ald0uljj3u4,k0a2dwd9ohq6hd,2019-09-07T21:32:31Z,{},hw1
1183,no,<p>Thank you so much! That example really put things in perspective.</p>,2019-09-08T15:48:53Z,42,Week 9/8 - 9/14,feedback,,jqmjg3txqw6ji,k0b5jtxgy6f5yy,2019-09-08T15:48:53Z,{},hw1
1184,no,"<p>If you haven&#39;t taken the Machine Learning course I can see how this assignment could be hard to start. You&#39;re being asked to model a Markov Decision Process, that is, define states, actions, transition probabilities and rewards. The homework description hints at looking at the changing bankroll as part of your state. What else can change? The actions at any time are clear, roll the die or quit. You have the option to reward each time you roll or only at the end of the game.</p>
<p></p>
<p>Once you have your model, you can perform value iteration on it to calculate what is the expected reward from each state on. Hope this few notes help you get started.</p>",2019-09-07T21:14:01Z,43,Week 9/1 - 9/7,followup,,is8ald0uljj3u4,k0a1q3nz83r46t,2019-09-07T21:14:01Z,{},hw1
1185,stud,"<p>Thanks, that&#39;s good to know! Feel free to make this public if you think it may help someone else.</p>",2019-09-07T03:04:14Z,45,Week 9/1 - 9/7,followup,a_0,,k08ysn0t3894x8,2019-09-07T03:04:14Z,{},project1
1186,no,<p>Good deal.</p>,2019-09-07T03:05:46Z,45,Week 9/1 - 9/7,feedback,,hyx9thiqa6j4nn,k08yulzczs97b7,2019-09-07T03:05:46Z,{},project1
1187,no,<p>Thank you very much for making this post public.</p>,2019-09-07T03:18:03Z,45,Week 9/1 - 9/7,followup,,jqkxzdmmolGf,k08zaeha2ik3i7,2019-09-07T03:18:03Z,{},project1
1188,no,<p>Can some one share the V values for N=21 case ?</p>,2019-09-07T03:30:37Z,43,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k08zqkl2jwqpw,2019-09-07T03:30:37Z,{},hw1
1189,no,"<p>Here&#39;s the expected values from mdptoolbox:</p>
<p>(7.379980563654033, 7.757801533311737, 8.144260878954757, 8.551884245761796, 8.965986394557822, 9.39909297052154, 9.836734693877549, 10.297052154195011, 10.780045351473923, 11.285714285714285, 11.80952380952381, 12.333333333333332, 12.857142857142858, 13.38095238095238, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 0.0, 0.0)</p>",2019-09-07T17:20:59Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k09tefmxsai3y2,2019-09-07T17:20:59Z,{},hw1
1190,no,"<p>I am seeing the states for N= 21 case as below , is this right?</p>
<p> [&#39;start&#39;, &#39;bad&#39;, 5, 6, 7, 8, 10, 12, 15, 17, 18, 19, 21, 11, 13, 20, 22, 23, 24, 26, 14, 16, 25, 27, 28, 29, 31, 30, 33, 32, 34, 36, 35, 38, 37, 39, 40, 42, &#39;end&#39;]</p>
<p></p>
<p>I am seeing 63 V value in this answer above, is there a state for 63 which can be derived considering a T =2 roll ?</p>
<p></p>
<p>My V values comes to below and no 63 in this</p>
<p> [ 7.37047835 0. 9.31746032 9.75510204 10.27891156 10.76190476   11.80952381 12.85714286 15. 17. 18. 19. 21. 11. 13. 20. 22. 23. 24. 26. 14. 16. 25. 27. 28. 29. 31. 30. 33. 32. 34. 36. 35. 38. 37. 39.  40. 42. 0. ]</p>
<p></p>",2019-09-07T20:36:59Z,43,Week 9/1 - 9/7,feedback,,jc6s23mr74r45q,k0a0eh4lhx36o3,2019-09-07T20:36:59Z,{},hw1
1191,no,"<p>Well, maybe you need more states then ... to simulate one more roll.  </p>
<p></p>
<p></p>",2019-09-07T22:04:53Z,43,Week 9/1 - 9/7,feedback,,jzh6k6o994a6dh,k0a3jiiyp9v1e0,2019-09-07T22:04:53Z,{},hw1
1192,no,"<p>So how do we define that, some cases work for rolls = 2 and some scenarios need more rolls ?</p>",2019-09-07T23:31:38Z,43,Week 9/1 - 9/7,feedback,,jc6s23mr74r45q,k0a6n3gb111176,2019-09-07T23:31:38Z,{},hw1
1193,no,<p>Also if I define  rolls = 3 then the max possible state (value)  would be 88 so max V value for state would be 88. </p>,2019-09-07T23:38:03Z,43,Week 9/1 - 9/7,feedback,,jc6s23mr74r45q,k0a6vcc1jd63hq,2019-09-07T23:38:03Z,{},hw1
1194,no,"<p>&#64;anu... if you want 3 rolls, I&#39;m pretty sure the max possible bankroll (state) is 3 * 21... </p>
<p></p>
<p>You do not define that &#39;some cases work for 2 rolls, and some scenarios need more&#39;... this is just an exercise... if the VI iteration converges, the algo stops and you get your value... if it doesn&#39;t converge, I guess you add more states to allow more episodes.  </p>
<p></p>
<p>I don&#39;t know what you guys are doing, but after I defined the P and R matrices, I tried with 2 rolls, and I got the EXACTLY the same values, all of them, down to the last decimal.  I just re-did it to be sure.  </p>
<p></p>",2019-09-08T00:37:02Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0a8z6p1f1y19e,2019-09-08T00:37:02Z,{},hw1
1195,no,"<p>I&#39;m having the same problem, my solution works for the other two cases, and for N=21 I get 7.3605. I tried messing with my convergence definition and the total number of states I&#39;m using, neither had any effect. </p>",2019-09-07T15:29:52Z,43,Week 9/1 - 9/7,followup,,jl2bq5rf8b67pq,k09pfj9e9ievi,2019-09-07T15:29:52Z,{},hw1
1196,no,"<p>If your P and R matrices are good, maybe you want to use mdptoolbox to get the calculations done for you and submit on time.  </p>
<p>To the very least, that would give you something to compare with.  </p>",2019-09-08T00:38:37Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0a9182p3as2ke,2019-09-08T00:38:37Z,{},hw1
1197,no,"<p>The values from mdp is matching with manual calculation, so it might be that P and R itself has some issue</p>",2019-09-08T03:11:38Z,42,Week 9/8 - 9/14,feedback,,jc6s23mr74r45q,k0aei0fcaoy636,2019-09-08T03:11:38Z,{},hw1
1198,no,"<p>I was facing the same problem with N =22 test case in the hw file, while getting correct answers for the other two. I would suggest printing out each transition matrix ( if using mdptoolbox) , I had a problem with the probabilities calculations.</p>",2019-09-08T05:15:23Z,42,Week 9/8 - 9/14,feedback,,jc5n7oq5zop3e8,k0aix550odj7da,2019-09-08T05:15:23Z,{},hw1
1199,no,"<p>You guys may want to look at &#64;164.  There&#39;s an interesting graph there, showing how VI could give you the impression that it has converged when it hasn&#39;t.</p>",2019-09-08T10:34:02Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0auaxf6jgn2g6,2019-09-08T10:34:02Z,{},hw1
1200,stud,"<p>How do you know what values to put in for mdptoolbox? I keep getting stochastic errs. </p>
<p>For the example problem, I was trying with the below. Is that even close to right? I feel like I am closer with my own python code, but I keep coming back to this because I feel like I *should* be able to more easily do it this way.  <br /><br /></p>
<p>prob = np.zeros((2, 6, 6))</p>
<p>prob[0] = [[0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667]]</p>
<p>prob[1] = [[0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667],<br /> [0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667, 0.1666666667]]</p>
<p>rewards = np.zeros((6, 2))<br />rewards[0] = [0., 0.] #0<br />rewards[1] = [4.5, 4] #4<br />rewards[2] = [5., 5.] #5<br />rewards[3] = [5.5, 6.] #6<br />rewards[4] = [8, 0] #8<br />rewards[5] = [9, 0] #9</p>",2019-09-09T05:36:56Z,42,Week 9/8 - 9/14,feedback,a_0,,k0bz4q3dvr66j6,2019-09-09T05:36:56Z,{},hw1
1201,no,"<p>You get stochastics errors because the sum of the probabilities for every line is not 1.</p>
<p></p>
<p>I don&#39;t know what you&#39;re trying to do but this doesn&#39;t look like a prob matrix for this problem.  </p>
<p>Your states don&#39;t seem to reflect the bankroll... see other posts for hints.</p>",2019-09-09T13:49:57Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0cgqqghu791xa,2019-09-09T13:49:57Z,{},hw1
1202,no,"<p>Thanks, I&#39;ll give that method a try. Anything is better than what I am currently doing. I&#39;ve reread this paragraph five times and struggle to understand what it actually says: <br /><br /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixpwxv7xdgi1u6%2Fk09tzh0vvkg6%2FScreenshot_from_20190907_133629.png"" alt="""" /></p>",2019-09-07T17:37:31Z,41,Week 9/1 - 9/7,followup,,ixpwxv7xdgi1u6,k09tzouzek73a9,2019-09-07T17:37:31Z,{},hw1
1203,no,"<p>Please anyone correct me if I&#39;m wrong, but I believe in this paragraph Sutton formally defines meanings of variables for describing <a href=""https://en.wikipedia.org/wiki/Absorbing_Markov_chain"" target=""_blank"">absorbing Markov chain</a> that he uses later in this chapter for his proofs. You&#39;ve skipped beginning of the definitions (prev paragraph, so it&#39;s hard to make sense reading just one this piece).</p>",2019-09-07T21:28:21Z,41,Week 9/1 - 9/7,feedback,,jqkxzdmmolGf,k0a28jdtkjk6bs,2019-09-07T21:28:21Z,{},hw1
1204,no,"<p>It does not help that it has grammar errors (each numerical vector is chosen dependent only <strong>on</strong> the corresponding....) and that the writing is dry and repetitive. Proof reading is scarce and harder than writing, just like code reviewing.</p>",2019-09-08T05:19:01Z,41,Week 9/1 - 9/7,feedback,,is8ald0uljj3u4,k0aj1thyf173h1,2019-09-08T05:19:01Z,{},hw1
1205,no,"<p>You will never understand a paper on the first read. My advice is you read it and pretend you understood. Keep moving forward then come back and read it again in a few days. Then again in a few weeks. Every time you read it, you&#39;ll be ready to understand more and more.</p>
<p></p>
<p>Don&#39;t spend too much time trying to understand everything on a paper. Just read, pretend, and come back to it. But you gotta start somewhere...</p>",2019-09-07T21:44:25Z,41,Week 9/1 - 9/7,followup,,hyx9thiqa6j4nn,k0a2t7eksf15kv,2019-09-07T21:44:25Z,{},hw1
1206,no,You mean bootstrapping? &#x1f609;,2019-09-13T12:55:09Z,40,Week 9/8 - 9/14,feedback,,i4jbttw9ru63ot,k0i4jo2o1ij6c8,2019-09-13T12:55:09Z,{},hw1
1207,no,<p>Ahhhh... I like this one!!! I&#39;m stealing this one!</p>,2019-09-14T02:24:55Z,40,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ixh1lhgbv4yb,2019-09-14T02:24:55Z,{},hw1
1208,no,"<p>I wish we had few weeks,,,,,,Time is precious!</p>
<p></p>",2019-09-15T22:50:13Z,39,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lkomugos653,2019-09-15T22:50:13Z,{},hw1
1209,no,"<p>This is my 8th class and I can say I <em>feel</em> you. The first thing I want to say to you is that if you have made it thus far, you likely understand more than you give yourself credit for. This is the third class I come across reinforcement learning (first was ML, second ML for trading). I felt that I got about half of what was said in ML and some more in ML4T. Now I can read the paragraph you quoted and have a good idea of what is being said. Just like with any form of machine learning, reiteration matters. I don&#39;t think reading the same thing over and over helps me, so I look for  other sources to have it explained in a different way. I still won&#39;t fully understand it, but I&#39;ll have a marginally better grasp at worst. Then I&#39;ll sleep on it, talk to someone about it, think about it in the shower, forget about it, then come across it again. To some, learning this stuff might be as easy as one read from a single source. To me, It helps to know that I&#39;m not the only one who doesn&#39;t get it after the first read, but over time I will if I just keep at it.</p>",2019-09-08T05:39:25Z,41,Week 9/1 - 9/7,followup,,is8ald0uljj3u4,k0ajs24dirf182,2019-09-08T05:39:25Z,{},hw1
1210,no,"<p>Re: &#34;I don&#39;t think reading the same thing over and over helps me, so I look for  other sources to have it explained in a different way.&#34;</p>
<p></p>
<p>Agreed!</p>",2019-09-30T04:11:13Z,37,Week 9/29 - 10/5,feedback,,jl8i3ypijUPC,k15wbdidlkl5pq,2019-09-30T04:11:13Z,{},hw1
1211,no,"<p>Is it ok if we put our homework code in that repository though, or would that be distracting to reviewers?</p>",2019-09-07T20:56:09Z,31,Week 9/1 - 9/7,followup,,is8ald0uljj3u4,k0a134y2k301dj,2019-09-07T20:56:09Z,{},hw6
1212,no,Ofcourse. There is a folder for each project. As long as its outside of those project folders its fine. ,2019-09-07T21:20:43Z,31,Week 9/1 - 9/7,feedback,,i4i9bi8rFqk,k0a1yqejqhp1iw,2019-09-07T21:20:43Z,{},hw6
1213,no,<p>I am getting same error. I guess the sum of probabilities for a given action from a state has to be 1. Just a guess not sure though.</p>,2019-09-07T20:17:52Z,43,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k09zpwrc6ham8,2019-09-07T20:17:52Z,{},hw1
1214,no,"<p>I guess we are both stuck at same point. </p>
<p>Roll in state 8 to bankrupt state (1,2,3)  = 0.5. </p>
<p>But roll and got to state 12,13,14 each is 0.167. </p>
<p>I used states - 0,4,5,6,8,9,10,11,12,19(Bankrupt), 20(Quit) and not adding these. </p>
<p></p>
<p>i am going to try add 0.16 each under state 4,5,6.</p>",2019-09-07T20:45:58Z,43,Week 9/1 - 9/7,feedback,,jfzaqnqvtQ1m,k0a0q1n2fx450h,2019-09-07T20:45:58Z,{},hw1
1215,no,<p>The sum of probabilities for each state has to be equal to 1. if you use 12 states you can roll 4 from state 8 then the chance of loosing all the money is 1 - 1/6.</p>,2019-09-07T21:56:22Z,43,Week 9/1 - 9/7,feedback,,jla4c8d6bj94ou,k0a38kyvsf04qo,2019-09-07T21:56:22Z,{},hw1
1216,no,"<p>If 12 is a terminal state, you cannot keep rolling. So the only thing you can do is quit. </p>
<p></p>
<p>Same thing happens to 8. 8 is a terminal state in your configuration, you cannot roll again. So you should just quit here.</p>
<p></p>",2019-09-07T22:09:25Z,43,Week 9/1 - 9/7,feedback,,jqmfnc46kl26eg,k0a3pd1evjr1rh,2019-09-07T22:09:25Z,{},hw1
1217,no,"<p>Thanks Artem/Omar, Terminal state is 20 in my case and I quit there as per logic you mentioned.</p>
<p></p>
<p>From 4, you can roll 4 and reach state 8, and </p>
<p></p>
<p>again from 8, you can roll 4 and reach state 12.</p>
<p></p>
<p>Artem,</p>
<p>I thought chance of losing from State 8 is 0.5 i.e. if you roll a 1,2 or 3 i.e. 0.5 </p>
<p></p>",2019-09-07T22:22:17Z,43,Week 9/1 - 9/7,feedback,,jfzaqnqvtQ1m,k0a45w93yvf4uh,2019-09-07T22:22:17Z,{},hw1
1218,no,"<p>Let&#39;s say that you only roll twice for this problem. Then, you cannot roll once you&#39;re in 8, since you already rolled twice. The only thing you can do here is quit. That&#39;s how I solved it, but I&#39;m sure there are many different approaches</p>",2019-09-08T10:34:48Z,42,Week 9/8 - 9/14,feedback,,jqmfnc46kl26eg,k0aubxk5uvf2uf,2019-09-08T10:34:48Z,{},hw1
1219,no,<p>Is there a convenient way of building the transition and reward arrays to feed to PyMDPToolbox without doing it by hand lol?</p>,2019-09-08T00:58:52Z,42,Week 9/8 - 9/14,followup,,gx3c8l7z7r72zl,k0a9r9zehm635y,2019-09-08T00:58:52Z,{},hw1
1220,no,"<p>I am not sure but I would let others answer that.</p>
<p></p>
<p>My first goal here is to get it working for N=6 example in homework :)</p>",2019-09-08T01:02:48Z,42,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0a9wbsuewy6p6,2019-09-08T01:02:48Z,{},hw1
1221,no,"<p>Since we do not know the number of sides before hand , the only way is to program the creation of transition and reward matrix, best to do the simple case by hand to figure out the logic.</p>",2019-09-08T05:19:02Z,42,Week 9/8 - 9/14,feedback,,jc5n7oq5zop3e8,k0aj1u89gsq3sk,2019-09-08T05:19:02Z,{},hw1
1222,no,That’s correct. I am also planning to do the same.,2019-09-08T05:39:02Z,42,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0ajrkfldaqxy,2019-09-08T05:39:02Z,{},hw1
1223,no,"<p>Yes, it takes 2 lines of code.</p>
<p>For every state, you add the probability to go to a reachable state.  </p>
<p>We know the probability, so it&#39;s just a matter of going through all possible states.</p>",2019-09-08T19:59:12Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0behqn1b113ne,2019-09-08T19:59:12Z,{},hw1
1224,no,"<p>Hi Jean,</p>
<p>Thanks.</p>
<p></p>
<p>Agreed.</p>
<p></p>
<p>Stuck in P for case where I am in good side (0) state and roll and reach state in bad side (mask of 1). At this point, should I add  this state in P matrix or ignore ? </p>
<p></p>
<p>If I go from badside state to good side, it is straightforward to add it.</p>
<p></p>
<p>I went ahead and added bad side state prob but does not give me correct answer.</p>",2019-09-08T20:39:07Z,42,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0bfx2pfwfu1e3,2019-09-08T20:39:07Z,{},hw1
1225,no,"<p>&#64;Anurag if I understand your question, I&#39;d say that no you don&#39;t have to add a state every time you get a &#39;bad side&#39;, you simply go to a unique &#39;lose everything&#39; state, and you stay there.</p>
<p>You never go from badside state to good side/state because the game is over when you get a bad side. </p>
<p>Maybe you need to read slowly the exercise, seems like you missed something.  </p>
<p></p>
<p>But you also say &#39;I went ahead and added bad side state prob but does not give me correct answer&#39;.</p>
<p>So if you added that absorbing state, then my bad.</p>
<p>I can&#39;t help you with not getting the correct answer, since I don&#39;t even know if you are calculating the value by iteration or if you use mdptoolbox to do it.</p>
<p>At this stage, i would encourage you to use mdptoolbox, which will give you the solution immediately, and focus on your matrices.  </p>",2019-09-08T21:52:37Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0bijlm2b3s4gc,2019-09-08T21:52:37Z,{},hw1
1226,no,<p>How do we know the average winning reward before playing a round ?</p>,2019-09-07T23:41:04Z,43,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k0a6z7mf7ig528,2019-09-07T23:41:04Z,{},hw1
1227,no,"<p>In the case of the six sided die problem it would be:</p>
<p></p>
<p>1/6(0) &#43; 1/6(0) &#43; 1/6(0) &#43; 1/6(4) &#43; 1/6(5) &#43; 1/6(6) = 2.5</p>
<p></p>
<p>2.5 - B * 0.5 = 0. </p>
<p></p>
<p>B = 5. </p>",2019-09-08T02:12:57Z,42,Week 9/8 - 9/14,feedback,,jzkifxpxau95an,k0acejfgnoa59x,2019-09-08T02:12:57Z,{},hw1
1228,no,"notice, “average on winning states”.<div>1/3 (4&#43;5&#43;6) = 5</div>",2019-09-08T04:37:46Z,42,Week 9/8 - 9/14,feedback,,jl1d3uyrhAts,k0ahkrmilcy60u,2019-09-08T04:37:46Z,{},hw1
1229,no,"<p>I&#39;m not sure if I am not coding it correctly or something wrong with how I have defined the values.</p>
<p>When I start with</p>
<p></p>
<p>$$V_0(s) = 0 $$</p>
<p>I end up with $$ V_1(s) = s&#43;1 $$ for all $$s$$ except for terminal state which is 0.</p>
<p>I think my code is correct, so something wrong with how I am defining the values.</p>",2019-09-07T23:48:02Z,43,Week 9/1 - 9/7,followup,,jque4s9u2b82s0,k0a7861ism670h,2019-09-07T23:48:02Z,{},hw1
1230,no,<p>Your representation is right. I used the same and it worked for me. I&#39;m pretty sure something wrong with your code. </p>,2019-09-08T18:30:18Z,42,Week 9/8 - 9/14,feedback,,jzih0fdt4sn1cq,k0bbbfilq9c4lv,2019-09-08T18:30:18Z,{},hw1
1231,no,"<p>Stuck on same issue here. </p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=169"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=169</a></p>
<p>Getting same output as yours. </p>",2019-09-07T23:30:34Z,31,Week 9/1 - 9/7,followup,,jc6s23mr74r45q,k0a6lpx6ok47gb,2019-09-07T23:30:34Z,{},hw1
1232,no,<p>I fixed my problem by changing my code to add more states to simulate additional rolls. Hope that helps!</p>,2019-09-07T23:50:16Z,31,Week 9/1 - 9/7,feedback,,j6m1jeidndu6wq,k0a7b24grk6109,2019-09-07T23:50:16Z,{},hw1
1233,no,"<p>I got for this example :</p>
<pre>7.379980563654032</pre>
<p>which is 7.38 if I round it to 3 decimals. I have states if I rolled the die 100 times. My code worked on Heroku site. So I don&#39;t understand why the answer is 7.3799 instead of 7.38</p>",2019-09-08T18:20:11Z,30,Week 9/8 - 9/14,feedback,,jzih0fdt4sn1cq,k0bayf54kn86ig,2019-09-08T18:20:11Z,{},hw1
1234,no,"<p>We are not rounding to 3 decimals and we are not checking for your answer to be equal to ours... We have some range, but if you round it to 3 decimal places, your should be good.</p>
<p></p>
<p>$$7.3799 \approx 7.38$$</p>",2019-09-08T18:31:01Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bbcbyttje5lc,2019-09-08T18:31:01Z,{},hw1
1235,stud,"<p>kind of dumb question which is a bit late - when we enter our answers, should we be entering all the places we have or rounding to 2 or 3?</p>
<p>So for Aida&#39;s example - should we enter </p>
<p>a) 7.379980563654032</p>
<p>b) 7.3799</p>
<p>c) 7.38</p>",2019-09-08T23:56:44Z,30,Week 9/8 - 9/14,feedback,a_0,,k0bmz80ssir2xo,2019-09-08T23:56:44Z,{},hw1
1236,no,"<p>Good question, actually. You should enter <em><strong>a</strong></em>, we take care of checking for the full range. Though are range is loose (for lack of a better word), so either should work well if it is accurate enough.</p>",2019-09-09T01:02:25Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bpboiwmwfok,2019-09-09T01:02:25Z,{},hw1
1237,no,"<p>I think we even check for a range of $$\pm 0.05$$. So, just enter your best, most accurate, answer.</p>",2019-09-09T01:05:07Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bpf5ga4y4eg,2019-09-09T01:05:07Z,{},hw1
1238,stud,"<p>ty <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  ! </p>",2019-09-09T01:05:23Z,30,Week 9/8 - 9/14,feedback,a_0,,k0bpfi8c7w24tm,2019-09-09T01:05:23Z,{},hw1
1239,no,<p>Yw.</p>,2019-09-09T01:34:34Z,30,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bqh12lqbr2yc,2019-09-09T01:34:34Z,{},hw1
1240,no,"<p>I&#39;ve encountered the same thing, I think it&#39;s because we are accumulating all the $$\Delta w$$ over the training set and update at the end, so if there are 10 sequences in one training set, it&#39;s very possible that the same observation will appear more than 10 times in the training set, and if $$\alpha $$ is chosen to be 0.1, so $$\Delta w$$ over the whole training set is &gt;= 10*0.1*$$(P_{t&#43;1} - P_t) \Sigma \lambda ^{t-k}\triangledown _wP_k$$ =  $$(P_{t&#43;1} - P_t) \Sigma \lambda ^{t-k}\triangledown _wP_k$$  which very likely not to converge. So $$\alpha $$ maybe a number &lt; 0.1</p>",2019-09-08T02:33:22Z,62,Week 9/8 - 9/14,followup,,jzj4205g7gd2fw,k0ad4sdp2ov4en,2019-09-08T02:33:22Z,{},project1
1241,no,"<p>Thanks Miguel. </p>
<p></p>
<p>For reward on quits, looks like</p>
<p>R(9, Quit, Terminal State) = 9. </p>",2019-09-08T02:36:31Z,42,Week 9/8 - 9/14,followup,,jfzaqnqvtQ1m,k0ad8uqzi9n2s6,2019-09-08T02:36:31Z,{},hw1
1242,no,<p>Yes!</p>,2019-09-08T02:36:56Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ad9dopqng34y,2019-09-08T02:36:56Z,{},hw1
1243,no,"<p>Thanks. Changing this got answer to 2.4512. Much closer to 2.5833.</p>
<p></p>
<p>Let me find out what else am I doing incorrect now.</p>",2019-09-08T02:40:25Z,42,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0addur4d634s,2019-09-08T02:40:25Z,{},hw1
1244,no,"<p>Add more states, perhaps!?</p>",2019-09-08T02:59:15Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ae23cc9c6183,2019-09-08T02:59:15Z,{},hw1
1245,no,"<p>Got it working finally. I had 0.16 instead 1/6 in Prob matrix and that messed up those numbers after decimal points. lol</p>
<p></p>
<p>Can&#39;t believe I wasted so much time due to this. But your Rewards help above got it working finally. </p>
<p></p>
<p>Thank you!</p>",2019-09-08T03:07:23Z,42,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0aecjqenzo1nl,2019-09-08T03:07:23Z,{},hw1
1246,no,<p>Enjoy!</p>,2019-09-08T03:32:48Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0af981p7vle1,2019-09-08T03:32:48Z,{},hw1
1247,no,"Can you elaborate why the reward would be 0 on quits on the first case? &#34;R(s,a,s′)=s′−s, unless on a bad state in which you will have to do: R(s,a,s′)=−s, and 0 on quits&#34;",2019-09-09T01:29:42Z,42,Week 9/8 - 9/14,followup,,jqkuetouttn5,k0bqas30nv75bh,2019-09-09T01:29:42Z,{},hw1
1248,no,"<p>Because in this model you are paying the reward as you go. So, if the agent quits, you already paid what you had to, you shouldn&#39;t pay any more.</p>",2019-09-09T01:34:21Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bqgqr0old2pc,2019-09-09T01:34:21Z,{},hw1
1249,no,"Ah! (facepalm) literally spent an hour trying to figure out why my code worked when setting R=0 when quitting and why it didn&#39;t without it! That makes a lot of sense, thank you Miguel! :)",2019-09-09T01:39:58Z,42,Week 9/8 - 9/14,feedback,,jqkuetouttn5,k0bqnzf3xdc3sq,2019-09-09T01:39:58Z,{},hw1
1250,no,"<p>Yw, Esteban!</p>",2019-09-09T02:36:15Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bsocyb56c6qi,2019-09-09T02:36:15Z,{},hw1
1251,no,"<p>Value iteration is not difficult to implement. You can even code a <em>generic</em> version if you look at sources such as the <a href=""http://aima.cs.berkeley.edu/python/mdp.html"" target=""_blank"">AI: a modern approach repository</a>. I did not use pymdptoolbox to solve this assignment, so you are right, it might we worth your while to roll your own code.</p>
<p></p>
<p>I solved two small cases by hand for validation, but I would not try that for larger problems without a computer.</p>
<p></p>
<p>You do not have to complete the problems in one sitting. But why would you not, if you have a solver program.</p>",2019-09-08T05:04:56Z,42,Week 9/8 - 9/14,followup,,is8ald0uljj3u4,k0aijpe32dh4fx,2019-09-08T05:04:56Z,{},hw1
1252,no,"<p>Thanks for giving a reference to AIMA, I didn&#39;t know such resource exists!</p>",2019-09-10T03:46:20Z,42,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0damcc1n9967x,2019-09-10T03:46:20Z,{},hw1
1253,no,"<p>can you clarify? If you have a vector like {0,0,0,1}. probability of facing an end any any state is .25. What else can it be</p>",2019-09-08T07:08:11Z,42,Week 9/8 - 9/14,followup,,jqrr36mqfm8M,k0amy7ozfro6vq,2019-09-08T07:08:11Z,{},hw1
1254,no,"<p>I was facing a problem sometimes with longer bit masks , it worked fine for shorter ones, due to float decimal points, not sure if even or odd made a difference</p>",2019-09-08T07:25:22Z,42,Week 9/8 - 9/14,feedback,,jc5n7oq5zop3e8,k0ankblay6x4dr,2019-09-08T07:25:22Z,{},hw1
1255,no,"<p>Thanks Aida.</p>
<p></p>
<p>I think I am confused comparing this with N=6 example where:</p>
<p>state 4 -&gt; Roll 4 -&gt; state 8 which is a new state. and I add R(8, quit) = 8</p>
<p></p>
<p>In this, we go from</p>
<p>state1 -&gt; Roll 1 -&gt; state 2, which is existing state and bad side (because it is 1 in bitmask). At this point, I am confused on whether I will have a Transition:</p>
<p>state 1, Roll 1, state 2 with prob 0.5</p>
<p>and what will be R(2, quit) = 2 or 1 ?</p>
<p></p>
<p></p>",2019-09-08T18:13:52Z,41,Week 9/8 - 9/14,followup,,jfzaqnqvtQ1m,k0baqacnwuc1pk,2019-09-08T18:13:52Z,{},hw1
1256,no,"<p>You don&#39;t force add R(8, quit), you don&#39;t force the agent to do that. Simply model the problem, and then have the agent find the answer.</p>",2019-09-09T01:07:55Z,41,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bpirptzhbul,2019-09-09T01:07:55Z,{},hw1
1257,no,"Yep, that did it for me. Thank you so much!",2019-09-08T17:23:24Z,42,Week 9/8 - 9/14,followup,,jnjgrn6usm9x,k0b8xeavo43r1,2019-09-08T17:23:24Z,{},hw1
1258,no,"<p></p>
<pre> The error measure used is the RMS error between the ideal predictions and those found by the learning procedure after being repeatedly presented with the training set until convergence of the weight vector.</pre>
<p>From the figure text for figure 3. My understanding (backed up by the experiments) is that (for the first experiment) for each training-set of 10 sequence every sequence is presented to the algorithm (in turn, only updating the the weight vector after showing the full set) until the weight vector no longer changes (defined by some epsilon). You then take that weight vector and calculate the RMS error against the ideal vector. Repeat for all sets and then average the errors for all the sets.</p>
<p></p>
<p>RMSE: $$\sqrt{\sum (ideal - predicted)^{2}/n}$$, practically since both are vectors you can just take the difference of them, you don´t need to do predicted_d = dot(w, (0, 0, 1, 0, 0)).</p>
<p>So for example given (assuming Suttons random walk):</p>
<p>ideal = array([[0.16666667],<br />                      [0.33333333],<br />                      [0.5       ],<br />                      [0.66666667],<br />                      [0.83333333]])</p>
<p></p>
<p>predicted= array([[0.22880682],<br />                            [0.34781761],<br />                            [0.49441543],<br />                            [0.64161237],<br />                            [0.81082703]])</p>
<p></p>
<p>You can do:</p>
<p></p>
<p>ideal - predicted = diff =</p>
<p>array([[-0.06214016],<br />       [-0.01448428],<br />       [ 0.00558457],<br />       [ 0.02505429],<br />       [ 0.0225063 ]])</p>
<p></p>
<p>$$diff^{2} =$$</p>
<p>array([[3.86139897e-03],<br />       [2.09794361e-04],<br />       [3.11874042e-05],<br />       [6.27717535e-04],<br />       [5.06533743e-04]])</p>
<p></p>
<p>$$\sum diff^{2} =$$</p>
<p>0.005236632015848839</p>
<p></p>
<p>$$(\sum diff^{2})/5 =$$</p>
<p>0.0010473264031697679</p>
<p></p>
<p>$$\sqrt{(\sum diff^{2})/5} =$$</p>
<p>0.032362422702414725</p>
<p></p>
<p>So RMSE = 0.032362422702414725</p>",2019-09-09T09:47:39Z,62,Week 9/8 - 9/14,followup,,jzjwcq2u8o7110,k0c834zgtjh2ay,2019-09-09T09:47:39Z,{},project1
1259,no,"<p>Yeah, it&#39;s similar to the 2nd way I proposed in my post - sqrt((ideal-weights).dot((ideal-weights)) / 5).  That&#39;s what I ultimately went with as it made the most sense.  That said, I do get a curve with similar behaviour to Sutton but the values are lower and it has a minima around .5 for my figure 3.  My convergence epsilon .000000001, I guess I could play with that to make things worse to get higher values - not sure if it&#39;ll make the minima go away.  My weight vector starts at zero too not sure if that matters and who knows what Sutton set his to.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixpclzk97jg2fo%2Fk0cc9jm1hbgr%2FScreenshot_from_20190909_074425.png"" alt="""" width=""594"" height=""392"" /></p>",2019-09-09T11:45:53Z,62,Week 9/8 - 9/14,feedback,,ixpclzk97jg2fo,k0ccb6u7nxw10o,2019-09-09T11:45:53Z,{},project1
1260,no,"<p>Yeah it´s exactly the same :)</p>
<p></p>
<p>I don´t think the point is to match Sutton. And 1988 was a long time ago and floating point calculations are better now, which I think explains the lower error.</p>",2019-09-09T12:07:23Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0cd2uabd0o2p4,2019-09-09T12:07:23Z,{},project1
1261,no,"<p>At least for the second experiment Sutton (page 22) says he initialized the weights to 0.5. I don´t think it matters for the first experiment, it will converge to the same value anyway.</p>",2019-09-09T12:10:22Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0cd6o6bbcq4bc,2019-09-09T12:10:22Z,{},project1
1262,no,"<p>When you guys mention convergence episilon, as per Sutton&#39;s experiments, didn&#39;t he fix the number of episodes (he calls iterations), rather than iterate until some convergence?</p>
<p></p>
<p>Also what learning rates / discount factors are being applied? Not very clear in the first experiment...</p>
<p></p>",2019-09-10T17:38:59Z,62,Week 9/8 - 9/14,feedback,,is6e83bsfvk,k0e4d4dx6x31q5,2019-09-10T17:38:59Z,{},project1
1263,no,"<p>No, he ran until convergence as defined by no changes larger then some epsilon (which he does not specify). Page 20:</p>
<pre>...until the procedure no longer produced any significant changes in the weight vector.</pre>
<p>For the second experiment each sequence was only presented once.</p>
<p></p>
<p>He does not specify learning rates for the first experiment except that alpha should be small. And as far as I can tell he does not decay the learning rate. Though he mentions in passing that if $$\alpha = \frac{1}{n}$$ then the variance around the convergence can be reduced to 0 (in the limit).</p>",2019-09-10T19:36:26Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0e8k5xuj2e1h3,2019-09-10T19:36:26Z,{},project1
1264,no,"<p>I see, so when he mentions following that statement:</p>
<blockquote><md><br />&#34;This measure was averaged over 100 training sets to produce the data shown.&#34;<br /></md></blockquote>
<p>So he means then to run the experiment until convergence 100 times and then average the the errors RMS(true values, estimates)? </p>",2019-09-10T20:17:45Z,62,Week 9/8 - 9/14,feedback,,is6e83bsfvk,k0ea1ard5294hw,2019-09-10T20:17:45Z,{},project1
1265,no,"<p>Yes, he uses 100 training-sets and each training-set consists of 10 sequences. So 10 sequences are presented until convergence, and an error is calculated. This is repeated for 100 different sets and the errors are then averaged.</p>",2019-09-10T21:18:03Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0ec6ur63dk663,2019-09-10T21:18:03Z,{},project1
1266,no,"<p>Are we supposed to sum the RMSE for all 100 training sets then divided by 100 then plot? I ask because I am calculating RMSE the same way &#64;Oscar is above ans summing with each of the 100 training sets and dividing by 100 for each RMSE for each lambda but I am getting a crazy graph below.</p>
<p></p>
<p>Does anyone have any suggestions?</p>
<p></p>
<p>Here are my predicted weights, they are very close:</p>
<p>       weights =  [ 0.16666667  0.35        0.44736842  0.625       0.9       ]<br />       actual_values =  [ 0.16666667  0.33333333  0.5         0.66666667  0.83333333]<br /><br /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk143x5lyt7jj%2FshittyGraph.PNG"" alt="""" /></p>
<p></p>
<p>Are we supposed to reset of weight vector with each new training set?</p>
<p></p>",2019-09-28T22:08:43Z,60,Week 9/22 - 9/28,followup,,gx3c8l7z7r72zl,k143xc1sk5f71e,2019-09-28T22:08:43Z,{},project1
1267,no,"<p>&gt; Are we supposed to sum the RMSE for all 100 training sets then divided by 100 then plot?</p>
<p>Yes, sum of 100 RMSE then divide them by 100 is what it means by taking average across 100 trainsets.</p>
<p></p>
<p>&gt; Are we supposed to reset of weight vector with each new training set?</p>
<p>Yes, for repeated representation we fit each trainset individually and take average across all of them to balance out the outliers.</p>
<p></p>",2019-09-29T20:58:57Z,59,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k15gvh95ye57gb,2019-09-29T20:58:57Z,{},project1
1268,no,"Sorry, is that &#34;no, we _don&#39;t_ have to do these things&#34;, or &#34;no, we _should_ do these things&#34;?",2019-09-08T22:44:30Z,62,Week 9/8 - 9/14,followup,,jl3oi5v7qkSk,k0bkebrrf1c6ou,2019-09-08T22:44:30Z,{},project1
1269,no,"<p>Okay, to clarify. You should not assume we, the TAs, have read thousands of these papers. Your paper <em>should</em> contain brief comments about the problem, the experiments, the methods, and so on.</p>
<p></p>
<p>But don&#39;t stop telling us: &#34;This is the random walk, there is an initial state in the middle, and two terminal states...blah blah.&#34;</p>
<p></p>
<p>How about you tell us what&#39;s special about this environment. What if it wasn&#39;t 50:50 random?! What is the average length of a trajectory in this random walk? How about if the walk was 10x longer, what is the average trajectory length? What is it was 99:1?! or 1:99?!</p>
<p></p>
<p>Bottom line is, yes, you should introduce the problem, the methods, and so on, but no, that won&#39;t give you 100 points. Experiment yourself, question the results, question the experiments themselves!!!</p>
<p></p>
<p>There is so much to explore...</p>",2019-09-08T23:58:44Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bn1squ7or1m0,2019-09-08T23:58:44Z,{},project1
1270,no,Would it be possible to have this available for download? Have a long commute that would love to use to catch up with the office hours!,2019-09-09T02:05:06Z,42,Week 9/8 - 9/14,followup,,jqkuetouttn5,k0brkasq73r6w3,2019-09-09T02:05:06Z,{},hw1
1271,no,"<p>We have been recommended not to, due to privacy concerns. Sorry.</p>",2019-09-09T02:33:21Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0bskmka96732v,2019-09-09T02:33:21Z,{},hw1
1272,no,<p>He was our student a few semesters back.</p>,2019-09-08T18:26:13Z,62,Week 9/8 - 9/14,followup,,hyx9thiqa6j4nn,k0bb65qlr4m6cv,2019-09-08T18:26:13Z,{},hw2
1273,no,Oh good to know !,2019-09-09T01:51:01Z,62,Week 9/8 - 9/14,feedback,,jc5n7oq5zop3e8,k0br26mpgu4453,2019-09-09T01:51:01Z,{},hw2
1274,no,"<p>Thanks for posting this. Read it a few times and one paragraph stood out as most helpful in getting the right intuition for implementing the algorithm. Sharing the excerpt here in case it helps someone else:</p>
<p></p>
<p>&#34;The backward view of TD(λ) updates values at each step. So after each step in the episode you make updates to all prior steps. The question is how do you weight or assign credit to all prior steps appropriately? The answer is by using something called Eligibility Traces (ET). ET basically keeps a record of the frequency and recency of entering a given state (Figure 4). It assigns credit to states that are both visited frequently as well as visited recently with respect to our terminal state. The lambda (λ) and gamma (γ) term are used to discount those traces.&#34;</p>",2019-09-12T15:54:19Z,62,Week 9/8 - 9/14,followup,,isde34zracb1mz,k0gvi83svev5oh,2019-09-12T15:54:19Z,{},hw2
1275,no,"<p>Makes sense, thanks Miguel.  It&#39;s a lot easier to say &#34;neural networks&#34; than &#34;multi-layer connectionist networks&#34;.</p>",2019-09-08T22:22:38Z,38,Week 9/8 - 9/14,followup,,is9so9huTMp,k0bjm7i5let4j4,2019-09-08T22:22:38Z,{},hw2
1276,no,"<p>All this updating from intermediate states feels a lot like back propogation, doesn&#39;t it?</p>",2019-09-24T03:45:32Z,36,Week 9/22 - 9/28,followup,,jqmfuaidej9155,k0xar8gsim76pe,2019-09-24T03:45:32Z,{},hw2
1277,no,<p>Yes it sure does.</p>,2019-09-24T06:26:11Z,36,Week 9/22 - 9/28,feedback,,is9so9huTMp,k0xghu2ptpb1cz,2019-09-24T06:26:11Z,{},hw2
1278,no,"<p>Likely possibilities: your formula could be wrong or you need more states and/or your are not converging and need to run more iterations</p>
<p></p>
<p>I was generating states for about 10-20 or sometimes even more dice rolls. But these many may not be needed</p>
<p></p>
<p>Convergence: I was letting the iterations go on for at least 50-100 and all problems converged well within that iteration count</p>
<p></p>
<p>But I had my formula slightly wrong but still was passing 4/10 questions.</p>
<p></p>
<p>1. r(s,a,s&#39;) vs r(s,a) confusion</p>
<p>2. rewarding while quitting vs ongoing rewards</p>
<p></p>
<p>I was messing up the above two points / so the formula i was using was a hybrid one (invalid) and hence 6/10 of the questions were failing</p>",2019-09-09T07:54:37Z,42,Week 9/8 - 9/14,followup,,jqrr36mqfm8M,k0c41s474rk2h8,2019-09-09T07:54:37Z,{},hw1
1279,no,"<p>that is correct. Some required 3, and the ones I haven&#39;t produced an accurate enough answer will require at least 4</p>",2019-09-09T00:05:17Z,42,Week 9/8 - 9/14,followup,,jqn5nxiacet6tz,k0bna7ilwcw5eu,2019-09-09T00:05:17Z,{},hw1
1280,no,<p>Are you using mdptoolbox?</p>,2019-09-09T00:12:36Z,42,Week 9/8 - 9/14,feedback,,jzkpm3i0pxp2sg,k0bnjmdu1ga2lf,2019-09-09T00:12:36Z,{},hw1
1281,no,"<p>Yes. I do have a small update. I solved a small issue with how I was initially allocating the array&#39;s. So the execution time is much faster. But I still get an out of memory exception.</p>
<p></p>
<p>my arrays have multiple states sometimes for each value in the bank, could this be the cause of the issue I&#39;m having? But if I reduce this, I&#39;m not sure how to track how many rolls have occurred.</p>
<p></p>
<p>i.e. in the N=6 example problem. My arrays will have multiple representations of 12 (rolling 4-&gt;4-&gt;4 and another for rolling 6-&gt;6)</p>",2019-09-09T00:27:33Z,42,Week 9/8 - 9/14,feedback,,jqn5nxiacet6tz,k0bo2ustbxf4fx,2019-09-09T00:27:33Z,{},hw1
1282,no,"<p>I just ran into the multiple 12 issues for N=6 case with rolls=3. I don&#39;t think deduping states is the correct approach. You need to keep multiple 12s and adjust transition probabilities based on the first, second and third roll. </p>
<p></p>
<p>I&#39;ll try to add a &#34;roll_number&#34; subscript next to the state so they are distinguishable.</p>",2019-09-09T00:36:59Z,42,Week 9/8 - 9/14,feedback,,jzkpm3i0pxp2sg,k0boeyyzvdu2zo,2019-09-09T00:36:59Z,{},hw1
1283,no,<p>My solution is value iteration</p>,2019-09-09T01:45:32Z,42,Week 9/8 - 9/14,followup,,jqn5nxiacet6tz,k0bqv53fpel2iw,2019-09-09T01:45:32Z,{},hw1
1284,no,"<p>Same here. </p>
<p></p>
<p>And for one of the problem I had to do 10 rolls. Then got the answer.</p>",2019-09-09T03:29:24Z,42,Week 9/8 - 9/14,feedback,,jzkpm3i0pxp2sg,k0bukpesfnb4is,2019-09-09T03:29:24Z,{},hw1
1285,no,"<p>If we&#39;re adding podcast links <a href=""https://www.talkrl.com/episodes/michael-littman"" target=""_blank"" rel=""noopener noreferrer"">here&#39;s an interview</a> with Prof. Littman and <a href=""https://podcasts.apple.com/us/podcast/episode-11-richard-sutton/id1438378439?i=1000432467700&amp;mt=2"" target=""_blank"" rel=""noopener noreferrer"">here&#39;s an interview</a> with Sutton.</p>",2019-09-10T18:46:02Z,62,Week 9/8 - 9/14,followup,,i4op5p9vfbq5yz,k0e6rd5rz5s1sm,2019-09-10T18:46:02Z,{},other
1286,no,"<p>Thanks for this.</p>
<p></p>
<p>Just listened to the Sutton podcast...inspiring.</p>",2019-09-14T00:21:35Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0it2fchrkr2me,2019-09-14T00:21:35Z,{},other
1287,no,"<p>I am having difficulty understanding P<sub>t</sub> and P<sub>t&#43;1</sub>.</p>
<p>According to section 3.2 : P<sub>t</sub> is simply the value of i<sup>th</sup> component of w</p>
<p></p>
<p>If the weights are initialized to be uniformly distributed [0.2, 0.2, 0.2, 0.2, 0.2], then P will be always be the same and (P<sub>t&#43;1</sub> -P<sub>t</sub>) will always be zero.</p>
<p></p>
<p>What am I missing?</p>",2019-09-13T19:52:48Z,62,Week 9/8 - 9/14,followup,,jl2egn5k4zo4lp,k0ijgry3ms25zl,2019-09-13T19:52:48Z,{},project1
1288,no,"<p>$$P_t$$ and $$P_{t&#43;1}$$ both represent the same variable, but at different time steps.  So you can say that one is the value of the $$i^{th}$$ component of $$w$$, but then you should say that the other is the value of the $$j^{th}$$ component of $$w$$, where $$i$$ and $$j$$ don&#39;t necessarily have to be the same, but depend on the non-zero component of the observation vector at each of the two time steps.</p>",2019-09-13T21:08:09Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0im5on25lqhb,2019-09-13T21:08:09Z,{},project1
1289,no,<p>I&#39;m sort of confused here too. P<sub>T  </sub>will be the dot product between the initial weights and the observation vector at time t. P<sub>T&#43;1 </sub>will be the dot product between the same initial weights and the observation vector at time t &#43; 1. Won&#39;t this always be 0 if the initial weight vector is uniformly distributed?</p>,2019-09-14T21:22:37Z,62,Week 9/8 - 9/14,followup,,jl3we43d3bp15p,k0k244pnzquz4,2019-09-14T21:22:37Z,{},project1
1290,no,"<p>Even if the weights never changed (which they do, once in a while), if you&#39;re multiplying two <strong>different</strong> observation vectors with the same weights, and subtracting the two dot products, why would the difference always be $$0$$?</p>
<p></p>
<p>Also, not sure what you mean by &#34;uniformly distributed.&#34;  Do you mean initialized to the same value?  Regardless, the above holds.</p>",2019-09-14T21:30:16Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0k2dzbpkl4sl,2019-09-14T21:30:16Z,{},project1
1291,no,"<p>Thanks Vahe,</p>
<p></p>
<p>I&#39;m obviously misunderstanding something here. If the initial weight vector is [0.2 0.2 0.2 0.2 0.2]</p>
<p>And say Xt is [ 0 0 1 0 0], and Xt&#43;1 is [0 0 0 1 0]</p>
<p></p>
<p>Then the dot product would be 0.2 for both, no?</p>
<p></p>
<p></p>
<p></p>",2019-09-14T21:50:30Z,62,Week 9/8 - 9/14,feedback,,jl3we43d3bp15p,k0k34015b9v14m,2019-09-14T21:50:30Z,{},project1
1292,no,"<p>No, I&#39;m sorry.  You&#39;re absolutely right.  With those unit basis observation vectors, if all the weight components are the same, then the errors will all be zero.  That will only change after the first update and the weights take on different values for the first time.</p>
<p></p>
<p>This makes sense.  If you always predict the same outcome regardless of state, then there is never any difference in two predictions.  This situation will only get a dose of reality when you hit a terminal state.</p>",2019-09-14T22:05:01Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0k3mnh1fyc1ju,2019-09-14T22:05:01Z,{},project1
1293,no,"<p>Ah got it, its the dose of reality that will make this work I believe. Thanks!</p>",2019-09-14T22:06:03Z,62,Week 9/8 - 9/14,feedback,,jl3we43d3bp15p,k0k3nzx7el06nx,2019-09-14T22:06:03Z,{},project1
1294,no,"<p>Thank you Tianhang, for replying to my question even when you are having flu. I really appreciate the effort!</p>
<p></p>
<p>1) I will need to read up on RKHS to understand your suggestion, since I have no idea what it is. Meanwhile, I have follow up questions that I think can be discussed.</p>
<p>&gt; &#34;You might want to assume finite actions&#34;</p>
<p>Can I assume you mean finite states? So, the conversion between Value-Iter and LP is not as straight forward. In Value-Iter, (with N=6 in our homework as toy example) we can ignore the fact that there are infinite states, and rely on $$ \varepsilon $$ as gate-stopper so that we don&#39;t explore too far. When I think about the problem in LP, there is no mention of $$ \varepsilon $$ and I don&#39;t know if making an assumption of finite states is just to make the problem easier, or because we wouldn&#39;t be able to solve it otherwise?</p>
<p></p>
<p>Furthermore, can you elaborate on how to turn the infinite states into finite states?</p>
<p>I think in the lecture, there was a mention of $$ \beta $$ to discount future value, which I also see in the LP equation. But if we include future discount, would it behave more like log-function instead of linear function?</p>
<p></p>
<p>2) I am having trouble connecting my thought with yours. Let me explain my thought first, then I will try to reflect my understanding on your answer.</p>
<p>Without understanding the math needed to solve the LP, I am imagining the idea would be like:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl5wq8mca7o0%2Fk0ek6adw4x1x%2Flp.jpg"" alt="""" width=""273"" height=""268"" /></p>
<p>Where L1 is the value we possibly earn, L2 is the value we possibly loose. And at some point, we can deduce an utility like &#34;keep rolling, until you get X dollar&#34;. However, I have trouble seeing such solution by looking at the LP equation. At most, we would just have the optimal $$v$$ at each state, without insight on what we should do.</p>
<p></p>
<p>And back to your answer, can you elaborate more on obtaining Q from T and R?</p>
<p>And what do you mean by solving Q(s, a) over variable a? Maybe a few step in solving the N=6 would help me see the process much clearer.</p>
<p></p>
<p></p>",2019-09-11T01:14:54Z,35,Week 9/8 - 9/14,followup,,jl5wq8mca7o0,k0ekng6snbk2lh,2019-09-11T01:14:54Z,{},hw1
1295,no,"<p>Addressing your 2)</p>
<p></p>
<p>If you already have the optimal $$v$$ for all states, then it&#39;s conceptually trivial to find the best action in any state (i.e. whether to roll or quit).  This is because, for any action you take from a given state $$s$$, you will be thrown to a new state $$s&#39;$$ (perhaps stochastically), where you have the optimal value of <em>that</em> state, $$v(s&#39;)$$.</p>
<p></p>
<p>Concretely, you try every possible action from state $$s$$, and compute the expected return for that action.  This is the definition of $$Q(s,a)$$, the action value function.  Computing this expected return involves computing the expected reward for taking that action (this requires knowledge of $$R$$), and adding to that the expected value of the next state that we transition to (this requires knowledge of $$T$$).  But we have the optimal values for all states by assumption, so we have all the information we need to make those computations.  Now we look at our table of $$Q(s,a)$$&#39;s, and pick the $$a$$ that gives us the maximum $$Q$$ (our $$\text{arg}\max$$).  That forms our policy.</p>",2019-09-11T07:34:00Z,35,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ey6z6zuan5gc,2019-09-11T07:34:00Z,{},hw1
1296,no,"<p>That makes sense! Thanks Vahe!</p>
<p>To rephrase, in  our homework 1 the policy could be like &#34;keep rolling, until we get X dollar&#34; or &#34;keep rolling, if we have better expected return&#34;. But in a boarder and more general concept, the policy is &#34;pick the action among all actions, that will give best result in current state&#34;. With that, having optimal values is equivalent to having a policy.</p>
<p>Did I get it right?</p>
<p></p>
<p>Edit: I also see &#64;235 have good info regard to my question here.</p>",2019-09-11T16:27:10Z,35,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0fh8mnuq392lv,2019-09-11T16:27:10Z,{},hw1
1297,no,"<p>A policy can only be defined once you&#39;ve defined your state space.  If your state is defined as the number of dollars in your bankroll, then a policy will simply be a mapping of each state to one of $$\{\text{quit}, \text{roll}\}$$.</p>
<p></p>
<p>It&#39;s not clear that you could extend that to a rule of &#34;keep rolling until we get X dollars.&#34;  For example, If you had a 100-sided die, and every side was bad except for $$1$$ and $$100$$, then it may be optimal to quit at $$2$$ dollars or $$100$$ or $$101$$ dollars, but not to try for $$3$$ dollars or $$4$$ dollars.  In other words, &#34;keep rolling until we get 100 dollars&#34; would be a very sub-optimal rule to follow.</p>
<p></p>
<p>Your other heuristic &#34;Keep rolling, if we have a better expected return&#34; is just another way of saying to take the $$\text{arg}\max$$ of the action values $$Q(s,a)$$ at a given state.  You would roll if the expected return due to rolling is greater than the expected return due to quitting, and vice-versa.  This is exactly what an optimal policy would do.</p>",2019-09-11T16:56:39Z,35,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0fiajrp7ax6b7,2019-09-11T16:56:39Z,{},hw1
1298,no,<p>That makes lot of sense! Thank you for taking time to explain it to me from another angle. That clears up many things!</p>,2019-09-11T17:10:40Z,35,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0fiskfn8866hq,2019-09-11T17:10:40Z,{},hw1
1299,no,"<p>flow through.  it is basically glorified policy iteration (with constraints).</p>
<p>for any step t, calculate results t&#43;1, given policy x being examined at t=1. </p>
<p>maximize your returns.</p>
<p></p>
<p>the key, is to solve your lp, with conservation of bank from t to t&#43;1. </p>
<p></p>
<p>then iterate over all possible values for your (constrained) policy (categorical) using gradient descent (on the resulting field).</p>
<p></p>
<p>it would be interesting to see, if you could this strictly with LP (integer only),  should be possible to do (with expected returns).</p>
<p></p>
<p></p>",2019-09-14T23:10:56Z,35,Week 9/8 - 9/14,followup,,jzivtxcbl6964n,k0k5zfsecvl62k,2019-09-14T23:10:56Z,{},hw1
1300,no,"<p>Interesting graph but I´m not quite sure if I follow what your point is? That more episodes(sequences in Suttons terminology I assume? Or do you mean training-set?) produce more stable results by having proportionally less noise? What do you mean is the bias?</p>
<p></p>
<p>Making the starting position random shouldn´t affect the probabilities, I might be mistaken, because it averages out. The average starting position is going to be the middle so the only thing that happens is that it takes longer(it´s going to need more examples) to converge from the added noise.</p>",2019-09-09T16:01:24Z,62,Week 9/8 - 9/14,followup,,jzjwcq2u8o7110,k0clfsml925ra,2019-09-09T16:01:24Z,{},project1
1301,no,"<p>Each episode is a training set. Each sequence is a step in the episode.</p>
<p></p>
<p>Sutton chose 100 episodes starting at the midpoint of the space. Starting at this point with only 100 episodes meant that there was a lot of uncertainty in the prediction (P) space that he was trying to learn with TD(lambda). I understand why he couldn&#39;t do more episodes, but he should have done 500 or 1000 episodes to get the full random probabilities of the walks.</p>
<p></p>
<p>The graphs show that the noisiest part of the random walk is in the first 100 episodes. I think this was a poor choice for the analysis.</p>",2019-09-09T20:53:26Z,62,Week 9/8 - 9/14,feedback,,jc554vxmyuy3pt,k0cvvc9sr0p3hg,2019-09-09T20:53:26Z,{},project1
1302,no,"<p>Okey, generally an episode is a single observation I think, ex : (D-E-D-E-F-G, 1), so a sequence in Sutton&#39;s terms. Sutton&#39;s use of however many training-sets of 10 sequences does not matter I think, he just averages over them to reduce bias from individual sets.</p>
<p></p>
<p>If you want to reduce noise in the underlying data you need to use more sequences in every set. For example this is figure 3 using 500 sequences per set (with 100 sets):<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzjwcq2u8o7110%2Fk0dg4ic2a8kj%2F500sequences.png"" alt="""" width=""564"" height=""423"" /></p>
<p>As compared to this, which is figure 3 using 500 training-sets(it&#39;s virtually identical to the plot when using 100 sets):</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzjwcq2u8o7110%2Fk0dkxdkjukq4%2F500_sets.png"" alt="""" width=""515"" height=""386"" /></p>
<p>So increasing the number of sequences does indeed decrease the error, since the maximum likelihood estimate (which TD converges to according to Sutton) is closer to the actual ideal predictions, ie has less noise.</p>
<p></p>
<p>But I´m not sure it really matters anyway. Sutton is trying to demonstrate that TD methods are efficient and that they do converge (in the finite case). So the error being in the range (.1 , .17) or (.016 , .024) is not the main point, I think.</p>",2019-09-10T08:51:30Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0dlirx1j82eh,2019-09-10T08:51:30Z,{},project1
1303,no,"<p>I re-read the paper a few more times and each observation is a step, then each episode is a sequence, and then the training sets are just a collection of repeatedly doing the episodes.</p>
<p></p>
<p>observation: D:0 (observation : $$x_{t,i}$$)</p>
<p>Sequence: BCBCDE:1 (Episode : $$X_t$$)</p>
<p>Training Set: $$X$$</p>
<p></p>
<p>$$\alpha = f(\lambda)$$ in the range (0.0, 1.0], but notably closer to 0 than 1.</p>",2019-09-11T00:12:32Z,62,Week 9/8 - 9/14,feedback,,jc554vxmyuy3pt,k0eif8a1feb4s9,2019-09-11T00:12:32Z,{},project1
1304,no,"<p>I haven&#39;t gotten to that part yet; but I presume given the commentary that it is a markovian walk; he&#39;s saying that the likelihood of winning, is influenced heavily by both where you start, and how long you walk.   and the choice in parameters, inherently produces a bias.</p>
<p></p>
<p>I discussed something similar in channel last night; that is, you can get a false convergence (false V*) based on not having sufficient &#34;headspace&#34; for far (dist from start) goal states with high positive rewards.   this is both effected by your epsilon (which implies, given any fixed mdp, a certain number of iterations to hit that epsilon), vs the number of  states intervening between your start state, and the reward states (particularly a unique rmax).  mdp convergence is based on iterating into infinity; or rather reward discounting that effectively simulates this in a fixed horizon.</p>
<p></p>
<p>sutton picked markovian walks because the optimal values are known (there is another way to computer the iteration).</p>
<p></p>
<p>if you want to read something interesting; look at the genesis of why markovian walks were actually constructed, with respect to a then-nascent (statistical) theory.   It was a prod to the eye to contemporary mathematicians (a pointed counterexample, in a heated debate), similar to Charles Bool and Dodgeson, creating &#34;binary arithmetic&#34; (as useless to applied mathematics).</p>
<p></p>
<p>quite fascinating.</p>
<p>--</p>
<p></p>
<p>if he is not talking about a markovian walk (I presume he is based on his introduction on pg 19).... then ignore this comment.</p>",2019-09-09T16:47:59Z,62,Week 9/8 - 9/14,followup,,jzivtxcbl6964n,k0cn3osrowv1l9,2019-09-09T16:47:59Z,{},project1
1305,no,<p>Very nice graphs and analysis.  There can be some bias based on the training sets.  This is all very good stuff to discuss in your paper.</p>,2019-09-10T01:18:29Z,62,Week 9/8 - 9/14,followup,,hz7meu55mi8sd,k0d5c7etuf2fv,2019-09-10T01:18:29Z,{},project1
1306,no,"<p>You may have had to increase the number of states (i.e. number of rolls you were considering.) I know I initially had this problem with higher N values as well, but they worked when I increased the number of states.</p>",2019-09-09T21:07:11Z,42,Week 9/8 - 9/14,followup,,jccyemecUB8q,k0cwd0u22604go,2019-09-09T21:07:11Z,{},hw1
1307,no,"<p>I did it. However, I am not sure where was my problem with the probability matrix. </p>
<p></p>
<p></p>",2019-09-10T00:13:26Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0d30jem1ur1za,2019-09-10T00:13:26Z,{},hw1
1308,no,"<p>My problem was just using the incorrect formula.</p>
<p></p>
<p>I had my initial formula messed up. Formula changes based on the two points below. I kept thinking it was my code problem since I was not using any mdptoolbox. But once I fixed it, things worked and converged </p>
<p></p>
<p> - r(s,a) vs r(s, a, s&#39;)</p>
<p> - ongoing rewards vs reward during quit</p>",2019-09-10T01:23:00Z,42,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0d5i0axpg83ot,2019-09-10T01:23:00Z,{},hw1
1309,no,"<p>I did it as reward as quitting, that&#39;s why I am concerned where did I took the wrong approach. Thanks , though.</p>",2019-09-10T02:34:40Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0d825vw78y5xn,2019-09-10T02:34:40Z,{},hw1
1310,no,"<p>I also used with zero rewards until we quit.  It pretty  much made the reward matrix empty except r(Si,quit) column, which is simply the number of the state.</p>
<p></p>
<p>If you got that right, my guess is that you didn&#39;t correctly initialize the probability matrix.  </p>
<p>It took me a while, but as soon as I got it right, mdptoolbox spewed the right answer.  </p>
<p></p>
<p>I tried with 2 or 3 rolls, and I got the same results.  </p>
<p></p>",2019-09-10T05:03:31Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0dddllwvi57a2,2019-09-10T05:03:31Z,{},hw1
1311,no,"<p>Since I wasn&#39;t using mdptoolbox, I simulated states that would be generated by 50-100 rolls since I wasn&#39;t sure what was causing my initial inaccurate result. Good to know that 2-3 rolls were enough with mdptoolbox</p>
<p></p>
<p>I was doing reward for quitting but also had reward inside the expectation, which was one of the problems</p>",2019-09-10T06:24:46Z,42,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0dga2xsn3nb8,2019-09-10T06:24:46Z,{},hw1
1312,no,"<p>Jean-Perre, that´s probably my mistake. I will double check the probability matrix.</p>
<p></p>
<p>I will try your approach as well Anand.</p>
<p></p>
<p>Thanks guy for your help.</p>
<p></p>
<p></p>",2019-09-10T16:03:20Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0e0y47wjpe7jo,2019-09-10T16:03:20Z,{},hw1
1313,stud,<p>We should go over a solution as a class.</p>,2019-09-10T02:57:02Z,42,Week 9/8 - 9/14,followup,a_0,,k0d8uxieu2823h,2019-09-10T02:57:02Z,{},hw1
1314,no,"<p>I&#39;m in. It is faster and you can learn from your mistakes. I do not even know which one is my MISTAKE.</p>
<p></p>
<p>Besides, I think for people who do not have a lot of time to check ALL the messages, it is important to summarize relevant information.</p>
<p></p>
<p></p>",2019-09-10T16:05:05Z,42,Week 9/8 - 9/14,feedback,,jzygktecb0e3i1,k0e10dq2cms1uc,2019-09-10T16:05:05Z,{},hw1
1315,no,"<p>is a probability transition matrix, considered code? I would imagine, that it would be (in the spirit, if not the text).  but I have to ask, for completeness sake.</p>",2019-09-10T05:30:09Z,42,Week 9/8 - 9/14,followup,,jzivtxcbl6964n,k0debuldqcs77q,2019-09-10T05:30:09Z,{},hw1
1316,no,"<p>I&#39;ve seen people post their transition and reward matrix.  </p>
<p>It wouldn&#39;t take a genius to figure out how the states were defined and how the matrices were built, but I was surprised to see the TA&#39;s let it fly.  </p>",2019-09-10T05:44:52Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0deurjeh4661x,2019-09-10T05:44:52Z,{},hw1
1317,no,"<p>I&#39;ve seen partial matrices being shared a couple of times, which is borderline what we want sharing. I haven&#39;t seen complete matrices, do PM where they are, and we will address them. We don&#39;t think there is value in sharing the solution to the homework. There is value in sharing concepts, etc. But a solution is not solving the homework and students that weren&#39;t able to solve the homework should not get a complete solution; they should get to solve it, instead. Anyway. Please, don&#39;t share it.</p>
<p></p>
<p>You are free and encouraged to discuss other things like graphs, designs, and so on — no need to show your trophies.</p>
<p></p>
<p></p>",2019-09-10T22:26:29Z,42,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0eemv59d0i1p,2019-09-10T22:26:29Z,{},hw1
1318,no,"<p>This is a really nice way to put it!</p>
<p>I also want to add: if you struggle with some concepts, please do ask your question. Many times I thought I understood something well, until other people asked related questions and I couldn&#39;t answer it, then I realized I had not understood it as well as I thought. Your questions help us reinforce our learning as well.</p>",2019-09-11T00:20:14Z,42,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0eip5gjysk7jl,2019-09-11T00:20:14Z,{},hw1
1319,no,"<p>I&#39;m not sure that the student answer is correct.  If the initial value of the state we are estimating is $$0$$, which it is in all of the homework cases and example cases, then yes, the $$\alpha$$&#39;s do indeed cancel out when you equate $$TD(1)$$ to $$TD(\lambda)$$.  But in general, for non-zero $$V(S_T)$$, I don&#39;t think this would be true.</p>",2019-09-11T04:38:32Z,32,Week 9/8 - 9/14,followup,,jzfsa4a37jf4aq,k0erxb9rub7c,2019-09-11T04:38:32Z,{},hw2
1320,no,"<p>You are correct Vahe. The learning rate is not needed because you are not being asked to learn the value function!!!</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhyx9thiqa6j4nn%2Fk0es6keof4u3%2FCS_7642.png"" alt="""" width=""794"" height=""477"" /></p>
<p>The &#34;estimator&#34; is something that is used to estimate, in particular, it estimates the value function. It&#39;s the target. In the box above without the $$-V(S_t)$$, otherwise, that&#39;s called the error, not target.</p>
<p></p>
<p>The value function $$V(S_t)$$ can be estimated using one-step targets: $$V_{t&#43;1}(S_t) = V_t(S_t) &#43; \alpha_t \Bigg[ \overbrace{\underbrace{R_{t&#43;1} &#43; \gamma V_t(S_{t&#43;1})}_{\substack{\text{TD} \\ \text{target}}}- V_t(S_t)}^{\substack{\text{TD} \\ \text{error}}} \Bigg]$$</p>
<p></p>
<p>The value function $$V(S_t)$$ can be estimated using infinite-step targets: $$V_{t&#43;1}(S_t) = V_t(S_t) &#43; \alpha_t \Bigg[ \overbrace{\underbrace{G_{t:T}}_\text{MC target} -\ \ V_t(S_t)}^\text{MC error} \Bigg]$$</p>
<p></p>
<p>In general, the value function $$V(S_t)$$ can be estimated using n-step targets: $$V_{t&#43;n}(S_t) = V_{t&#43;n-1}(S_t) &#43; \alpha_t \Bigg[ \overbrace{\underbrace{G_{t:t&#43;n}}_{\substack{\text{n-step} \\ \text{target}}} - V_{t&#43;n-1}(S_t)}^{\substack{\text{n-step} \\ \text{error}}} \Bigg]$$</p>
<p></p>
<p>The n-step target is calculated like so: $$G_{t:t&#43;n} = R_{t&#43;1} &#43; ... &#43; \gamma^{n-1} R_{t&#43;n} &#43; \gamma^n V(S_{t&#43;n})$$</p>
<p></p>
<p>So a one: $$G_{t:t&#43;1} = R_{t&#43;1} &#43; \gamma V_t(S_{t&#43;1})$$ &lt;--- TD(0)/TD/E1</p>
<p></p>
<p>A two: $$G_{t:t&#43;2} = R_{t&#43;1} &#43; \gamma R_{t&#43;2} &#43; \gamma^2 V_{t&#43;1}(S_{t&#43;2})$$ </p>
<p></p>
<p>A three: $$G_{t:t&#43;3} = R_{t&#43;1} &#43; \gamma R_{t&#43;2} &#43; \gamma^2 R_{t&#43;3} &#43; \gamma^3 V_{t&#43;2}(S_{t&#43;3})$$</p>
<p></p>
<p>...</p>
<p></p>
<p>An inf: $$G_{t:T} = R_{t&#43;1} &#43; \gamma R_{t&#43;2} &#43; ... &#43; \gamma^{T-1} R_T$$ &lt;--- TD(1)/MC/Einf</p>
<p></p>
<p>And in fact, you can use something called a lambda return to estimate the value function $$V(S_t)$$: $$V_T(S_t) = V_{T-1}(S_t) &#43; \alpha_t \Bigg[ \overbrace{\underbrace{G^\lambda_{t:T}}_{\substack{\lambda\text{-return}}} - V_{T-1}(S_t)}^{\lambda-\text{error}} \Bigg]$$</p>
<p></p>
<p>How do you calculate the lambda return? This way: $$G^\lambda_{t:T} = \underbrace{(1-\lambda) \sum_{n=1}^{T-t-1}{\lambda^{n-1} G_{t:t&#43;n}}}_{\substack{\text{Sum of weighted returns} \\ \text{from 1-step to T-1 steps}}} &#43; \underbrace{ \lambda^{T-t-1} G_{t:T}}_{\substack{\text{Weighted} \\ \text{final return (T)}}}$$</p>
<p></p>
<p>Basically, using all the targets and mixing them with $$\lambda$$ as in the equation above.</p>
<p></p>
<p>This should do for now...</p>",2019-09-11T04:56:49Z,32,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0esktnidl17kw,2019-09-11T04:56:49Z,{},hw2
1321,no,<p>Thanks Miguel!</p>,2019-09-11T05:29:33Z,32,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0etqxpgxx76m3,2019-09-11T05:29:33Z,{},hw2
1322,no,<p>Np!</p>,2019-09-11T19:51:09Z,32,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0foixzk6ez172,2019-09-11T19:51:09Z,{},hw2
1323,no,"<p>thanks all, this makes sense. So when you add these terms together, though it seems to be boiling down to solving a fifth degree polynomial equation on lambda. Is that right?</p>",2019-09-12T16:37:07Z,32,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0gx1a3n3l71ss,2019-09-12T16:37:07Z,{},hw2
1324,no,<p>What do you mean by equate TD(λ) to TD(1)? Wouldn&#39;t that just mean setting λ = 1?</p>,2019-09-13T16:54:12Z,32,Week 9/8 - 9/14,feedback,,jqqssjikI511,k0id339wfx71hi,2019-09-13T16:54:12Z,{},hw2
1325,no,"<p>No, that would be equating $$\lambda$$ to $$1$$.</p>",2019-09-13T17:50:13Z,32,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0if3552l15228,2019-09-13T17:50:13Z,{},hw2
1326,no,"<p>Why does the final return at step T doesn&#39;t have (1-lambda) term? </p>
<p></p>
<p>In homework, I suppose what we have been discussing about (E1, ... Ek) is the (G1, G2, ... Gk). Which E is the final step? I think it&#39;s E5 but it doesn&#39;t work out. </p>",2019-09-14T05:42:38Z,32,Week 9/8 - 9/14,feedback,,j6ln9puq99s5uv,k0j4jarg80b6c5,2019-09-14T05:42:38Z,{},hw2
1327,no,"<p>For this homework purpose, can someone share how to calculate TD(1), which is estimated by E6? </p>
<p>I have this. Looks like they are not correct. </p>
<p>E5 = p*(R[0]&#43;R[2]) &#43; (1-p)*(R[1]&#43;R[3]) &#43; R[4] &#43; R[5] &#43; R[6] &#43; V[6]<br />E6 = p*(R[0]&#43;R[2]) &#43; (1-p)*(R[1]&#43;R[3]) &#43; R[4] &#43; R[5] &#43; R[6] &#43; 0 &#43; V[6]</p>",2019-09-14T06:13:53Z,32,Week 9/8 - 9/14,feedback,,j6ln9puq99s5uv,k0j5nhk52665on,2019-09-14T06:13:53Z,{},hw2
1328,no,<p>Wouldn&#39;t TD(1) be estimated by E5 ? We can only have 5 transitions in HW2 MDP ?</p>,2019-09-14T08:54:43Z,32,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0jbebps6hg370,2019-09-14T08:54:43Z,{},hw2
1329,no,<p>TD(1) is the <strong>return </strong>(for this problem with our settings of $$\gamma$$ and $$\alpha$$).  The return does not bootstrap on the value estimate of any states.</p>,2019-09-14T16:52:05Z,32,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jsg8499ijzo,2019-09-14T16:52:05Z,{},hw2
1330,no,"<p>Thanks Vahe. Sorry little bit confused.</p>
<p></p>
<p>Does it mean TD(1) = Gt = E6 ?</p>",2019-09-14T20:02:30Z,32,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0jz93ne9r21b5,2019-09-14T20:02:30Z,{},hw2
1331,no,"<p>Without directly answering your question, I&#39;ll say that</p>
<p></p>
<p>$$E_k$$, applied to a particular state, sums the first $$k$$ rewards received upon leaving that state then stops and uses whatever the state value estimate is for the state it has reached.</p>
<p></p>
<p>The $$G_t$$ of a particular state is the sum of all rewards upon leaving that state and transitioning until it reaches a terminal state.</p>
<p></p>
<p>TD(1) is just the return, yes.</p>",2019-09-14T20:39:26Z,32,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0k0klxnk8j5gz,2019-09-14T20:39:26Z,{},hw2
1332,no,"<p>In particular, #2 needs a breakdown.</p>",2019-09-10T01:37:38Z,62,Week 9/8 - 9/14,followup,,hyx9thiqa6j4nn,k0d60tvn8dj2f,2019-09-10T01:37:38Z,{},hw2
1333,no,"<p>Can I have some help breaking down #2? </p>
<p></p>
<p>I understand that TD(λ) is a weighted measure of each k-step estimator, but I&#39;m unsure how to actually solve for λ. If it doesn&#39;t converge, what exactly are we looking for in this infinite sum? </p>
<p></p>
<p>Suppose I&#39;ve calculated E<sub>1 </sub>- E<sub>4</sub>, like we&#39;re given in &#64;155. I could write</p>
<p></p>
<p>TD(1) = (1 - λ)(λ<sup>1-1</sup>)E<sub>1 </sub>&#43; (1 - λ)(λ<sup>2</sup><sup>-1</sup>)E<sub>2</sub><sub> </sub>&#43; (1 - λ)(λ<sup>3</sup><sup>-1</sup>)E<sub>3</sub><sub> </sub>&#43; (1 - λ)(λ<sup>4</sup><sup>-1</sup>)E<sub>4</sub><sub> </sub></p>
<p></p>
<p>and solve for λ after calculating TD(1). But this is only because I&#39;ve used a finite amount for k. At k = infinity, how do we solve for λ?</p>",2019-09-10T18:03:33Z,62,Week 9/8 - 9/14,followup,,jl284xdcifz44g,k0e58q3d709444,2019-09-10T18:03:33Z,{},hw2
1334,no,"<p>I&#39;d say your equation is missing terms.  An obvious one is $$E\infty$$ which is also TD(1).</p>
<p></p>",2019-09-10T18:41:20Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0e6lbiszku1sm,2019-09-10T18:41:20Z,{},hw2
1335,no,"<p>I guess that&#39;s what I&#39;m trying to understand, how many terms do we need in the summation of (1 - λ)(λ<sup>k-1</sup>)E<sub>k </sub>to accurately calculate TD(λ)? </p>",2019-09-10T19:22:41Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0e82hk0arf58f,2019-09-10T19:22:41Z,{},hw2
1336,no,"Wait, if E_inf = TD(1), can we do some trickery involving subtracting TD(1) from each side?",2019-09-10T20:07:24Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0e9nzjbiw830y,2019-09-10T20:07:24Z,{},hw2
1337,no,"<p>Sure, but the coefficient of $$E\infty$$ in TD(λ) is not one. </p>
<p>I&#39;d say the &#39;trickery&#39; is not substracting one from it, it&#39;s finding it.  </p>
<p>I would also ask myself about E5, E6, E7, E8... </p>",2019-09-10T20:30:54Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0eai7pc91r1o,2019-09-10T20:30:54Z,{},hw2
1338,no,<p>JPB has it.</p>,2019-09-10T22:20:52Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0eefn2xc1a265,2019-09-10T22:20:52Z,{},hw2
1339,no,"<p>Thanks for the hint &#64;JPB.  I think I see some series, which can be truncated. </p>",2019-09-11T02:47:18Z,62,Week 9/8 - 9/14,feedback,,jl8j7vzvUNs2,k0eny9qgmeq3kn,2019-09-11T02:47:18Z,{},hw2
1340,no,Can someone shed a little more light on this series? I&#39;m looking over my notes but I don&#39;t understand how to rewrite the equations to solve for lambda,2019-09-11T03:51:11Z,62,Week 9/8 - 9/14,feedback,,jl284xdcifz44g,k0eq8fjiorn6z0,2019-09-11T03:51:11Z,{},hw2
1341,no,"<p>Try partitioning the series (the weighted sum of estimators) into two separate sums: one that includes estimators that bootstrap, and one that doesn&#39;t.</p>",2019-09-11T04:09:24Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eqvutezq31rj,2019-09-11T04:09:24Z,{},hw2
1342,no,<p>&#64;218</p>,2019-09-11T05:14:11Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0et75zan8m11o,2019-09-11T05:14:11Z,{},hw2
1343,no,"<p>Any suggestion on how to solve equation below</p>
<p> TD($$\lambda $$) = TD(1)</p>
<p>TD(1) = (1-$$\lambda $$)E1 &#43; (1-$$\lambda $$)$$\lambda $$ E2  &#43; (1-$$\lambda $$)$$\lambda $$^2 E3.... (1-$$\lambda $$)$$\lambda $$^n-1E infinity</p>
<p></p>",2019-09-14T03:04:31Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0iyvz33ndm67r,2019-09-14T03:04:31Z,{},hw2
1344,no,"<p>Use algebra and multiple out the equation. Then try to isolate the term you are solving for. As others have mentioned in &#64;256, then you end up with a polynomial that you can solve for.</p>",2019-09-14T03:53:35Z,62,Week 9/8 - 9/14,feedback,,ixty1midfufhd,k0j0n2pp4zbjz,2019-09-14T03:53:35Z,{},hw2
1345,no,"<p>Actually, I listened to David Silver&#39;s ch4 on TD, and he explains that TD1 <span style=""text-decoration:underline"">is</span> Monte Carlo.  </p>
<p>It&#39;s a very interesting video.  90&#39; well invested.  </p>",2019-09-10T04:49:48Z,42,Week 9/8 - 9/14,followup,,jzh6k6o994a6dh,k0dcvyrpvmq2pz,2019-09-10T04:49:48Z,{},hw1
1346,no,<p>I&#39;ll have to check that out. I did see that in my research online about TD1 estimates. Essentially it&#39;s because both TD1 and MC do not update until the end of an episode. But it seemed like with Value Iteration we are moving through the tree of states _all at once_ and updating in real time. </p>,2019-09-10T16:23:58Z,42,Week 9/8 - 9/14,feedback,,jzkifxpxau95an,k0e1onn6nge1jv,2019-09-10T16:23:58Z,{},hw1
1347,no,"<p>Yes, we do move through all the states at once.</p>
<p>But Silver explains that the advantage of TD is that you can update at each iteration without waiting to be done.</p>
<p>He says it&#39;s an advantage because it allows to learn in cases such as when there&#39;s no termination at all, ie the learning process is interrupted.  </p>",2019-09-10T17:01:53Z,42,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0e31f16hgz7a3,2019-09-10T17:01:53Z,{},hw1
1348,no,"<p>A good discussion of the advantages of TD vs Monte Carlo is in B&amp;S, Example 6.1 and Section 6.2 (starting on page 122).</p>",2019-09-11T00:18:25Z,42,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eimsn9u1v560,2019-09-11T00:18:25Z,{},hw1
1349,no,<p>&#64;Timothy Bail.  Thanks I just wanted to make sure I was understanding how it should labeled based on the parameters given.</p>,2019-09-10T01:26:08Z,62,Week 9/8 - 9/14,followup,,is9so9huTMp,k0d5m1cgx8xf9,2019-09-10T01:26:08Z,{},hw2
1350,no,"<p>You need the MDP, but only to calculate the k-step estimators.</p>",2019-09-10T01:32:42Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0d5uh6dq2235,2019-09-10T01:32:42Z,{},hw2
1351,no,<p>Got it... I just had to make sure I was interpreting the MDP correctly.  Thanks everyone!</p>,2019-09-10T02:00:54Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0d6uqztacm4wi,2019-09-10T02:00:54Z,{},hw2
1352,no,<p>Got ya. You are!</p>,2019-09-10T02:13:18Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0d7ap6myjz58m,2019-09-10T02:13:18Z,{},hw2
1353,no,"<p>&#64;Nick It may help, for understanding purposes, to realize that in this particular MDP, there is only one action to take from each state.  Another way of looking at it, and a more realistic scenario, is that the policy for some original MDP has been given to us, and the above graph is what the MDP gets truncated to if we have to follow <em>this</em> particular policy.  So edges between states corresponding to actions that don&#39;t follow our policy have been removed.</p>",2019-09-10T22:24:47Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eekohjyni6sw,2019-09-10T22:24:47Z,{},hw2
1354,no,"<p>I&#39;m confused on the value estimates given for the examples. Are these calculated or given? If calculated, I&#39;m confused how this is done</p>",2019-09-13T02:19:45Z,62,Week 9/8 - 9/14,followup,,j6m1jeidndu6wq,k0hhujw18cb2vw,2019-09-13T02:19:45Z,{},hw2
1355,no,"<p>Given, not calculated</p>",2019-09-13T02:34:01Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0hicvshd0v7nm,2019-09-13T02:34:01Z,{},hw2
1356,no,"<p>&#64;jacob... thanks for the thanks, but I wonder, is this the first time you see a TD(λ) curve?</p>
<p>Because they all should look like the first one.  </p>
<p>If you haven&#39;t, I recommend video4 from David Silver who explains quite in depth TD(λ), or, of course the Sutton 1988 paper on which project 2 is based.  </p>
<p></p>
<p>I tried to submit one answer to the grader, and it was accepted  !!?? </p>
<p>I&#39;ll stop for now because it really annoys me the plots are so wrong.  </p>
<p></p>
<p>Thanks for your suggestion, but I can&#39;t wrap my mind around this for now because the equation is full of λ&#39;s and rewards and estimates that the slightest mistake should deeply modify TD(λ) (and the graphs are witness to that I guess) and YET, that graph still crosses TD1 at exactly the right place.  ??? And at λ = 1 on top of that.  </p>
<p></p>
<p>Maybe the graphs are not wrong after all.  I need to take a look at that tomorrow.  </p>
<p></p>
<p></p>",2019-09-10T04:45:07Z,37,Week 9/8 - 9/14,followup,,jzh6k6o994a6dh,k0dcpxfnwwd752,2019-09-10T04:45:07Z,{},hw2
1357,no,<p>Expand the finite TD($$\lambda$$) formula. You will see that it is a polynomial. The different values and rewards (coefficients) in the formula will determine the polynomial&#39;s degree...</p>,2019-09-10T14:27:43Z,37,Week 9/8 - 9/14,feedback,,jc554vxmyuy3pt,k0dxj5hi8w26v,2019-09-10T14:27:43Z,{},hw2
1358,no,"<p>Thanks &#64;Jacob, I know it&#39;s a polynomial, and I can find its root, and it&#39;s the right answer.</p>
<p>What I can&#39;t figure out, is why when I plot TD(λ), I&#39;m not getting the typical curve like the first one.</p>
<p></p>
<p>Would you be kind enough and print TD(λ) for the second test in the exercise to see if you come up with something like me?</p>",2019-09-10T15:30:20Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0dzroj2y0t3os,2019-09-10T15:30:20Z,{},hw2
1359,no,"<p>They&#39;re &#34;atypical&#34; because of the effect of correspondingly &#34;atypical&#34; state value estimates.</p>
<p></p>
<p>Remember that $$\lambda$$ determines how much weight we&#39;re assigning to the different estimators.  For values of $$\lambda$$ near $$0$$, where we are heavily weighting short-lookahead estimators, we will over-represent the short-lookahead state value estimates - those that are close to state $$0$$.  For values of $$\lambda$$ closer to $$1$$, we will more evenly weight <strong>all</strong> the rewards and state value estimates.</p>
<p></p>
<p>So, if our particular reward/value estimate structure is such that there are large positive (negative) state value estimates for early states that don&#39;t match the reward structure of the MDP, then early values of $$\lambda$$ will be over-inflated (under-inflated).</p>
<p></p>
<p>Edit:  I think maybe the confusion comes from the fact that your plots are different than the plots in Sutton&#39;s paper.  His plots are for error vs $$\lambda$$.  Your plots are for state value estimate vs $$\lambda$$.</p>",2019-09-10T23:06:36Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0eg2g8f99j128,2019-09-10T23:06:36Z,{},hw2
1360,no,"<p>Haaaa, yes, he plots the errors, silly me!</p>
<p>So my plots were good then, mystery solved.  </p>
<p></p>
<p>Thanks a lot.</p>
<p></p>",2019-09-14T04:26:07Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0j1swyh6pf3el,2019-09-14T04:26:07Z,{},hw2
1361,no,<p></p>,2019-09-10T05:28:15Z,37,Week 9/8 - 9/14,followup,,j6ll2xkiDJf,k0de9e53sco5qj,2019-09-10T05:28:15Z,{},hw2
1362,no,<p></p>,2019-09-10T02:05:13Z,62,Week 9/8 - 9/14,followup,,ixpclzk97jg2fo,k0d70algbry4t5,2019-09-10T02:05:13Z,{},project1
1363,no,"<p>Hey,</p>
<p></p>
<p>Some of my lambda and alpha combinations also blow up. In general I get the same shape as Sutton, but with lower alpha&#39;s. Its as if I took Suttons graph and compressed it along the x-axis:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl3we43d3bp15p%2Fk0lu4bdnm7u6%2Ffigure_02.png"" alt="""" /></p>
<p></p>
<p>Not sure how some of his values were able to converge to RMS &lt; 1 for alpha &gt; 0.5.</p>",2019-09-16T03:15:08Z,61,Week 9/15 - 9/21,followup,,jl3we43d3bp15p,k0lu5bv2akn2co,2019-09-16T03:15:08Z,{},project1
1364,no,"<p>I get a similar thing when looking at lambda = 0</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzjr6qef6i06tp%2Fk0m0cwez5oiw%2Ffigure_4.png"" alt="""" /></p>
<p>Here is what my total graph looks like </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzjr6qef6i06tp%2Fk0m0oiojzptc%2Ffigure_4_all.png"" alt="""" /></p>
<p></p>
<p>Anybody have any ideas?</p>
<p></p>",2019-09-16T06:09:40Z,61,Week 9/15 - 9/21,followup,,jzjr6qef6i06tp,k0m0ds7enl25nu,2019-09-16T06:09:40Z,{},project1
1365,no,<p>did you solve this?</p>,2019-09-22T23:11:30Z,60,Week 9/22 - 9/28,feedback,,jc6mqevhagl262,k0vlizd3foy5b2,2019-09-22T23:11:30Z,{},project1
1366,no,<p>See what happens when you limit the length of your sequences.</p>,2019-09-24T15:42:08Z,60,Week 9/22 - 9/28,feedback,,is8ald0uljj3u4,k0y0csn1rxb6el,2019-09-24T15:42:08Z,{},project1
1367,no,"<p>Are you all using an alpha equals to zero for the first point in the figure 4? If we use alpha=0, all the weights are not updated and the rms error would be much bigger than 0.2.</p>",2019-09-19T03:06:24Z,61,Week 9/15 - 9/21,followup,,jc6xvgjncoey,k0q45myu4ww55m,2019-09-19T03:06:24Z,{},project1
1368,no,<p>I was not calculating RMSE right.</p>,2019-09-19T03:52:26Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0q5su9f7fj54t,2019-09-19T03:52:26Z,{},project1
1369,no,<p>What do you mean by this? It looks like Figure 4 of the paper starts at zero. Did you not?</p>,2019-09-23T15:42:44Z,60,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0wkxp4t2mt1mm,2019-09-23T15:42:44Z,{},project1
1370,no,"<p>I have the same issue. Anyone has any suggestion on solving this? Thanks</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9puq99s5uv%2Fk116dtiblk5j%2Ffigure4.png"" alt="""" /></p>",2019-09-26T22:42:27Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k11a90guy9964m,2019-09-26T22:42:27Z,{},project1
1371,no,<p>discussion in &#64;307 might help you</p>,2019-09-29T18:47:26Z,59,Week 9/29 - 10/5,feedback,,j6la9jim7e23dq,k15c6cgf5715vu,2019-09-29T18:47:26Z,{},project1
1372,no,<p>I wish I&#39;d seen this a month ago.  :-)</p>,2019-10-11T19:30:12Z,58,Week 10/6 - 10/12,followup,,jqmfuaidej9155,k1mizkge7lj5ke,2019-10-11T19:30:12Z,{},hw2
1373,no,<p>:-)</p>,2019-10-11T20:49:02Z,58,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1mlsy37vob5ye,2019-10-11T20:49:02Z,{},hw2
1374,no,<p>I believe it&#39;s because we are essentially counting the number of times we have seen that state. So over time we are getting those observations so we can calculate the TD(1) estimate. </p>,2019-09-10T16:21:06Z,62,Week 9/8 - 9/14,followup,,jzkifxpxau95an,k0e1kyrd7c24qf,2019-09-10T16:21:06Z,{},hw2
1375,no,"<p>Thanks, but I still don&#39;t know why it updated to <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9q5kphc5uz%2Fk0e4fqri60sh%2FScreen_Shot_20190910_at_12.38.04_PM.png"" alt="""" width=""14"" height=""21"" /> if e(s) means the number of times we have seen that state. </p>",2019-09-10T17:42:09Z,62,Week 9/8 - 9/14,feedback,,j6ln9q5kphc5uz,k0e4h76weue766,2019-09-10T17:42:09Z,{},hw2
1376,no,<p>I&#39;m still digesting the lecture as well so I might be wrong. But we updated eligibility to 1 and since we were multiplying all of that by $$\gamma$$ then $$\gamma * 1 = \gamma$$. </p>,2019-09-10T17:45:09Z,62,Week 9/8 - 9/14,feedback,,jzkifxpxau95an,k0e4l2kbrng2on,2019-09-10T17:45:09Z,{},hw2
1377,no,<p>I think I need to digesting the lecture as well:) Thank you! </p>,2019-09-10T17:47:41Z,62,Week 9/8 - 9/14,feedback,,j6ln9q5kphc5uz,k0e4oboc8uoqv,2019-09-10T17:47:41Z,{},hw2
1378,no,"<p>$$e(s)$$ takes into account not <em>just</em> the number of times we have seen state $$s$$, but also how long ago we have seen it.  Adding $$1$$ accounts for the former variable, multiplying by $$\gamma$$ accounts for the latter.</p>",2019-09-10T22:08:03Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0edz5bqtj444x,2019-09-10T22:08:03Z,{},hw2
1379,no,<p>Thanks! I think it looks like the discounted rate for each state we have seen. </p>,2019-09-11T03:12:20Z,62,Week 9/8 - 9/14,feedback,,j6ln9q5kphc5uz,k0eougyc7kzrs,2019-09-11T03:12:20Z,{},hw2
1380,stud,<p>I scoured piazza to confirm that we needed to use the Estimates. also noticed the &#34;best&#34; lambda. You are not alone.</p>,2019-09-12T06:28:45Z,37,Week 9/8 - 9/14,followup,a_0,,k0gbawwg72r222,2019-09-12T06:28:45Z,{},hw2
1381,no,"<p>I will address these issues with the documents. Thanks for pointing out ways to make the instructions better. If you guys have comments for HW1, please create a new post! I&#39;m always up for clarifying instructions and helping out future students.</p>",2019-09-12T12:50:17Z,37,Week 9/8 - 9/14,followup,,hyx9thiqa6j4nn,k0goxkcop9k4di,2019-09-12T12:50:17Z,{},hw2
1382,no,"<p>Another thing in the hw2 description that confused me for a second about what to do is that since $$V(S_{t&#43;k})$$ and $$E_k$$ are usually considered random variables, $$\text{TD}(\lambda)$$, the weighted sum, is expected to be a random variable, too. We are actually asked to work on the expected value of $$\text{TD}$$. I thought it might be clearer to explicitly say it, i.e. we are to find $$\lambda$$, such that $$E[\text{TD}(\lambda)]=E[\text{TD}(1)]$$. Just a tiny thing. </p>
<p></p>",2019-09-14T04:35:21Z,37,Week 9/8 - 9/14,feedback,,jzhs489b10i78c,k0j24rr41z616,2019-09-14T04:35:21Z,{},hw2
1383,stud,<p></p>,2019-09-14T04:45:43Z,37,Week 9/8 - 9/14,feedback,a_1,,k0j2i3v58om5lh,2019-09-14T04:45:43Z,{},hw2
1384,no,<p>Hmm. Valid point.</p>,2019-09-14T04:46:03Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0j2ijglfn3wi,2019-09-14T04:46:03Z,{},hw2
1385,no,"<p>I agree with this, see: &#64;259 . </p>
<p></p>
<p>I suspect a concern was to not overcomplicate the statement of the problem / the notation, since perhaps different people have different comfort levels with probability theory, but handling the stochasticity in HW2 gets put on a very precise foundation if we just call those variables out for what they are, random variables.</p>",2019-09-14T04:59:55Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0j30dstpty6i0,2019-09-14T04:59:55Z,{},hw2
1386,no,"<p>It&#39;s really awesome to see the instructors are taking student feedback seriously.  Even better, they are considering that feedback during the course rather than having to wait until the end of the course.  It&#39;s as if they are using $$TD(\lambda )$$  rather than $$TD(1)$$ to make the course better.</p>
<p></p>
<p>Brilliant!</p>
<p></p>",2019-09-15T20:53:54Z,36,Week 9/15 - 9/21,feedback,,is9so9huTMp,k0lgj1rqwp23sk,2019-09-15T20:53:54Z,{},hw2
1387,no,"<p>Sutton absolutely plots the error for the best α for each of the λs:<br /><br /><em>Figure 5 plots the best error level achieved for each λ value, that is, using </em><em>the α value that was best for that λ value.</em></p>",2019-09-13T04:55:33Z,59,Week 9/8 - 9/14,followup,,jl3oi5v7qkSk,k0hnew7s5pd5s3,2019-09-13T04:55:33Z,{},project1
1388,no,"<p>Figure 5 and Figure 3 have different test set-ups. For figure 3 Sutton does whatever he needs to do to get to convergence, and I agree with Jacob that there is no reason to believe he had any restrictions on $$\alpha$$ for generating that graph.  $$\alpha$$ doesn&#39;t matter if you&#39;re running to convergence.</p>",2019-09-13T05:55:30Z,59,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0hpk03ao9w4nn,2019-09-13T05:55:30Z,{},project1
1389,no,<p>Thanks for finding that needle in the haystack of Sutton 1988!</p>,2019-09-13T15:03:13Z,59,Week 9/8 - 9/14,feedback,,jc554vxmyuy3pt,k0i94dblxf31b1,2019-09-13T15:03:13Z,{},project1
1390,no,"<p>I can&#39;t imagine how much money Rich Sutton spent on this paper. He did a grid search on alpha and convergence on lambda during that grid search, likely on a Cray XMP because his tiny Sparc station didn&#39;t have a vector processor and it ran at a paltry speed. That&#39;s likely why he is so memory sensitive in his algorithms, because the Cray time would also limit how much memory he could use, in megabytes (ugh).</p>
<p></p>
<p>I wrote a recursive algorithm as an undergrad that ran on an XMP one night. That cost our group $10k over 6 hours. Sutton 1988 must have been more...</p>
<p></p>
<p>He could have written out his algorithms a little better, though. In hindsight, everyone&#39;s a critic...</p>",2019-09-13T15:07:03Z,59,Week 9/8 - 9/14,feedback,,jc554vxmyuy3pt,k0i99aj9okv5zz,2019-09-13T15:07:03Z,{},project1
1391,no,<p>One follow-up for the report. I see &#34;The rubric includes a few points for formatting.&#34; but I cannot seem to find the rubric anywhere. Can someone please link to the rubric and/or formatting guidelines? </p>,2019-09-21T17:44:40Z,40,Week 9/15 - 9/21,followup,,jqr9leosvw8P,k0tuet67mtq1os,2019-09-21T17:44:40Z,{},project1
1392,no,"<p>We do not release a formal rubric with the points breakdown.   The reference you are referring to is about the readability of the report.  We normally do not deduct those points unless the paper is really disjointed, font too small, graphs unreadable, etc.</p>",2019-09-22T00:21:27Z,39,Week 9/22 - 9/28,feedback,,hz7meu55mi8sd,k0u8l33zlwx31p,2019-09-22T00:21:27Z,{},project1
1393,no,<p>Is week four 9/16 - 9/20 or is it this past week 9/9 - 9/13?</p>,2019-09-14T17:23:17Z,62,Week 9/8 - 9/14,followup,,gx3c8l7z7r72zl,k0jtkceouj05ct,2019-09-14T17:23:17Z,{},logistics
1394,no,<p>This past week.</p>,2019-09-14T18:28:31Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jvw8ms5d662t,2019-09-14T18:28:31Z,{},logistics
1395,no,"<p>Having also mulled potential applications of the principles we&#39;re learning in this class, I get the feeling that it&#39;s premature to offer advice on real-world RL use.  Put another way, I feel like we&#39;re just starting to learn the framework for RL, and that anything I said now would pale in quality to what I would be able to say a month from now, or two months from now, etc.  I think after actually applying RL principles to the learning of an agent to maximize some kind of non-trivial goal (i.e. a reasonably complex project), I&#39;d be in a much better position to make valuable contributions to this kind of discussion.  I&#39;m sure this is true for most of the other students as well.</p>
<p></p>
<p>I know this doesn&#39;t answer your question, but I&#39;ve been telling myself to just keep my head down, learn, and wait for a few months before trying to apply this stuff.</p>",2019-09-11T16:37:30Z,62,Week 9/8 - 9/14,followup,,jzfsa4a37jf4aq,k0fhlwuj2635qs,2019-09-11T16:37:30Z,{},other
1396,no,"<p>Fair point.  Well, I guess this post will live forever so maybe people can check back on it in a couple months and we can see what we learned.</p>",2019-09-11T16:42:07Z,62,Week 9/8 - 9/14,feedback,,jl2d544u1wcE,k0fhrv0umv15xd,2019-09-11T16:42:07Z,{},other
1397,no,<p>Thanks!</p>,2019-09-11T23:44:57Z,62,Week 9/8 - 9/14,followup,,jl3we43d3bp15p,k0fwvme216c5zc,2019-09-11T23:44:57Z,{},project1
1398,no,"<p>&#64;Shayan, I think this means Sutton was going crazy... Thus the erratum section being required.  LOL</p>",2019-09-12T02:42:04Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g37e266xg27c,2019-09-12T02:42:04Z,{},project1
1399,no,"<p>Ok so alpha can be say 0.01</p>
<p>While $$P_{t&#43;1} $$ would be 1 if for that sequence led to one or 0 if that sequence lead to 0 right</p>
<p>And you are saying $$P_{t} $$ is simply the weight right?</p>
<p>Makes sense but the $$\nabla_wP_k $$ how can we do that?</p>",2019-09-11T18:38:26Z,62,Week 9/8 - 9/14,followup,,jzj7y1ofgsro1,k0flxg1qp774dv,2019-09-11T18:38:26Z,{},project1
1400,no,"<p>That&#39;s absolutely on possible value for alpha.</p>
<p>$$P_{t&#43;1}$$ is the prediction at every the next time step so no it is not simply 1 or 0, you are going over the entire sequence. It is the dot product between the transposed weight vector and the observation vector a time step t.</p>
<p>$$\nabla_{w}P_{k}$$ is the gradient (so the vector of partial derivates) with respect to w. Think about what P is a function of. I don&#39;t think I can just tell you how to do it, but it&#39;s not complicated to do.</p>",2019-09-11T19:33:22Z,62,Week 9/8 - 9/14,feedback,,jzjwcq2u8o7110,k0fnw2y2b844a5,2019-09-11T19:33:22Z,{},project1
1401,no,To calculate the gradient you&#39;ll take the partial derivative of P with respect to each component of w. If this feels unfamiliar or if calc was some time ago KhanAcademy has a short series on vector calc concepts that would be good to review.,2019-09-11T20:02:45Z,62,Week 9/8 - 9/14,feedback,,i4op5p9vfbq5yz,k0foxvloj33414,2019-09-11T20:02:45Z,{},project1
1402,no,<p>thanks </p>,2019-09-12T15:55:56Z,62,Week 9/8 - 9/14,feedback,,jzj7y1ofgsro1,k0gvkbcqbwx11q,2019-09-12T15:55:56Z,{},project1
1403,no,"<p>I understand that when the sequence leads to 1, P<sub>t</sub> = w<sup>T</sup>x<sub>t</sub></p>
<p></p>
<p>What is P<sub>t</sub> when the sequence leads to 0? Should it be (1 - w<sup>T</sup>x<sub>t</sub>)</p>",2019-09-14T13:06:38Z,62,Week 9/8 - 9/14,feedback,,jl2egn5k4zo4lp,k0jkea8q9q85us,2019-09-14T13:06:38Z,{},project1
1404,no,"<p>$$P_t$$ is always $$w^Tx_t$$ regardless of where the sequence leads to.   The difference comes only when you reach the end of an episode, where you receive a different reward for the two possible outcomes.</p>",2019-09-14T16:49:14Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jscjwry9z688,2019-09-14T16:49:14Z,{},project1
1405,no,"<p>With regards to the partial derivatives, i just watched the Khan videos and want to make sure i am understanding this correctly:</p>
<p></p>
<p>Give example weight of [.1666, .3333, .5, .6666, .8333,], example observation vector [0,0,1,0,0], and k of 2</p>
<p></p>
<p>P<sub>t</sub> = w<sup>T</sup>x<sub>t</sub></p>
<p><br />We are searching for the vector of partial derivatives with respect to w so the derivative would be 1*x<sub>k</sub></p>
<p></p>
<p>We get this because any example weight is reduced to 1 as it is solely a single variable and we would be left with the observation vector at time k. So either i don&#39;t understand why there is complicated math around this or i am missing some key ingredient?</p>
<p></p>
<p><span style=""vertical-align:-0.266em"">since</span></p>",2019-09-16T21:29:20Z,61,Week 9/15 - 9/21,feedback,,jzttp1ojahj6ju,k0mx8h31gec26a,2019-09-16T21:29:20Z,{},project1
1406,no,"<p>Yeah there&#39;s some unnecessary confusion.</p>
<p></p>
<p>First of all, numbers don&#39;t come into play when deriving the gradient because derivatives are taken with respect to the functions, not their values at any given time.  You can later evaluate a derivative at a value of its independent variable, but that&#39;s something different.</p>
<p></p>
<p>Secondly, you really need to remove the vector notation before taking the derivative.  So expand out</p>
<p></p>
<p>$$P_t = w^T\cdot x_t = w_1\cdot x_1 &#43; w_2\cdot x_2 &#43; w_3\cdot x_3 &#43; w_4\cdot x_4 &#43; w_5\cdot x_5$$, where each $$w_i \text{ and } x_i \text{ are the } i^{th}$$ components of their respective vectors.  As you can see $$P_t$$ is a function of ten independent variables</p>
<p></p>
<p>Now you can take the partial derivative of $$P_t$$ with respect to any of its five weight components, and the gradient is the vector formed by placing each of those five partial derivatives into the appropriate component.</p>
<p></p>
<p>For instance, one partial derivative would be</p>
<p></p>
<p>$$\frac{d}{dw_1} P_t = x_1$$.</p>",2019-09-16T22:36:57Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0mznf59oyd4rz,2019-09-16T22:36:57Z,{},project1
1407,no,"<p>Following this along:</p>
<p>$$ \Delta w_t = \alpha (w^{T} \textbf x_{t&#43;1} - w^{T} \textbf x_{t}) \textbf x_t$$</p>
<p>Where $$\textbf x_t$$ is our current state.</p>
<p></p>
<p>Is that equation correct?</p>
<p>Thanks to Vahe and the rest for clearly explaining it.</p>",2019-09-18T12:23:26Z,61,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0p8m50cogb1pt,2019-09-18T12:23:26Z,{},project1
1408,stud,"<p><em>&#34;Is that equation</em> correct?&#34;</p>
<p></p>
<p>Yes, it pretty much is. Note only that if we&#39;re applying the rule for weight updates to the following observation outcome sequence, $$x_{1}, x_{2}, x_{3}, ..., x_{m}, z$$, things will change slightly for $$t=m$$. When $$t=m$$, the $$t &#43; 1^{st}$$ entry in the observation-outcome sequence is the actual <em>outcome</em>, $$z$$, and not another observation vector (i.e., there is no $$x_{m&#43;1}$$). </p>
<p></p>
<p>So if you have $$m$$ observations, the <em>final</em> weight update will be $$\Delta w_{t=m} = \alpha(z - w^{T}x_{m})x_{m}$$. This should make intutive sense. What should we update the prediction corresponding to the state <em>immediately prior to</em> the actual outcome in the direction of? Well, we should update it in the direction of the actual outcome! That&#39;s the next thing that happens, after all. We see the outcome. But the outcome is just the outcome. It&#39;s a scalar (and is the &#34;ground truth&#34; for the sequence we just saw). So no need to take a dot product with a weight vector (you can&#39;t anyway), or fuss with approximating $$z$$. At the end of the sequence, you&#39;ve already <em>got</em> $$z$$. </p>",2019-09-19T15:34:59Z,61,Week 9/15 - 9/21,feedback,a_0,,k0quwc0qoy23lz,2019-09-19T15:34:59Z,{},project1
1409,no,"<p>&#64;Sriram and &#64;Vahe <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>Are we saying that when we take the ∇wPk  <span style=""vertical-align:-0.266em""></span>it reduces down to xt<span style=""vertical-align:-0.377em""></span>?</p>
<p></p>
<p>&#64;Sriram</p>
<p></p>
<p>in your:  Δwt=α(wTxt&#43;1−wTxt)xt what happened to for summation and lambda?</p>",2019-09-28T20:17:25Z,60,Week 9/22 - 9/28,feedback,,gx3c8l7z7r72zl,k13zy7myf322tc,2019-09-28T20:17:25Z,{},project1
1410,no,"<p>I had this same question myself, &#64;Dalton beat me to the point of asking it.</p>
<p></p>
<p>Is the &#34;loopback reward&#34; worth 0 because we don&#39;t see a reward given for r7?  For instance, the state diagram in HW2 has representations for rewards r0 - r6.  If we were to go beyond state 6, we would be essentially be obtaining r7 (which isn&#39;t given in our parameters) and thus it returns a reward of 0?  If that&#39;s the case, then we could go to k = 40 and we would be looping 34 times with a reward of 0 for each loopback into state 6.</p>
<p></p>
<p>Am I thinking about this correctly?  ^^^ seems pretty straight forward, I hope I&#39;m not over simplifying.</p>",2019-09-12T02:39:41Z,62,Week 9/8 - 9/14,followup,,is9so9huTMp,k0g34bprgip4g1,2019-09-12T02:39:41Z,{},hw2
1411,no,"<p>All terminal states in episodic MDPs are <em>defined</em> to loop back on themselves and grant no reward for that loop.  It&#39;s just a way of making the episodic task look like a continuous (infinite-horizon) task, where essentially nothing happens after you reach that terminal state.  But the important point is that once you reach a terminal state, there are no more rewards, by definition.</p>",2019-09-12T02:46:12Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0g3cptb8ql1vw,2019-09-12T02:46:12Z,{},hw2
1412,no,"<p>Thanks &#64;Vahe, that makes sense, it&#39;s good to have that understanding confirmed.</p>
<p></p>",2019-09-12T02:49:49Z,62,Week 9/8 - 9/14,feedback,,is9so9huTMp,k0g3hcx6l6t477,2019-09-12T02:49:49Z,{},hw2
1413,no,"<p>oh so we are saying all states after the terminal state retain the Gt value of the terminal state? Because the reward might be zero, but the Gt value would not be 0, rather will be the Gt value of the terminal state. Right?</p>",2019-09-13T07:36:59Z,62,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0ht6i25i9a2pp,2019-09-13T07:36:59Z,{},hw2
1414,no,<p>$$G_t$$ is defined as the discounted sum of all future rewards.  How could it be nonzero if you&#39;re guaranteed to never get a reward again?</p>,2019-09-13T07:43:11Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0htehnql6470i,2019-09-13T07:43:11Z,{},hw2
1415,no,"<p>But looking at the Gt formula posted above, Gt:t&#43;x = r<sub>t&#43;1</sub> &#43; r<sub>t&#43;2</sub> &#43; .. r<sub>t&#43;x</sub> &#43;...</p>
<p>r<sub>t&#43;1</sub> &#43; r<sub>t&#43;2</sub> ... could be non-zero</p>
<p>So what I mean to ask is the k step target  will retain the value of the last terminal step..</p>",2019-09-13T07:57:22Z,62,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0htwq29k9c2mh,2019-09-13T07:57:22Z,{},hw2
1416,no,"<p>I think you&#39;re getting tripped up on the concept of state values.  When doing lookahead that spans <em>past</em> the time at which you first reach the terminal state, then each new time-step after you reach it, it loops back on itself (remember, terminal states can be thought of as perpetually looping back on themselves with zero reward), and you enter a <em>new</em> state as far as the state values in the lookahead calculation are concerned, because those state values are referenced by <em>time.</em></p>",2019-09-13T23:13:05Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0iqmc322wv26d,2019-09-13T23:13:05Z,{},hw2
1417,no,"<p>Well I got it all working. So my questions were more academic. </p>
<p></p>
<p>So agent is at State0 and trying to figure out the possible states and future awards</p>
<p></p>
<p>Assuming we hit the terminal step at  x. Then what do the step estimators Gt&#43;x to Gt inf represent?</p>
<p>a: the set of virtual &#39;states&#39; after the terminal state x in mdp</p>
<p>b: time or iterations of mdp</p>
<p></p>
<p>either way intuitively not sure why we need to track anything after we have reached the terminal state. What benefit can the agent get by estimating the states after terminal state</p>",2019-09-14T07:32:33Z,62,Week 9/8 - 9/14,feedback,,jqrr36mqfm8M,k0j8gnmcnkg3a3,2019-09-14T07:32:33Z,{},hw2
1418,no,"<p>So, first off, if you were actually <em>in</em> a terminal state, you <em>wouldn&#39;t</em> care about your return or any future rewards.  They&#39;d all be zero.  Like you said - there&#39;s no benefit.</p>
<p></p>
<p>We care about the states after the terminal states when:</p>
<p></p>
<p>1. We&#39;re not in a terminal state, and</p>
<p></p>
<p>2. We&#39;re using the $$TD(\lambda)$$ algorithm.</p>
<p></p>
<p>$$TD(\lambda)$$ is <em>defined</em> to be an infinite series of weighted returns, each term being weighted by a higher and higher power of $$\lambda$$.  So if we are going to use that algorithm, we need to care about those states.  Fortunately, since they&#39;re all $$0$$ value states, computing with them is easy and we can actually sum that infinite sum.</p>",2019-09-14T08:30:11Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jais79bh97an,2019-09-14T08:30:11Z,{},hw2
1419,no,"<p><a href=""https://math.stackexchange.com/questions/1693584/how-to-find-sum-of-infinite-geometric-series-without-n-1"">https://math.stackexchange.com/questions/1693584/how-to-find-sum-of-infinite-geometric-series-without-n-1</a></p>
<p></p>
<p>I was getting stuck on finding the formula for a convergent geometric starting at a value other than 1, the above link clarified it for me. Actually pretty funny that I am teaching sequences and series to my Algebra II class this week.</p>",2019-09-13T23:06:01Z,62,Week 9/8 - 9/14,followup,,jl2bq5rf8b67pq,k0iqd8vsg435kk,2019-09-13T23:06:01Z,{},hw2
1420,no,<p>Great!</p>,2019-09-12T17:14:29Z,62,Week 9/8 - 9/14,followup,,jzygktecb0e3i1,k0gydbny3vr5tk,2019-09-12T17:14:29Z,{},logistics
1421,no,<p>For your reference here is the link to the presentation: https://docs.google.com/presentation/d/1Di3UxtT3RfTBXwZ6NGKRWwtdMQsyw0cl7tHhdqbcS_A/edit?usp=sharing</p>,2019-09-14T18:09:23Z,62,Week 9/8 - 9/14,followup,,i4i9bi8rFqk,k0jv7n3ix7a752,2019-09-14T18:09:23Z,{},logistics
1422,stud,<p>When will the recording be available ? </p>,2019-09-15T03:37:38Z,61,Week 9/15 - 9/21,feedback,a_0,,k0kfif1ezs4661,2019-09-15T03:37:38Z,{},logistics
1423,no,Its available now check &#64;132,2019-09-15T10:40:55Z,61,Week 9/15 - 9/21,feedback,,i4i9bi8rFqk,k0kumr7430mnd,2019-09-15T10:40:55Z,{},logistics
1424,no,<p>Quick q. If we export a python notebook do we need to clean up anything? ex: if I use magic cells %%time etc. which may not work outside of ipython and I export to py will that be an issue? or can we expect that TA&#39;s can just eval all cells in a notebook and be fine?</p>,2019-09-22T20:29:32Z,36,Week 9/22 - 9/28,followup,,jcg0nzvdk8272b,k0vfqotntth253,2019-09-22T20:29:32Z,{},project1
1425,no,<p>your readme should describe the steps necessary to create/view the outputs used in the report. </p>,2019-09-23T17:49:27Z,36,Week 9/22 - 9/28,feedback,,jl1acpoc4HA9,k0wpgo3l1gg6xu,2019-09-23T17:49:27Z,{},project1
1426,no,"<p>This is a great question, and a great answer. Clicked thanks, but sometimes it seems better to say thanks as well. Appreciate it!</p>",2019-09-13T16:05:47Z,62,Week 9/8 - 9/14,followup,,isde34zracb1mz,k0ibcts6e5k2ea,2019-09-13T16:05:47Z,{},project1
1427,no,<p>&#43;1.</p>,2019-09-14T04:41:54Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0j2d7nbyst7jz,2019-09-14T04:41:54Z,{},project1
1428,no,<p></p>,2019-09-14T03:35:04Z,62,Week 9/8 - 9/14,followup,,jvfpllmsggt7p4,k0izz8sz6dg7i,2019-09-14T03:35:04Z,{},project1
1429,no,<p>It&#39;s covered pretty well in the Mitchell ML book in section 13.3.  I&#39;m curious to hear the lectures in this course on that subject though.  When I first saw it in action in ML4T I was completely amazed.  Really cool stuff.</p>,2019-09-13T02:18:15Z,47,Week 9/8 - 9/14,followup,,is9so9huTMp,k0hhslsz3l84wy,2019-09-13T02:18:15Z,{},other
1430,no,"<p>How is it a 5th degree, there is a term in solving TD($$\lambda $$)= TD(1)</p>
<p>(1-$$\lambda $$)$$\lambda $$$$^{5}$$E$$^{6}$$</p>",2019-09-14T18:55:26Z,62,Week 9/8 - 9/14,followup,,jc6s23mr74r45q,k0jwuuuv55m2mk,2019-09-14T18:55:26Z,{},hw2
1431,no,<p>Write down what $$E_6$$ equals.</p>,2019-09-14T19:49:36Z,62,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jysiqlyhz4nk,2019-09-14T19:49:36Z,{},hw2
1432,no,"<p>I did clone the repo but got the same result. I am new to docker. It seems to me that the docker image </p>
<ol><li><code>docker pull mimoralea/gdrl:v0.12</code></li></ol>
<p>should have the notebooks in it?</p>
<p></p>",2019-09-13T02:49:31Z,62,Week 9/8 - 9/14,followup,,j6ll2xkiDJf,k0hiwtmf5c865m,2019-09-13T02:49:31Z,{},other
1433,no,"<p>Clone the GitHub repo first.</p>
<p></p>
<p>It doesn&#39;t have the Notebooks inside because I want your changes to persist. If I put them inside and you extend/edit/play with the Notebooks, then as soon as the container shuts down all changes are lost.</p>
<p></p>
<p>What the container has is everything installed and configured for you. OpenAI Gym with a virtual display, numpy, PyTorch, custom environment packages, etc. But you must clone the GitHub repo and run the command from inside the repo folder.</p>
<p></p>
<p>  </p>
<ol start=""0""><li>Get this repo: <code>git clone --depth 1 https://github.com/mimoralea/gdrl.git &amp;&amp; cd gdrl</code></li><li>Pull the gdrl image with: <code>docker pull mimoralea/gdrl:v0.12</code></li><li>Spin up a container: <code>docker run -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12</code> (remember to use <code>nvidia-docker</code> if you are using a GPU.)</li><li>Open a browser and go to the URL shown in the terminal (likely to be: <a href=""http://localhost:8888/"">http://localhost:8888</a>). The password is: <code>gdrl</code></li></ol>
<p></p>
<p>#0 above is what the issue you mention seems to be related to.</p>",2019-09-13T02:54:52Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0hj3p77ppk3wk,2019-09-13T02:54:52Z,{},other
1434,no,"<p>Miguel;</p>
<p> step 0 and 1 works fine.</p>
<p>Step 2 errors if I run it as is above:</p>
<p>(venv) C:\Users\Kamran\PycharmProjects\miguelnotebooks\gdrl&gt;docker run -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12 docker: Error response from daemon: create $PWD/notebooks: &#34;$PWD/notebooks&#34; includes invalid characters for a local volume name, only &#34;[a-zA-Z0-9][a-zA-Z0-9_.-]&#34; are allowed. If you inte nded to pass a host directory, use absolute path. See &#39;docker run --help&#39;.</p>
<p></p>
<p>If I remove &#34;$PWD&#34;, then it runs </p>
<p>(venv) C:\Users\Kamran\PycharmProjects\miguelnotebooks\gdrl&gt;docker run -it --rm -p 8888:8888 -v /notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12<br />Container must be run with group &#34;root&#34; to update passwd file<br />Executing the command: jupyter notebook<br />[I 03:12:06.263 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret<br />[I 03:12:08.018 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab<br />[I 03:12:08.018 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab<br />[I 03:12:08.022 NotebookApp] Serving notebooks from local directory: /mnt/notebooks<br />[I 03:12:08.022 NotebookApp] The Jupyter Notebook is running at:<br />[I 03:12:08.022 NotebookApp] http://(5ddb70a4941b or 127.0.0.1):8888/<br />[I 03:12:08.022 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</p>
<p></p>
<p></p>
<p></p>
<p>But when I go to the browser and load localhost:8888, it asks for the password. I enter &#34;gdrl&#34; and then it displays the following:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ll2xkiDJf%2Fk0hjw34jzirp%2Fjupyter.jpg"" alt=""jupyter notebook missing"" /></p>",2019-09-13T03:17:29Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0hjwsdlhx952d,2019-09-13T03:17:29Z,{},other
1435,no,"<p>I ran into this problem a while back on my windows machine and since then switched to my mac. I believe the &#39;mnt&#39; should be switched to &#39;gdrl&#39; and quotes around pwd replaced with squiggly brackets<b>.</b></p>
<p></p>
<pre>docker run -it --rm -p 8888:8888 -v ${pwd}/notebooks/:/gdrl/notebooks/ mimoralea/gdrl:v0.12</pre>",2019-09-13T16:40:16Z,62,Week 9/8 - 9/14,feedback,,jc73qxwxnYpJ,k0icl69mq696y3,2019-09-13T16:40:16Z,{},other
1436,no,<p>that did not work for me.</p>,2019-09-13T18:06:12Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0ifnojlo8420o,2019-09-13T18:06:12Z,{},other
1437,no,"<p>$pwd is a unix command for &#34;print working directory&#34;. If you are inside the repo, it will give the full absolute path of the repo.</p>
<p>-v on docker will mount volume data into the docker container.</p>
<p></p>
<p>So, step 2 is basically mount the notebook in the repo, on your host machine, into docker container.</p>
<p>But since you are on window machine, the $pwd wouldn&#39;t work for you. With your command above, you are mounting a non exist folder &#34;/notebooks/:/mnt/notebooks/&#34; into your container, that&#39;s why you don&#39;t see anything in your container.</p>
<p>What you should do, is passing in your (Window) path:</p>
<p></p>
<pre>docker run -it --rm -p 8888:8888 -v &#34;C:\Users\Kamran\PycharmProjects\miguelnotebooks\gdrl\notebooks\&#34;:/mnt/notebooks/ mimoralea/gdrl:v0.12</pre>
<p>I don&#39;t have a Windows machine to test it, but at least I hope this helps you with some understand about the problem.</p>
<p></p>",2019-09-13T18:20:08Z,62,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0ig5lkka976ix,2019-09-13T18:20:08Z,{},other
1438,no,"<p>Quang, I followed your tip and I am still getting an empty notebook directory... I am wondering if anyone have this working on Windows? what about mac?</p>",2019-09-13T22:21:35Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0ios3tr67p22u,2019-09-13T22:21:35Z,{},other
1439,no,"<p>I tried it on an ubuntu TLS 18.04 Oracle VM Virtual box and don&#39;t seem to have any luck either. </p>
<p></p>
<p>kamran&#64;kamran-VirtualBox:~/gdrl$ docker run -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/      (my error here - this is not the full cmd)<br />&#34;docker run&#34; requires at least 1 argument.<br />See &#39;docker run --help&#39;.</p>
<p>Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</p>
<p>Run a command in a new container<br />kamran&#64;kamran-VirtualBox:~/gdrl$</p>
<p></p>
<p>The error is different though.....</p>
<p></p>
<p>I guess, I wait for a response from Miguel on this....</p>
<p>Has anyone successfully loaded and used miguel&#39;s notebooks on any OS?</p>
<p></p>",2019-09-14T01:20:06Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0iv5ocjhtx3kw,2019-09-14T01:20:06Z,{},other
1440,no,"<p>Kamran, that&#39;s not the docker command I have on my repo.</p>
<p></p>
<p>Do you mind getting on a Hangouts with me? I&#39;d like to take a look.</p>",2019-09-14T01:30:38Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivj8gek1c46p,2019-09-14T01:30:38Z,{},other
1441,no,<p>sure how can I do that?</p>,2019-09-14T01:32:26Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0ivljdm9py1uo,2019-09-14T01:32:26Z,{},other
1442,no,"<p><a href=""https://hangouts.google.com/call/lqqxjdM-TBeujD0PpeLOAEEE"">https://hangouts.google.com/call/lqqxjdM-TBeujD0PpeLOAEEE</a></p>
<p></p>",2019-09-14T01:32:46Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivlze813g6in,2019-09-14T01:32:46Z,{},other
1443,no,"<p>Thanks Miguel. Problem solved!</p>
<p>For those that have issues with windows, I recommend going to Ubuntu. </p>
<p>In my earlier post, I had copied the &#34;run&#34; command incorrectly (only line 1) and that was causing the problem in Ubuntu.</p>
<p>the full command should be:</p>
<p>sudo docker run -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12</p>
<p></p>",2019-09-14T01:54:24Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0iwdsc32gb5t2,2019-09-14T01:54:24Z,{},other
1444,no,"<p>After you successfully setup docker, you can also follow the suggestion post-setup, so that you don&#39;t have to run it with sudo:</p>
<p><a href=""https://docs.docker.com/install/linux/linux-postinstall/"">https://docs.docker.com/install/linux/linux-postinstall/</a></p>
<p></p>
<p>Run docker with sudo will set root permission on some of your configuration. And plus, it will be 5 more chars to type everytime you want to run :)</p>",2019-09-14T03:17:15Z,62,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0izccerfyy6t3,2019-09-14T03:17:15Z,{},other
1445,no,"<p>From step 2:</p>
<blockquote>
<p>Spin up a container: <code>docker run -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12</code> (remember to use <code>nvidia-docker</code> if you are using a GPU.)</p>
</blockquote>
<p>Would that mean we will run with:</p>
<pre>docker run --gpus all -it --rm -p 8888:8888 -v &#34;$PWD&#34;/notebooks/:/mnt/notebooks/ mimoralea/gdrl:v0.12</pre>
<p>Or is there another way to instruct the docker to use GPU?</p>
<p></p>",2019-09-13T16:01:07Z,62,Week 9/8 - 9/14,followup,,jl5wq8mca7o0,k0ib6tnu9tg72o,2019-09-13T16:01:07Z,{},other
1446,no,"<p>Quang, are you able to do this docker installation successfully?</p>
<p></p>",2019-09-14T01:21:46Z,62,Week 9/8 - 9/14,feedback,,j6ll2xkiDJf,k0iv7u6d30g2ez,2019-09-14T01:21:46Z,{},other
1447,no,"<p>For the GPU, use &#96;nvidia-docker&#96;: <a href=""https://github.com/NVIDIA/nvidia-docker"">https://github.com/NVIDIA/nvidia-docker</a></p>
<p></p>",2019-09-14T01:31:36Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivkha117n52x,2019-09-14T01:31:36Z,{},other
1448,no,"<p><a href=""https://hangouts.google.com/call/lqqxjdM-TBeujD0PpeLOAEEE"">https://hangouts.google.com/call/lqqxjdM-TBeujD0PpeLOAEEE</a></p>
<p></p>",2019-09-14T01:32:53Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivm4h26ds6lh,2019-09-14T01:32:53Z,{},other
1449,no,"<p>Feel free to join, Quang.</p>",2019-09-14T01:33:08Z,62,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivmg4yde96rk,2019-09-14T01:33:08Z,{},other
1450,no,"<p>Sorry I just miss the hangout. Can someone rewind the finding we had in the session?</p>
<p></p>
<p>&#64;Kamran, yes I was able to run the docker mentioned in the repo successfully. I just wasn&#39;t sure I was using CPU or GPU. Running notebook for ch.8, I was able to see my GPU through nvidia-smi, but the training bumps my CPU load instead.</p>",2019-09-14T03:12:35Z,62,Week 9/8 - 9/14,feedback,,jl5wq8mca7o0,k0iz6bwtvr84lo,2019-09-14T03:12:35Z,{},other
1451,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  I just saw this and installed it but I don&#39;t have a nvidia gpu, but an AMD RX580, which is supported by the <a href=""https://rocm.github.io/"" target=""_blank"" rel=""noopener noreferrer"">ROCm</a> platform.  </p>
<p>Have you or anyone used it? I&#39;d like to give it a shot but I worry to waste hours trying to make it work on a mac.  </p>
<p></p>",2019-10-14T15:43:14Z,57,Week 10/13 - 10/19,followup,,jzh6k6o994a6dh,k1ql78dbm7n4th,2019-10-14T15:43:14Z,{},other
1452,no,"<p>Hi, JP. I have not used AMD graphics cards for DL...</p>
<p></p>
<p>Hopefully, someone else can chime in.</p>",2019-10-14T15:45:35Z,57,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1qla90c12b15k,2019-10-14T15:45:35Z,{},other
1453,no,"<p>I have a nvidia RTX 2060 in a windows build, and ran keras and tensorflow with the gpu option enabled for a small neural net. It took me a couple hours to get it setup, and then once I did; the algorithm ran quick a bit slower per episode. (from ~.6 seconds per episode to ~1.3 seconds per episode). Not sure if I did something wrong, but was guessing the overhead of packaging it up to the gpu and such added more time then just the cpu running it directly. I didn&#39;t let it run very long once I noticed the loss in performance though, so maybe it would have picked up. but overall not impressed at all. Maybe I&#39;ll fiddle with it some more later.</p>",2019-10-15T01:27:39Z,57,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1r62snqpdv5eu,2019-10-15T01:27:39Z,{},other
1454,no,<p>So then maybe I just do not know what to do with this. Do we find a polynomial function for each $$t$$?</p>,2019-09-13T19:10:13Z,37,Week 9/8 - 9/14,followup,,jqmjg3txqw6ji,k0ihy07l3vq6z,2019-09-13T19:10:13Z,{},hw2
1455,no,"<p>$$t$$ refers to time in the evolution of the MDP if the agent were moving through states.  Our homework is asking us to do something at one particular instant in time.  We&#39;re given a snapshot of the valueEstimates right at that instant in time.  So, as I understand it, there is only one value of $$t$$ in play.</p>
<p></p>
<p>Keep in mind that $$T$$ also refers to a time - the time when you reach the terminal state.  So if you were, say, to set $$t$$ to 1,000 or 1,000,000, but you&#39;re at state $$0$$ and you know that you won&#39;t be reaching the terminal state for another, say, $$10$$ time-steps, then $$T$$ in the first case would be 1,010 and in the second case 1,000,010, i.e. $$T$$ is not fixed, but relative to $$t$$ - it depends more on where the state you&#39;re in is relative to the terminal state.</p>",2019-09-13T19:17:30Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ii7dtn1k72wu,2019-09-13T19:17:30Z,{},hw2
1456,no,"<p>hm, &#64;Vahe, afaik T refers to the number of the episode... T-1 means &#39;the previous episode&#39; and T &#39;the current&#39; episode.</p>",2019-09-13T19:50:40Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0ije1gkbg81zp,2019-09-13T19:50:40Z,{},hw2
1457,no,"<p>&#64;JPB No, $$T$$ here is the time at which the episode terminates.</p>",2019-09-13T21:03:19Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ilzghmpx96xy,2019-09-13T21:03:19Z,{},hw2
1458,no,"<p>Well, the equation above says &#39;1-step to T-1 steps&#39; so I&#39;m pretty sure T is not a measure of time... </p>
<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  what do you say about this?</p>",2019-09-13T21:47:32Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0inkbizdl4l9,2019-09-13T21:47:32Z,{},hw2
1459,no,"<p>Interested to know as well. Though the way I am currently thinking about it T is related to the terminal state, not different episodes.</p>
<p></p>
<p>Either way, the equation above looks right? What are people using as solvers? I cannot get mine to work.</p>",2019-09-13T21:49:03Z,37,Week 9/8 - 9/14,feedback,,jqmjg3txqw6ji,k0inm9linus2tu,2019-09-13T21:49:03Z,{},hw2
1460,no,<p>It&#39;s $$1$$ <strong>time-step </strong>to $$T-1$$ <strong>time-steps</strong>.</p>,2019-09-13T21:51:33Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0inphi39uc69r,2019-09-13T21:51:33Z,{},hw2
1461,no,"<p>Nevermind, got it. Wolfram&#39;s interface was being strange. Still curious to hear Miguel&#39;s input on this.</p>",2019-09-13T22:02:37Z,37,Week 9/8 - 9/14,feedback,,jqmjg3txqw6ji,k0io3pz6fcb5we,2019-09-13T22:02:37Z,{},hw2
1462,no,"<p>His input on whether $$T$$ represents the time of of an event?  I&#39;m taking all bets on this, laying 3:1 up to $1000.</p>",2019-09-13T22:10:45Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0ioe67mwuagb,2019-09-13T22:10:45Z,{},hw2
1463,no,"<p>We are talking about time <strong>STEPS</strong> here, steps are counted 1,2,3... here, we do not care about the amount of time between every step, therefore T and t&#39;s are not time, just numbers to designate a step in the sequence.  </p>
<p>That&#39;s why we can write t&#43;1, t&#43;2... it&#39;s a sequence of events, in time, but it doesn&#39;t mean t is time, but just the next thing after t-1 event.  </p>
<p>Just like you would say &#39;after 10 steps, turn right&#39;, it&#39;s 10 steps, not 10 meters, even if those steps are measured in meters. </p>
<p>The fact that t and T are unit less is the reason why you can add 1,2,3 to them btw.  </p>
<p></p>
<p>It&#39;s an abuse of language here, all of this is timeless, just events happening after each other, in a sequence.  </p>
<p>You wrote it yourself when you say &#39;1 to T-1 <strong>time steps&#39;</strong>... you&#39;re using T to count the time steps.  The unit of time is inside the time step, not in T. </p>
<p>In other words, t,T $$\in\mathbb{N}$$  </p>
<p></p>
<p>The time factor will probably be introduced when we deal with a real system, when a digital system tests a real system, then engineers have to decide if the time step is 1s or 1 ms.  </p>
<p>But until then, we are just counting/indexing them.  </p>",2019-09-13T23:34:26Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0irdsldknf6pn,2019-09-13T23:34:26Z,{},hw2
1464,no,"<p>So let me get this straight. </p>
<p></p>
<p>First you were arguing that $$T$$ was some kind of &#39;episode number&#39;.</p>
<p></p>
<p>&#34;hm, &#64;Vahe, afaik T refers to the number of the episode... T-1 means &#39;the previous episode&#39; and T &#39;the current&#39; episode.&#34;</p>
<p></p>
<p>It seems you&#39;ve pretty much thrown that idea away and now you&#39;re onto the more pedantic argument that time can&#39;t be discrete, only continuous.  We&#39;ve been in the discrete time domain this entire course.  That&#39;s clearly what anyone means at any point in any of these posts when they say &#34;time.&#34;</p>
<p></p>
<p><strong>Edit:</strong>  Just noticed this: &#34;The fact that t and T are unit less is the reason why you can add 1,2,3 to them btw. &#34;</p>
<p></p>
<p>This is hogwash. You can add constants to variables, as long as the constants have the same units as the variables.  It&#39;s done pretty much any time anyone writes a mathematical expression involving any physical quantity.</p>",2019-09-13T23:40:17Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0irlbl6l0v55k,2019-09-13T23:40:17Z,{},hw2
1465,no,"<p>$$t$$ and $$T$$, in this case, unfortunately, are a measure of time. $$t$$ denotes a single time step; a time step is a single interaction cycle. There are episodes, and those are collections of time steps from the initial time step to the terminal time step $$T$$. Little $$t$$ is used as a variable denoting any time step in an episode. But capital $$T$$ is used to explicitly indicate the last time step in an episode, that is, once you have reached a terminal state. For instance, $$S_T$$ is the state sampled at the last time step $$T$$. Therefore $$S$$, in this case, is a terminal state.</p>
<p></p>",2019-09-14T01:24:36Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivbha768q4e1,2019-09-14T01:24:36Z,{},hw2
1466,no,"<p>BTW, I knew adding the subscripts will bring these conversations... Enjoy them, we are learning a very cool (and difficult) field. And there is more ahead!</p>
<p></p>
<p>Let&#39;s keep working together, and only kill each other afterward!</p>",2019-09-14T01:28:46Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0ivgttrbmu2ph,2019-09-14T01:28:46Z,{},hw2
1467,no,"<p>With respect <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  , if the dimension of t,T is time, and since a time step dimension is also time, then &#39;T time-steps&#39; would have the dimension $$time^2$$... </p>
<p>I think Sutton &#39;abuses&#39; the language, like EE&#39;s like me do all the time when we talk about digital states machines, when we say &#39;at time Tth step&#39; to avoid saying &#39;at a time that is the result of the multiplication of the time unit (the time step) by its index T&#39;. </p>
<p>When you write &#39;t denotes any time step any time step&#39; or &#39;terminal time step T&#39;, you&#39;re using t and T as indices because &#39;denoting&#39; means &#39;indicate&#39;, &#39;show, ie &#39;indexing&#39;.</p>
<p></p>
<p>The dimension of &#39;Tth step&#39; is time, because the dimension of a time-step is time and T is just the index, the number that defines the position of a particular time step compared to another.  That&#39;s why we add natural number to t and T imho, and we abuse language by calling the &#39;end of the Tth time-step&#39; simply &#39;time T&#39;.</p>",2019-09-14T15:29:04Z,37,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0jphh0xbnp27c,2019-09-14T15:29:04Z,{},hw2
1468,no,"<p>Too many words! </p>
<p></p>
<p>Understand what you may. I&#39;m just a messenger.</p>",2019-09-14T16:41:00Z,37,Week 9/8 - 9/14,feedback,,hyx9thiqa6j4nn,k0js1yso8po4ks,2019-09-14T16:41:00Z,{},hw2
1469,no,"<p>My question was not about what those things are, I did google them, but what we are supposed to do with the prof mentioning them and debating it with Prof Isbell, like it&#39;s obvious things.  </p>
<p>I was about to read the paper, so thanks, maybe it&#39;ll be more obvious after that.  </p>",2019-09-14T00:29:54Z,62,Week 9/8 - 9/14,followup,,jzh6k6o994a6dh,k0itd4q016x25k,2019-09-14T00:29:54Z,{},other
1470,stud,"<p>Welcome! One thing, though. It&#39;s Professor <em>Isbell.</em> Not Lisbell. Looks like you ran <em>Littman</em> and <em>Isbell</em> together. </p>",2019-09-14T00:34:51Z,62,Week 9/8 - 9/14,feedback,a_0,,k0itjhy4uth7kc,2019-09-14T00:34:51Z,{},other
1471,no,"<p>Thanks, haha, let me fix that.  </p>",2019-09-14T01:22:14Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0iv8fsnl3z251,2019-09-14T01:22:14Z,{},other
1472,no,"<p>I just watched this lecture and actually thought the generalized MDP stuff was really cool.  What I got from it was</p>
<p></p>
<p>1. You can write Bellman-like equations that <em>aren&#39;t</em> the Bellman optimality equation, but still are very useful.  For example, if you&#39;re doing an $$\epsilon$$-exploration policy, you still know that the state action values are going to converge, and you can find what that value (for that policy) is.</p>
<p></p>
<p>2. Both the minimax and the min equation are there to suggest a version of a policy where your agent doesn&#39;t have complete control:  either the MDP is completely adversarial (in the case of the min) and always puts you in the worst state that you can transition to, or you&#39;re competing with another agent and have <em>some</em> control, in the case of the minimax.</p>
<p></p>
<p>3. The $$\rho (ord())$$ I took as purely a generalization of all the possible functions we could think of applying to our choice of actions once we enter the next state.</p>
<p></p>
<p>Also, a comment on the lectures in general.  I think that, with it being 2019 with the vast amount of resources we have (Sutton and Barto, David Silver lectures, Google search and countless websites on reinforcement learning, Stack Exchange, etc., etc.) having lectures that are off-beat or give some high-level ideas that we generally wouldn&#39;t find anywhere else is actually preferable to just having <em>another</em> clone of the material that is already out there in countless forms.</p>",2019-09-16T15:37:38Z,61,Week 9/15 - 9/21,followup,,jzfsa4a37jf4aq,k0mko6ipn091z9,2019-09-16T15:37:38Z,{},other
1473,stud,<p>Right?! Thanks for confirming my suspicion.</p>,2019-09-13T23:11:20Z,62,Week 9/8 - 9/14,followup,a_0,,k0iqk2z2gpr25t,2019-09-13T23:11:20Z,{},hw2
1474,no,"<p>Simple technique to improve reproducibility: record seed for RNG and make it easily configurable, so you can rerun code with the exact results. Maybe even record &#34;interesting&#34; seeds in the README.</p>
<p></p>
<p>Much more elaborate: use tools like</p>
<ul><li>sacred - <a href=""https://sacred.readthedocs.io/en/latest/"">https://sacred.readthedocs.io/en/latest/</a></li><li>lab - https://lab.readthedocs.io/en/latest/index.htm</li><li>etc. (many alternatives available for Python, most probably for other langs too)</li></ul>
<p>To track and record more information.</p>
<p></p>",2019-09-14T10:12:03Z,62,Week 9/8 - 9/14,followup,,jzhwdil7ssn443,k0je5scptap40,2019-09-14T10:12:03Z,{},project1
1475,no,"<p>Sacred also comes with a Docker example, in theory it can be used to package everything in an image (dependencies, scripts, visualization, data) - https://sacred.readthedocs.io/en/latest/examples.html#docker-setup</p>",2019-09-14T10:15:03Z,62,Week 9/8 - 9/14,feedback,,jzhwdil7ssn443,k0je9mt6uuw15z,2019-09-14T10:15:03Z,{},project1
1476,no,"<p>Very belated, but I just saw this. Thanks for the intro to sacred, I&#39;ve never come across something like this before. It might be overkill for this project, but it&#39;s good to know this sort of thing exists :D</p>
<p></p>",2019-09-22T11:06:32Z,60,Week 9/22 - 9/28,feedback,,jl3oi5v7qkSk,k0uvmnssxc25jq,2019-09-22T11:06:32Z,{},project1
1477,no,<p>Scary! How did you find this?! :)</p>,2019-09-14T04:43:05Z,62,Week 9/8 - 9/14,followup,,hyx9thiqa6j4nn,k0j2epy0vcz74m,2019-09-14T04:43:05Z,{},other
1478,no,"<p>It was actually a tab that I opened a week or two ago, but I can&#39;t remember where I got it.</p>
<p>Then I visited it today and I saw (s)he had added the corresponding S&amp;B chapters.</p>",2019-09-14T05:16:27Z,62,Week 9/8 - 9/14,feedback,,jzh6k6o994a6dh,k0j3ln0en1b194,2019-09-14T05:16:27Z,{},other
1479,no,<p>Haha I made in and post it in &#64;43. Glad to hear that you think it&#39;s useful :)</p>,2019-09-19T09:35:31Z,61,Week 9/15 - 9/21,feedback,,jl2bqebwzel2s,k0qi22a66x863t,2019-09-19T09:35:31Z,{},other
1480,no,"<p>Aaaah, that&#39;s how I got it!</p>
<p>Thx for doing that.  You save me the burden of finding out which S&amp;B chapter to read.</p>
<p>Great job! </p>",2019-09-19T12:44:18Z,61,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0qostz0j5n6jj,2019-09-19T12:44:18Z,{},other
1481,no,I&#39;ll definitely do that. Thank you. ,2019-09-14T16:41:23Z,62,Week 9/8 - 9/14,followup,,jc8mgkzqyyso,k0js2gqxd374vu,2019-09-14T16:41:23Z,{},hw2
1482,no,"<p>I don&#39;t use sympy but it looks like solve in on the path to being deprecated in favor of solveset:</p>
<p></p>
<p><a href=""https://github.com/scipy-lectures/scipy-lecture-notes/issues/308"">https://github.com/scipy-lectures/scipy-lecture-notes/issues/308</a></p>",2019-09-14T18:49:13Z,37,Week 9/8 - 9/14,followup,,jzfsa4a37jf4aq,k0jwmuylja21y8,2019-09-14T18:49:13Z,{},hw2
1483,no,"<p>Another good alternative is numpy.roots:</p>
<p>https://docs.scipy.org/doc/numpy/reference/generated/numpy.roots.html</p>",2019-09-14T18:54:32Z,37,Week 9/8 - 9/14,followup,,is8ald0uljj3u4,k0jwtp95xo16d9,2019-09-14T18:54:32Z,{},hw2
1484,no,"<p>I also ended up using np.roots. Easy to use.</p>
<p></p>
<p>I got little confused with the coefficients it returns though. Ended up using the positive ones under 1 and it seems to get right answer :)</p>",2019-09-14T19:57:46Z,37,Week 9/8 - 9/14,feedback,,jfzaqnqvtQ1m,k0jz309gxk79m,2019-09-14T19:57:46Z,{},hw2
1485,no,"<p>If you see a $$j$$, that denotes the imaginary unit $$j = \sqrt{-1}$$.  Since our $$\lambda$$ is only defined between zero and one, we can ignore any complex roots.</p>",2019-09-14T20:14:44Z,37,Week 9/8 - 9/14,feedback,,jzfsa4a37jf4aq,k0jzou42izu775,2019-09-14T20:14:44Z,{},hw2
1486,no,"<p>l = Symbol(&#39;l&#39;, real=True) makes it return only real numbers</p>
<p></p>",2019-09-14T20:52:06Z,37,Week 9/8 - 9/14,feedback,,jzhnj3cj4pl2uy,k0k10vu2e0953h,2019-09-14T20:52:06Z,{},hw2
1487,no,"Thanks Clayton, that helped !",2019-09-16T03:15:32Z,36,Week 9/15 - 9/21,followup,,jc5n7oq5zop3e8,k0lu5ug32xr2qo,2019-09-16T03:15:32Z,{},hw2
1488,no,<p>My single-thread code produces all three figures in about 20 seconds with a 3-year old laptop. This is just informative: my results somewhat resemble the graphs from the article but I might still be missing something important.</p>,2019-09-15T23:00:33Z,61,Week 9/15 - 9/21,followup,,is8ald0uljj3u4,k0ll1xiz8afur,2019-09-15T23:00:33Z,{},project1
1489,no,"<p>Bingo, thank you kindly.</p>",2019-09-15T11:40:18Z,61,Week 9/15 - 9/21,followup,,jl3oi5v7qkSk,k0kwr410qjo7fk,2019-09-15T11:40:18Z,{},other
1490,no,"<p>if you are having issues with convergence, try experimenting with some of the hyper parameters. </p>",2019-09-17T00:13:30Z,61,Week 9/15 - 9/21,followup,,hz7meu55mi8sd,k0n33l62xb942o,2019-09-17T00:13:30Z,{},project1
1491,no,"<p>For equation 4, how do we get the delta_w as a vector of size 5 from the equation? Is Pk a scaler? Based on the above explanation, it looks like it, but not sure how the equation end up producing a vector of 5. </p>
<p>Can anyone give an example of calculating gradient of Pk to w? </p>
<p></p>",2019-09-25T05:21:39Z,42,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k0ytmp7uc1r58n,2019-09-25T05:21:39Z,{},project1
1492,no,"<p>&#64;Germayne,</p>
<p></p>
<p>When you don&#39;t reset the eligibility trace every episode, how do you handle the boundaries between episodes?  What&#39;s your error term for the time-step moving from the terminal state at the end of one episode to the beginning state at the start of the next episode, and do you decay that term?</p>",2019-09-15T21:50:57Z,61,Week 9/15 - 9/21,followup,,jzfsa4a37jf4aq,k0likeuilgk2v7,2019-09-15T21:50:57Z,{},project1
1493,no,"<p>We use eligibility traces here !!?? I&#39;m confused now.  </p>
<p></p>
<p></p>",2019-09-21T00:29:44Z,61,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0stfvq91qu2lv,2019-09-21T00:29:44Z,{},project1
1494,no,"Hi Vahe, I guess I was confused by the error present in various pseudo codes of TD lambda , Sarsa ...etc <div><br /></div><div>From a theoretical point of view, it is clear that we compute each TD on a episode rather than across 10 episode. </div><div><br /></div><div>Known pseudo code error: </div><div><a href=""https://stackoverflow.com/questions/29904270/eligibility-trace-reinitialization-between-episodes-in-sarsa-lambda-implementati"">https://stackoverflow.com/questions/29904270/eligibility-trace-reinitialization-between-episodes-in-sarsa-lambda-implementati</a></div>",2019-09-16T01:53:56Z,61,Week 9/15 - 9/21,followup,,jvfpllmsggt7p4,k0lr8wpfmze3u8,2019-09-16T01:53:56Z,{},project1
1495,no,"<p>Thank you for that link.  From what I can glean, that&#39;s a pseudocode from an old S&amp;B that isn&#39;t in the book anymore.  All the appropriate 2nd ed book algos clearly have a $$z \leftarrow 0$$ at the start of each episode.</p>",2019-09-16T02:06:29Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lrp14f6bl1gf,2019-09-16T02:06:29Z,{},project1
1496,no,<div><br /></div><div>TLDR; please reset eligibility trace matrix every episode / sequence! </div>,2019-09-16T02:18:54Z,61,Week 9/15 - 9/21,feedback,,jvfpllmsggt7p4,k0ls507qos56kb,2019-09-16T02:18:54Z,{},project1
1497,stud,"<p>Here&#39;s my reasoning:</p>
<p></p>
<p>I partitioned the TD($$\lambda$$) weighted sum of k-step estimators into two sub-sums like so:</p>
<p>(1 - $$\lambda $$)$$\sum_{k=1}^{\infty }\lambda^{k-1}E_{k}$$ = (1 - $$\lambda $$)($$\sum_{k=1}^{4}\lambda^{k-1}E_{k}$$ &#43; $$\sum_{k=5}^{\infty }\lambda^{k-1}E_{k}$$)</p>
<p></p>
<p>I chose to partition at that point because, I reasoned, $$E_{k} = E_{5}$$ for $$k\geq 5$$.</p>
<p></p>
<p>Then, that second part simplifies to $$E_{5}\sum_{k=5}^{\infty }\lambda^{k-1} = E_{5}\lambda^{4}/(1-\lambda)$$.</p>
<p></p>
<p>Plugging that result back into the partitioned sum and simplifying, yielded the 4th degree polynomial: (1 - $$\lambda $$)($$\sum_{k=1}^{4}\lambda^{k-1}E_{k}$$) &#43; $$E_{5}\lambda^{4}$$.</p>
<p></p>
<p>Where did I err?</p>",2019-09-15T22:50:08Z,36,Week 9/15 - 9/21,followup,a_0,,k0lkoirxzr37k2,2019-09-15T22:50:08Z,{},hw2
1498,no,"<p>&#34;I chose to partition at that point because, I reasoned, Ek=E5 for k≥5.&#34;</p>",2019-09-15T22:51:42Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lkqjbxq2j2d4,2019-09-15T22:51:42Z,{},hw2
1499,stud,"<p>After taking 5 steps from state 0, we reach and stay at the terminal state (state 6). So, if gamma =1, $$r_{k\gt6} = 0$$, and V(state after $$\geq5$$ steps) = V(6), then it seems like $$E_{k}$$ should works out to be $$E_{5}$$ for all 5&#43; step look-ahead estimators.<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgrnw1wjrysg%2Fk0ll3u9vx26y%2FScreen_Shot_20190915_at_1.36.54_AM.png"" alt="""" width=""483"" height=""226"" /></p>",2019-09-15T23:20:59Z,36,Week 9/15 - 9/21,feedback,a_0,,k0lls7kz70f6an,2019-09-15T23:20:59Z,{},hw2
1500,no,"<p>&#34; V(state after ≥5 steps) = V(6)&#34;</p>
<p></p>
<p>This is not true.</p>
<p></p>
<p>Edit: I know this is confusing.  The value of a state is the expected sum of(discounted) future rewards.  When we reach state 6 and stop there, we&#39;re forced to use whatever value we have for it from our previous iteration/initialization.  But when we proceed <em>past</em> state 6, we know exactly what our sum of future rewards is, since it&#39;s a terminal state.  Think of terminal states as looping back on themselves and granting a reward of $$0$$.</p>",2019-09-15T23:33:34Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lm8dpm4v031z,2019-09-15T23:33:34Z,{},hw2
1501,stud,"<p>Thank a million, Vahe! Your help in this and other threads finally made it click for me, I think. If Value = expected discounted future rewards, then Value after termination = 0, and thus, $$E_{k} \neq E_{5}$$ for any $$k\geq6$$. Correct?</p>
<p></p>
<p></p>",2019-09-16T00:45:51Z,36,Week 9/15 - 9/21,feedback,a_0,,k0lotck8ynl4uc,2019-09-16T00:45:51Z,{},hw2
1502,no,"<p>Glad you got it!<br /><br />That&#39;s my interpretation, yeah.  I do still find it a little confusing because, for example, if we looped back onto a non-terminal state, then we would use that state&#39;s current state value as a proxy for future rewards.  But in the case of examples 2 and 3 on the homework handout where there&#39;s a non-zero terminal state value, when we&#39;re looping back on the terminal state, we <strong>don&#39;t</strong> use the given value for state 6.  Terminal states are handled differently it seems.  Each loop back onto it brings you to a different &#34;virtual state&#34; even though it&#39;s still the same node in the graph.<br /><br />But going back to the definition of state value -- expected sum of future rewards -- seems to be the best way to see this clearly.</p>",2019-09-16T01:38:34Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lqp54umo46us,2019-09-16T01:38:34Z,{},hw2
1503,no,"<p>I ended up having to do a &#34;dummy state&#34; where the utility of s6 is used.  and then another state on top of that, to signify the null utility (the actual).</p>",2019-09-16T03:49:16Z,36,Week 9/15 - 9/21,followup,,jzivtxcbl6964n,k0lvd80fes35rh,2019-09-16T03:49:16Z,{},hw2
1504,no,"<p>If you already solved all the questions, why would you need to recheck when you know what the answer is? I wouldn&#39;t risk getting locked out of the account. </p>",2019-09-15T23:28:15Z,61,Week 9/15 - 9/21,followup,,jzjaw2l4l6m5ns,k0lm1jwgbg710k,2019-09-15T23:28:15Z,{},hw2
1505,no,<p>I&#39;m really just wondering how the system for the hw works</p>,2019-09-16T03:07:30Z,61,Week 9/15 - 9/21,feedback,,j6m1jeidndu6wq,k0ltvig14o86wu,2019-09-16T03:07:30Z,{},hw2
1506,no,"<p>You have 10 submission attempts.  Your &#34;checks&#39; would count as another submission attempt, right or wrong, and if you re-check again to fix an error that you introduced on your check, then that would be a third attempt.  I believe that&#39;s how it works.</p>",2019-09-16T03:08:57Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0ltxdqspgz3g8,2019-09-16T03:08:57Z,{},hw2
1507,no,"<p>take care, the first and third values given (for the examples in HM2) have a higher effective tolerance than the middle one.  the middle one, either you nail it, or your alg is wrong.</p>
<p></p>",2019-09-16T03:50:19Z,61,Week 9/15 - 9/21,followup,,jzivtxcbl6964n,k0lvekjlpoe73z,2019-09-16T03:50:19Z,{},hw2
1508,no,"<p>to reproduce the graphs in Sutton paper, what value of gamma (discount rate) should be used?</p>",2019-09-16T02:22:10Z,43,Week 9/15 - 9/21,followup,,j6ll2xkiDJf,k0ls97roc9a2bx,2019-09-16T02:22:10Z,{},project1
1509,no,"<p>This is a question that is best answered by reading his paper and seeing what value he used, and in general, what methods he used.  After all, we&#39;re trying to replicate his experiment, right?</p>",2019-09-16T02:49:56Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lt8wqhh956za,2019-09-16T02:49:56Z,{},project1
1510,no,<p>OK. So there is some trial and error.....</p>,2019-09-16T03:45:46Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lv8q5lhp12av,2019-09-16T03:45:46Z,{},project1
1511,no,"<p>Actually I don&#39;t think there&#39;s any trial and error.  Sutton&#39;s model, algorithm, and experiments are completely specified in the paper.</p>",2019-09-16T04:01:15Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lvsmd3qjk9h,2019-09-16T04:01:15Z,{},project1
1512,no,"<p>Vahe if you look at the lecture videos, one needs to have some gamma (discount factor) to compute the TD(Lambda). Hence, Sutton must have used some value for gamma. I could not pin point what that could be (hence trial and error).</p>",2019-09-16T04:04:07Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lvwbdunsc5ly,2019-09-16T04:04:07Z,{},project1
1513,no,<p>What is the value of $$\gamma$$ if it&#39;s invisible in a mathematical expression?</p>,2019-09-16T04:06:01Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lvyrg93ud7kc,2019-09-16T04:06:01Z,{},project1
1514,no,"<p>Not sure what you mean by invisible. However, I believe he used gamma = 1. I need to do the project in order to say that with great confidence.. </p>",2019-09-16T04:13:39Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lw8km6q475jg,2019-09-16T04:13:39Z,{},project1
1515,no,"<p>See my edit to the Instructor&#39;s answer above.  You will want to follow Sutton&#39;s paper as closely as possible (algorithms and experiments) for your best chance at replicating the results.  As you dig deeper into the assignment, you will find that there is some trial and error involved.</p>",2019-09-16T13:22:41Z,43,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0mfumlx6fe2q5,2019-09-16T13:22:41Z,{},project1
1516,no,<p>I&#39;m having trouble relating the equations in Sutton&#39;s paper to the algorithm in the lectures video. Can anyone help? Is it best to implement how they write in in the lectures video or how sutton writes things in his paper?</p>,2019-09-16T03:54:59Z,43,Week 9/15 - 9/21,followup,,jzjr6qef6i06tp,k0lvkkqcvhu6la,2019-09-16T03:54:59Z,{},project1
1517,no,"<p>I personally found Sutton&#39;s paper to be harder to understand than the lecture, It may be that it is condensed and intense on Math.....</p>
<p></p>",2019-09-16T04:01:07Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lvsg8h6z42dk,2019-09-16T04:01:07Z,{},project1
1518,no,<p>I would recommend Sutton&#39;s paper with the lectures as supporting material since you are replicating the results of his paper. </p>,2019-09-16T13:23:50Z,43,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0mfw45avh551b,2019-09-16T13:23:50Z,{},project1
1519,no,"<p>Tim, I did not mean to say that one should ignore Sutton&#39;s paper. When it comes to implementing the TD(lambda) algorithm (regardless of the test cases such as random walk in Sutton&#39;s paper), I felt that the lecture did a good job. Ofcourse, one needs to apply that to sutton&#39;s paper for project 1. I also don&#39;t believe that following the lecture to implement TD(Lambda) algorithm would give a different results as the underlying math is the same. Am I correct in saying this? or there are two TD(lambda) algorithms, one in the lecture and one in Sutton&#39;s paper <strong>and the two are different</strong> and produce different results? The analogy is implementing value iteration following littman&#39;s paper or some other paper when applied to the same MDP should produce the same exact results. Correct? (assuming test conditions and hyper parameters are the same when applied to two implementations of value iteration)</p>",2019-09-16T19:50:57Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0mtpy6xd1v3g0,2019-09-16T19:50:57Z,{},project1
1520,no,<p>you are correct. I was not saying that they were different. My main point was that you might confused trying to switch between the lecture and paper since some of the notation is different.   My recommendation is to follow Sutton&#39;s paper.  You can certainly follow the lecture equations if you want and in theory you should be able to get the correct replication.</p>,2019-09-16T23:56:09Z,43,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0n2h9vhs431yx,2019-09-16T23:56:09Z,{},project1
1521,no,"<p>yes, I still have sometime to immerse myself into the math and how Sutton explains the differences in updating the weights among various TD methods. I wish I had a Math Phd though to go through it without permanently damaging my brain!!!</p>",2019-09-17T00:21:13Z,43,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0n3divw9xarp,2019-09-17T00:21:13Z,{},project1
1522,no,"<p>I am trying to implement TD(1) per lecture video algorithm:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ll2xkiDJf%2Fk0nf8zmbf8o3%2Ftd1.jpg"" alt=""td(1)"" />.</p>
<p>is there an error in this slide at the top right? should one have <a href=""https://www.codecogs.com/eqnedit.php?latex=V_%7bT-1%7d%28s%29=V_%7bT%7d%28s%29"" target=""_blank"" rel=""noopener noreferrer""><img src=""https://latex.codecogs.com/gif.latex?V_%7bT-1%7d%28s%29=V_%7bT%7d%28s%29"" /></a>   </p>
<p></p>
<p>Also the learning rate as described in the lecture, <a target=""_blank""></a> is  1/T where T is incremented at every episode?</p>
<p></p>
<p>While in Sutton&#39;s paper learning rate is a variable between 0 and 1.</p>
<p></p>
<p>So which one is it?  </p>",2019-09-17T06:08:14Z,43,Week 9/15 - 9/21,followup,,j6ll2xkiDJf,k0nfrruqerm500,2019-09-17T06:08:14Z,{},project1
1523,no,<p>Sutton&#39;s approach in computing TD(Lambda) is different than the one in the lecture. I am implementing what is in the paper and not in the lecture for this project.</p>,2019-09-27T08:44:00Z,42,Week 9/22 - 9/28,feedback,,j6ll2xkiDJf,k11vqm7bwif5nl,2019-09-27T08:44:00Z,{},project1
1524,no,"<p>They&#39;re actually not different.  The notation is different, and the vectorization in the paper makes the connection between the two notations difficult to see.</p>",2019-09-27T08:51:34Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k11w0ch71ls160,2019-09-27T08:51:34Z,{},project1
1525,no,"<p>I believe for the project 1 purposes, using the lecture video may not assist in replicating the three figures.</p>",2019-10-04T20:10:21Z,41,Week 9/29 - 10/5,feedback,,j6ll2xkiDJf,k1ckc8ia5mg28l,2019-10-04T20:10:21Z,{},project1
1526,no,"<p>&#34;The true probabilities of right-side termination the ideal predictions - for each of the non terminal states can be computed as described in section 4.1. These are 1/6, 1/3, 1/2, 2/3, 5/6  for states B, C, D, E and F, respectively.&#34; Wondering if these are the V(s) for the random walk MDP computed by value iteration?  The math in sec 4.1 is somewhat hard to follow.....</p>
<p></p>
<p></p>",2019-09-16T02:11:13Z,61,Week 9/15 - 9/21,followup,,j6ll2xkiDJf,k0lrv4mod783sd,2019-09-16T02:11:13Z,{},project1
1527,no,"<p>You can derive this by solving a linear system of equations, by hand.  It&#39;s just a Markov chain with 5 states.  Each state&#39;s value is the probability it reaches the rightmost state.</p>",2019-09-16T02:46:56Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lt520aksu11p,2019-09-16T02:46:56Z,{},project1
1528,no,"<p>OK though if the number of states changes and the probabilities change, then one needs to programmatically arrive to ideal predictions....</p>",2019-09-16T03:50:37Z,61,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lveyrsta31fn,2019-09-16T03:50:37Z,{},project1
1529,no,"<p>Sorry, I don&#39;t quite understand.  The number of states is fixed (5 states).  The probability model is fixed (random walk).  If you&#39;re talking about solving some general problem that doesn&#39;t involve this project then yes, solving a system of equations for <em>this</em> model wouldn&#39;t translate to something else.</p>",2019-09-16T03:54:26Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lvjv7wo9b2mq,2019-09-16T03:54:26Z,{},project1
1530,no,"<p>I understand but say I wanted to do a similar analysis on a MDP with more states, I do not want to do things by hand....</p>",2019-09-16T04:10:57Z,61,Week 9/15 - 9/21,feedback,,j6ll2xkiDJf,k0lw540npwj571,2019-09-16T04:10:57Z,{},project1
1531,no,"<p>Sorry, I was under the impression you wanted to know where Sutton got the ideal prediction values from.</p>",2019-09-16T04:13:15Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0lw82mz6czq,2019-09-16T04:13:15Z,{},project1
1532,no,"<p>Do we take the ideal prediction values as given, or should we show work that derives these values as well?</p>",2019-09-17T05:12:59Z,61,Week 9/15 - 9/21,followup,,jqq4kjckTEy3,k0ndsq7h50p8r,2019-09-17T05:12:59Z,{},project1
1533,no,<p>you can use them as specified in his paper.</p>,2019-09-17T11:45:37Z,61,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0nrtnmnams61z,2019-09-17T11:45:37Z,{},project1
1534,no,"<p>Mine also blow up for some combinations of lambda and alpha greater than 0.5. My shape looks the same as Sutton&#39;s but for lower values of alpha than what he achieved:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl3we43d3bp15p%2Fk0lu4bdnm7u6%2Ffigure_02.png"" alt="""" /></p>",2019-09-16T03:31:48Z,43,Week 9/15 - 9/21,followup,,jl3we43d3bp15p,k0luqqzclcv5nx,2019-09-16T03:31:48Z,{},project1
1535,no,<p></p>,2019-09-16T04:19:31Z,43,Week 9/15 - 9/21,feedback,,jvfpllmsggt7p4,k0lwg4e3n366f,2019-09-16T04:19:31Z,{},project1
1536,no,"<p></p><div>
<div></div>
</div>
<p>I have exactly the same problem</p>
<div></div>",2019-09-22T05:43:09Z,43,Week 9/15 - 9/21,feedback,,ixpcm4a3mo22i2,k0uk2s8dh3w6n6,2019-09-22T05:43:09Z,{},project1
1537,no,"<p>Here&#39;s mine, very similar to Shayan&#39;s:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk0m3ln1iulcm%2FFigure4.PNG"" alt="""" /></p>
<p></p>
<p></p>
<p>There&#39;s enough variance due to the randomness of the input sequences that it&#39;s silly to try to replicate the<em> exact</em> graph that Sutton has I think.  But I&#39;d definitely like to get my 0.45 closer to 0.6.</p>",2019-09-16T07:44:25Z,43,Week 9/15 - 9/21,followup,,jzfsa4a37jf4aq,k0m3rmk5j9c72h,2019-09-16T07:44:25Z,{},project1
1538,no,"<p>Better...</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk0m4y085gji3%2FFigure4_4.PNG"" alt="""" /></p>
<p></p>",2019-09-16T08:17:45Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0m4yhl8faj5f0,2019-09-16T08:17:45Z,{},project1
1539,no,<p>Lol how&#39;d you do this Vahe? Just different seed - or something more involved?</p>,2019-09-16T16:17:04Z,43,Week 9/15 - 9/21,feedback,,jl3we43d3bp15p,k0mm2we946o6r9,2019-09-16T16:17:04Z,{},project1
1540,no,<p>I found a hidden hyperparameter... I wonder if Sutton used this also.</p>,2019-09-16T18:18:51Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0mqfik99hm4bz,2019-09-16T18:18:51Z,{},project1
1541,no,<p>Oh! Was that in the readings? I&#39;m behind on those. </p>,2019-09-16T20:32:15Z,43,Week 9/15 - 9/21,feedback,,jl3we43d3bp15p,k0mv72nedv63rd,2019-09-16T20:32:15Z,{},project1
1542,no,<p>What even is a hyperparameter? First time I&#39;ve heard that word</p>,2019-09-16T20:52:34Z,43,Week 9/15 - 9/21,feedback,,jzttp1ojahj6ju,k0mvx76fe0a5og,2019-09-16T20:52:34Z,{},project1
1543,no,"<p>Nothing from any readings.  Just something I discovered playing around with the setup.</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Hyperparameter_%28machine_learning%29"">https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)</a></p>",2019-09-16T21:11:58Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0mwm55a62e5ar,2019-09-16T21:11:58Z,{},project1
1544,no,"<p>I superimposed Vahe&#39;s result image atop Sutton&#39;s Figure 4:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0qy22eqwxnn%2Fvahesuperimposed.png"" alt="""" /></p>",2019-09-19T17:03:31Z,43,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qy26qgw3528,2019-09-19T17:03:31Z,{},project1
1545,no,"<p>Here is mine: my lambda 1 seems to blow up more for some reason... </p>
<p></p>
<p></p>
<p></p>
<p>Edit: Fixed a bug and here is mine: </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjvfpllmsggt7p4%2Fk0macv429d6h%2FUntitled.png"" alt="""" width=""490"" height=""248"" /></p>",2019-09-16T08:16:12Z,43,Week 9/15 - 9/21,followup,,jvfpllmsggt7p4,k0m4whkr4jr2qa,2019-09-16T08:16:12Z,{},project1
1546,no,<p>Sutton&#39;s blows up too.  He conveniently stops graphing it after 0.35...</p>,2019-09-16T08:29:07Z,43,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0m5d45t23f49m,2019-09-16T08:29:07Z,{},project1
1547,no,"<p>Good call! However, i noticed that my error starts to diverge at 0.05 :&lt;</p>",2019-09-16T08:29:52Z,43,Week 9/15 - 9/21,feedback,,jvfpllmsggt7p4,k0m5e2v0de91bi,2019-09-16T08:29:52Z,{},project1
1548,no,<p></p>,2019-09-16T08:40:33Z,43,Week 9/15 - 9/21,feedback,,jvfpllmsggt7p4,k0m5rtk64vj3ew,2019-09-16T08:40:33Z,{},project1
1549,no,"<p><strong>EDIT - solved!</strong> In case anyone else runs into the same issue, a clue is that no weight updates should be applied mid-sequence!  (guess that&#39;s a pretty big clue...)</p>
<p><strong>EDIT2:</strong> This raises a question: why shouldn&#39;t we apply weight updates mid-sequence? Appears it achieves lower errors... (Temporarily unresolving in hopes of hearing some thoughts here)</p>
<p></p>
<p>Has anyone else seen behavior like this before? Any hints on what the issue was?</p>
<p>Lambda 0 error doesn&#39;t rise to surpass lambdas of 0.3 or 0.8 as alpha increases.</p>
<p></p>
<p>I&#39;ve double checked that I&#39;m implemented experiment 2 correctly: single pass over each training set (no convergence), weights re-initialized to 0.5 starting for each training set, weights updating after each sequence.</p>
<p></p>
<p>One of the only things I appear to have done differently (as far as I can tell) is implementing TD according to Sutton&#39;s <a href=""http://incompleteideas.net/book/ebook/node75.html"" target=""_blank"" rel=""noopener noreferrer"">Backward View of TD</a> Fig7.7. This appears identical to the lecture, and appears very close (if not mathematically equivalent) to the approach described in the 1988 paper. Reviewed section 2.3 of the paper a few times (&#34;The TD(Lambda) family of learning procedures&#34;) to see if I can identify differences, but the approaches appear similar mathematically as far as I can tell. Appreciate any insights on what might be off.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fk0r6lw62aqlr%2Ff4.png"" alt="""" /></p>",2019-09-19T21:12:03Z,43,Week 9/15 - 9/21,followup,,isde34zracb1mz,k0r6xtb0ftg6i2,2019-09-19T21:12:03Z,{},project1
1550,no,"<p>Weight changes are often applied intra-sequence (it&#39;s in section 5.2 of the paper). But since this causes the weight vector to be different for each update step, I think it messes up the proof of convergence :(</p>",2019-09-19T23:02:40Z,43,Week 9/15 - 9/21,feedback,,jzifg1e23c29s,k0raw2g31cf6wa,2019-09-19T23:02:40Z,{},project1
1551,no,"<p>Makes sense. Thanks, Steph!</p>",2019-09-20T01:02:40Z,43,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0rf6drhghl1pg,2019-09-20T01:02:40Z,{},project1
1552,no,"<p>For anyone plotting with matplotlib, how do you get the plot for lambda = 1 to stop at 0.4?</p>",2019-09-23T14:51:32Z,42,Week 9/22 - 9/28,followup,,jqmjg3txqw6ji,k0wj3v815et9i,2019-09-23T14:51:32Z,{},project1
1553,no,"<p>This might be kind of sloppy, but when I don&#39;t want to plot some numbers in a sequence, but not disturb the indices, I just set the unwanted data points to np.nan.</p>",2019-09-23T15:00:33Z,42,Week 9/22 - 9/28,feedback,,jc9hybafy446pq,k0wjfg6q7af45j,2019-09-23T15:00:33Z,{},project1
1554,no,"<p>That worked, thanks!</p>",2019-09-23T15:15:07Z,42,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0wjy6rpmqh9s,2019-09-23T15:15:07Z,{},project1
1555,no,"<p>you could also set the ylim to be the range of y values you are interested in, that will automatically drop the points outside the range from the plot</p>",2019-09-27T01:26:07Z,42,Week 9/22 - 9/28,feedback,,j6pmq1sglzo35i,k11g3huqvfg1gr,2019-09-27T01:26:07Z,{},project1
1556,no,"<p>Has anyone figured this out? My graph looks exactly like Vahe&#39;s first graph. I&#39;ve been looking into hyperparameters, but nothing is clicking. Did others solve with hyperparameters or some other method? </p>",2019-09-23T15:57:10Z,42,Week 9/22 - 9/28,followup,,jqmjg3txqw6ji,k0wlg9fc2dd32x,2019-09-23T15:57:10Z,{},project1
1557,no,<p>&#64;307</p>,2019-09-23T20:44:10Z,42,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0wvpcekgvy7hi,2019-09-23T20:44:10Z,{},project1
1558,no,"<p>In addition to the great answer above, there&#39;s a gorilla in the room wrt convergence issues.  Something you haven&#39;t mentioned that you can play with...</p>",2019-09-16T17:25:52Z,61,Week 9/15 - 9/21,followup,,jzfsa4a37jf4aq,k0mojddnn9k2d3,2019-09-16T17:25:52Z,{},project1
1559,no,"<p>It was indeed a good reply! That&#39;s clarify that at least I&#39;m in the right direction, although I haven&#39;t found yet the gorilla in the room that is messing with my numbers.</p>",2019-09-16T18:41:39Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0mr8tpb3xq6wd,2019-09-16T18:41:39Z,{},project1
1560,no,"<p>Just to be registered for other students that may have in the same situation.</p>
<p></p>
<p>I found out what was going on... I was resetting my delta_w vector to zeros in the wrong loop, so I was carrying left overs even after updating my weights.</p>",2019-09-17T02:50:21Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0n8pb1ogqk4a0,2019-09-17T02:50:21Z,{},project1
1561,no,"<p>Hi, &#64;Timothy Bail </p>
<p>I am having more than massive convergence issues, even negative weights despite checking my code and double-checking the equations with the paper.</p>
<p>Thanks for the tip but to my understanding, hyperparameters are the parameters one sets before starting the algorithm, and I can&#39;t see anything else than alpha.  </p>
<p>There&#39;s no policy other than &#39;walk&#39;, the probabilities of the walk itself are fixed... </p>
<p>I thought TD was always converging.</p>",2019-09-21T02:00:10Z,61,Week 9/15 - 9/21,followup,,jzh6k6o994a6dh,k0swo64l1mr65v,2019-09-21T02:00:10Z,{},project1
1562,no,<p>yeah alpha is one of the important parameters.  How low are you going with alpha?</p>,2019-09-22T00:23:29Z,60,Week 9/22 - 9/28,feedback,,hz7meu55mi8sd,k0u8np92l92kt,2019-09-22T00:23:29Z,{},project1
1563,no,"<p>Just wanted to check if I&#39;m on the right track.</p>
<p></p>
<p>For my implementation of figure 3,  I observed that any alpha &gt; 0.05 doesn&#39;t seem to converge. Anyone else experienced the same? </p>
<p></p>
<p>Convergence for me is defined as a maximum(taken across all weight elemetns) weight difference of &lt;1% across iterations.</p>",2019-09-21T09:24:49Z,61,Week 9/15 - 9/21,followup,,ijctp4ucNy8,k0tcjzsq5jk4bc,2019-09-21T09:24:49Z,{},project1
1564,no,"<p>You may have the same problem as me see &#64;334.</p>
<p>If we don&#39;t backup the weights until the 10 sequences are done, then the delta w for F is the only one to change in the first training set since all the weights (therefore the P&#39;s) are zero.</p>
<p>But F gets a $$ 1 * \alpha$$ everytime it reaches G, so it creeps up way above 1 before anything gets done. </p>
<p>And when the weights get finally updated, it goes downhill from there, the &gt;1 value spreads to other states, and one can see the weights truly expand to high numbers at every sequence.</p>
<p>And it all gets to the value of alpha, so I&#39;m not surprised about what you say.  </p>
<p>Maybe we have to use eligibility traces to make the delta w fade and never allow a w to go above 1.</p>
<p>I haven&#39;t figured it out yet, and no one has replied to my question so far.  </p>",2019-09-21T12:59:45Z,61,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0tk8ev4wuj1v3,2019-09-21T12:59:45Z,{},project1
1565,no,"<p>As I understand from that paragraph, there is a sequence of observations $$x_{1}, x_{2}, x_{3},.. x_{m} $$ which the learner uses to make $$ P_{1}, P_{2}, P_{3},.. P_{m} $$. My confusion lies in the value of $$P_{m&#43;1} $$ when solving for $$ \Delta w_{m} $$. Do you treat it as a 100% prediction of state A or G as the outcome dictates?</p>
<p></p>",2019-09-16T22:30:49Z,61,Week 9/15 - 9/21,followup,,i4lzw47xh1u57w,k0mzfjdb8zt4d1,2019-09-16T22:30:49Z,{},project1
1566,no,"<p>I want to also recommend a recorded set of RL course lectures from IIT Madras on NPTEL - <a href=""https://nptel.ac.in/courses/106106143/"">https://nptel.ac.in/courses/106106143/</a> - great content and structure.</p>",2019-09-29T19:15:10Z,33,Week 9/29 - 10/5,followup,,jzhwdil7ssn443,k15d6073vra6ln,2019-09-29T19:15:10Z,{},hw3
1567,no,<p>And of course DeepMind&#39;s specific course on DeepRL - https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs</p>,2019-10-27T19:19:13Z,29,Week 10/27 - 11/2,followup,,jzhwdil7ssn443,k29dn2iqqad59o,2019-10-27T19:19:13Z,{},hw3
1568,no,<p>:)</p>,2019-09-17T04:03:43Z,61,Week 9/15 - 9/21,followup,,jqrr36mqfm8M,k0nbbnl296557r,2019-09-17T04:03:43Z,{},project1
1569,no,"<p>I ask because I don&#39;t know and also because I spent way too much time trying to understand the incorrect Figure 3, but is it not standard to just change the figures in the paper (as opposed to mentioning fixes in the erratum)?</p>",2019-09-17T04:04:44Z,61,Week 9/15 - 9/21,followup,,hbmelkhwx5a5d3,k0nbcy9ex304d5,2019-09-17T04:04:44Z,{},project1
1570,no,"<p>LOL...  I found this &#39;digitally remastered&#39; version, with all new images and no errata:</p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk0ne2ggjaxse%2FSutton1998_dr.pdf"" target=""_blank"" rel=""noopener noreferrer"">Sutton1998_dr.pdf</a></p>
<p></p>
<p>I downloaded it from this site, where apparently it was uploaded by R. Sutton himself:</p>
<p></p>
<p><a href=""https://www.researchgate.net/publication/225264698_Learning_to_Predict_by_the_Method_of_Temporal_Differences"">https://www.researchgate.net/publication/225264698_Learning_to_Predict_by_the_Method_of_Temporal_Differences</a></p>",2019-09-17T05:22:10Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0ne4jcyteq7l3,2019-09-17T05:22:10Z,{},project1
1571,no,<p>The quality of the PDF is much higher than the one offered on canvas too. It is an electronic version (probably a much better way to refer to it) rather than a scanned and pixelated print </p>,2019-09-17T11:57:37Z,61,Week 9/15 - 9/21,feedback,,jc554uhtm3s3p3,k0ns939s8ln5xl,2019-09-17T11:57:37Z,{},project1
1572,no,"<p>I agree. Why not fix the paper and not confuse the people who will read it for generations to come or at least mention somewhere on that page to please refer to correct figure in erratum :)</p>
<p></p>",2019-09-20T17:44:25Z,61,Week 9/15 - 9/21,feedback,,jfzaqnqvtQ1m,k0seyn5rftd7a2,2019-09-20T17:44:25Z,{},project1
1573,no,<p>Well think of it this way.  Everyone who takes this course will have learned a lesson that people who only look at the correct paper won&#39;t learn:  always check the errata at the end of the paper first!</p>,2019-09-20T17:48:46Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0sf48fafbq55,2019-09-20T17:48:46Z,{},project1
1574,no,<p>True that!</p>,2019-09-21T02:15:23Z,61,Week 9/15 - 9/21,feedback,,jfzaqnqvtQ1m,k0sx7qki9s330n,2019-09-21T02:15:23Z,{},project1
1575,no,"<p>Thank you for this, I&#39;ve found this to be the most useful lecture video by far. Before his paper gets math heavy, I find the way he speaks on the intuition of RL and TD is quite enchanting. I think this combines the best of both.</p>",2019-09-18T01:37:13Z,61,Week 9/15 - 9/21,followup,,jzg6j8k09gm3zm,k0olj3u78aw2mp,2019-09-18T01:37:13Z,{},project1
1576,no,<p>so in that equation we use the next sequence dot product for $$P_{t&#43;1}$$?</p>,2019-09-17T15:51:02Z,61,Week 9/15 - 9/21,followup,,jzj7y1ofgsro1,k0o0la03gaf685,2019-09-17T15:51:02Z,{},project1
1577,no,"<p>Yeah, in that equation you can substitute all of the $$t$$&#39;s with any value, so for the next observation in the sequence (at time $$t&#43;1$$): $$P_{t&#43;1}=w^T x_{t&#43;1}$$</p>",2019-09-17T16:06:26Z,61,Week 9/15 - 9/21,feedback,,jzlyi4e55bz5kj,k0o152cn5rd2c2,2019-09-17T16:06:26Z,{},project1
1578,no,<p>what about the last and first observations?</p>,2019-09-17T16:09:12Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0o18mynbo75ji,2019-09-17T16:09:12Z,{},project1
1579,no,<p>what about the last and first observations?</p>,2019-09-17T16:12:49Z,61,Week 9/15 - 9/21,followup,,jzj7y1ofgsro1,k0o1dajw7j72y2,2019-09-17T16:12:49Z,{},project1
1580,no,"<p>I just thought to skip those, </p>",2019-09-17T16:17:51Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0o1jrecaqh6ao,2019-09-17T16:17:51Z,{},project1
1581,no,<p>But I can&#39;t as the first will always be D right.</p>,2019-09-17T16:19:28Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0o1lu34hwo8p,2019-09-17T16:19:28Z,{},project1
1582,no,"<p>The first observation is just D, like you say. For the last observation, the t&#43;1 predication should actually be your reward value received, which is iether 0 or 1. Make sense?</p>",2019-09-17T16:27:20Z,61,Week 9/15 - 9/21,feedback,,jl3we43d3bp15p,k0o1vy0q7hr3y9,2019-09-17T16:27:20Z,{},project1
1583,no,<p>yes thank you</p>,2019-09-17T16:34:43Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0o25fw3bqs4m6,2019-09-17T16:34:43Z,{},project1
1584,stud,"<p>More or less. The only thing to watch out for is that if $$T&#43;1$$ is the length of an entire observation-outcome sequence (i.e., we&#39;ve got $$T$$ observations, $$x_{1}, x_{2}, ..., x_{T}$$ followed by one outcome, $$z$$, which is either $$0$$ or $$1$$ depending upon whether we saw a left termination or a right termination) then $$P_{T&#43;1} = z$$, not $$\vec{w} \cdot \vec{x_{T&#43;1}}$$. In truth, there is no $$x_{T&#43;1}$$. The last element of any observation outcome sequence is an <em>outcome </em>and <em>not</em> another observation vector. </p>",2019-09-17T16:38:00Z,61,Week 9/15 - 9/21,followup,a_0,,k0o29of2wv23kj,2019-09-17T16:38:00Z,{},project1
1585,no,<p>so $$P_{t&#43;1}$$ is the termination vector(either [1 0...] or [... 0 1] always? I thought it was next step in a sequence? </p>,2019-09-17T17:30:50Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0o45m8x10k4rs,2019-09-17T17:30:50Z,{},project1
1586,stud,"<p>The outcome of a sequence is <em>not</em> a vector for Sutton&#39;s Markov walk (which is all we care about here). Look at the paper. We&#39;re looking at observation-<em>outcome</em> sequences. The last step ($$T &#43; 1$$) of any observation outcome sequence is an <em>outcome</em>, not an observation vector. There is no &#34;termination vector&#34;. The last element of each sequence is a scalar, either $$1$$ or $$0$$. Which of these it is will depend upon whether we terminated on the right or the left. </p>
<p></p>
<p>This is all spelled out in the paper, btw. </p>",2019-09-17T17:35:10Z,61,Week 9/15 - 9/21,feedback,a_0,,k0o4b6qj96126j,2019-09-17T17:35:10Z,{},project1
1587,stud,"<p>Notice also that $$t$$ in $$P_{t&#43;1}$$ is a <em>variable. </em>$$T$$ is a constant, and is the number of observations in an observation-outcome sequence of length $$T &#43; 1$$. Sutton uses $$m$$ for this value earlier in the paper, and <em>defines</em> $$P_{m&#43;1}$$ to be $$z$$, the outcome of the sequence. So, if you like, $$P_{t&#43;1} |_{t=T} = z$$, whereas $$P_{t&#43;1}|_{t &lt; T} = \vec{w} \cdot \vec{x}_{t&#43;1}$$</p>",2019-09-17T17:51:37Z,61,Week 9/15 - 9/21,feedback,a_0,,k0o4wcmisvx144,2019-09-17T17:51:37Z,{},project1
1588,no,"<p>&#64;Angel Javier - $$P_{t&#43;1}$$ is a scalar, $$z$$. Only then can it be possible that we are calculating the error $$w^T.*x_{t&#43;1} - w^T.*x_{t}$$ </p>
<p>This error term is scaled with our current observation $$\textbf x_t$$.</p>",2019-09-18T12:32:05Z,61,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0p8x9opf6m4w4,2019-09-18T12:32:05Z,{},project1
1589,no,<p>Can you explain how you go from ∇<sub>w</sub>P<sub>k</sub> to x<sub>t</sub>? I&#39;m still not clear on how to use the gradient operator here</p>,2019-09-19T14:33:59Z,61,Week 9/15 - 9/21,followup,,jl284xdcifz44g,k0qspw1s1ku6w1,2019-09-19T14:33:59Z,{},project1
1590,no,"<p>Vahe explains it here:</p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=246"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=246</a></p>",2019-09-19T15:20:47Z,61,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0que2a3nb14px,2019-09-19T15:20:47Z,{},project1
1591,no,"<p>When you say $$P_t$$ do you mean the probs of each state or do u mean the values $$(1/6, 2/6, .. , 5/6)$$ ?</p>",2019-09-18T00:28:22Z,61,Week 9/15 - 9/21,followup,,jl8j7vzvUNs2,k0oj2k1tlii3xx,2019-09-18T00:28:22Z,{},project1
1592,no,<p>$$P_t$$ is the prediction at step t.</p>,2019-09-18T02:37:24Z,61,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0onoi14hs02ws,2019-09-18T02:37:24Z,{},project1
1593,no,"<p>weight vector wt is all zeroes to begin with. The prediction P will also be zeroes to begin with. </p>
<p></p>
<p>P=w⋅x</p>
<p></p>
<p>Also w itself depends on alpha*(Pt&#43;1 - Pt)*sum_of_products</p>
<p></p>
<p>So how will be prediction values ever get non-zero values? </p>
<p></p>
<p>Also in sutton&#39;s formula, there is no use of transition probability ? That is suspicious as well. Maybe its implicitly assumed to reflect in P values maybe?</p>
<p></p>",2019-09-21T04:35:44Z,61,Week 9/15 - 9/21,followup,,jqrr36mqfm8M,k0t2886tejh4d2,2019-09-21T04:35:44Z,{},project1
1594,no,"<p>I also see a formula Pm&#43;1 = z</p>
<p></p>
<p>z is the outcome of the sequence</p>
<p></p>
<p>This implies when we are at a state whose next state is a terminal state with value  =1,  then Pm&#43;1 gets updated to 1?</p>
<p></p>
<p>I think I am mixing up different P definitions...</p>
<p></p>
<p>When do you update Pm&#43;1 = z vs P = w.x</p>
<p></p>
<p></p>",2019-09-21T04:56:36Z,61,Week 9/15 - 9/21,feedback,,jqrr36mqfm8M,k0t2z2sjxfy3vq,2019-09-21T04:56:36Z,{},project1
1595,no,<p>Have you found the answer to this?</p>,2019-09-22T14:10:43Z,60,Week 9/22 - 9/28,feedback,,jc7wfm89uv844j,k0v27itlvpd31t,2019-09-22T14:10:43Z,{},project1
1596,no,"<p>We have to do both, since they calculate P values at different states. Internal state use P = w.x, terminal state Pm&#43;1 = z</p>",2019-09-22T16:08:36Z,60,Week 9/22 - 9/28,feedback,,jqrr36mqfm8M,k0v6f3xvhuf5s1,2019-09-22T16:08:36Z,{},project1
1597,stud,"<p>Sooo I have read the paragraph on the 19th page about 25 times now. </p>
<p>For Example, if I had a walk D--&gt;E--&gt;D--&gt;E--&gt;F--&gt;G</p>
<p>and a weight vector [0.11,0.22,0.33,0.44,0.55] (making things up for arguments sake)</p>
<p>It says &#34;Thus, if the state the walk was in at time t has its 1 at the i th component of its observation vector, then the prediction Pt = &#39;wTxt was simply the value of the ith component of w. &#34; <br /><br />so, in this case, my prediction for D is 0.33 ? </p>
<p>and I just keep updating this new weight vector....? </p>",2019-09-24T01:09:38Z,60,Week 9/22 - 9/28,followup,a_0,,k0x56qy526n6tw,2019-09-24T01:09:38Z,{},project1
1598,stud,<p>Won&#39;t this mess things up? Like I am constantly updating the weight vector * 1........</p>,2019-09-24T01:10:33Z,60,Week 9/22 - 9/28,feedback,a_0,,k0x57x03l1fkn,2019-09-24T01:10:33Z,{},project1
1599,no,"<p>Here I placed the Figure 5 line on the surface to demonstrate how the optimal alphas follow the inflection line on the surface.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0p8wmwe2dyb%2F3dtdlambdaspacewithfigure4share.png"" alt="""" /></p>",2019-09-18T12:32:01Z,29,Week 9/15 - 9/21,followup,,jc554vxmyuy3pt,k0p8x6edge516a,2019-09-18T12:32:01Z,{},project1
1600,no,<p>Awesome!</p>,2019-09-18T20:38:23Z,29,Week 9/15 - 9/21,followup,,jzygktecb0e3i1,k0pqancdd7z4di,2019-09-18T20:38:23Z,{},project1
1601,no,<p>Thanks!</p>,2019-09-18T22:41:27Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0puowtaufd51f,2019-09-18T22:41:27Z,{},project1
1602,no,<p>It is interesting to see the figure in this way. Thanks for sharing!</p>,2019-09-19T07:18:03Z,29,Week 9/15 - 9/21,feedback,,jl5wq8mca7o0,k0qd59iu65x6gu,2019-09-19T07:18:03Z,{},project1
1603,no,"<p>Jacob,</p>
<p></p>
<p>Very interesting charts.</p>
<p>I have a result very close to yours related to the chart in the bottom right (which should be equivalent to figure 4 from Sutton). I also have the error going high for lambdas equal to 0 or 0.3 for alpha=0.6.</p>
<p></p>
<p>Sutton&#39;s figure 4 has much lower errors for those. Have you figured the reason?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk0qs8ebjshiu%2FCapture.PNG"" alt="""" /></p>",2019-09-19T14:21:07Z,29,Week 9/15 - 9/21,followup,,jc6xvgjncoey,k0qs9cldatr51z,2019-09-19T14:21:07Z,{},project1
1604,no,"<p>Same errors Danilo, just Sutton adjusted the bounds of his graph to highlight the area of interest instead of leaving it open like I did. Take a look at where my $$\lambda(1)$$ ends, it&#39;s at the same eyeball value that Sutton has.</p>
<p></p>
<p>note the logarithmic scale on my 3d image. I found log(rmse) worked well to open the detail that Sutton was trying to get at.</p>",2019-09-19T14:31:27Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qsmmxb7pa6wm,2019-09-19T14:31:27Z,{},project1
1605,no,"<p>Jacob,</p>
<p></p>
<p>I was actually talking about when alpha equals 0.6, Sutton&#39;s chart shows an error of around 0.25 for lambda=0.3. The same occurs to lambda=0, Sutton has an error of about 0.5 for alpha 0.6, this is much lower than you have in the chart (me as well).</p>
<p></p>
<p>In your chart (and in my chart as well), this error is above 1.0. So, it is not the case Sutton adjusted the bounds of the chart like for lambda 1.</p>
<p></p>
<p>So, I spent some time yesterday night trying to figure why in those two cases the error in my chart is going much further than Sutton&#39;s chart.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk0qtjbaf50um%2FCapture2.PNG"" alt="""" /></p>",2019-09-19T15:00:55Z,29,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0qtoip2ltf502,2019-09-19T15:00:55Z,{},project1
1606,no,"<p>You&#39;re correct, the $$\lambda$$=0 has a weird error in Sutton&#39;s chart. I don&#39;t think his chart is accurate. Look at the intersection of $$\lambda$$=0 and $$\lambda$$=0.8. Your version and mine match Sutton&#39;s, which is expected. Sutton clearly did some data framing on his part to make the graphs exemplify the result he was trying to convey. The two exponential boundary conditions - $$\lambda$$=0 and $$\lambda$$=1 - made his graphs look horrible (like a hockey stick). I saw this in the 3D before I switched to logarithmic scale.</p>",2019-09-19T16:04:53Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qvyslmyopzt,2019-09-19T16:04:53Z,{},project1
1607,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0qxlv432dlq%2Fsuperimposedfigure4.png"" alt="""" /></p>",2019-09-19T16:26:28Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qwqjn1tklzd,2019-09-19T16:26:28Z,{},project1
1608,no,"<p>Thanks for the great data visualizations. Any idea why I&#39;m unable to get lambda=0 to catch up and exceed the error on vs lambda=0.3 or 0.8?  I&#39;ve tried various &#34;treatments&#34; on the input and my figure 4 always seems to have lambda=0 at the lowest error even at higher alphas. Couple examples below... any insights would be much appreciated.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fk0qx0w9em2de%2Fwm_EXP2_fig4_12.png"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde34zracb1mz%2Fk0qx1c7cx97f%2Fwm_EXP2_fig4_17.png"" alt="""" /></p>",2019-09-19T16:35:15Z,29,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0qx1twlufi5tq,2019-09-19T16:35:15Z,{},project1
1609,no,"<p>Check that you did not invert the application of the lambda exponent expansion in the summation.</p>
<p></p>
<p>Check that you are applying the weight changes properly according to his algorithm description for experiment 2. This is a significant change from experiment 1, and it&#39;s not just removing convergence.</p>
<p></p>
<p>Don&#39;t massage the data - you don&#39;t need to modify the data in any way.</p>
<p></p>",2019-09-19T16:52:41Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qxo8zq2in41p,2019-09-19T16:52:41Z,{},project1
1610,no,"<p>Thanks, Jacob. I think my root issue may be that I implemented TD(lambda) using the algorithm from lecture (with eligibility traces) as opposed to as described in the paper. Going to give it a shot from the description in the paper.</p>",2019-09-19T17:01:14Z,29,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0qxz91kdor1dv,2019-09-19T17:01:14Z,{},project1
1611,no,<p>Over in &#64;292 I superimposed Vahe&#39;s figure 4 atop Sutton&#39;s and his follows a similar pattern of agreement/disagreement.</p>,2019-09-19T17:05:36Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qy4uvlkyghm,2019-09-19T17:05:36Z,{},project1
1612,no,"<p>I think one of the fascinating things about reading old papers is that you get to see how the experts in a field looked at things when the field was in its infancy.  This includes notational differences.</p>
<p></p>
<p>I think you&#39;ll find that there is an eligibility trace there...it probably just wasn&#39;t formalized (and called that) yet.</p>",2019-09-19T17:39:46Z,29,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0qzctb3bt03ab,2019-09-19T17:39:46Z,{},project1
1613,no,"<p>Yeah, I see how the near-equivalent of an eligibility trace appears to be present in the original paper, but can&#39;t say I&#39;m confident yet that they are absolutely equivalent. Can&#39;t find any other explanation (yet) for my curious Fig4 plots above so I&#39;m trying to revisit the nuances.</p>",2019-09-19T17:51:16Z,29,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0qzrl893s2jh,2019-09-19T17:51:16Z,{},project1
1614,no,"<p>Related: the only &#39;hidden&#39; hyperparameter I was tuning was artificially capping maximum sequence length. I don&#39;t recall exactly where, but remember Sutton describing properties of TD sequences with certain lengths and thought it may at least be interesting to see how the error curves shift with min &amp; max sequence lengths enforced. Appears it doesn&#39;t have much to do with getting TD(0) error increasing faster at higher alphas.</p>",2019-09-19T17:53:42Z,29,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0qzuq7nq5l5z4,2019-09-19T17:53:42Z,{},project1
1615,no,"<p>I&#39;ve come closer to Sutton&#39;s results in Figure 4 by playing with the method of error calculation. His figure 1 states &#34;Average error on random walk problem after experiencing 10 sequences.&#34; That tells me he just did a quick mean of errors from each sequence. When I did this, the result was marginally different from my figure 4.</p>",2019-09-19T18:57:44Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0r252x7dy83ef,2019-09-19T18:57:44Z,{},project1
1616,no,<p>Are you saying you take average over each training set? That is what I&#39;m doing currently.</p>,2019-09-19T20:00:37Z,29,Week 9/15 - 9/21,feedback,,jl2zkad6fs661b,k0r4dxfyqhu5ps,2019-09-19T20:00:37Z,{},project1
1617,no,<p>I am saying that you should experiment with different methods of measuring error.</p>,2019-09-19T20:23:07Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0r56v795b7yx,2019-09-19T20:23:07Z,{},project1
1618,no,"<p>When I&#39;ve seen the P and dW blow up enormously it&#39;s because of some really long walk. I&#39;ve seen 39 step walks that produce huge predictions:</p>
<pre>|| ERROR ERROR t= 39 ,dW is really big,P= [406.37811185 286.32655108 406.37811185 214.195883   406.37811185
 286.32655108 406.37811185 286.32655108 104.14843115 286.32655108
 406.37811185 214.195883   406.37811185 286.32655108 406.37811185
 214.195883   406.37811185 286.32655108 104.14843115 286.32655108
 104.14843115 286.32655108 406.37811185 286.32655108 104.14843115
 286.32655108 406.37811185 214.195883   406.37811185 286.32655108
 406.37811185 214.195883   406.37811185 214.195883    25.15310256
 214.195883   406.37811185 214.195883   406.37811185 214.195883
 406.37811185 214.195883   406.37811185 286.32655108 406.37811185
 214.195883   406.37811185 214.195883    25.15310256   0.        ]</pre>
<p>When I implemented section 5&#39;s intra-sequence weight update, the weights for $$\lambda$$=1 gets stupid big. Not even logarithmic scale helps it.</p>
<p></p>
<p>I wonder if Sutton actually limited his walks to &lt; 10 steps. Hmmm. Will have to try that next.</p>
<p></p>
<p></p>",2019-09-19T23:46:10Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0rcg078i4n40d,2019-09-19T23:46:10Z,{},project1
1619,no,"<p>Doesn&#39;t look like limiting the sequence length is useful....</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0rcrvv6mpqk%2F10stepmaxwalkfig4.png"" alt="""" /></p>",2019-09-19T23:55:38Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0rcs64or685i8,2019-09-19T23:55:38Z,{},project1
1620,no,"<p>Jacob,</p>
<p></p>
<p>When I limit the creation of each sequences to a max of 15, I actually get a chart pretty close to Sutton. I suspect this is the hidden hyperparameter that Vahe was talking about in another thread.</p>
<p></p>
<p>See below:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk0rdd4bo11ln%2FFigure_4.png"" alt="""" /></p>",2019-09-20T00:12:35Z,29,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0rddzbfdje4e6,2019-09-20T00:12:35Z,{},project1
1621,no,"<p>Danilo! I did min=5 and max=15 for the walk length, and I get a really good match on Sutton. That dude ..... he cooked his data.</p>
<p></p>
<p>Here&#39;s the result of my impl with min-walk = 5 and max-walk = 14. I did 10 - 20 for the max walk and the changes are interesting. Too bad I already submitted and I&#39;m out of town in about 2 hours.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0sfmvy9kt3y%2Ffig4514share.png"" alt="""" /></p>",2019-09-20T17:45:53Z,29,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0sf0itwkxyvm,2019-09-20T17:45:53Z,{},project1
1622,no,<p>!!!!!!!!!!!!!!!</p>,2019-09-20T19:08:21Z,29,Week 9/15 - 9/21,feedback,,isde34zracb1mz,k0shyl46w041jj,2019-09-20T19:08:21Z,{},project1
1623,no,<p>Thank you for sharing!!</p>,2019-09-22T16:14:32Z,28,Week 9/22 - 9/28,feedback,,jl284th59gv42o,k0v6mqudwal563,2019-09-22T16:14:32Z,{},project1
1624,no,"<p>So, &#34;cooking his data&#34; was a mischaracterization of Sutton&#39;s method. He was employing a windowing technique that he thought would reduce the variance of his learning. The problem was, it did not do that because he wasn&#39;t sampling enough data. I prepared a 10 episode animation of 10 - 15 length walks. I also did one for the 1000 episode test. You can see that after 100 walks of length 10 - 15 the variance dies down and he would have had a good sampling. The problem though is that in 1988 doing 10 training sets of 1000 walks would have cost him 10x more money. So that&#39;s just not economical. I think he was trying to save money and reduce variance at the same time.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0x5mqwat77i%2Fwinpctmw515_10_episodes.gif"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0x5nay2mmfc%2FWM_rw_prob_win_max1015_min5_1000walks.gif"" alt="""" /></p>",2019-09-24T01:22:41Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0x5niossoqx9,2019-09-24T01:22:41Z,{},project1
1625,no,"<p>I did a movie of the 3D space for the 10 sets of 100 episodes with a 15 walk limit. If I don&#39;t limit the walk then the errors at $$\lambda$$=0 become very large even at $$log_{10}$$ scale.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk0xwe3ygwg3l%2F3d_logrmse_100x10.gif"" alt="""" /></p>",2019-09-24T13:51:32Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0xwejrifhq4eo,2019-09-24T13:51:32Z,{},project1
1626,no,"<p>Sutton, I feel so betrayed...</p>
<p></p>
<p>This is such a cool discussion!</p>
<p></p>
<p>Does anyone see where he talks about censoring the training set on the length of the walks?  I&#39;m not seeing it in the paper.</p>",2019-09-25T02:14:00Z,28,Week 9/22 - 9/28,feedback,,jqmfuaidej9155,k0ymxdn0r6d5tx,2019-09-25T02:14:00Z,{},project1
1627,no,"<p>Does anyone have an idea why I&#39;d get the following graph?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2zkad6fs661b%2Fk0tzc9zrc34x%2FFigure_4_with_Watermark.png"" alt="""" /></p>
<p></p>
<p>It always kind of looks like this, even when limiting the sequence min/max lengths. There is always a big dip at alpha 0.05 and it also blows up at earlier values of alpha.</p>",2019-09-21T20:04:03Z,29,Week 9/15 - 9/21,followup,,jl2zkad6fs661b,k0tze1tzzrv6s6,2019-09-21T20:04:03Z,{},project1
1628,no,<p>did you figure this out I have the same issue?</p>,2019-09-22T23:10:03Z,28,Week 9/22 - 9/28,feedback,,jc6mqevhagl262,k0vlh3vaear34y,2019-09-22T23:10:03Z,{},project1
1629,no,"<p>No, I haven&#39;t figured it out yet. At least it&#39;s nice to know someone&#39;s in the same boat as me. I wonder if there is some precision issue as it accumulates the error? I don&#39;t really know.. I haven&#39;t looked at it recently, just focused more on the report as I was stuck for so long.</p>
<p></p>
<p>Do you have any ideas?</p>",2019-09-23T01:25:49Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k0vqbpmear438y,2019-09-23T01:25:49Z,{},project1
1630,no,Changing the sequence length was key for me,2019-09-23T01:43:39Z,28,Week 9/22 - 9/28,feedback,,jc6mqevhagl262,k0vqyn604x11gg,2019-09-23T01:43:39Z,{},project1
1631,no,<p>Wait so you figured it out? By changing the sequence length how? Making sure they are the same length or having a min and max length?</p>,2019-09-23T12:46:02Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k0wemh2tti66m,2019-09-23T12:46:02Z,{},project1
1632,no,"<p>The max length will control the divergence of the errors at lower $$\lambda$$ values.</p>
<p></p>
<p>You might be experiencing a problem with the $$\lambda$$ summation. Check that you are properly applying that exponent.</p>",2019-09-24T13:53:44Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0xwhdmb1uz1o4,2019-09-24T13:53:44Z,{},project1
1633,no,"<p>Thanks for the suggestions. I was starting my lambda from the 0th term and not the 1st term, so I think that was one issue. I also played with the parameters (this one in particular is 4 - 12) and got the following:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2zkad6fs661b%2Fk0y8gnuxlpn1%2FFigure_4_with_Watermark.png"" alt="""" /></p>
<p></p>
<p>It looks much closer to what had been posted by others. However, I still can&#39;t figure out why I have such a huge error dip in the beginning compared to the others...</p>
<p></p>
<p>Also, my figure 5 looks different as well (it always increases):</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2zkad6fs661b%2Fk0y8gyeupqoy%2FFigure_5_with_Watermark.png"" alt="""" /></p>
<p></p>
<p></p>",2019-09-24T19:29:26Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k0y8h3hm2e919z,2019-09-24T19:29:26Z,{},project1
1634,no,"<p>The first one is clearly not applying a polynomial blending function to the back history of the weights. The straight line means that you are pretty much just applying a constant discount factor to the back history. Definitely look at how you are computing that summation with the lambda term.</p>
<p></p>
<p>Your figure 5 looks like the figure 5 I got when I used a logarithmic scale. ha! Once you figure out the lambda bug that appears to be in your code then the graphs will start to make sense. Remember that you are adding vectors, not scalars.</p>
<p></p>
<p>A 3 level deep history would look like:</p>
<p></p>
<p>$$\Delta w_3 = K * (\lambda^{2}*\bar{x}_1 &#43; \lambda^{1}\bar{x}_2 &#43; \lambda^{0}\bar{x}_3)$$ for $$t=3$$ and $$K$$ is just that $$\alpha*(P_{3&#43;1}-P_3)$$ factor which is going to be $$\alpha * (1 - P_t)$$ or $$\alpha * (0 - P_t)$$  when the $$t=T$$ right?</p>
<p></p>",2019-09-24T20:15:39Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0ya4j8bjxz3nk,2019-09-24T20:15:39Z,{},project1
1635,no,"<p>I am certain that I am doing some kind of discount factor onto my lambda coefficients, although maybe I&#39;m not doing it correctly. I think my graph looks similar to one that Danilo posted earlier except for the values from alpha = 0 to 0.1. I think these might be a little more linear looking because of the parameters as well as my issue near those alpha values. Here is an example with different parameters, using the same code:<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl2zkad6fs661b%2Fk0ycb8m8gabo%2FFigure_4_with_Watermark.png"" alt="""" /></p>
<p></p>
<p>The thing is, yes, I don&#39;t know if I&#39;m doing something incorrectly with this summation or something else. I have checked the code many times, unfortunately, haven&#39;t figured out what the issue is. In fact, I have done two different implementations of the lambda term summation - one using a loop and one using numpy arrays, and they both come to the same results. Guess I&#39;ll keep looking. I don&#39;t know why for these specific values</p>
<p></p>
<p>What you posted at the bottom is what I have to be understood to be correct up until now. But maybe my implementation is incorrect. I&#39;ll keep searching. Thanks for the info.</p>",2019-09-24T21:18:34Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k0ycdfys22t6tc,2019-09-24T21:18:34Z,{},project1
1636,no,"<p>Your graph is following the same trends as all of the graphs posted on this thread. Mine don&#39;t have the stark drop at 0.15 because my max is 1.2 instead of 0.7. I don&#39;t get a 0.7 max unless I limit the walk to [5,14] steps. All of my curves are based upon unbounded walks, which have a very high variance.</p>
<p></p>
<p></p>",2019-09-24T22:18:39Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0yeip7f45i5v9,2019-09-24T22:18:39Z,{},project1
1637,no,<p>I will try and see if I can keep tweaking to get similar results. Gotta see if I can get some of those error values closer I guess... I have done with unbounded walks as well but it still comes out similarly.</p>,2019-09-25T14:45:20Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k0zdrlcs55v412,2019-09-25T14:45:20Z,{},project1
1638,no,"<p>Well, I finally figured out my issue. It was pretty dumb, as this kind of thing usually goes. I was not resetting my weights after each training set run. I only reset it once for each alpha-lambda combination. Thank you Jacob for your help along the way.</p>",2019-09-27T18:49:15Z,28,Week 9/22 - 9/28,feedback,,jl2zkad6fs661b,k12hcyv02696zm,2019-09-27T18:49:15Z,{},project1
1639,no,"<p>&#64;Joshua, I have been stuck with the same problem for several days, doing the same thing you did, and face the big drop which seems to be the optimal value for $$\alpha$$</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl5wq8mca7o0%2Fk14oj3o6anbg%2Ftemp.png"" alt="""" /></p>
<p>I am confused of why we need to reset the weight per each training set, vs each alpha-lambda.</p>
<p>- With resetting per each alpha-lambda, we are taking fitted weight from one training to another training, like we are &#34;transfer&#34; our learning so that we don&#39;t have to start off from the beginning for each training data. That may explain why we get drop to seemingly optimal error even with very small $$\alpha$$.</p>
<p>- With resetting per each training set, we are taking the average of all fitted weight, which may cancel some of the adjustments that we previously learnt, which will make the learning process slower. But why is this a right thing to do?</p>",2019-09-29T07:57:26Z,28,Week 9/22 - 9/28,feedback,,jl5wq8mca7o0,k14oyfga708572,2019-09-29T07:57:26Z,{},project1
1640,no,"<p>&#64;Joshua, I had the exact same problem as you. The description for Fig. 4 says &#34;Average error after experiencing 10 sequences&#34;. This implies that you are starting from fresh every training set, instead of carrying on from previous training. Thus, you need to reset your weights after each training set</p>",2019-09-29T13:28:16Z,27,Week 9/29 - 10/5,feedback,,jzlf32odc2k20h,k150rw5kkng13p,2019-09-29T13:28:16Z,{},project1
1641,no,"<p>Oh, that&#39;s right! For Fig.4 we want to see how fast an $$ \alpha $$ value will help the model learn. So we let it run on only 10 sequences, then take the average over 100 sets of data. Carrying the weight like Joshua and I did, we effectively increase the size of the training into 1000 sequences. No wonder why our model learn the optimal values so fast, even with very low $$ \alpha $$.</p>
<p></p>
<p>Thank you Joshua and Ben for spotlighting the issue and for the reference!</p>",2019-09-29T17:03:40Z,27,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k158gwhojhk5o8,2019-09-29T17:03:40Z,{},project1
1642,no,"<p>What do you mean by limiting the walk to [5,14] steps? Do you mean that if the sequence length (walk steps) is out of this range, you simply ignore this sequence? </p>
<p></p>
<p>How could the large number of walks cause the error to blow up At small lambda values? </p>
<p>Here is my graph. Will it be acceptable?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9puq99s5uv%2Fk11bk3efzyzc%2Ffigure4.png"" alt="""" /></p>
<p></p>",2019-09-26T23:19:08Z,28,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k11bk6rcbmc7fl,2019-09-26T23:19:08Z,{},project1
1643,no,"<p>If I limit the sequence to max of 15 steps, the figure 4 looks still similarly okay, but figure 5 becomes more like figure 3, where the best lambda is 0 instead of 0.3.</p>
<p></p>",2019-09-27T00:11:16Z,28,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k11df8npdya6ay,2019-09-27T00:11:16Z,{},project1
1644,no,"<p>Limiting the walk means we ignore any walks of length L, which in the case I proposed was 15 steps. Having a lower min-walk bound meant I ignored those walks that were less than length B, e.g. 5.</p>
<p></p>
<p>A common variance reduction technique is to throw out the outliers and only experiment on the tighter range of data. Sutton may have done this to reduce the tendency of the random walk to create extreme outliers. I&#39;ve had paths that were 38 steps deep, and these caused very large errors.</p>
<p></p>
<p></p>",2019-09-27T00:40:56Z,28,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k11ehe1rebo1jc,2019-09-27T00:40:56Z,{},project1
1645,no,"<p>I think there could be another reason why he did that, beside reducing variance and outlier.</p>
<p></p>
<p>The first issue I think, is with cycling walk, where we get to a node, then after some steps we get back to the same node again. What we should do is forget the previous steps and restart collection weight-diff. This is the big problem for TD(1), where we remember everything.</p>
<p></p>
<p>The second issue is when we adjusting the weight diff according to $$P_{t} -&gt; P_{t&#43;1}$$ , there is only one to terminal state per sequence, but there can be more than one step <em>away </em>from the terminal state, toward another state. Ex: DEFEFEFG. The step toward the final state (F-&gt;G) would propagate the right value, since terminal state&#39;s value won&#39;t change, and is the ground truth. The step away from the terminal state (F-&gt;E) would update state F toward unknown value. The ratio between the two could throw the model off,and that&#39;s why TD(0) performs so bad on this, while other values tolerate better. The longer the sequence, the higher change there will be steps away from terminal states, and that will cancel all the &#34;good&#34; values that has been propagated.</p>
<p></p>
<p>I think the two issues could be specific to the random walk, and could be a distraction from his theory of sequentially adjusting weight, and maybe he didn&#39;t want to spotlight it too much.</p>",2019-09-29T08:28:28Z,28,Week 9/22 - 9/28,feedback,,jl5wq8mca7o0,k14q2cvozcc289,2019-09-29T08:28:28Z,{},project1
1646,no,"Could explain why figure 2 in Sutton (1998) can become the Widrow-Hoff, in essence it substitutes the vector of partial derivatives of Pt for Xt?",2019-09-20T00:59:37Z,61,Week 9/15 - 9/21,followup,,jl2s2ehalMmK,k0rf2gdlcpc2nm,2019-09-20T00:59:37Z,{},logistics
1647,no,The explanation given didn&#39;t make sense to me why it was reasonsable to do.,2019-09-20T01:01:35Z,61,Week 9/15 - 9/21,feedback,,jl2s2ehalMmK,k0rf4zntb225or,2019-09-20T01:01:35Z,{},logistics
1648,no,"<p>I think you mean Equation (2), not Figure 2</p>",2019-09-20T01:04:00Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0rf83ckm1n6ya,2019-09-20T01:04:00Z,{},logistics
1649,no,<p>Can you explain when do you stop iterating over your training sets?</p>,2019-09-20T01:01:15Z,61,Week 9/15 - 9/21,followup,,jc6mqevhagl262,k0rf4kbmtf8r,2019-09-20T01:01:15Z,{},logistics
1650,no,<p>for experiment one</p>,2019-09-20T01:08:58Z,61,Week 9/15 - 9/21,feedback,,jc6mqevhagl262,k0rfegz4w8r1om,2019-09-20T01:08:58Z,{},logistics
1651,no,<p>Could you please explain the relationship between the vector of partial derivatives of P_t with respect to each component of w defined in the Sutton paper and the eligibility discussed in lecture? </p>,2019-09-22T23:58:32Z,60,Week 9/22 - 9/28,followup,,jl9q3tj6G53J,k0vn7gruf1s1bl,2019-09-22T23:58:32Z,{},logistics
1652,no,"<p>Eligibility traces are a record of how often and how recent a state was visited, so that the TD algorithm can assign the proper credit of a prediction error to states that were or were not visited frequently/recently.</p>
<p></p>
<p>If you look at the vector of partial derivatives and its role in that equation, this is exactly what it is doing.</p>",2019-09-23T00:15:58Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0vntvydjzd36c,2019-09-23T00:15:58Z,{},logistics
1653,no,When will the link be available? ,2019-09-23T08:57:22Z,60,Week 9/22 - 9/28,followup,,jqi9pq31o4p6dt,k0w6gerpin78v,2019-09-23T08:57:22Z,{},logistics
1654,no,<p>I thINK SO!</p>,2019-09-18T20:44:41Z,61,Week 9/15 - 9/21,followup,,jzj7y1ofgsro1,k0pqiqzrtzo1ki,2019-09-18T20:44:41Z,{},project1
1655,no,<p>What are you using for you observations $$x_{t}$$? Since the only time we see a nonzero reward is entering the nonzero terminal state I don&#39;t really understand how $$\Delta w_{t}$$ is suppose to be nonzero since it seems like the summation term will always be zero?</p>,2019-09-20T17:07:04Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0sdmm322mt1gj,2019-09-20T17:07:04Z,{},project1
1656,no,<p>I think you have a type. The power of lambda is t-k. </p>,2019-09-19T01:02:17Z,61,Week 9/15 - 9/21,followup,,jzn63vvy8x67lj,k0pzq0trmnt1jy,2019-09-19T01:02:17Z,{},project1
1657,no,"<p>Thanks Angel for bringing it up.  I&#39;m trying to move forward along the path. </p>
<p>Quoting the paper &#34;Given that $$e_t$$ is the value of the sum in (4) for t, we can incrementally compute $$e_{t&#43;1}$$ using only current information, as:</p>
<p>$$e_{t&#43;1} = \sum\limits_{k=1}^t \lambda^{t&#43;1-k} \nabla_{w} P_{k}$$</p>
<p></p>
<p>So then:</p>
<p>$$e_0 = \alpha(w^T.x_1 - w^T.x_0).x_1$$</p>
<p>$$e_1 = \lambda.e_0 &#43; x_2$$</p>
<p>$$e_2 = \lambda.e_0 &#43; x_3$$</p>
<p>....</p>
<p>...</p>
<p></p>
<p>And the final summation of weights $$e0, e1, e2$$ is $$\Delta w$$</p>
<p></p>
<p>That&#39;s my understanding, but it seems to be a bit too simple, i think i&#39;m missing something there.</p>",2019-09-19T03:38:15Z,61,Week 9/15 - 9/21,followup,,jl8j7vzvUNs2,k0q5alrm5n2qs,2019-09-19T03:38:15Z,{},project1
1658,no,<p>What value of $$t$$ in the equation you wrote above leads to $$e_0$$?</p>,2019-09-19T07:13:19Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0qcz6xw1eo6dm,2019-09-19T07:13:19Z,{},project1
1659,no,"<p>The final sum of weights isn&#39;t the sum of $$e_t$$, but the sum of $$\alpha (P_{t&#43;1} - P_t) e_t$$. Also, check your subscripts and remember the formula is recursive. I&#39;d work out the first few terms using the equation on p. 16 to get a better understanding.</p>",2019-09-19T12:59:39Z,61,Week 9/15 - 9/21,feedback,,jc9hybafy446pq,k0qpckqvy4b64h,2019-09-19T12:59:39Z,{},project1
1660,no,"<p>Thanks Vahe and Wade. I&#39;m ignoring that equation.</p>
<p>I feel I got a hang on the recursion.</p>
<p>And so the total delta w should just be summation of all the individual delta w terms.</p>
<p>I don&#39;t think there is a need to further simply this, at least to implement it easily in code.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl8j7vzvUNs2%2Fk0qpnuznecxe%2FIMG_20190918_223325.jpg"" alt="""" /></p>",2019-09-19T13:14:01Z,61,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0qpv1nxtr6ev,2019-09-19T13:14:01Z,{},project1
1661,no,"<p>This looks good to me. Now factor out the lambas in your terms for $$e_t$$ to see the recursion. The Markov chains aren&#39;t very long, but Sutton does make the point that doing the recursion saves time and space.</p>",2019-09-19T14:40:36Z,61,Week 9/15 - 9/21,feedback,,jc9hybafy446pq,k0qsyeeionq6bu,2019-09-19T14:40:36Z,{},project1
1662,no,"<p>typo for generalized form right, should be xt &#43; lambda*xt-1</p>",2019-09-19T14:57:59Z,61,Week 9/15 - 9/21,feedback,,jzj7y1ofgsro1,k0qtkr7js7v32e,2019-09-19T14:57:59Z,{},project1
1663,no,"<p>Yes, it should be:</p>
<p>$$x_t &#43; \lambda x_{t-1} &#43; ... $$</p>",2019-09-19T15:51:26Z,61,Week 9/15 - 9/21,feedback,,jl8j7vzvUNs2,k0qvhhaierr5kq,2019-09-19T15:51:26Z,{},project1
1664,no,"<p>I am a bit confused by this. If we initiate w = [0.2, 0.2, 0.2, 0.2, 0.2], then won&#39;t all of the values for $$\Delta w_{t}$$ = 0? Since $$P_{t&#43;1} = P_{t}$$</p>
<p></p>
<p>So when will the values of $$\Delta w_{t}$$ get updated?</p>
<p></p>
<p>Edit: Is it that at t=m, we will need to incorporate the final reward?  </p>",2019-09-21T01:24:12Z,61,Week 9/15 - 9/21,feedback,,jqmjg3txqw6ji,k0svdx3ip7w4y9,2019-09-21T01:24:12Z,{},project1
1665,no,"<p>&#64;Alonso, check out &#64;334. Yes, for homogeneous values the $$ \Delta w_t $$ will be 0, until the terminal state. Then on next run, the next-to-terminal state will be non-zero. Then so on. The update value of $$ \Delta w_t $$ get propagate from the terminal state.</p>",2019-09-25T07:41:12Z,60,Week 9/22 - 9/28,feedback,,jl5wq8mca7o0,k0yym5k4q2h4sh,2019-09-25T07:41:12Z,{},project1
1666,no,"<p>Pt corresponds to our current estimate at a given time step which is the value of our weight vector at the same time step and our observations xt correspond to the reward we get for transition to a given xt right? If so, are our observations not always 0 since the reward for transitioning between non terminal states is always 0? I&#39;m a bit confused as to how the terminal state which is greater than zero gets incorporated into delta_w.</p>",2019-09-20T03:40:23Z,61,Week 9/15 - 9/21,followup,,h6e6q7ftqr747f,k0rkt6yhpk36em,2019-09-20T03:40:23Z,{},project1
1667,no,"<p>Your first sentence is a bit confusing so I&#39;m going to try to answer your question by first rewriting that sentence.</p>
<p></p>
<p>&#34;$$P_t$$ corresponds to our current estimate at a given time-step which is the dot-product of our weight vector and our observation, $$x_t$$, at the same time-step.  $$P_t$$, also referred to as a prediction, corresponds to our expected reward, which also happens to be the probability of reaching state $$G$$.&#34;</p>
<p></p>
<p>&#34;If so, are our observations not always 0 since the reward for transitioning between non terminal states is always 0?&#34;</p>
<p></p>
<p>The observations are defined to be the states we&#39;re in at the different time-steps, so they can&#39;t be $$0$$.  I think you meant &#34;Are our <strong>predictions</strong>, $$P_t$$ always $$0$$ since the rewards are all $$0$$ when tranistioning between non-terminal states?&#34;</p>
<p></p>
<p>You kind of answered the question yourself right at the end.  The key to the predictions moving in the right direction is to discover that terminal state which is greater than zero.  But that does happen.  The terminal states are a very important part of the weight update operation.  And so their values will affect the weight changes. Take a look at, for example, the last big paragraph on page 19.</p>",2019-09-20T04:19:00Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0rm6v9jmy62vi,2019-09-20T04:19:00Z,{},project1
1668,no,"<p>In the above calculations for $$\Delta w$$, $$P_{t}$$ equates to the value in our current estimate of w for whatever state we happen to be in at time t, but it&#39;s unclear to me after reading the paragraph you referenced what our observation values $$x_{t}$$ are? If the reward for transitioning into a non terminal state is 0, how will these observation values $$x_{t}$$ be anything, but 0? Unless $$x_{t}$$ also reference some value in w and we choose non zero values for our initial estimates of w.</p>",2019-09-20T04:50:09Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0rnaxkq47h352,2019-09-20T04:50:09Z,{},project1
1669,no,"<p>&#34;If the reward for transitioning into a non terminal state is 0, how will these observation values xt be anything, but 0?&#34;</p>
<p></p>
<p>If our current predictions in all states are $$0$$, and we only transition to the terminal state with reward $$0$$, then yes, we will never predict anything but zero, ever.  But that&#39;s not the case.  Sometimes we&#39;ll transition to the terminal state with reward $$1$$.</p>
<p></p>
<p>The observation values $$x_t$$ are the states we&#39;re in at time $$t$$.  They can&#39;t be $$0$$ (see below).</p>
<p></p>
<p>&#34;it&#39;s unclear to me after reading the paragraph you referenced what our observation values xt are&#34;</p>
<p></p>
<p>Each observation is the state we were in at that time.  State $$B$$ maybe, or state $$D$$, etc.  We have some leeway in how we represent those states in our code, but I think they have to be linearly independent vectors.  In section $$4$$ Sutton gives mathematical conditions on what the vectors have to look like.  For these experiments, he uses a particularly simple implementation outlined in the paragraph I mentioned in the previous post.</p>",2019-09-20T05:18:00Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0roaqhe9j57gw,2019-09-20T05:18:00Z,{},project1
1670,no,<p>My understanding of the paragraph on page 19 is that $$x_{t}$$ is simply a 1 for being in that state? Not the reward for transitioning into that state.</p>,2019-09-20T16:56:20Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0sd8soair062s,2019-09-20T16:56:20Z,{},project1
1671,no,"<p>The variable $$x_t$$ is a <strong>vector</strong>.  It&#39;s a structure that has $$5$$ components, each of which can be a $$1$$ or a $$0$$:</p>
<p></p>
<p>$$x_t = [x_{t1}, x_{t2}, x_{t3}, x_{t4}, x_{t5}]$$</p>
<p></p>
<p>There are five states in these experiments, $$B, C, D, E, F.$$  They are encoded into the $$x_t$$&#39;s as follows:</p>
<p></p>
<p>If the agent is in state $$B$$ at time $$t$$, then $$x_t = [1,0,0,0,0]$$.</p>
<p>If the agent is in state $$C$$ at time $$t$$, then $$x_t = [0,1,0,0,0]$$.</p>
<p>If the agent is in state $$D$$ at time $$t$$, then $$x_t = [0,0,1,0,0]$$.</p>
<p>If the agent is in state $$E$$ at time $$t$$, then $$x_t = [0,0,0,1,0]$$.</p>
<p>If the agent is in state $$F$$ at time $$t$$, then $$x_t = [0,0,0,0,1]$$.</p>
<p></p>
<p>So you can see that the the state that the agent was in at time $$t$$ can be discerned by seeing which <strong>component</strong> of $$x_t$$ is a $$1$$.</p>",2019-09-20T17:16:45Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0sdz1t3ivt76j,2019-09-20T17:16:45Z,{},project1
1672,no,"<p>So if </p>
<p></p>
<p>  $$x_{0} = [1, 0, 0, 0, 0]$$</p>
<p></p>
<p></p>
<p>we get</p>
<p></p>
<p>  $$w_{0} = \alpha (P_{1} - P_{0}) \ast \lambda^{0 - 0} * [1, 0, 0, 0, 0]$$ ?</p>
<p></p>
<p>This summation components doesn&#39;t make sense to me. We&#39;re multiplying a scalar by some summation of scalars (lambda) multipled by a state vector?</p>",2019-09-20T19:22:21Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0sigkvllkv3pb,2019-09-20T19:22:21Z,{},project1
1673,no,"<p>The summation starts at $$k=1$$ and ends at $$k=t$$.  So in the case of your example above, since you&#39;re computing $$\Delta w_0$$, which is the weight increment for time $$t=0$$, you wouldn&#39;t have any terms in your summation (the lower index is actually greater than the upper index).</p>
<p></p>
<p>I think Sutton defines his first observation as the one that happens at $$t=1$$, so your first observation vector would be $$x_1$$, and the first weight increment would be:</p>
<p></p>
<p>$$\Delta w_1 = \alpha(P_2 - P_1)\cdot \lambda^{1-1}\Delta P_1 = \alpha(P_2-P_1)\cdot x_1 = \alpha(P_2-P_1)\cdot [1,0,0,0,0]$$, for your example of $$x_1 = [1,0,0,0,0]$$,</p>
<p></p>
<p>which is scalars multiplied by a vector.  And that&#39;s what you want because the left side, $$\Delta w_1$$ is also a vector.</p>",2019-09-20T19:43:13Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0sj7f26cky5g4,2019-09-20T19:43:13Z,{},project1
1674,no,<p>Oh okay I think I got. I appreciate the help Vahe!</p>,2019-09-20T19:58:17Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0sjqsnc9fl5zl,2019-09-20T19:58:17Z,{},project1
1675,no,"<p>The key was adjusting where I checked for convergence </p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-21T21:22:57Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0u27itzma36nm,2019-09-21T21:22:57Z,{},project1
1676,no,"<p>Hi,</p>
<p></p>
<p>I am getting a 404 not found error. Any way to resolve this issue?</p>
<p></p>
<p>Thank you.</p>",2019-09-19T16:54:33Z,61,Week 9/15 - 9/21,followup,,jzkpm3i0pxp2sg,k0qxqo055ge4i,2019-09-19T16:54:33Z,{},logistics
1677,no,<p>Did you login? These are private repos.</p>,2019-09-19T16:55:24Z,61,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qxrr073xq1sy,2019-09-19T16:55:24Z,{},logistics
1678,no,"<p>Do you login here?: <a href=""https://github.gatech.edu/"">https://github.gatech.edu/</a></p>
<p></p>
<p>I am able to login using my gatech username.</p>",2019-09-19T16:56:44Z,61,Week 9/15 - 9/21,feedback,,jzkpm3i0pxp2sg,k0qxtgwl6xe2jb,2019-09-19T16:56:44Z,{},logistics
1679,no,"<p>after you login, when you visit the repository using the name they provided, does it work or do you get 404? make sure you are using the gt userid that you are logging in with, not the gtid.</p>",2019-09-19T17:12:49Z,61,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0qye5n6iz66yr,2019-09-19T17:12:49Z,{},logistics
1680,no,<p>No it gives the 404 error. I am using the gt username at the end of the URL they provided.</p>,2019-09-19T17:38:37Z,61,Week 9/15 - 9/21,feedback,,jzkpm3i0pxp2sg,k0qzbbvwi0h1h6,2019-09-19T17:38:37Z,{},logistics
1681,no,"<p>for instance:</p>
<p></p>
<p><a href=""https://github.gatech.edu/gt-omscs-rldm/7642Fall2019bpurusho3"">https://github.gatech.edu/gt-omscs-rldm/7642Fall2019bpurusho3</a> would be Binod&#39;s repository</p>
<p></p>
<p>note the &#34;gt-omscs-rldm&#34; as the organization</p>",2019-09-19T20:19:28Z,61,Week 9/15 - 9/21,feedback,,jc554vxmyuy3pt,k0r526mrlb87nm,2019-09-19T20:19:28Z,{},logistics
1682,no,Please follow up with a private message to All Instructors as stated in &#64;129,2019-09-19T22:27:34Z,61,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0r9mwp4nao704,2019-09-19T22:27:34Z,{},logistics
1683,no,"<p>Hi,</p>
<p></p>
<p>I have the same issue.</p>
<p></p>
<p></p>",2019-09-19T18:35:11Z,61,Week 9/15 - 9/21,followup,,jzygktecb0e3i1,k0r1c2jacy91xg,2019-09-19T18:35:11Z,{},logistics
1684,no,Please follow up with a private message to All Instructors as stated in &#64;129,2019-09-19T22:27:42Z,61,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0r9n34pnio73z,2019-09-19T22:27:42Z,{},logistics
1685,no,"<p>I did it. But no response, yet.</p>",2019-09-19T22:50:15Z,61,Week 9/15 - 9/21,feedback,,jzygktecb0e3i1,k0rag3fthtx6cy,2019-09-19T22:50:15Z,{},logistics
1686,no,"<p>Hi</p>
<p></p>
<p>I have the same issue as well - bpurusho3</p>
<p></p>
<p>Regards</p>
<p></p>
<p>Binod</p>",2019-09-19T19:42:13Z,61,Week 9/15 - 9/21,followup,,jzk2u4t3n5o5el,k0r3qa9omvp6ju,2019-09-19T19:42:13Z,{},logistics
1687,no,Please follow up with a private message to All Instructors as stated in &#64;129,2019-09-19T22:27:49Z,61,Week 9/15 - 9/21,feedback,,hz7meu55mi8sd,k0r9n8fvnsr7by,2019-09-19T22:27:49Z,{},logistics
1688,no,"<p>Hello, I&#39;m able to log in to my gatech account but it looks like I&#39;m having the same issue accessing the class repository.</p>
<p></p>
<p>lpasqualin3</p>",2019-09-22T14:45:51Z,60,Week 9/22 - 9/28,followup,,jzk4afmbx073hw,k0v3gp6420v67h,2019-09-22T14:45:51Z,{},logistics
1689,no,"<p><span style=""display:!important""></span>It should be a private repo that TAs/Instructors and yourself can see. Please follow up with a private message to All Instructors as stated in <a href=""/class/jzh9tkzzxkd7ph?cid=129"">&#64;129</a></p>",2019-09-22T21:53:23Z,60,Week 9/22 - 9/28,feedback,,jl1acpoc4HA9,k0viqirog6t3cr,2019-09-22T21:53:23Z,{},logistics
1690,no,<p>I&#39;m still having this issue. I sent a message to all instructors but have no received a response.</p>,2019-09-25T05:37:51Z,60,Week 9/22 - 9/28,followup,,k0i2gsf6my96ge,k0yu7ixz9ay2jz,2019-09-25T05:37:51Z,{},logistics
1691,stud,"<p>Regarding 1, above, It also seems worth noting that Sutton provides examples two further advantages of TD methods besides not having to wait to see the outcome of an observation-outcome sequence in order to update estimates.</p>
<p></p>
<p>First, he considers cases in which the penultimate state in a sequence of states leads to a bad outcome most of the time (say, $$\geq 90\%$$ of the time). If we consider a sequence where we get lucky and (improbably) wind up with a positive outcome from the bad penulitmate state, we can see another example of how TD methods will sometimes best supervised learning methods. Since the penultimate state is bad, TD methods will take this into account. They&#39;ll learn to avoid the bad state much quicker than supervised learning methods. The latter will ignore the fact that the penultimate state leads to bad outcomes most of the time in those cases where the <em>actual</em> outcome is positive. They&#39;ll instead update estimated values of the bad state in the direction of a <em>positive</em> outcome in such instances. But the fact that we got lucky on <em>this</em> run shouldn&#39;t make us believe that a state that mostly leads to bad outcomes is a good state to be in. TD methods can capture this. Supervised learning methods can, presumably, <em>eventually</em> capture this, but only with considerably more effort than TD. </p>
<p></p>
<p>Second, Sutton also notes that TD methods can be more <em>computationally efficient</em> than supervised learning methods. Some forms of TD can be implemented so that they use <em>incremental</em> procedures for weight updates. This can translate into fewer demands on memory, since less information has to be retained at each time step (in comparison with supervised learning or, I think, monte carlo methods). Sutton also claims that the computations required to implement the weight updates using an incremental implementation of TD will be distributed more evenly than is the case with competing supervised learning methods. This may also be a computational benefit.  </p>
<p></p>
<p>I think Vahe is correct above. The benefits and advantages of TD methods he mentions should be considered primary. But it seems worth noting that there are other differences between TD methods and non-TD methods that Sutton thought important enough to merit mention in the paper. </p>",2019-09-19T15:21:26Z,61,Week 9/15 - 9/21,followup,a_0,,k0quewf4zhv46f,2019-09-19T15:21:26Z,{},project1
1692,no,"<p>I think your first point is a special case of TD using more information than SL.  There&#39;s a reason we associate the penultimate state with a bad outcome - because that&#39;s the explanation that maximizes our expected rewards.  When ignoring that in favor of a lucky outcome, we&#39;re simply not using all the information we have, and as you said, that will slow down convergence.</p>
<p></p>
<p>Your second point about computation is, of course, completely independent and another notch in the belt of TD.</p>
<p></p>
<p>As a devil&#39;s advocate to the first point (and Sutton mentions this for the stationary MDP case), an adversarial environment can trip up TD much more than SL.  It could put us in a state that leads to that penultimate state, but such that that <i>sequence of states</i> always leads to a good outcome, even though the penultimate state by itself usually in the past led to a bad outcome.  In a stationary MDP, as Sutton mentions, this should necessarily happen very rarely, so it&#39;s not something that we should really worry about.</p>",2019-09-19T15:47:34Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0qvci9v6sw3ws,2019-09-19T15:47:34Z,{},project1
1693,stud,"<p><em>&#34;I think your first point is a special case of TD using more information [than] SL&#34;.</em></p>
<p><em></em></p>
<p>That seems right. </p>
<p></p>
<p>&#34;<em>As a devil&#39;s advocate to the first point (and Sutton mentions this for the stationary MDP case), an adversarial environment can trip up TD much more than SL.  It could put us in a state that leads to that penultimate state, but such that that </em><em>sequence of states</em><em> always leads to a good outcome, even though the penultimate state by itself usually in the past led to a bad outcome.  In a stationary MDP, as Sutton mentions, this should necessarily happen very rarely, so it&#39;s not something that we should really worry about.</em>&#34;</p>
<p></p>
<p>I agree. Good point. Certainly, I don&#39;t mean to attribute to Sutton the view that TD methods are always necessarily better than SL methods. My interest was in outlining some of the additional/particular advantages Sutton claims for TD methods in the paper for the benefit of the OP. But certainly there is no suggestion on Sutton&#39;s part (or mine, for that matter) that we&#39;d do well to completely abdandon SL methods for TD methods across the board. As you note, TD methods have their limitations as well. </p>",2019-09-19T16:03:20Z,61,Week 9/15 - 9/21,feedback,a_0,,k0qvwsifa8e5yk,2019-09-19T16:03:20Z,{},project1
1694,no,"<p>1) Thank you for your response but I&#39;m still stuck a bit, pardon the thick brain. Let&#39;s revisit the Saturday example. I get that we have information about weather on Friday (ex. cloud formation, etc..) but my understanding is that we&#39;d still be using that information in a supervised learning approach. I&#39;m still missing something. Put differently, with TD are we restricted to predicting only one day in advance? what if we wanted some time buffer to &#34;do something about the weather&#34; (ex. buy an umbrella) and somehow that takes a few days to ship. We&#39;d have to make a prediction a few days in advance. Does the one TD advantage vanish in this case?</p>
<p></p>
<p>2) Is how I proposed to visualize the results a prudent way to proceed? should I think of observation pairs instead where each row is just two columns with two successive states?</p>
<p></p>
<p>3) Noted, thank you. Re-reading is indeed the plan. Thanks again for all the discourse. </p>",2019-09-19T15:53:04Z,61,Week 9/15 - 9/21,followup,,j6vegc4sb0v33d,k0qvjkwxss01uz,2019-09-19T15:53:04Z,{},project1
1695,no,"<p>&#34;I get that we have information about weather on Friday (ex. cloud formation, etc..) but my understanding is that we&#39;d still be using that information in a supervised learning approach.&#34;</p>
<p></p>
<p>Nope, you&#39;d only be using the Saturday (outcome) information in an SL approach.</p>
<p></p>
<p>&#34;with TD are we restricted to predicting only one day in advance?&#34;</p>
<p></p>
<p>With <strong>1-step TD</strong> we&#39;re using only the next day&#39;s information for the current day&#39;s prediction (for the weather example), which is still arguably much, much better than SL for a temporal problem (look at Sutton&#39;s curves for $$\lambda = 0$$ vs $$\lambda = 1$$, for example).  But with $$TD(\lambda)$$ we&#39;re using a linear combination of $$1$$-step lookahead, $$2$$-step lookahead, etc.  So we&#39;re using multiple days&#39; results for the current day&#39;s prediction.  $$TD(1)$$ is just using the Monte Carlo return, which can be thought of as applying supervised learning to a temporal problem.</p>",2019-09-19T16:03:40Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0qvx86pmi76fu,2019-09-19T16:03:40Z,{},project1
1696,no,<p>Super helpful. Thank you Vahe for this input. I&#39;m working my way through a better understanding and this is helping. </p>,2019-09-19T17:54:08Z,61,Week 9/15 - 9/21,feedback,,j6vegc4sb0v33d,k0qzva9md0l6iq,2019-09-19T17:54:08Z,{},project1
1697,no,"<p>Now I&#39;m confusing myself again. I had settled on the fact that ω was a scalar, but then realized that P<sub>t</sub> = ω<sup>T</sup>x<sub>t</sub> wouldn&#39;t make any sense. If we&#39;re transposing ω with a superscript T, doesn&#39;t that mean it has to be an array?</p>",2019-09-19T13:55:28Z,61,Week 9/15 - 9/21,followup,,jl284xdcifz44g,k0qrccnc50n66e,2019-09-19T13:55:28Z,{},project1
1698,no,"<p>Dalton,</p>
<p></p>
<p>w and delta_w are vectors of size 5. I had a similar question &#64;296.</p>
<p></p>
<p>Answering your questions:</p>
<p>1 - It depends on which Suttons experiment you are talking about. Experiment 1 and Experiment 2 have different ways of updating the weights. What you are describing matches with experiment 2.</p>
<p></p>
<p>2 - This is what I&#39;m doing as well, but know that experiment 1 and 2 updates that differently.</p>
<p></p>
<p>3 - P is just the result of w.transpose * xt. So when you update w, P will be different.</p>
<p></p>
<p>I hope it helps.</p>
<p></p>
<p></p>",2019-09-19T14:08:59Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0qrtq5ju1q6b7,2019-09-19T14:08:59Z,{},project1
1699,no,"<p>Ok thanks, I think that clears up a few things. I&#39;ll keep chugging</p>",2019-09-19T19:12:22Z,61,Week 9/15 - 9/21,feedback,,jl284xdcifz44g,k0r2nw9xeat7jc,2019-09-19T19:12:22Z,{},project1
1700,no,"<p>How is ω a vector of size 5 if Δω is also a vector of size 5? </p>
<p></p>
<p>For example, if Δω = [1, 2, 3, 4, 5] and we update ω, wouldn&#39;t ω = 1 &#43; 2 &#43; 3 &#43; 4 &#43; 5, making it a scalar? </p>",2019-09-19T20:46:52Z,61,Week 9/15 - 9/21,followup,,jl284xdcifz44g,k0r61f9uei82ua,2019-09-19T20:46:52Z,{},project1
1701,no,"<p>What does the subscript t in equation (4) above represent? If Δω is always an array of size 5, then it would seem that t represents states B, C, D, E, F.</p>
<p></p>
<p>But on the right side of the equation, using subscript t and subscript t &#43; 1 implies that t represents the &#34;step&#34; on the walk, which could be a number much larger than 5. </p>
<p></p>
<p>If t represents a state, how do we get t &#43; 1? If it represents a step, does that mean Δω is an array with size equal to the number of steps on the walk?</p>",2019-09-19T20:56:43Z,61,Week 9/15 - 9/21,feedback,,jl284xdcifz44g,k0r6e3cvhzn3zr,2019-09-19T20:56:43Z,{},project1
1702,no,"<p>&#34;For example, if Δω = [1, 2, 3, 4, 5] and we update ω, wouldn&#39;t ω = 1 &#43; 2 &#43; 3 &#43; 4 &#43; 5, making it a scalar? &#34;</p>
<p></p>
<p>The summation, $$\sum$$, doesn&#39;t sum the components of the vector $$\Delta w$$, but rather different $$\Delta w$$&#39;s which are indexed, as you said, by the time-step of the walk.</p>",2019-09-19T21:33:51Z,61,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0r7ptyu8g4j4,2019-09-19T21:33:51Z,{},project1
1703,no,"<p>Dalton,</p>
<p>Addingo to Vahe&#39;s explanation above, t represents the time step. It happens that the gradient of Pt is a vector of size 5 which will make delta_wt (and also w) to be a vector of size 5.</p>
<p></p>
<p>Think about this way, it wouldn&#39;t make sense to update the whole vector w by a scalar, otherwise all the w positions would change the same quantity. Every position on w is actually updated differently according to the direction determined by the gradient of P in time t.</p>",2019-09-19T23:25:52Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0rbpwhw85u4ec,2019-09-19T23:25:52Z,{},project1
1704,no,"<p>I&#39;m still not totally clear on this, but I appreciate all the help. Let&#39;s say our random walk took 10 steps to reach a terminal state. Wouldn&#39;t that mean we have Δw<sub>1</sub>, Δw<sub>2</sub>, ... , Δw<sub>10</sub> ? So wouldn&#39;t Δw be a vector of length 10? </p>
<p></p>
<p>I now understand how the summation works, but I&#39;m still not sure how to apply equation (4). </p>",2019-09-20T01:16:09Z,61,Week 9/15 - 9/21,feedback,,jl284xdcifz44g,k0rfnq4ld9s1kt,2019-09-20T01:16:09Z,{},project1
1705,no,"<p>each delta_wt would have a size of 5, because you are multiplying a scalar with the gradiente of Pt (which is size 5). You would have 10 delta_w of size 5 each... then you just need to sum all of them.</p>",2019-09-20T01:27:43Z,61,Week 9/15 - 9/21,feedback,,jc6xvgjncoey,k0rg2la3iiz1lp,2019-09-20T01:27:43Z,{},project1
1706,no,"<p>Hmmmmm...ok, I think this is starting to make sense. Many thanks, I&#39;ll dig into this more tomorrow</p>",2019-09-20T01:38:40Z,61,Week 9/15 - 9/21,feedback,,jl284xdcifz44g,k0rggo5e98n7lt,2019-09-20T01:38:40Z,{},project1
1707,no,"Can someone give an example of handwriting one update sequence in terms of x, w, and P ? These parameters are not clear to me when getting into a specific experiment.<p></p><div><br /></div><div>My current understand is,</div><div>Xt is a vector of size 5 like [0,0,1,0,0] indicating where the current state is </div><div><br /></div><div>w is a vector of 5 as well. Therefore delta-w is also a vector of 5</div><div><br /></div><div>Pt: I’m a litttle confused. It should be a vector of 5 to me since it’s the predicted value for each letter state.  equation (4) also implies  Pt is a vector of size 5 otherwise delta-w won’t be a vector. </div><div><br /></div><div>However  people mention Pt = w.T * X. So this will make Pt a scalar. What’s wrong here ? </div>",2019-09-22T00:05:08Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k0u803hkl4q3vc,2019-09-22T00:05:08Z,{},project1
1708,no,"<p>The prediction, either 1 or 0, is a scalar at time t. That&#39;s where $$P_t = w^{T} * X$$ comes from. Sutton appears to be graphing the deviation of the weights relative to the ideal probability. You could also graph the $$P_t$$ versus the $$P_m$$ as the prediction error. The prediction error was not very interesting to me, but maybe you can find some hidden nuance.</p>",2019-09-24T14:01:35Z,60,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0xwrh47pfi23d,2019-09-24T14:01:35Z,{},project1
1709,no,"<p>Thanks &#64;Vahe, </p>
<p>but you say the algorithm is not iterative ?? the pseudocode has two nested loops to first iterate over every state to update its value, then to do the same for every episode.  Why is this not iterative?</p>
<p></p>
<p>About the value always increasing, ok, you&#39;re right (G - V(St)) can be negative, my bad, but, as I just said, the algo says to keep calculating V, online, from one episode to another, so what we end up with is the estimated value function, no averaging needing to be done, correct? </p>
<p></p>
<p>Maybe one does the averaging from a batch of episodes to the next, of the RMS error, like Sutton does in his paper for instance, right? </p>",2019-09-20T04:57:37Z,36,Week 9/15 - 9/21,followup,,jzh6k6o994a6dh,k0rnkiyvzq24t9,2019-09-20T04:57:37Z,{},hw2
1710,no,"<p>&#34;but you say the algorithm is not iterative ?? the pseudocode has two nested loops to first iterate over every state to update its value, then to do the same for every episode.  Why is this not iterative?&#34;</p>
<p></p>
<p>If you want to define any algorithm that contains a loop as an iterative algorithm, then it&#39;s iterative.  I tried to answer your question about why updating a state twice in a loop isn&#39;t &#34;weird.&#34;   Did you understand my reasoning there?  I would think that that is the important take-away, not whether the algorithm can properly be called iterative or not.</p>
<p></p>
<p>&#34;no averaging needing to be done, correct? &#34;</p>
<p></p>
<p>Correct.</p>
<p></p>
<p>&#34;Maybe one does the averaging from a batch of episodes to the next,&#34;</p>
<p></p>
<p>Nope.</p>",2019-09-20T05:26:19Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0rolflu1ky18u,2019-09-20T05:26:19Z,{},hw2
1711,no,"<p>Sure &#64;vahe, I understood what you said about updating a state twice during an episode wasn&#39;t weird at all because, well, it&#39;s an update.</p>
<p>The &#39;&#43;&#39; sign in the equation made me see it as &#39;always increasing the value&#39; but I got to my senses now, so thanks for that. </p>
<p></p>
<p>Also my impression that there had to be some sort of averaging from one episode to the next.  This is still new to me, I understand the concepts when I read, then it blurs a bit.</p>
<p>I guess I lack practice, so I&#39;ll keep learning harder.  It&#39;s logical after all since we know those algorithms converge, meaning the more we test the system, the closer we get.  </p>
<p></p>
<p>But I&#39;d still like to understand what you meant by &#39;the algo isn&#39;t iterative&#39;.  Could you elaborate and maybe give me an example?</p>",2019-09-20T06:32:08Z,36,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0rqy2ddeeu1e6,2019-09-20T06:32:08Z,{},hw2
1712,no,"<p>&#34;But I&#39;d still like to understand what you meant by &#39;the algo isn&#39;t iterative&#39;. Could you elaborate and maybe give me an example?&#34;</p>
<p></p>
<p>The algorithm doesn&#39;t have to be iterative.  I&#39;ll explain below.</p>
<p></p>
<p>For true iterative algorithms, I gave examples of value/policy iteration in my original post.  For example, in value iteration, each iteration updates the values of all the states, and all the assignments use the old values (you can actually shortcut that and mix new and old values and still converge, but that&#39;s another story).  Once all states are updated, you move onto the next iteration and do the same thing over again.</p>
<p></p>
<p>&#34; the pseudocode has two nested loops to first iterate over every state to update its value&#34;</p>
<p></p>
<p>This isn&#39;t true.  In the case of the $$n$$-step TD algorithm, there&#39;s no such iterative process for updating state values.  You move sequentially through the episode until you hit a terminal state.  If a particular episode has you looping from one non-terminal node back to itself 100 times in a row before finally transitioning to a terminal state, then the <em>only</em> state value that would have been updated by the algorithm is the one for that one state.  You <em>could</em> have iterations, but they would be over <em>episodes, </em>not states.  But you could also run $$n$$-step $$TD$$ without ever repeating the same episode.  I could feed you a stream of different episodes and you would run through them one-by-one, updating state values as you go through the corresponding states, and never iterate through the same episode more than once.  When you get to Project 1, you&#39;ll see both approaches used.</p>
<p></p>
<p>Edit:  Incidentally, the &#34;averaging&#34; you intuitively want to have happen is already happening by virtue of updating state values using the error between between the current return and the old estimate.  That kind of incremental update is averaging in disguise.  The learning rate $$\alpha$$ controls how we weight that &#34;average.&#34;</p>",2019-09-20T07:27:04Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0rswq7ahn5o,2019-09-20T07:27:04Z,{},hw2
1713,no,"<p>Thanks a lot &#64;vahe... these questions actually arise because I&#39;m trying to figure out the project.  </p>
<p>But many people have started some time ago so there&#39;s a lot of resources already here.</p>",2019-09-20T19:10:56Z,36,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0si1wf1zml5jz,2019-09-20T19:10:56Z,{},hw2
1714,no,"<p>&#34;since we only compute Pt, ie a single number, for the whole sequence.&#34;</p>
<p></p>
<p>This isn&#39;t true. $$P_t$$ refers to the prediction at time $$t$$.  A sequence spans multiple time-steps.</p>",2019-09-20T19:14:08Z,36,Week 9/15 - 9/21,feedback,,jzfsa4a37jf4aq,k0si60gss5g169,2019-09-20T19:14:08Z,{},hw2
1715,no,<p>For large values of lambda occasionally I&#39;m unable to converge and error results for experiment one appear to be the opposite of what Sutton experience. Any ideas for debugging this? I&#39;m at a loss as to how to proceed.. </p>,2019-09-20T23:04:08Z,61,Week 9/15 - 9/21,followup,,h6e6q7ftqr747f,k0sqdsctmzb66z,2019-09-20T23:04:08Z,{},project1
1716,no,"<p>Hey Dylan,</p>
<p></p>
<p>Check your P assignments. Those might be the culprit.</p>
<p></p>
<p>It would help to change your seq # from 10 -&gt; 1 and training set also from 100 -&gt; 1.</p>
<p></p>
<p>Add additional print statements for value of P at every state and that should help you narrow down your problem.</p>
<p></p>
<p>This helped me find my error today that I mentioned in the above comment</p>
<p></p>
<p></p>",2019-09-21T02:20:22Z,61,Week 9/15 - 9/21,feedback,,jfzaqnqvtQ1m,k0sxe5gg6wl6xn,2019-09-21T02:20:22Z,{},project1
1717,no,"<p>Here&#39;s some output for one iteration over a single episode with 6 moves:</p>
<p></p>
<pre>starting lambda: 1
*****************************
mdp length: 10

episode pos: 0
state: 5
episode length: 6
state_error: 0.0
summation: [0 0 0 0 0 1 0 0 0 0]
delta_w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
W: [0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]

episode pos: 1
state: 4
episode length: 6
state_error: 0.0
summation: [0 0 0 0 1 1 0 0 0 0]
delta_w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
W: [0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]

episode pos: 2
state: 3
episode length: 6
state_error: 0.0
summation: [0 0 0 1 1 1 0 0 0 0]
delta_w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
W: [0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]

episode pos: 3
state: 2
episode length: 6
state_error: 0.0
summation: [0 0 1 1 1 1 0 0 0 0]
delta_w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
W: [0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]

episode pos: 4
state: 1
episode length: 6
state_error: -0.00125
summation: [0 1 1 1 1 1 0 0 0 0]
delta_w: [ 0.      -0.00125 -0.00125 -0.00125 -0.00125 -0.00125  0.       0.
  0.       0.     ]
W: [0.  0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 1. ]</pre>
<p></p>
<p>Here:</p>
<p></p>
<p>lambda = 1</p>
<p>alpha = .0025</p>
<p>state_error = $$\alpha (P_{t &#43; 1} - P_{t})$$ with $$P_{t &#43; 1} and P_{t}$$ being taken directly from W. For example in the first output block state_error = W[4] - W[5]<br />delta_w = state_error * summation</p>
<p>summation = $$\sum_{k=1}^{t}\lambda_{t-k} \nabla wP_{k}$$</p>
<p></p>
<p>so $$\Delta w =$$ state_error * summation</p>
<p></p>
<p>After W is updated to [0. , 0.49875, 0.49875, 0.49875, 0.49875, 0.49875, 0.5 ,0.5 , 0.5 , 1. ]. The index 0 and 9 in W never change as these are used to represent the values for terminal states.<br /><br />This seems correct to me. Maybe I&#39;m not understanding this summation component correctly? </p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-21T20:59:46Z,61,Week 9/15 - 9/21,feedback,,h6e6q7ftqr747f,k0u1dppqkx831b,2019-09-21T20:59:46Z,{},project1
1718,no,<p>I had to adjust where I was checking for convergence!</p>,2019-09-22T13:45:34Z,60,Week 9/22 - 9/28,feedback,,h6e6q7ftqr747f,k0v1b6j0ght686,2019-09-22T13:45:34Z,{},project1
1719,no,"Hi Dalton, I initially had a similar problem which arose because I was only modelling the non-terminal states. I later included the terminal states and initialised their value functions to zero, with the reward of 0 or 1 given on the transition into the terminal state. Basically the values of the terminal states weren’t bounded so could drift off to infinity. I also had some instability problems with large alpha but I changed things to update the weights after each sequence, not waiting until after all 10 sequences. Good luck.",2019-09-23T20:50:54Z,60,Week 9/22 - 9/28,feedback,,jzvoaw0pl6326,k0wvy09yvdkbe,2019-09-23T20:50:54Z,{},project1
1720,no,<p>Remember to set the seed so you can use the same training set as you debug.</p>,2019-09-21T03:53:01Z,59,Week 9/15 - 9/21,followup,,is9so9huTMp,k0t0pb6x24f5u6,2019-09-21T03:53:01Z,{},project1
1721,no,<p>Thanks!</p>,2019-09-23T12:43:30Z,58,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0wej7el3y01ln,2019-09-23T12:43:30Z,{},project1
1722,no,Can I get clarification on the 10 sequences? Is one sequence from start to reaching the far left position? It’s like playing the game from start to end. And repeat it 10 times. This will be one training set? Is my understanding correct? <div><br /></div><div>This sounds like value iteration problem but in the paper it makes like a update process in supervised learning </div><div><br /></div>,2019-09-21T23:48:31Z,59,Week 9/15 - 9/21,followup,,j6ln9puq99s5uv,k0u7epqozxn60s,2019-09-21T23:48:31Z,{},project1
1723,no,<p>10 sequences means playing the game from start to end 10 times (meaning until it reaches the far left <em>or </em>the far right position). This will be one training set.</p>,2019-09-21T23:53:36Z,59,Week 9/15 - 9/21,feedback,,jqmjg3txqw6ji,k0u7l9heudx3hl,2019-09-21T23:53:36Z,{},project1
1724,no,Thanks <div><br /></div>,2019-09-23T19:02:13Z,58,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k0ws28qa9c440f,2019-09-23T19:02:13Z,{},project1
1725,no,"For experiment 1 in the analysis of repeated presentation, my understanding is that repeat each training set and update the weight vector until the weight converges. In figure 3 legend it says the error is average of 100 training sets. My questions are <div>1. How are the RMSE calculated? Do we calcaulte the P for each state at convergent and compare with ideal predictions and then get the error? My understanding is that P is just a scalar because Pt=wX     So should we calculate the P for each state in the end or just get P for starting state D? </div><div><br /></div><div>2. Since we need to run many iterations of a training set to get convergence, will these be counted towards the 100 training set? The definition of the “learning procedure” is confusing. </div>",2019-09-24T02:50:49Z,58,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k0x8sv3zlpi2dy,2019-09-24T02:50:49Z,{},project1
1726,no,<p></p><ol><li>&#64;192</li><li>No they will not. For experiment 1 (Figure 3) each training set will need to be run over and over until convergence before moving on to the next training set. You repeat that 100 times for the 100 training sets.</li></ol>,2019-09-24T12:27:14Z,58,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0xte50mwlfmn,2019-09-24T12:27:14Z,{},project1
1727,no,"<p>Should the last bullet read something like &#34;get $$\sum \Delta w_{t}$$ for each sequence, then after going through all sequences in the training set, update the w vector for each sequence (w = w &#43; $$\sum \Delta w_{t}$$)&#34;?</p>",2019-09-21T01:07:14Z,61,Week 9/15 - 9/21,followup,,jqmjg3txqw6ji,k0sus4332ly5zg,2019-09-21T01:07:14Z,{},project1
1728,no,<p>correct</p>,2019-09-21T13:18:45Z,61,Week 9/15 - 9/21,feedback,,is5gzbotXmz,k0tkwtzci3q70w,2019-09-21T13:18:45Z,{},project1
1729,no,<p>Is it an overall w vector that gets updated or does each sequence have its own w vector?</p>,2019-09-21T16:30:58Z,61,Week 9/15 - 9/21,feedback,,jqmjg3txqw6ji,k0trs18hu1k2hx,2019-09-21T16:30:58Z,{},project1
1730,no,"<p>Afaik, there&#39;s only one w vector per training set.  </p>",2019-09-21T22:00:32Z,61,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0u3jv83dnw5nn,2019-09-21T22:00:32Z,{},project1
1731,no,"<p>Yes but trend, but what about his axis limits? I know that some are cut off as they grow exponentially but mine are growing way before his are.</p>",2019-09-21T13:28:53Z,61,Week 9/15 - 9/21,followup,,jzj7y1ofgsro1,k0tl9vohtpu4qm,2019-09-21T13:28:53Z,{},project1
1732,no,<p>I had the same issue. The error is over 1-10. How did you fix your bug? Need shining light. Thanks</p>,2019-09-26T22:36:05Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k11a0u5zh9m42a,2019-09-26T22:36:05Z,{},project1
1733,no,<p>Worth mentioning that &#34;Pro Git&#34; is up to date and completely free - https://git-scm.com/book/en/v2</p>,2019-09-21T07:17:49Z,61,Week 9/15 - 9/21,followup,,jzhwdil7ssn443,k0t80o0s7ys2xf,2019-09-21T07:17:49Z,{},project1
1734,no,"<p>Hi all,</p>
<p></p>
<p></p>
<p>Is it possible to find it through the Git Hub desktop?</p>
<p></p>
<p>Thanks in advance.</p>",2019-09-27T04:10:40Z,60,Week 9/22 - 9/28,followup,,jzygktecb0e3i1,k11lz4a3o2r5ao,2019-09-27T04:10:40Z,{},project1
1735,no,"<p>I didn&#39;t find a way from the desktop app, but maybe this will help. (Hopefully this is correct, as it&#39;s what I did).</p>
<p>1. Navigate to the repository in your browser. github.gatech.edu/gt-omscs-rldm/7642Fall2019USERNAME </p>
<p>2. Click # Commits (The # is some number)</p>
<p>3. You should see a list of commits. On the most recent one (at the top) look at the right. You should see a clipboard, some numbers/letters, and a &lt;&gt;. Click on the numbers/letters.</p>
<p>4. Near the top half of the screen you&#39;ll see &#34;commit&#34; followed by a ridiculously long string of numbers and letters. Copy the whole thing.</p>
<p>5. Paste it somewhere in your paper. </p>",2019-09-28T19:17:45Z,60,Week 9/22 - 9/28,feedback,,jl2bq5rf8b67pq,k13xthfdcvb4oh,2019-09-28T19:17:45Z,{},project1
1736,no,"<p>I am doing both. Pm&#43;1 for terminal state, and P = w.x for every state</p>",2019-09-21T07:35:34Z,61,Week 9/15 - 9/21,followup,,jqrr36mqfm8M,k0t8ni4g9hx4y4,2019-09-21T07:35:34Z,{},project1
1737,no,<p>I&#39;m confused by the definition of P as well. Is P = w.x just the weight of that x state? Is Pm&#43;1 always the terminal state? When I used the P = w.x method I am multiplying all my calculations by 0 except for the state before the G state. I feel like I&#39;m mixing something up here</p>,2019-09-24T18:46:28Z,60,Week 9/22 - 9/28,followup,,j6m1jeidndu6wq,k0y6xudgyq52sk,2019-09-24T18:46:28Z,{},project1
1738,no,"<p>Remember that for Experiment 1, we are running the algorithm multiple times, until it &#34;converges.&#34;  As you noticed, if you initialize $$w = 0$$, then all your predictions will be $$0$$ until the end of the first iteration.  But what happens on the next iteration?</p>",2019-09-24T18:56:09Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0y7aal8nkk2zn,2019-09-24T18:56:09Z,{},project1
1739,no,"<p>Oh ok I think I understand, that 1 reward from state G will slowly propagate through to the other states incrementally starting with state F then going backward. So this means after the first run of the algorithm state F and G will be non-zero then it will continue as I complete more iterations. So this means Pt and Pt&#43;1 are the weights of the states I am in and will go into at step t, correct? I for some reason was previously thinking P was the reward received which would be 0 at all except G and those of course were not getting updated. </p>",2019-09-24T19:08:24Z,60,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k0y7q1pah3d3ad,2019-09-24T19:08:24Z,{},project1
1740,no,"<p>You may see more than just a propagation to just state F, since TD$$(\lambda)$$ assigns credit for prediction errors to all states which were observed in the past, depending on the value of $$\lambda$$.  But yes, I think you get the general idea.</p>",2019-09-24T19:48:30Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0y95mhhzz43xi,2019-09-24T19:48:30Z,{},project1
1741,no,"I&#39;ve had this issue at the beginning, changing to a much smaller alpha will solve this.",2019-09-21T18:58:36Z,59,Week 9/15 - 9/21,followup,,jzj4205g7gd2fw,k0tx1vyj9u26xo,2019-09-21T18:58:36Z,{},project1
1742,stud,"<p>I believe Ava is correct. Concretely, suppose again that we had the observation-outcome sequence $$x_{1}, x_{2}, x_{3}, x_{4}, z \rightarrow x_{D}, x_{E}, x_{F}, 1 \rightarrow (0,0,1,0,0), (0,0,0,1,0), (0,0,0,0,1), 1$$ repeated ten times in a particular training set. But now, let&#39;s suppose that $$\alpha = 0.1$$, while $$\lambda = 1$$. In that case, the <em>total</em> increment due to one complete sweep over the training set will be $$10(0,0,0.1,0.1,0.1) = (0,0,1,1,1)$$.</p>
<p></p>
<p>And that&#39;s good, since any vector of the form $$(w_{1}, w_{2}, 1,1,1)$$ for arbitrary $$w_{1}, w_{2}$$ is going to be a <em>fixed point</em> of the update procedure for this particular training set. After we hit $$(w_{1}, w_{2}, 1,1,1)$$, all the future total increments will be $$\vec{0}$$, and so we&#39;ll stop updating (of course) and declare ourselves &#34;converged&#34;. Granted, we sort of converged to garbage ($$(w_{1}, w_{2}, 1,1,1)$$ is not really anywhere near the &#34;ground truth&#34; we&#39;re looking for here), but (again) it&#39;s a weird training set. </p>
<p></p>
<p>Any smaller $$\alpha$$ should work as well. Concretely, take $$\alpha = 0.01$$. Starting with $$\vec{w} = \vec{0}$$, the first total increment will be $$(0,0,0.1,0.1,0.1)$$. And when we feed <em>that</em> to the update procedure, we wind up with a total increment from the second sweep equal to $$(0, 0, 0.009,0.009,0.009)$$. Then a new weights vector of $$(0,0,0.1009,0.1009,0.1009)$$. With any luck, the three rightmost weights will each converge to $$1$$ from below, and we&#39;ll eventually get to the same fixed point after (maybe) many more sweeps. </p>
<p></p>
<p>So $$\alpha$$ should presumably be pretty small here (and likewise, $$\vec{w}$$ should presumably consist of smaller values). </p>",2019-09-21T21:34:40Z,59,Week 9/15 - 9/21,feedback,a_0,,k0u2mlh8pcg3jz,2019-09-21T21:34:40Z,{},project1
1743,no,"<p>I am getting convergence but want to be sure I haven&#39;t made any silly errors.. I ran your example. </p>
<p>alpha=0.01, w starts at 0, 1st iteration weight vector through &#39;your&#39; training set, I get same as you (0,0,0.1, 0.1, 0.1).  </p>
<p></p>
<p>Increment is (0,0,0.09, 0.09, 0.09)</p>
<p>Hence, 2nd iteration weight vector is (0,0, 0.19, 0.19, 0.19)</p>
<p></p>
<p></p>",2019-09-22T07:23:33Z,59,Week 9/15 - 9/21,feedback,,jqrr36mqfm8M,k0unnw3hlit649,2019-09-22T07:23:33Z,{},project1
1744,stud,"<p>Well, let&#39;s see. I may have made a mistake above somewhere.</p>
<p></p>
<p>Coming out of the first sweep, the updated weights vector will be $$(0,0,0.1,0.1,0.1)$$. That&#39;s right. </p>
<p></p>
<p>What happens next? On the second sweep, it&#39;s again the case that only the third increment (i.e., $$\Delta w_{t=3}$$) induces any changes to the weights vector. That comes from the fact that $$z \neq P_{3}$$. Here, $$z = 1$$ and $$P_{3} = 0.1$$. So the error factor should be $$z - P_{3} = 1 - 0.1 = 0.9$$.</p>
<p></p>
<p>Thus the update (for a single sequence) will be $$\alpha(z - P_{t})\sum_{k=1}^{t}x_{k}|_{\alpha = 0.01, z = 1.0, t = 3}$$.</p>
<p>This is $$0.01(0.9)(0,0,1,1,1) = 0.009(0,0,1,1,1) = (0,0,0.009,0.009,0.009)$$. This gets multiplied by a factor of $$10$$ as we sweep over the training set. So the the <em>total</em> increment should be $$(0, 0, .09, .09, .09)$$. Adding this to the previous weights vector gets us $$(0, 0, .19, .19, .19)$$.</p>
<p></p>
<p>So, it appears that you&#39;re correct Anand. The second calculation in the previous post is off (it adds the increment due to a single sequence rather than the increment due to the whole training set). Thanks for catching this. </p>",2019-09-22T14:11:37Z,58,Week 9/22 - 9/28,feedback,a_0,,k0v28ok9nam4y,2019-09-22T14:11:37Z,{},project1
1745,no,"<p>nice you derive the same answer as well. Its all tricky even if we get convergence and the numbers appear close enough to actual predictions, we can still have bugs. I fixed a few like that</p>",2019-09-22T14:32:13Z,58,Week 9/22 - 9/28,feedback,,jqrr36mqfm8M,k0v2z5vpoyb6tu,2019-09-22T14:32:13Z,{},project1
1746,no,"<p>&#64;anonymous, thx for your lengthy reply.</p>
<p>I apologize for the very shady notation $$\Delta_{w}(F)$$ which meant the  contribution of $$\Delta w_{t=x_{F}}$$ to $$w_{F}$$.  </p>
<p>I use a small lambda, so I didn&#39;t consider the contribution to the previous states&#39; weights.  </p>
<p>No excuse, but I posted the question just before going to bed, my mind was foggy already.</p>
<p></p>
<p>Anyway, without changing anything but reading carefully again how Sutton handles one training set, I realized I had overlooked sthg that turned out to be the key to convergence. </p>
<p>Then, suddenly, the $$\Delta w$$ started to converge really quickly, a few iterations top, even if I set the error to a very small number, like 0.0005.  Without eligibility trace.</p>
<p>I think this for sure is stgh we need to look into for the report.  There&#39;s a reason he did it like he did, with 10 sequences and not 50.  </p>
<p></p>
<p>However, the probabilities are quite far from the theoretical values.</p>
<p>For instance, I got</p>
<p>&#34;Weights = B: 0.31 C: 0.64 D: 0.79 E: 0.92 F: 0.98&#34; with alpha = 0.05, lambda = 0.3</p>
<p></p>
<p>So, the RMS is 1.14, not good, but we can see the pattern of the weights increasing from B to F, so I&#39;m getting near now I hope.  </p>
<p></p>
<p>About alpha, yes, for sure, it needs to be small so that even if you get ten &#39;wins&#39; in a row, the $$\Delta_{w}(F)$$ (sorry, couldn&#39;t resist) don&#39;t add up to a high number before the weights get updated.  </p>
<p>Now I need to figure out why I&#39;m so far off.  All the values are too high, but now that I think of it, it could be because I&#39;m letting states who didn&#39;t end with a win contribute too much. </p>
<p>For instance, if you go to F 5 times before winning, those 5 times will add to the delta&#39;s although, ideally none should.  </p>
<p>Maybe eligibility traces will fix this.  More to experiment.  I&#39;m starting to find this problem fun.  </p>
<p></p>
<p></p>
<p></p>",2019-09-21T21:49:39Z,59,Week 9/15 - 9/21,followup,,jzh6k6o994a6dh,k0u35v3vvbe3fw,2019-09-21T21:49:39Z,{},project1
1747,stud,"<p>Got it. </p>
<p></p>
<p>For what it&#39;s worth, I don&#39;t think the notation $$\Delta w_{t = x_{F}}$$ makes much sense, either. The subscript $$t$$ here is supposed to index the (discrete) time step at which we received a particular observation. So it should be positive integer valued. But the observation we received at time $$t$$, $$x_{t}$$ (which might happen to equal $$x_{F}$$) is a <em>vector</em>. </p>
<p></p>
<p>I also think that there&#39;s a possible confusion evidenced when you say you&#39;re trying to represent the &#34;contribution  of [$$\Delta w_{t}$$] to $$x_{F}$$&#34;. The updates don&#39;t &#34;contribute&#34; to any of the (nonterminal) state observation vectors. Those are represented by the standard basis vectors for $$\mathbb{R}^{5}$$, and they are all <em>constants</em>. Rather, the <em>weights vector</em> is what (possibly) changes due to an increment $$\Delta w_{t}$$, and <em>how</em> it changes is determined by both the temporal difference for some particular value of $$t$$ <em>and</em> which state observation vectors appear in the sum $$\sum_{k=1}^{t}x_{t}$$. </p>",2019-09-22T14:28:02Z,58,Week 9/22 - 9/28,feedback,a_0,,k0v2ts4y22j179,2019-09-22T14:28:02Z,{},project1
1748,no,"<p>Mate, I wrote &#39;the contribution of $$\Delta w_{t}\ to\ w_{F}$$, not to $$x_{F}$$.  </p>
<p>And the notation $$t = x_{F}$$ obviously means &#39;every time t when we update state $$x_{F}$$ when we go to F.  </p>
<p>Let&#39;s focus on the algorithm.  </p>
<p>Did you manage to get numbers close to the theoretical probabilities?</p>",2019-09-22T14:55:58Z,58,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0v3tpzrnub3ne,2019-09-22T14:55:58Z,{},project1
1749,stud,"<p><em>&#34;I wrote &#39;the contribution of Δwt to wF, not to xF.&#34;</em></p>
<p><em> </em></p>
<p>Whoops! My bad. That makes a bit more sense. Though, it&#39;d probably be clearer if you just said $$w_{5}$$, since I think that&#39;s what you&#39;re talking about (i.e., the fifth componenet of $$\vec{w}$$). Remember that these posts are visible to <em>everyone</em>, and the introduction of new notation which differes from Sutton&#39;s might confuse some people. If you really need it, then fine. But I don&#39;t think it&#39;s <em>really</em> necessary here. </p>
<p></p>
<p><em>&#34;And the notation t=xF obviously means &#39;every time t when we update state xF when we go to F.&#34;</em></p>
<p><em></em></p>
<p>To be completely fair, the obvious (or plain) interpretation of $$t = x_{F}$$ is that the time index for some unspecified $$t$$ is equal to the observation vector for state $$F$$ (because that&#39;s what $t = x_{F}$$ <em>literally</em> says). If you want to <em>stipulate</em> another meaning for that particular combination of symbols you&#39;re free to do so, I suppose. But again, not sure I see the benefits of doing this. But your choice, ultimately. </p>
<p></p>
<p><em>&#34;Did you manage to get numbers close to the theoretical probabilities?&#34;</em></p>
<p><em></em></p>
<p>Not yet!</p>",2019-09-22T16:26:14Z,58,Week 9/22 - 9/28,feedback,a_0,,k0v71sth5wy1s8,2019-09-22T16:26:14Z,{},project1
1750,stud,"<p>Now I&#39;m reliably reproducing the figure for Sutton&#39;s experiment 1. I&#39;m happy to share whatever details I can, up to what&#39;s allowed. </p>",2019-09-23T00:57:33Z,58,Week 9/22 - 9/28,feedback,a_0,,k0vpbcumfs1ba,2019-09-23T00:57:33Z,{},project1
1751,no,"<p>That&#39;s great! Me, I do get the theoretical values at times, but also pretty awful values.  Take a look at this, and let me know what you think.</p>
<p></p>
<p>I printed the weights only when RMS &lt; 0.2.  Lambda = 0.3, alpha = 0.01, gamma (eligibility traces) = 0.5.</p>
<p>I stop looping when the sum of all the weight delta&#39;s &lt; 0.000001 at the end of a training set.</p>
<p></p>
<p>I honestly don&#39;t know what to play with anymore.</p>
<p></p>
<p>RMS = 0.561<br />RMS = 0.076 Weights = B: 0.14 C: 0.34 D: 0.51 E: 0.67 F: 0.8<br />RMS = 0.598<br />RMS = 1.9<br />RMS = 0.077 Weights = B: 0.16 C: 0.33 D: 0.49 E: 0.63 F: 0.81<br />RMS = 0.476<br />RMS = 0.843<br />RMS = 0.364<br />RMS = 0.776<br />RMS = 0.275<br /><br />RMS mean = 0.595<br />RMS stdev= 0.502<br /><br /></p>",2019-09-23T01:30:56Z,58,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0vqi9zms2d4v4,2019-09-23T01:30:56Z,{},project1
1752,stud,"<p>I&#39;m a little confused by your having <em>both</em> a $$\lambda$$ and a $$\gamma$$. I know that the version of TD we saw in lecture had (I think) both an eligibility trace, $$e$$, and a discount factor, $$\gamma$$, but Sutton doesn&#39;t seem to use both of these. Rather, in Sutton&#39;s implementation, $$\lambda$$ <em>is</em> what we&#39;re calling the &#39;eligibility trace&#39; (Sutton calls it &#34;exponential weighting with recency&#34;, but it&#39;s the same idea so far as I can tell). I don&#39;t think we need discount factors here (I didn&#39;t use one). </p>
<p></p>
<p>It sounds like your overall implementation approach is different from mine. Are you trying to confirm your approach is correct by running a single trial for particular values of the parameters, and then looking at how close you get to the <em>ideal predictions</em>? If so, can I suggest a different approach?</p>
<p></p>
<p>I thought about it like this. </p>
<p></p>
<p>The most basic thing we need to do is return (i.e., <em>calculate</em>) the weight update for a <em>single sequence</em>. Code a funtion that does just that. Test it on some simple sequences with convenient values of some of the parameters and initial weights vector until you&#39;re <em>very </em>confident that it&#39;s calculating the increment for <em>a single markov walk</em> correctly. Check that off.</p>
<p></p>
<p>The next thing we need to be able to do (for experiment 1) is calculate the <em>total</em> increment <em>over a training set</em>. So code up a function that takes in a training set comprised of ten observation-outcome sequences (and whatever parameters/initial weights you need) and returns the <em>total</em> increment due to that training set. This function should use <em>the first function</em> above (i.e., the one that computes the increment from a single sequence), because all we&#39;re doing is computing the increment for a single sequence <em>ten times </em>(once for each sequence in the set) and adding all that up. Again, make up small training sets (I used some suggested by your remarks above, for what it&#39;s worth. That is, &#34;degenerate&#34; sets that just repeated the same short sequence over and over again. These are easy to reason about and it&#39;s easy to verify that you&#39;re getting the correct result on them), and verify that your <em>per training set</em> increments are correct until you&#39;re <em>very</em> confident that this is true in general. Now check that off. </p>
<p></p>
<p>Next, you need to be able to update an initial weights vector by repeatedly adding the per training set increment you <em>just found</em> until the weight vector from the previous iteration is sufficiently close to the weight vector from the present iteration. Code a function to do this. This function should basically just be a &#34;wrapper&#34; for your function which computes the per training set increment (i.e., it should be a function that says, basically, &#34;keep getting the total increment for this training set, adding it to the present weight vector, comparing <em>that</em> to the old weight vector, and repeating this process for the new weight vector until the old weight vector and the new weight vector are sufficiently close in value.&#34;) Have this function return the value of the weight vector when you stop iterating over the training set. This is a bit harder to check (convergence and all that), but if you did due dillegence with testing the other functions, it should be easy to code (and it should do what it&#39;s supposed to). </p>
<p></p>
<p>Next, we need to run the experiment for a particular value of $$\lambda$$ (and a sensible value of $$\alpha$$). This should just consist of running the last function above on the <em>entire training batch, </em>i.e., all one hundred training sets. You&#39;ll need to persist all of the final weights vectors for each of those sets and calculate the average rms-error the way Sutton describes in the paper (Be <em>very</em> careful here. If you don&#39;t calculate the rms_error correctly, your results will look wrong when you plot them, even if the algorithm implementation is correct.)</p>
<p></p>
<p>I found it convenient to implement experiment 1 as a <em>class</em>. I had one <em>instance</em> of that class for each value of $$\lambda$$, and so all of the important data associated with a particular $$\lambda$$ value for experiment 1 was encapsulated in a corresponding instance of the experiment 1 class. You may likewise find it convenient to do things this way, but it shouldn&#39;t be strictly necessary. </p>
<p></p>
<p>Vectorize with numpy wherever you can, but be prepared to wait for a while until all of your results are being returned and plotted. This is <em>far</em> from the most efficient approach, I&#39;d wager. It is, however, pretty straightforward and easy(ish) to get right.</p>
<p></p>
<p>You&#39;re going to have to find a &#34;sweet spot&#34; for $$\alpha$$, but others have already mentioned values that work well for this experiment elsewhere on Piazza. </p>
<p></p>
<p>If you want to make sure your results aren&#39;t just a fluke, run the experiment multiple times with different initial weights vectors and <em>don&#39;t set a seed</em>. Your graph shouldn&#39;t be a &#34;function&#34; of your seed value. The result should be robust in the sense that <em>different </em>training batches (so, different seeds) produce identical (or, at least, <em>nearly so</em>) figures.</p>
<p></p>
<p>That&#39;s everything I&#39;ve got. Hope it&#39;s helpful. </p>
<p></p>
<p>Now I&#39;m going to bed : )</p>",2019-09-23T02:32:05Z,58,Week 9/22 - 9/28,feedback,a_0,,k0vsowxyoy3mr,2019-09-23T02:32:05Z,{},project1
1753,no,"<p>Thanks for your answer.</p>
<p>Can you believe that all my pain was coming from a wrongly implemented RMSE??? How silly is that! </p>
<p>I forgot to divide by 5 before the sqrt.</p>
<p>Now, I have the opposite issue, my RMSE&#39;s are way smaller than Sutton&#39;s... a quality problem I guess.  </p>",2019-09-23T23:09:00Z,58,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0x0vlulb7n2dk,2019-09-23T23:09:00Z,{},project1
1754,stud,"<p>I can, actually! Something similar happened to me as well (hence the warining about carefully calculating the rmse).</p>
<p></p>
<p>When I was getting values too small, the rmse was again the culprit (for what it&#39;s worth). Look and see if your errors get closer to Sutton&#39;s if you multiply by a factor of ten in the <em>average</em> RMSE calculation. I was off there too, so everything wound up being smaller by a factor of ten than it should have been. Fixed that, and everything fell into place. </p>",2019-09-23T23:44:26Z,58,Week 9/22 - 9/28,feedback,a_0,,k0x256k9uom1nt,2019-09-23T23:44:26Z,{},project1
1755,stud,"<p>So - I am kind of trying to compare this to the last homework. I am definitely struggling. I feel like a couple of things are throwing me off:</p>
<p>1. What is the value estimate? I&#39;m somehow computing that based on vectors?</p>
<p>2. How am I supposed to track the val estimate for each B,C,D,E,F? Can I just go through the whole sequence and update the value everytime that specific one occurs? That whole piece is not jiving for me. </p>",2019-09-24T00:54:03Z,58,Week 9/22 - 9/28,followup,a_1,,k0x4mph4xx41zg,2019-09-24T00:54:03Z,{},project1
1756,no,"<p>Here the &#39;value estimate&#39; P of a state is its probability to win when you are in that state.  Read the maths in ch4 if you want to be convinced, or take it at face value.</p>
<p></p>
<p>It is all a bit confusing here because, unlike hw2, Sutton starts by using vectors xt for states to be general, ie to record several measurements from the real world.</p>
<p>And then he uses a general formula Pt=f(w,xt) to estimate the outcome z at each step t, but for our random walk problem, he uses a linear function for P and vectors with only a &#39;1&#39; placed at the location corresponding to the state. </p>
<p>For instance, xC = (0,1,0,0,0). See his explanations p19.</p>
<p>This turns Pt into wt, so the value estimate of a state becomes simply its weight, and (Pt&#43;1 - Pt) is simply (wt&#43;1 - wt), but, be careful, the last term in eq 4 is a sum of gradients of P, ie vectors xi.  </p>
<p></p>
<p>The equations are given at &#64;314 if you have issues with the gradients.  </p>
<p></p>",2019-09-24T01:18:22Z,58,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0x5hywvj6t4zz,2019-09-24T01:18:22Z,{},project1
1757,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2FjfzaqnqvtQ1m%2Fk0w4us0ehzpm%2Ffigure_4.png"" alt="""" /></p>
<p></p>
<p>Thanks Farrukh. I updated it and it looks like above. It seems the error for different lambda varies a lot unlike the fig 4 in the paper.</p>
<p></p>
<p>Any pointers on what I might be doing wrong to get the correct figure ? Others seem to get the exact same fig as in the paper.</p>
<p></p>
<p>Appreciate your help here.</p>
<p></p>
<p></p>",2019-09-23T08:18:34Z,58,Week 9/22 - 9/28,followup,,jfzaqnqvtQ1m,k0w52icaetc73h,2019-09-23T08:18:34Z,{},project1
1758,no,"<p>Here&#39;s a newer version after reducing seq len as suggested in some piazza posts:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2FjfzaqnqvtQ1m%2Fk0xcvkcaz3n9%2Ffigure_4.png"" alt="""" /></p>
<p></p>
<p></p>
<p>It looks a lot like the fig 4 inthe paper now but I do see it still shooting up for lambda=1</p>",2019-09-24T04:46:50Z,58,Week 9/22 - 9/28,feedback,,jfzaqnqvtQ1m,k0xcy25c9n6iy,2019-09-24T04:46:50Z,{},project1
1759,no,<p>&#64;Anurag Tangri : what did u do to rectify the first problem you mentioned in the post?</p>,2019-09-24T17:34:31Z,58,Week 9/22 - 9/28,feedback,,jl2bv84c6FEX,k0y4dbedtlu5z1,2019-09-24T17:34:31Z,{},project1
1760,no,"Hi Shimlee,<div>I was not initializing weights to 0.5 after every training set.</div><div>Also I had incorrect formula for calculating rmse </div><div>Does this latest graph look ok ?</div><div><br /></div><p></p>",2019-09-24T18:59:16Z,58,Week 9/22 - 9/28,feedback,,jfzaqnqvtQ1m,k0y7eamapw03dv,2019-09-24T18:59:16Z,{},project1
1761,no,"<p>It does look similar to sutton,  though, 0.3 and 0.0 is crossing over for you instead of 0.8 and 0.0.</p>
<p></p>",2019-09-25T01:22:13Z,58,Week 9/22 - 9/28,feedback,,jl2bv84c6FEX,k0yl2s02v335fw,2019-09-25T01:22:13Z,{},project1
1762,no,"<p>Thanks!</p>
<p></p>
<p>Any pointers on what I might be doing wrong here ?</p>",2019-09-25T23:53:13Z,58,Week 9/22 - 9/28,feedback,,jfzaqnqvtQ1m,k0zxc64oh6c7fd,2019-09-25T23:53:13Z,{},project1
1763,no,"<p>Hello instructors,</p>
<p>I ended up with two implementations for fig4 since last time. </p>
<p></p>
<p>Here are the two graphs I see. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2FjfzaqnqvtQ1m%2Fk13uv0ueb1pp%2Ffigure_4.png"" alt="""" /></p>
<p></p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2FjfzaqnqvtQ1m%2Fk13uxcoa714d%2Ffigure_4_fin.png"" alt="""" /></p>
<p></p>
<p>Top one has the shape as in paper (the one I shared earlier) but lower one seems have to right order of curves for lambda.</p>
<p></p>
<p>I wanted to get your thoughts on which one to include in the report.</p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>",2019-09-28T18:01:10Z,58,Week 9/22 - 9/28,followup,,jfzaqnqvtQ1m,k13v2zneuvj39m,2019-09-28T18:01:10Z,{},project1
1764,no,"<p>I believe it&#39;s one global one. However, in experiment1, it&#39;s an delayed update till end of a training set. So yes, in code you may need different variables to account for a weight to use in your P =wx calculation and a temp weight vector that accumulates till end of the training set</p>
<p></p>
<p>Atleast that is what I am doing ..</p>",2019-09-21T20:29:06Z,61,Week 9/15 - 9/21,followup,,jqrr36mqfm8M,k0u0aa3u9qd75n,2019-09-21T20:29:06Z,{},project1
1765,no,<p>Should the sum of the weight vector always equal 1?</p>,2019-09-21T23:25:16Z,61,Week 9/15 - 9/21,followup,,jqmjg3txqw6ji,k0u6ktqtege3ai,2019-09-21T23:25:16Z,{},project1
1766,no,"<p>Yes because, here, they represent the probability to reach G from each position.  </p>",2019-09-21T23:30:16Z,61,Week 9/15 - 9/21,feedback,,jzh6k6o994a6dh,k0u6r8x7r92205,2019-09-21T23:30:16Z,{},project1
1767,no,"<p>That is what I thought. Does this happen naturally when updating the values for w? Or do you need to do something to ensure that everything adds up to one, like normalize the vector?</p>
<p></p>
<p>My w vectors are currently not adding up to 1 for any training set.</p>",2019-09-21T23:32:58Z,61,Week 9/15 - 9/21,feedback,,jqmjg3txqw6ji,k0u6uqh2g6s5ns,2019-09-21T23:32:58Z,{},project1
1768,no,"<p>In example 6.2 p125 in S&amp;B, about the random walk, Sutton wrote &#39;Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state.&#39;</p>
<p>I don&#39;t clearly understand why though, unless I read 4 pages of thick maths in Sutton&#39;s paper.  It&#39;s something I need to do, but it&#39;s not a priority until I have managed to do the project.  </p>
<p>If you can find it explained in an intuitive way, I&#39;m interested.  </p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-22T00:06:40Z,60,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0u822bgock40k,2019-09-22T00:06:40Z,{},project1
1769,stud,"<p></p><div>
<div>
<p>I don&#39;t think the above is correct. In particular, notice that Sutton (1988), page 20, tells us that the <em>ideal predictions</em> are $$(\frac{1}{6}, \frac{1}{3}, \frac{1}{2}, \frac{2}{3}, \frac{5}{6})$$. These weights <em>do not</em> sum to $$1$$.</p>
<p></p>
<p>I don&#39;t think that the weights vector is intended to define a probability distribution over states, even if each weight represents a probability. </p>
</div>
</div>
<div>
<div>
<div>
<div>
<p><em></em></p>
</div>
</div>
</div>
</div>",2019-09-22T02:01:33Z,60,Week 9/22 - 9/28,feedback,a_0,,k0uc5swsdyy3di,2019-09-22T02:01:33Z,{},project1
1770,no,<p>correct. each wi represent P(right | start at i) so doesnt have to sum to 1</p>,2019-09-22T04:34:19Z,60,Week 9/22 - 9/28,feedback,,is5gzbotXmz,k0uhm9mkz3t37l,2019-09-22T04:34:19Z,{},project1
1771,no,"<p>I have been having the divergence related issues as well, I recommend playing around with the alphas.</p>",2019-09-21T20:26:27Z,61,Week 9/15 - 9/21,followup,,jzzw8pi7a7t3d4,k0u06uzcsrf4bn,2019-09-21T20:26:27Z,{},project1
1772,no,"<p>I have been using alpha 0.01, and it has worked nicely for lambdaS less than 1.0</p>
<p></p>
<p></p>",2019-09-21T20:31:14Z,61,Week 9/15 - 9/21,followup,,jqrr36mqfm8M,k0u0d08cgqo70z,2019-09-21T20:31:14Z,{},project1
1773,no,<p>it was a bug in code around how P was calculated. </p>,2019-09-22T16:23:50Z,60,Week 9/22 - 9/28,feedback,,jqrr36mqfm8M,k0v6ypa5f251lb,2019-09-22T16:23:50Z,{},project1
1774,no,"<p>When my lambda is zero, the error is really big. Is there something wrong? </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9puq99s5uv%2Fk116dtiblk5j%2Ffigure4.png"" alt="""" /></p>",2019-09-26T20:54:20Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k116dz43zx4b8,2019-09-26T20:54:20Z,{},project1
1775,stud,<p>Having same problem. Have you found the reason?</p>,2019-09-27T15:10:59Z,60,Week 9/22 - 9/28,feedback,a_0,,k129ka6mg22yr,2019-09-27T15:10:59Z,{},project1
1776,no,"<p>After search though all related post here, I don&#39;t believe that&#39;s a problem. If you limit your y axis, the figure will somewhat like the one in the paper. Of Course, without knowing the exact experiment parameter, the result will never the the same. The trend is already there. </p>",2019-09-27T18:30:17Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k12gol1pd9u291,2019-09-27T18:30:17Z,{},project1
1777,no,<p>thanks &#64;Binod... I missed the OH (the calendar didn&#39;t push the planned notification... weird)</p>,2019-09-22T01:34:42Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k0ub7a0fjmt2ak,2019-09-22T01:34:42Z,{},project1
1778,no,"<p>I just noticed the same thing with my chart (unfortunately only after start writing my paper). I also don&#39;t know how to explain that and I couldn&#39;t  find any bug in my code yet.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk0ua75o0wp0z%2FFigure_4.png"" alt="""" /></p>",2019-09-22T01:06:54Z,59,Week 9/15 - 9/21,followup,,jc6xvgjncoey,k0ua7jaqo9o19w,2019-09-22T01:06:54Z,{},project1
1779,no,"<p>Thanks, I decided to take a stab at it again.  We look eerily similar now :)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixpclzk97jg2fo%2Fk0v9fa2nv5gi%2Ffigure4_5.png"" alt="""" /></p>
<p></p>",2019-09-22T17:33:04Z,58,Week 9/22 - 9/28,feedback,,ixpclzk97jg2fo,k0v9fr69d3qh8,2019-09-22T17:33:04Z,{},project1
1780,no,"<p>&#64;instructors, in the analysis should we weigh reproduction of graphs over reproduction of experiment? Ex: if I toggle sequence length to be different, should I do that for the sake of getting images closer to Sutton, or should I just replicate the experiment as stated and just mention this exploration in some auxiliary notes section? Seems to change results quite a bit.</p>",2019-09-22T05:30:17Z,59,Week 9/15 - 9/21,followup,,jcg0nzvdk8272b,k0ujm8gqnau1z8,2019-09-22T05:30:17Z,{},project1
1781,no,"<p>I&#39;ve been working on my code for several days now and I&#39;m confident that it is doing exactly what is outlined in the paper, but I still can&#39;t get it to reproduce the plots that are in the paper without some modifications to the procedure. My strategy is going to be to present plots from both the vanilla and modified procedures, then discuss, explain, analyze. This seems to be in line with the project description and I&#39;m also assuming that this course is graded like ML, where a large premium is placed on the analysis of your results.</p>",2019-09-22T17:14:38Z,58,Week 9/22 - 9/28,feedback,,jc9hybafy446pq,k0v8s16ue0657i,2019-09-22T17:14:38Z,{},project1
1782,no,"<p>Gotcha. Yea, I agree that it will likely be the same sort of deal as ML. Thanks!</p>",2019-09-22T18:06:53Z,58,Week 9/22 - 9/28,feedback,,jcg0nzvdk8272b,k0van86c5lx2b6,2019-09-22T18:06:53Z,{},project1
1783,no,"<p>Agree with Wade. We do look at plots but analysis is more important. </p>
<p>This is something you can choose to explore but you don&#39;t have to. You may play with sequence length to get plots closer to Sutton&#39;s. But you should make this explicit in the paper and provide some analysis around why this was required. </p>",2019-09-23T05:42:21Z,58,Week 9/22 - 9/28,feedback,,i4i9bi8rFqk,k0vzhm7d4bw5v5,2019-09-23T05:42:21Z,{},project1
1784,no,"<p>I&#39;ll try to provide an intuition behind these ideal predictions. We have 7 states and 6 probabilities between these 7 states (arrows between the states) . Also the system is symmetric around D state.</p>
<p></p>
<p>So if we are in the state D, there should be equal chance in ending up in either state A or G from the state D, because the system is symmetric and we have equal probabilities in the left and right sides. </p>
<p></p>
<p>Now if we are in the state C, the system is skewed to the left as we should have a higher chance to end up in the state A than in state G (we have less arrows in the left side than in the right side). Let&#39;s X be the ideal probability to end the game (end up in either state A or G), then to end up in the A state is 4/6 of X and to end up in the state G is 2/6 of X as we need to &#34;walk&#34; just 2 arrows in the left and 4 arrows on the right. That&#39;s why the ideal probability of the state G or &#34;frequency of ending up in the state G if we play this game infinite time&#34; equals 2/6=1/3.</p>
<p></p>
<p>Same logic applies for the state B. We have just 1 array on the left and 5 arrays on the right. If we play this game 1,000 or 1,000,000 or infinite times then we should end up in the state A more often than in the state G. How often? Well, it should be 5/6 times in the state A and 1/6 times in the state G.</p>
<p></p>
<p>Same goes for all remaining states.</p>
<p></p>
<p>Hope this is helpful!</p>",2019-09-22T16:25:55Z,60,Week 9/22 - 9/28,followup,,jzih0fdt4sn1cq,k0v71e60hgq1h1,2019-09-22T16:25:55Z,{},project1
1785,no,"<p>This is neat!  Counting arrows is definitely not a rigorous way to derive the ideal probabilities, and it wasn&#39;t obvious to me at first if this wasn&#39;t just a coincidence, but I actually think this works, even for arbitrary-sized versions of the walk.</p>",2019-09-22T21:07:01Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0vh2w9rfih2pl,2019-09-22T21:07:01Z,{},project1
1786,no,"<p>Let P<sub>a, </sub>P<sub>b, </sub>P<sub>c, </sub>P<sub>d</sub><sub>, </sub>P<sub>e, </sub>P<sub>f, </sub>P<sub>g </sub>be the true probability of the states A, B, C, D, E, F, G, we have following five equations:</p>
<p> </p>
<p>P<sub>f</sub> = 0.5P<sub>g</sub> &#43; 0.5P<sub>e</sub> = 0.5 &#43; 0.5P<sub>e</sub></p>
<p>P<sub>e</sub> = 0.5P<sub>f</sub> &#43; 0.5P<sub>d</sub></p>
<p>P<sub>d </sub>= 0.5P<sub>e</sub> &#43; 0.5P<sub>c</sub></p>
<p>P<sub>c </sub>= 0.5P<sub>d</sub> &#43; 0.5P<sub>b</sub></p>
<p>P<sub>b </sub>= 0.5P<sub>c </sub>&#43; 0.5P<sub>a </sub>= 0.5Pc</p>
<p></p>
<p>By solving them, we get 1/6, 2/6, 3/6, 4/6, 5/6.</p>",2019-09-23T01:20:24Z,60,Week 9/22 - 9/28,followup,,jzjabmjd3l2103,k0vq4qlfx66ue,2019-09-23T01:20:24Z,{},project1
1787,no,Hold on. How to come up with the 0.5 and 0 in the first and last equation? How is P defined here? Why the probability for A is 0? I thought the reward is 0 but it doesn’t mean the game won’t end up in state A. I think I’m confused with the P definition here. Can someone help to explain ? Thank <p></p>,2019-09-23T17:36:11Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k0wozlqd2l571t,2019-09-23T17:36:11Z,{},project1
1788,no,"<p>$$P_i$$ is the probability of ending at state $$G$$ starting at state $$i$$.  The probability of going from state $$G$$ to state $$G$$ is $$1$$.  LIkewise, the probability of going from state $$A$$ to state $$G$$ is $$0$$.</p>",2019-09-23T20:07:12Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0wudtc83sagz,2019-09-23T20:07:12Z,{},project1
1789,no,<p>Got it. Thanks!</p>,2019-09-27T18:08:15Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k12fw9cb1jw31,2019-09-27T18:08:15Z,{},project1
1790,no,"<p>Instructor does the following format work?</p>
<p>https://docs.google.com/document/d/1DJU6h6chzwibgqw06LXFXnxmiqlozXiYtesKiAsSWWA/edit</p>",2019-09-23T16:39:37Z,42,Week 9/22 - 9/28,followup,,jzj7y1ofgsro1,k0wmyuxks0vy2,2019-09-23T16:39:37Z,{},project1
1791,no,<p>ok that implies we may need to submit a readme with additional pip install instructions on how to install the modules and run the code.</p>,2019-09-22T17:50:55Z,58,Week 9/22 - 9/28,followup,,jqrr36mqfm8M,k0va2p1f4cr71a,2019-09-22T17:50:55Z,{},project1
1792,no,"<p>Hi Anand,</p>
<p></p>
<p>Yea, that is what my readme has. It&#39;s stated that the readme is to &#34;Include thorough and detailed instructions on how to run your source code in the README.md&#34;.</p>",2019-09-22T18:11:12Z,58,Week 9/22 - 9/28,feedback,,ixty1midfufhd,k0vasshujv63fw,2019-09-22T18:11:12Z,{},project1
1793,no,"<p>&gt; Your code is your proof that the work in your report is yours. It should include your figure generation and the easier it is for someone else to run it and verify your results the better it is for you. It does not need to be fully automated but in such a case I would include clear instructions for replication.</p>
<p></p>
<p>I did not see this post until after the project submission; I used excel and a tab delimited output; rerunning with the same seeds (which are fully automated) will result in the same dataset; and the excel files are included (as well as a .config and .output file).  this is sufficient I hope, to &#39;verify that the graphs are mine&#39;?  I&#39;m fairly certain it will be quite clear from the actual datadump that the program output is unique to my project.</p>
<p></p>
<p></p>
<p></p>",2019-10-06T08:47:25Z,57,Week 9/29 - 10/5,followup,,jzivtxcbl6964n,k1eqto5je66fb,2019-10-06T08:47:25Z,{},project1
1794,no,"<p>this graph looks good. However, I have questions on why you set the convergence to low precision at 0.001. I set it at 0.00000001</p>",2019-09-22T16:51:11Z,58,Week 9/22 - 9/28,followup,,jqrr36mqfm8M,k0v7xw3inl46mw,2019-09-22T16:51:11Z,{},project1
1795,no,"<p>With 0.00000001 I get this:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjqmjg3txqw6ji%2Fk0wef0d626cr%2FPicture1.png"" alt="""" width=""674"" height=""504"" /></p>",2019-09-22T17:18:11Z,58,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0v8wljudl13zk,2019-09-22T17:18:11Z,{},project1
1796,no,<p>This chart looks identical to mine.. </p>,2019-09-23T00:22:52Z,58,Week 9/22 - 9/28,feedback,,jqrr36mqfm8M,k0vo2qtxvlx4mp,2019-09-23T00:22:52Z,{},project1
1797,no,"<p>I get pretty much the same chart with alpha=0.01 and epsilon=0.00001. With both procedures, keeping one set of global weights or reinitialising for each train set. Well because convergence!</p>
<p></p>",2019-09-23T07:24:08Z,58,Week 9/22 - 9/28,feedback,,jzzw8pi7a7t3d4,k0w34hqf3eo5o7,2019-09-23T07:24:08Z,{},project1
1798,no,"<p>Okay cool, glad to have the sanity check. Thanks guys!</p>",2019-09-23T12:41:14Z,58,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0wegajaq576lq,2019-09-23T12:41:14Z,{},project1
1799,stud,"<p><em></em>&#34;<em>If we generate training sets as Sutton has described I would classify your example as a degenerate case. Sutton is making the point that under repeated presentation we always converge to the same ideal weights. Of course this is modulo some assumptions eg. the data contains the information we wish to learn.</em>&#34;</p>
<div>
<div>
<div></div>
</div>
</div>
<p></p>
<p>Ok.</p>
<p></p>
<p>But there are infinitely many such cases. I singled out that <em>particular</em> case  only because it&#39;s especially easy to think about (and I&#39;d already thought about it). But <em>any</em> training set that never visits one or more states is going to be &#34;degenerate&#34; in the sense at issue here.</p>
<p></p>
<p>These &#34;degenerate&#34; training sets can, moreover, be more or less arbitrarily complex (as long as at least one state doesn&#39;t appear in any sequence, what Sutton says is, strictly speaking, <em>false</em>).</p>
<p></p>
<p>So there are infinitely many degenerate training sets, and Sutton&#39;s claim holds modulo the (unstated) assumption that we don&#39;t <em>ever</em> have even a single one in our entire training batch? That seems like a signficant assumption to leave unstated. In particular, it&#39;s far from clear that it&#39;s at all likely that we&#39;d <em>never</em> see one of these &#34;degenerate&#34; training sets, even if we &#34;generate training sets as Sutton has described&#34; (and even if the <em>particular</em> example I provided is adjudged unlikely).</p>",2019-09-22T18:33:44Z,60,Week 9/22 - 9/28,followup,a_0,,k0vblrfccbr215,2019-09-22T18:33:44Z,{},project1
1800,no,"A good observation, and the intuitive takeaway is with TD we can&#39;t infer information that&#39;s not present in the data. That this assumption should be plainly started is a fair critique (and perhaps running some experiments comparing TD1 in these cases would strengthen your point), however, I find attempting to elevate your critique to Richard &#34;Sutton&#34; is &#34;false&#34;, &#34;misleading&#34; and &#34;fishy&#34; to be a logical leap.",2019-09-22T18:55:34Z,60,Week 9/22 - 9/28,feedback,,i4op5p9vfbq5yz,k0vcdugd88y4tb,2019-09-22T18:55:34Z,{},project1
1801,stud,"<p>To be fair, <em>I</em><em> </em>didn&#39;t <em>say</em> &#34;Richard Sutton is false, misleading and fishy&#34;. You did (just now). I <em>asked</em> (notice the question mark at the end of the post heading. It was put there intentionally) whether his convergece claim was false ( and intepreted with maximal generality,<em> </em>it seems that it is ), or could be considered misleading (and again, intepreted with maximal generality,<em> </em>it seems that it could be, a point you seem to grant above: &#34;A good observation, and the intuitive takeaway is with TD we can&#39;t infer information that&#39;s not present in the data. <strong>That this assumption should be plainly started is a fair critique</strong>&#34;).</p>
<p></p>
<p>I <em>did</em> use the word &#34;fishy&#34;, but only in the following lexical sense: <em>INFORMAL; arousing feelings of doubt or suspicion</em>. What meaning you took from that, I haven&#39;t a clue. But I did indeed doubt the claim (and found it suspicious, given what I&#39;d noticed), so &#34;fishy&#39; seems an appropiate (and suitably <em>value free</em>) descriptor. </p>
<p></p>
<p>&#34;<em>I find attempting to elevate your critique to &#34;Richard Sutton is false, misleading and fishy&#34; to be a logical leap.&#34;</em></p>
<p><em></em></p>
<p>I&#39;m not trying to elevate anything, really. And I wouldn&#39;t conisder it a critque, either. Not really. I&#39;m just asking a question about the material. This was allowed (and encouraged), last I checked, but maybe things have changed?</p>
<p></p>
<p>The only real &#34;leap&#34; here is from <em>what I actually said</em>, to <em>what you say I said</em> (which is another thing altogether). And for the record, I don&#39;t very much appreciate the (<em>extremley)</em> uncharitable interpretation. </p>
<p></p>
<p>Nevertheless, your overall point (&#34;with TD we can&#39;t infer information that&#39;s not present in the data&#34;) is well taken. </p>",2019-09-22T19:18:08Z,60,Week 9/22 - 9/28,feedback,a_0,,k0vd6v35pti19z,2019-09-22T19:18:08Z,{},project1
1802,no,You&#39;re right. My characterization was unfair and I&#39;ve edited my post. What I think I was trying to communicate is that we generally try to give each other the benefit of the doubt (as I failed to do). Honest mistakes (as evident in this very thread!) are common.,2019-09-22T19:31:38Z,60,Week 9/22 - 9/28,feedback,,i4op5p9vfbq5yz,k0vdo7v96ci2z0,2019-09-22T19:31:38Z,{},project1
1803,stud,"<p><em></em></p>
<div>
<div>
<blockquote>
<p><em>&#34;My take is that the whole process is about nudging the weight vectors to a value close enough to ideal prediction by adjusting for the errors iteratively (we learn). So the initial vector values shouldn&#39;t matter. Also the training sets are randomly generated and we are doing this over 1000 randomly generated sequences. </em></p>
<p><em> </em></p>
<p><em>Like you mention, yes for small hard coded training sets like you show, initial vectors may matter. That also means not much scope for learning as there are no differences in your set.&#34;</em></p>
<p><em></em></p>
</blockquote>
<p>I disagree. This seems like a more general phenomenon than my initial example perhaps suggests. See my response to Chris, above. </p>
</div>
<div>
<div></div>
</div>
</div>",2019-09-22T18:54:04Z,60,Week 9/22 - 9/28,followup,a_0,,k0vcbwkwri42rj,2019-09-22T18:54:04Z,{},project1
1804,no,"<p>I think this is a non-issue, and agree with Chris&#39;s original answer.</p>
<p></p>
<p>It is almost always the case that if we are approaching something looking to critique it, we can find something to critique.  In any real engineering problem, there are always going to be many unsaid assumptions when trying to communicate some result, because otherwise the communication would be too long, and would fail to focus on the desired points.  If we want every assumption spelled out, we can try pure math research (even there it&#39;s not the case).</p>
<p></p>
<p>We&#39;re not in R. Sutton&#39;s shoes at the time he wrote this paper in the late 1980s to know what kind of trade-offs he was considering in terms of paper length, topic coverage, desire to explain vs desire to keep simple, etc.  But regardless of all of that, I think that this particular issue is definitely not worth mentioning because:</p>
<p></p>
<p>(1) One can assume that over 10 sequences, not all episodes are degenerate.  It seems to me that for your potential problem to be realized we need at least one state (out of 5) to never be entered across all 10 episodes.  The chances of this happening are on the order of 0.1%.  It is highly likely that his statement</p>
<p></p>
<p>&#34;For small $$\alpha$$, the weight vector always converged in this way, and always to the same final value, independent of its initial value<strong>.&#34;</strong></p>
<p><strong></strong></p>
<p>was indeed borne out by his experiments.</p>
<p></p>
<p>(2) Even if a person isn&#39;t satisfied with (1), anyone who is at the level of this paper knows that the assumption of (1) is intended and would either excuse a discrepancy, or would regenerate their sequences in the unlikely case a training set of degenerate sequences was created.</p>
<p></p>
<p>(3) The choice of 10 sequences is somewhat arbitrary - if computational power wasn&#39;t an issue (this is 1988 after all) we could instead use training sets of 100 sequences, in which case this really is a non-issue.  The point of his statement is meant to transcend computational limitations, and reflect the fact that the initial conditions don&#39;t affect the final weight vector when TD is converging to the values of states in a random sequence of random walks under the repeated presentations paradigm.</p>",2019-09-22T20:27:19Z,60,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k0vfnu6s6ssbh,2019-09-22T20:27:19Z,{},project1
1805,stud,"<p>So, first of all, Vahe, I didn&#39;t claim that this is significant (that it was <em>not</em> a &#34;non-issue&#34;, as you put it). Disagree? All of my thoughts about this are up above. Quote me where I wrote above that this is some &#34;profound discovery&#34; on my part. </p>
<p></p>
<p>And asking a question about a paper we&#39;ve been asked to read (including whether a claim the author made might be false or misleading) and whose results we&#39;ve been asked to duplicate doesn&#39;t need to be interpreted as a personal attack on said author. I mean, if you and Chris want to see things that way, I can&#39;t stop you. There&#39;s no way, unfortuantely, to force people to read charitably. But I&#39;ve already responded to this above, and I&#39;ll not spill more ink on it. </p>
<p></p>
<p><em>&#34;It is almost always the case that if we are approaching something looking to critique it, we can find something to critique.  In any real engineering problem, there are always going to be many unsaid assumptions when trying to communicate some result, because otherwise the communication would be too long, and would fail to focus on the desired points.  If we want every assumption spelled out, we can try pure math research (even there it&#39;s not the case).&#34;</em></p>
<p><em></em></p>
<p>Granted. And there&#39;s no reason that we should have to refrain from asking a question about something that we find puzzling, least of all in a learning environment. And again, (as I said above in the post you likely <em>didn&#39;t</em> read) I wasn&#39;t offering a &#34;critique&#34; of Sutton&#39;s work. Again, you&#39;re free to take things anyway you like. But that&#39;s on you. I&#39;ve already made my view on this quite clear. </p>
<p></p>
<p><em>&#34;We&#39;re not in R. Sutton&#39;s shoes at the time he wrote this paper in the late 1980s to know what kind of trade-offs he was considering in terms of paper length, topic coverage, desire to explain vs desire to keep simple, etc.  But regardless of all of that, I think that this particular issue is definitely not worth mentioning because:</em></p>
<p><em></em></p>
<p><em>(1) One can assume that over 10 sequences, not all episodes are degenerate.  It seems to me that for your potential problem to be realized we need at least one state (out of 5) to never be entered across all 10 episodes.  The chances of this happening are on the order of 0.1%.  It is highly likely that his statement</em></p>
<p><em> </em></p>
<p><em>&#34;For small α, the weight vector always converged in this way, and always to the same final value, independent of its initial value<strong>.&#34;</strong></em></p>
<p></p>
<p><em>was indeed borne out by his experiments.</em></p>
<p><em> </em></p>
<p><em>(2) Even if a person isn&#39;t satisfied with (1), anyone who is at the level of this paper knows that the assumption of (1) is intended and would either excuse a discrepancy, or would regenerate their sequences in the unlikely case a training set of degenerate sequences was created.</em></p>
<p></p>
<p><em>3) The choice of 10 sequences is somewhat arbitrary - if computational power wasn&#39;t an issue (this is 1988 after all) we could instead use training sets of 100 sequences, in which case this really is a non-issue.  The point of his statement is meant to transcend computational limitations, and reflect the fact that the initial conditions don&#39;t affect the final weight vector when TD is converging to the values of states in a random sequence of random walks under the repeated presentations paradigm.&#34;</em></p>
<p><em></em></p>
<p>And you&#39;re not in my shoes. I am not Sutton. I dont&#39;t know this material like the back of my hand (and I certainly didn&#39;t write the paper that&#39;s considered the <em>locus classicus</em> for this sort of approach). I&#39;m not under any illusions that I&#39;m &#34;at the level of this paper&#34;, and I didn&#39;t make the decision to use it as the primary teaching material for this project. It actually does matter, by the way, for a novice who is trying to implement Sutton&#39;s algorithm by following his remarks in the paper what&#39;s going on here. At least, this  was something that confused me, and now it doesn&#39;t. </p>
<p></p>
<p>Finally, please do not <em>presume</em> to tell me what questions are &#34;worth mentioning&#34;. That is rude, patronizing, and condescending. I asked an honest question, and I accepted Chris&#39;s follow up response that <em>&#34;with TD we can&#39;t infer information that&#39;s not present in the data&#34;</em> as an explanation of what I had noticed. <em>You</em> are not an instructor, and I have every right to ask a question and request an answer from course staff. I pay tution to be able to be here and do just that. </p>
<p></p>
<p></p>",2019-09-22T20:57:12Z,60,Week 9/22 - 9/28,feedback,a_0,,k0vgq9otth966f,2019-09-22T20:57:12Z,{},project1
1806,stud,"<p><em>&#34;1) One can assume that over 10 sequences, not all episodes are degenerate&#34;</em></p>
<p><em></em></p>
<p>By the way, it&#39;s the <em>training sets</em> that can exhibit &#34;degeneracy&#34; in the sense at issue here. Not the individual sequences. It makes no sense to call them degenerate. Even if a particular sequence doesn&#39;t visit some states, we won&#39;t get this sort of &#34;non-issue&#34; if <i>other</i> sequences in the set visit those states. </p>",2019-09-22T21:12:42Z,60,Week 9/22 - 9/28,feedback,a_0,,k0vha73nka85eh,2019-09-22T21:12:42Z,{},project1
1807,no,"<p>My tone was a response to the tone of your title: &#34;False (or Misleading) Claim in Sutton (1988)?&#34; and tone of subsequent posts.</p>
<p></p>
<p>If you had phrased your post differently, e.g. &#34;Noticed an interesting quirk that violates Sutton&#39;s conclusion&#34;, then you would have probably gotten a different tone in my response.</p>
<p></p>
<p>Also, my saying that your observation is not an issue with the quality of the paper (whether the paper is misleading or false) is not the same as saying that your observation is a bad one or a question not worth asking.  In fact I think it&#39;s a great insight.  It made me really think about the dynamics of the algorithm, which is why I put so much effort into my response.  I even ran a simulation which is where I got my 0.1% figure.</p>
<p></p>
<p>I should have shown appreciation for the fact that you taught me something, and instead I guess I came across like I was criticizing you.</p>
<p></p>
<p>In any case, this isn&#39;t personal, at least it&#39;s not for me.</p>",2019-09-22T21:21:16Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0vhl7tjxan7gw,2019-09-22T21:21:16Z,{},project1
1808,stud,"<p><em>&#34;My tone was a response to the tone of your title: &#34;False (or Misleading) Claim in Sutton (1988)?&#34;</em></p>
<p><em></em></p>
<p>There is no &#34;tone&#34; problem here. I asked if there might be a false or misleading claim in <em>a paper</em>. This does not presuppose, entail, hint at, or even <em>suggest</em> that an accusation is being made that the author of said paper is guilty of deliberate dishonesty (or even that there is &#34;a problem with the paper&#34;, for that matter, unless we assume that only papers completely free of errors have value). </p>
<p></p>
<p>An accusation of deliberate dishonesty would require, <em>at minimum</em>, the assertion that a falsehood (or misleading statement) was made with <em>the <strong>deliberate</strong> intent to decieve or mislead</em>. And no such assertion was made, or even insinuated (not, at least, by me). </p>
<p></p>
<p>All that <em>needs</em> to be the case in order for there to be a &#34;False (or Misleading) Claim in Sutton (1988)?&#34; is, well...that the latter contains at least one false or misleading claim (and the misleading part is harder to judge, since that&#39;s arguably at least <em>somewhat</em> subjective, in at least <em>some</em> cases, and probably in this one). </p>
<p></p>
<p>Even people with well deserved reputations for general veracity and truth telling are prone to making false and/or misleading claims now and again, btw, if only because it is difficult to live the length of a typical human life and <em>never</em> say anything false (or anything which misleads anyone ever). To suppose otherwise seems to be to assume someone is, if not omniscient, then at least <em>infallible</em>. Hopefully, we can all agree that Sutton, as talented as he surely is, is still only human and thus in principle prone to the occasional error (whether or not we agree that the issue above constitutes an error). </p>
<p></p>
<p>That I would actually have to take the time to <em>explain</em> any of this, especially given that the above mostly consists of what I would take to be (essentially) platitudes, is a fact which I find utterly amazing (and not in a good way).</p>
<p></p>
<p>In any event, if you got <em>something </em>from thinking about the issue rasied, I am glad. </p>
<p></p>
<p>For my part, this has become tedious and uninteresting. I&#39;m unfollowing the post. </p>
<p></p>
<p></p>",2019-09-22T22:54:51Z,60,Week 9/22 - 9/28,feedback,a_0,,k0vkxkdwlim4wh,2019-09-22T22:54:51Z,{},project1
1809,no,"<p>When you repeat a training set over and over, the weights propagate no matter what, even with lambda = 0.</p>",2019-09-23T19:25:44Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k0wswh9obfz48t,2019-09-23T19:25:44Z,{},project1
1810,no,"<p>Hey, I mentioned in a previous post that mine takes ~6 minutes to generate all the graphs. My convergence criteria was within 10^-5. Someone else mentioned 30 seconds which makes me question my algorithm, but I went over it a lot and feel its correct.</p>",2019-09-22T23:37:55Z,60,Week 9/22 - 9/28,followup,,jl3we43d3bp15p,k0vmgy5okt95l,2019-09-22T23:37:55Z,{},project1
1811,stud,"<p>Thanks, Shayan. That&#39;s around what I&#39;m seeing too.</p>
<p></p>
<p>30 seconds is insanely fast, and I just can&#39;t see how to vectorize anymore than I already have. It might just be a really slick implementation. Who knows? </p>",2019-09-22T23:41:08Z,60,Week 9/22 - 9/28,followup,a_0,,k0vml36emud4wg,2019-09-22T23:41:08Z,{},project1
1812,no,"<p>It&#39;s less about vectorizing than taking advantage of the preconditions to reduce the amount of computation. Since gamma = 1 and all rewards are 0 except in one place, you might find ways to simplify your code. If I use the full version my code also needs several minutes to complete.</p>",2019-09-23T20:42:13Z,60,Week 9/22 - 9/28,feedback,,is8ald0uljj3u4,k0wvmul51dc4l8,2019-09-23T20:42:13Z,{},project1
1813,stud,"<p>It sounds like you have a deeper understanding of the algorithm than I do. I didn&#39;t even frame my implementation in these terms. Sutton (so far as I can tell) doesn&#39;t talk about a discount rate at all (or it&#39;s implied that it&#39;s $$1$$, in which case it might as well not be there) and I never quite made the full connection with the different formalism we had in lecture (which seems closer to everything we did before this, but further away from what Sutton is doing). I just did my best to implement the first version of the algorithm he describes (not the iterative one that uses the $$e_{t}$$ notation). I&#39;m sure you&#39;re right, though. Mine certainly doesn&#39;t <em>feel</em> like an especially efficient implementation. </p>",2019-09-23T22:34:57Z,60,Week 9/22 - 9/28,feedback,a_0,,k0wznt8nxhy2jr,2019-09-23T22:34:57Z,{},project1
1814,no,<p>I get around 30 seconds as well (per experiment) so roughly 1.5 min net.</p>,2019-09-23T07:35:12Z,60,Week 9/22 - 9/28,followup,,jcg0nzvdk8272b,k0w3iqgcnfu4g5,2019-09-23T07:35:12Z,{},project1
1815,no,"<p>Do you use the algorithm given in the paper, or some other version (e.g. the one in the Sutton and Barto textbook)?</p>",2019-09-23T13:41:27Z,60,Week 9/22 - 9/28,feedback,,jl3oi5v7qkSk,k0wglqtocjy72n,2019-09-23T13:41:27Z,{},project1
1816,stud,"<p>I <em>think</em> there&#39;s probably a way to further speed things up if you make all of the observation-outcome sequences the same length. The fact that (using what might arguably be the most &#34;natural&#34; way of generating them) they&#39;re typically not is what (I think) stopped me from further vectorizing the updates b/c it seems like you need a matrix of observation-outcome sequences that is not &#34;irregularly shaped&#34; (or maybe I should say just a matrix, since an &#34;irregularly shaped&#34; matrix seems like it&#39;s really just a numpy array of numpy arrays of differing lengths. So, not really a matrix at all.) , i.e., one that has fixed (and known) shape values for each dimension, rows and columns. To get that, I think you&#39;d either have to pick a length for all of your observation outcome sequences, or generate a bunch of them of all different lengths, find the length of the maximum length sequence, and then &#34;pad&#34; the shorter sequences with $$\vec{0}$$s so that you&#39;ve got a rectangular matrix. Otherwise, you&#39;ll hit a point where you can&#39;t further vectorize due (basically, I think) to broadcasting errors. I don&#39;t exactly know how to do this efficiently, and I&#39;m not convinced it&#39;s worth the additional effort if (like me) you find vectorized code a bit unnatural and hard to think about.</p>
<p></p>
<p>&lt;= 15 minutes to generate all the required results seems reasonable to me. That, at least, is in line with the upper limit on runtime in some other OMSCS courses (though I don&#39;t actually see that we&#39;ve been given a limit here).  If it does take 5-15 minutes, you can presumably just indicate that in your README file (so the TA who runs your code knows what to expect/is free to do something else while your figures are being produced). </p>",2019-09-23T14:56:23Z,60,Week 9/22 - 9/28,feedback,a_0,,k0wja3xj8rm4rc,2019-09-23T14:56:23Z,{},project1
1817,no,<p>I was reading a paper by Sutton around 2005 - his footnotes indicate each figure took 44 - 47 hours to generate LOL. Of course that was multiple connections of TD networks. </p>,2019-09-23T16:30:44Z,60,Week 9/22 - 9/28,feedback,,jl3we43d3bp15p,k0wmnfcy3h911x,2019-09-23T16:30:44Z,{},project1
1818,stud,<p>Yikes! It&#39;s nice to be living in the future (sometimes). </p>,2019-09-23T19:07:27Z,60,Week 9/22 - 9/28,feedback,a_0,,k0ws8yowr13126,2019-09-23T19:07:27Z,{},project1
1819,no,"<p>Yeah I was just contemplating this.  The though was... if Sutton heard us being concerned about a 6 minute run time to finish, he would likely shake his head in disbelief with today&#39;s computer scientists.. I suppose we can claim to be victims of Moore&#39;s Law.</p>",2019-09-23T23:46:34Z,60,Week 9/22 - 9/28,feedback,,is9so9huTMp,k0x27xhcndu4nc,2019-09-23T23:46:34Z,{},project1
1820,no,"<p>I have the same alpha, same threshold for convergence and more or less the same values.  </p>
<p>I thought it could come from the training sets, ie from the seed value, but nope.  </p>
<p>I&#39;m scratching my head at the moment.</p>",2019-09-24T01:56:20Z,42,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k0x6ushgsmhqm,2019-09-24T01:56:20Z,{},project1
1821,no,<p>Yeah I&#39;ve seen others with similar results. Having a hard time thinking of why the paper results are different error-wise though.</p>,2019-09-26T01:12:07Z,42,Week 9/22 - 9/28,feedback,,jc7qnuoloyn56p,k1005n0k92x4pg,2019-09-26T01:12:07Z,{},project1
1822,no,"<p>I think all the instructors need to do is reproduce your figures and see your code. If you can do this by just running your python file without any arguments that should be good - just specify exactly that in your git readme. Also, it may be better to save your figures than just show them - not sure. </p>",2019-09-23T16:25:57Z,42,Week 9/22 - 9/28,followup,,jl3we43d3bp15p,k0wmhalo3gc7a2,2019-09-23T16:25:57Z,{},project1
1823,no,"<p>True, convergence isn&#39;t what Sutton was going for in figure 4. Present a set of 10 episodes to your algorithm just once. After each episode, apply your weight updates to the weight vector. Calculate error, do this for the other 99 sets, and average the errors out (I think :) ). </p>",2019-09-23T16:28:15Z,42,Week 9/22 - 9/28,followup,,jl3we43d3bp15p,k0wmk8dgseu554,2019-09-23T16:28:15Z,{},project1
1824,no,"<p>Ah, I thought we were still using the parameters for experiment 1 on figure 4. I updated it and the results are looking much better. Thank you!</p>",2019-09-23T18:47:50Z,42,Week 9/22 - 9/28,feedback,,jl284xdcifz44g,k0wrjqip4hz4ax,2019-09-23T18:47:50Z,{},project1
1825,no,"<p>Zachary, did you ever figure out your issue? I&#39;m getting your issue but I can&#39;t seem to figure out why its opposite...</p>",2019-09-26T01:29:07Z,42,Week 9/22 - 9/28,followup,,jzkke6iz3cl28t,k100rhvbfhb424,2019-09-26T01:29:07Z,{},project1
1826,no,"<p>Yes.  I will be posting something interesting in a day or two, pending review (and commentary) by instructors for correctness. ;)</p>",2019-10-02T08:24:18Z,41,Week 9/29 - 10/5,followup,,jzivtxcbl6964n,k1908jmm6pw1ur,2019-10-02T08:24:18Z,{},project1
1827,no,<p>alright on monday I will send it to instructors. ;) need to reformat it &amp;&amp; correct one observation.</p>,2019-10-06T08:49:10Z,40,Week 10/6 - 10/12,feedback,,jzivtxcbl6964n,k1eqvxomv1y15d,2019-10-06T08:49:10Z,{},project1
1828,no,"<p>My Git gt-omscs-rldm repo is currently marked as private, do TAs have an access to the repo or I need to grant the access to them explicitly?</p>",2019-09-27T00:57:46Z,60,Week 9/22 - 9/28,followup,,jqkxzdmmolGf,k11f316bzr04xx,2019-09-27T00:57:46Z,{},logistics
1829,no,<p>We have access don&#39;t worry.</p>,2019-09-27T19:42:06Z,60,Week 9/22 - 9/28,feedback,,is6lq4mnzu02k5,k12j8y0pppe2ao,2019-09-27T19:42:06Z,{},logistics
1830,no,"<p>Did you record the last study session?</p>
<p>I missed it due to malfunctioning alerts so I&#39;d like to listen to it if it&#39;s available. Thx</p>",2019-09-23T19:35:53Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k0wt9jnbxc45ht,2019-09-23T19:35:53Z,{},logistics
1831,no,<p>&#64;132 for all the recordings links</p>,2019-09-23T19:36:44Z,60,Week 9/22 - 9/28,feedback,,is6lq4mnzu02k5,k0wtan49kmq4gt,2019-09-23T19:36:44Z,{},logistics
1832,no,"<p>Silly attempt to capture the chat log</p>
<p></p>
<blockquote>
<p>10:32 AM Quang Vu can I ask question about Sutton 1988 paper? <br />10:34 AM Quang Vu sure <br />10:35 AM Don Jacob Go head <br />10:35 AM Quang Vu I dont have microphone at the moment, I will have to type it out, sorry <br />10:35 AM Quang Vu at the end of page 14, Sutton make some Math, and I don&#39;t follow his derrive of the last 2 <br />10:36 AM Quang Vu can you walk through the last 2 steps, when he combines (1) and (2)? <br />10:38 AM Quang Vu it seems like there is an typo, but I am not sure <br />10:38 AM Quang Vu :) <br />10:38 AM Quang Vu yea, the rearrangement is confusing <br />10:39 AM Quang Vu that would be nice. Thanks! <br />10:39 AM Quang Vu Can you also work out the math when parital derrivative of Pt become xt? <br />10:45 AM Quang Vu It is on page 13 <br />10:46 AM Quang Vu at the end of page 13 <br />10:48 AM Quang Vu I try to get the general concept, I think Sutton means that concept also applies outside of the random-walk <br />10:48 AM Quang Vu thank you Don and Farruk! <br />10:49 AM Neal Kelly I&#39;ve got one! <br />10:50 AM Neal Kelly I&#39;ve experimented a lot with sequence lengths and stuff, but my errors are much lower than Sutton&#39;s <br />10:50 AM Neal Kelly Is this a bad sign? <br />10:50 AM Neal Kelly Cool <br />10:50 AM Quang Vu my confusion,is that the partial der of Pt is in respect to w change, then should pd of Pt = wt, instead of xt? <br />10:50 AM Binod Purushothaman Yup, my graphs look pretty similar, except the rmse is much lower <br />10:53 AM Quang Vu And I apologize for not prepare to present my question better. I realize how difficult it is to ask you guys the way I am doing now. Thanks for taking time to explain <br />10:53 AM Neal Kelly A question for later, if our valuation function had a form that didn&#39;t depend explicitly on being in a state, (for example if we overparametrized the valuation function and used &#39;distance from A&#39; * a new weight), is there an intuitive way to look at the eligibility trace? <br />10:56 AM Quang Vu Thanks Don and Farruk, so when taking pd of Pt, we hold wt constant? <br />10:57 AM Quang Vu thanks, I will look more into it <br />10:59 AM Neal Kelly Well, I&#39;m considering a non-linear value function I think <br />10:59 AM Neal Kelly Yeah, terminal state, not next action <br />10:59 AM Neal Kelly Sure, but then the gradient won&#39;t be constant I think <br />11:00 AM Andrew Radke In Pt = wTxt (bottom of page 13), is wT and Pt a vector? <br />11:00 AM Neal Kelly Sounds good! I&#39;ll take it there.</p>
</blockquote>",2019-09-28T18:55:49Z,60,Week 9/22 - 9/28,followup,,jl5wq8mca7o0,k13x19l91kn1rq,2019-09-28T18:55:49Z,{},logistics
1833,no,"<p>You use $$\frac{\sigma}{\sqrt{n}}$$ to estimate the deviation of n samples (normally independent), each having a distribution with a deviation $$\sigma$$.</p>
<p>That assumes knowing the distribution of the random variables... I wouldn&#39;t even know where to begin if I had to find the distribution of the weights generated by a TD algorithm.  </p>
<p>Also, the formula applies to the mean of the samples, and here, we have a RMS which is not the same.</p>
<p></p>
<p>But anyway, that formula is used to estimate the deviation of the mean of the samples when you know the distribution of the variable, so it&#39;s not applicable here.  </p>
<p></p>
<p>Here, honestly, I don&#39;t see what else it could be other than the sqrt of the variance of the samples, ie the value returned by np.std.  </p>
<p></p>
<p>I would have like so much to divide by $$\sqrt{100}$$ because, me too, I have values around 0.09.  </p>",2019-09-24T01:49:19Z,42,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k0x6lrqkap7ql,2019-09-24T01:49:19Z,{},project1
1834,no,"<p>&#34;But anyway, that formula is used to estimate the deviation of the mean of the samples when you know the distribution of the variable, so it&#39;s not applicable here. &#34;</p>
<p></p>
<p>I&#39;m pretty sure this is wrong.</p>
<p></p>
<p>We&#39;re sampling from an (unknown) distribution of RMSEs.  Let $$X_i$$ be our iid samples, distributed as $$X$$, the true RMSE distribution.</p>
<p></p>
<p>$$\text{Var}\bigg{[}\frac{\sum\limits_{i=1}^{100}X_i}{100}\bigg{]} = \frac{1}{100^2}\cdot 100\text{Var}[X] = \frac{\text{Var}[X]}{100}$$,</p>
<p></p>
<p>and the standard deviation is just $$\sigma = \sqrt{\frac{Var{X}}{100}} = \frac{\sigma_X}{10}$$</p>
<p></p>
<p>We don&#39;t know the underlying standard deviation of the population, but we can estimate it using the sample standard deviation we obtain from our $$100$$ samples.</p>
<p></p>
<p><strong>Edit</strong>: Found this on Wikipedia:</p>
<p></p>
<p>&#34;Since the <a href=""https://en.wikipedia.org/wiki/Standard_deviation"">population standard deviation</a> is seldom known, the standard error of the mean is usually estimated as the <a href=""https://en.wikipedia.org/wiki/Standard_deviation#Corrected_sample_standard_deviation"">sample standard deviation</a> divided by the square root of the sample size (assuming statistical independence of the values in the sample).&#34;</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Standard_error"">https://en.wikipedia.org/wiki/Standard_error</a></p>",2019-09-24T02:45:20Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0x8ltj2a0t6a,2019-09-24T02:45:20Z,{},project1
1835,no,"<p>Wait, so if I have $$X_i$$ as one calculation of the RMSE, and RMSE~$$ X$$ then wouldn&#39;t the calculation for variance be</p>
<p></p>
<p>$$\sigma_X^2 = Var(X) = \big[ \frac{\sum\limits^{100}_{i=1} (X_i - \overline{X})^2 }{100} \big]$$? </p>",2019-09-24T03:31:33Z,42,Week 9/22 - 9/28,feedback,,jqmfuaidej9155,k0xa9962otp39q,2019-09-24T03:31:33Z,{},project1
1836,no,"<p>We&#39;re finding the variance of the sample mean of the RMSEs we&#39;ve collected.  The sample mean is $$\frac{\sum\limits_{i=1}^{100}X_i}{100}$$, where $$X_i$$ is <em>one</em> sample RMSE.  So we want the variance of that fraction I believe.</p>",2019-09-24T03:44:32Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0xapy4qa7d5c7,2019-09-24T03:44:32Z,{},project1
1837,no,"<p>&#64;vahe how are you sure Sutton really calculated the deviation of the mean rather than simply taking the stdev of the samples? </p>
<p>Which he uses to compare the results with different lambdas.</p>
<p></p>
<p>Besides, the variables he compares here, the RMSE averages, are not independent if they were all generated using a seed, or the same batch of 100 sets as he did, no? </p>
<p>This experiment is actually very repeatable.  Given the test sequences, anyone would always find the same result if he doesn&#39;t change anything else ...</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-24T05:28:59Z,42,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0xeg9zsuxbbw,2019-09-24T05:28:59Z,{},project1
1838,no,"<p>Vahe, I&#39;m looking at this thread and I think we&#39;ve both absolutely blown it when it comes to notation.  </p>
<p></p>
<p>To those coming in afterward, this link should be useful: <a href=""http://onlinestatbook.com/2/sampling_distributions/samp_dist_mean.html"">http://onlinestatbook.com/2/sampling_distributions/samp_dist_mean.html</a></p>
<p></p>
<p>if you read the caption on figure 3, &#34;...For each data point, the standard error is approximately $$\sigma = 0.01$$, so the differences between the Widrow-Hoff procedure and the other procedures are highly significant...&#34;, these &#34;data points&#34; are estimates of the mean (sample means of each grouping of RMSE&#39;s). </p>
<p></p>
<p>This means that we want $$\sigma_{\overline{X}}$$, which Vahe correctly pointed out is</p>
<p></p>
<p>$$\frac{\sigma_X}{\sqrt{N}}$$.</p>
<p></p>
<p>(Note the absence of a line over the subscript in the numerator of that fraction.)</p>
<p></p>
<p>And on further inspection, Vahe had it right, I just didn&#39;t get his notation.  Sorry about the confusion, and thank you, Vahe, for your time!</p>",2019-09-24T05:29:53Z,42,Week 9/22 - 9/28,feedback,,jqmfuaidej9155,k0xehf7054w68b,2019-09-24T05:29:53Z,{},project1
1839,no,"<p>&#64;Tyler, imho it is not correct to conclude that Sutton wanted to or did calculate the stdev of the mean just because he calculated the mean.   </p>
<p>Sometimes, people just take the mean and stdev of samples, which are given by np.mean and np.std or any statistics library.</p>
<p>How can we be sure?</p>
<p>Has anyone reached $$\sigma$$ = 0.01 without dividing by 10?   </p>
<p></p>
<p><br /></p>",2019-09-24T05:44:43Z,42,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0xf0i29j0s7gq,2019-09-24T05:44:43Z,{},project1
1840,no,"<p>&#34;how are you sure Sutton really calculated the deviation of the mean rather than simply taking the stdev of the samples? &#34;</p>
<p></p>
<p>Because that&#39;s the definition of Standard Error</p>
<p></p>
<p>&#34;Besides, the variables he compares here, the RMSE averages, are not independent if they were all generated using a seed, or the same batch of 100 sets as he did, no? &#34;</p>
<p></p>
<p>Hard to understand what you&#39;re saying here, but each RMSE sample comes from a <em>completely different</em> set of $$10$$ episodes.</p>
<p></p>
<p>&#34;This experiment is actually very repeatable.  Given the test sequences, anyone would always find the same result if he doesn&#39;t change anything else&#34;</p>
<p></p>
<p>This has nothing to do with calculating the standard error.</p>",2019-09-24T05:46:41Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0xf313fy4q4td,2019-09-24T05:46:41Z,{},project1
1841,no,"<p>&#64;Jean-Pierre, think of it this way: if you were going to do a hypothesis test on the means he estimated for each class, you would use the distribution of $$\overline{RMSE}$$ to calculate the test statistic (assuming normality, which is valid in the case of sample averages by the law of large numbers, iirc), and the variance/stddev for $$\overline{RMSE}$$ is calculated in Vahe&#39;s post above.  You would not use the distribution of $$RMSE$$.</p>
<p></p>
<p>(Here, $$\overline{RMSE}$$ is the sample mean.)</p>
<p></p>
<p></p>",2019-09-25T01:47:05Z,42,Week 9/22 - 9/28,feedback,,jqmfuaidej9155,k0ylyrmpxsu17x,2019-09-25T01:47:05Z,{},project1
1842,no,"<p>To add to Tyler&#39;s post.  The standard deviation of the RMSE of a training set is what it is, some positive number - you can&#39;t change it by sampling.  The standard error of the average of the RMSEs over all training sets is something you can drive to zero by sampling.</p>",2019-09-25T03:10:48Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0yoyeyqn547pq,2019-09-25T03:10:48Z,{},project1
1843,no,"<p>&#64;vahe, i.e., by sampling RMSE over ever more training sets--if for example we were able to sample an infinite number of times, then the standard error of the estimate becomes zero--$$\lim\limits_{n\rightarrow\infty} \frac{\sigma}{\sqrt{n}} = 0$$.</p>",2019-09-25T03:38:48Z,42,Week 9/22 - 9/28,feedback,,jqmfuaidej9155,k0ypyfm5tnh5jf,2019-09-25T03:38:48Z,{},project1
1844,no,"<p>&#64;Vahe &amp; &#64;Tyler... I understand what you both say, but you both avoid my point which is can we consider the values we get from each training set independent when we use a seed to generate them?</p>
<p></p>
<p>I mean, if you know the seed, not only each training set depends on the previous one (by the fact that the n-th set always comes right after the n-1th set, even if you try a trillion times) and (for someone who knows the random algo) can even be predicted in advance.  </p>
<p>So, in these conditions, how can we consider any training set or anything derived from them independent?  If not, the CLT does not apply.  </p>
<p></p>
<p>If not, your points that &#39;the stderror can be driven to zero etc&#39; (because we divide by $$\sqrt{n}$$ and n can be increased at will) may not be true if it turns out we can&#39;t use that formula at all. </p>
<p>The sufficiency principle, which allows to use an average over samples as a meaningful statistics, does not always apply.  </p>
<p></p>
<p>Even if I&#39;m right, it doesn&#39;t mean that generating 100 sets to compare their RMSE is not a valuable method to evaluate the algorithm anyway.  This is a statistics debate, I&#39;m not forcing anyone into it.  </p>
<p></p>
<p>And we still don&#39;t know for sure what Sutton had in his mind since he says &#39;stderror&#39; and uses the deviation symbol... it&#39;d be interesting to know the best values people have reached.</p>
<p></p>
<div>
<div></div>
</div>",2019-09-25T20:25:36Z,42,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0zpx6lnpfn2d7,2019-09-25T20:25:36Z,{},project1
1845,no,"<p>&#34;<em>but you both avoid my point  which is can we consider the values we get from each training set independent when we use a seed to generate them?&#34;</em></p>
<p><em></em></p>
<p>I didn&#39;t realize that was your point.  I thought it was this:</p>
<p></p>
<p>&#34;But anyway, that formula is used to estimate the deviation of the mean of the samples when you know the distribution of the variable, so it&#39;s not applicable here.&#34;</p>
<p></p>
<p>As for your new point:</p>
<p></p>
<p>&#34;<em>I mean, if you know the seed, not only each training set depends on the previous one (by the fact that the n-th set always comes right after the n-1th set, even if you try a trillion times) and (for someone who knows the random algo) can even be predicted in advance.</em> &#34;</p>
<p></p>
<p>You do realize that the whole point of having multiple training sets is to simulate iid samples from a distribution, right?</p>
<p></p>
<p>As for random number generators not being random, I&#39;m not even going to go there. Here:</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Pseudorandom_number_generator"">https://en.wikipedia.org/wiki/Pseudorandom_number_generator</a></p>",2019-09-25T21:04:59Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0zrbtt445e6q6,2019-09-25T21:04:59Z,{},project1
1846,no,"<p>Maybe you didn&#39;t see one of my latest questions:</p>
<p></p>
<p>&#34;Besides, the variables he compares here, the RMSE averages, are not independent if they were all generated using a seed, or the same batch of 100 sets as he did, no? </p>
<p>This experiment is actually very repeatable.  Given the test sequences, anyone would always find the same result if he doesn&#39;t change anything else ...&#34;</p>
<p></p>
<p>I&#39;m trying to have a conversation about that, not about my very first statement that you went to pick to make me look like I made a new point, or to insinuate I can&#39;t understand we&#39;re trying to simulate random samples or what a pseudo generator is.  </p>
<p>Please try to take into consideration this is my 3rd language, and maybe see where I&#39;m trying to go rather than making things up... </p>
<p></p>
<p>My point is the independence of the samples when we use a seed, which makes the whole sequence predictable, compared to when we don&#39;t and let the random generator decide.  </p>
<p>It would be a better use of our time if you&#39;d rather address the points in my last post, for instance about the applicability of the sufficiency principle, which could make the use of the average not a good decision from a statistical viewpoint, although it could still be valid for the kind of experiment we do.  I remember reading a comment from a TA in 6420 saying that we can&#39;t always apply the sufficiency principle, or that the mean could not always be used (not sure and I can&#39;t find it anymore).  It&#39;s been my point all along, but I&#39;ve been met with affirmative statements &#39;it&#39;s like that, period&#39;.  I&#39;m just trying to learn, and it&#39;s a habit of mine to look at things from as many angles as possible, even silly ones, which usually disorients people.  </p>
<p></p>
<p></p>",2019-09-26T00:40:23Z,42,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k0zz0tr9piv2xt,2019-09-26T00:40:23Z,{},project1
1847,no,"<p>&#34;My point is the independence of the samples when we use a seed, which makes the whole sequence predictable, compared to when we don&#39;t and let the random generator decide.&#34;</p>
<p></p>
<p>If you are using the same seed to generate each sequence, or each training set, then I believe we have found a bug in your code.</p>",2019-09-26T02:24:53Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k102r7zg8w235,2019-09-26T02:24:53Z,{},project1
1848,no,"<p>&#34;Please try to take into consideration this is my 3rd language and maybe see where I&#39;m trying to go rather than making things up... &#34;</p>
<p></p>
<p>When you state things which are false as a fact, which misleads the class, I&#39;m going to call you out on it, if someone else hasn&#39;t done so already.  I hope you do the same when I state things which are false.  There is so much information in my posts addressing specific points you made, but you&#39;re telling me that I&#39;m making stuff up.  Why don&#39;t you try to understand what I wrote? And if I said something that is wrong, tell me what it was.</p>",2019-09-26T02:51:20Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k103p8518wb1yd,2019-09-26T02:51:20Z,{},project1
1849,no,"<p>Well, calling me out is a bit awkward when you made up the fact that I was bringing a new issue, and I called you out by taking a quote from a previous post proving it was not new, so, yeah, you made it up instead of saying &#39;sorry, I missed your point&#39; and politely replied to it.  </p>
<p>This is not facebook, I am trying to learn.  </p>
<p>I didn&#39;t say you said something that was not correct, I just said you had missed my point, and you still do.  </p>
<p>I give up, it&#39;s not worth it.  I received a notification by email that someone even said &#39;Math is your third language?&#39; to mock me instead of respecting the fact I&#39;m doing my best to express myself in a 3rd language.  </p>
<p>I truly was not expecting this kind of treatment here, but now I know.  I will adjust.  </p>",2019-09-27T04:32:08Z,42,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k11mqq3a4e31ec,2019-09-27T04:32:08Z,{},project1
1850,no,"<p>Jean,</p>
<p></p>
<p>I have addressed every one of your points, at least the ones that are decipherable, across multiple threads and multiple classes.  You have no appreciation or consideration for the amount of time other people have to put into refuting statements that make no sense while backing up their own statements which you attack with little logic.  If you really cared about learning, you would put more effort into actually reading and trying to understand what other people are saying.</p>",2019-09-27T17:54:32Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k12felp0ky07id,2019-09-27T17:54:32Z,{},project1
1851,no,<p>I got the same results. My RMSE average for 100 training set is 0.14 - 0.2 range. I used &#96;np.std()&#96; to calculate the error this the 100 training set result. The error is around 0.07 - 0.08. I wonder if this is due to my stop threshold in convergence. I used 0.0001 as stopping threshold. </p>,2019-09-26T20:16:18Z,42,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k11512ww5nt76b,2019-09-26T20:16:18Z,{},project1
1852,no,"<p>Only parallel I can see here is in understanding and be able to expand Equation 4 in the paper as you did in homework 2. </p>
<p></p>
<p>Once you start doing it, you will see it for yourself. Project builds up on the understanding on TD(lambda) that we got introduced to in homework 2.</p>
<p></p>
<p>Rest will be all just coding the experiments and creating figures same as in the paper and writing your report. Good luck !</p>
<p></p>
<p></p>",2019-09-24T04:51:51Z,35,Week 9/22 - 9/28,followup,,jfzaqnqvtQ1m,k0xd4ipk8kqyd,2019-09-24T04:51:51Z,{},hw2
1853,no,"<p>wild divergence, due to extreme hyperparameter sensitivity, and delayed feedback, does not lend itself to &#34;exploratory&#34; analysis.</p>
<p></p>
<p>that is: having the correct equation, does not gaurantee good results.</p>
<p></p>
<p></p>",2019-09-24T06:37:37Z,35,Week 9/22 - 9/28,feedback,,jzivtxcbl6964n,k0xgwiwzkfr1j7,2019-09-24T06:37:37Z,{},hw2
1854,no,"<p>&#34;and delayed feedback&#34;</p>
<p></p>
<p>Yeah, it can be a huge time sink... I feel like I need to develop better software engineering habits to counter this, like minimizing the delay when testing.</p>",2019-09-24T07:25:41Z,35,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0ximch6vdzcc,2019-09-24T07:25:41Z,{},hw2
1855,no,"<p>hello vahe.  I have read your comments with interest ;) if you are on the slack for the class, let me know. ;)</p>
<p></p>
<p>indeed I coded a &#34;max_ws&#34; specifically to avoid large amounts of iteration (in addition to the epsilon check).   came in handy in cases of divergence. </p>
<p></p>
<p>I am not quite sure if I nailed his algorithm &#34;correctly&#34; in that there is almost no mention of divergence or dynamic conditions in his paper, with respect to the algorithm (other than a cryptic &#34;small enough&#34; which leaves room for interpretation).  but my graphs could be cherrypicked (fig3) to match fairly close; albeit with less error.  </p>
<p></p>
<p>I did notice something, quite interesting, with respect to bias vs variance.</p>",2019-10-06T08:54:07Z,33,Week 10/6 - 10/12,feedback,,jzivtxcbl6964n,k1er2aapl5r5ug,2019-10-06T08:54:07Z,{},hw2
1856,no,"<p>I&#39;m not on slack - barely have the time to monitor this forum as it is.  I didn&#39;t have any issue with the &#34;small enough&#34;, since $$\alpha$$ is irrelevant, after the fact, when running to convergence, but I did have an issue related to this: &#34;almost no mention of divergence or dynamic conditions in his paper&#34; but with respect to Figure 4, and then a different issue with his Figure 3 results.  But I&#39;m waiting to see the TAs&#39; take on what I said.  Maybe I made some fundamental mistakes in my analysis...</p>",2019-10-06T13:36:26Z,33,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1f15d1rswo7g8,2019-10-06T13:36:26Z,{},hw2
1857,no,"<p>re:slack  yes ok :)</p>
<p></p>
<p>I did not find it to be irrelevant.  But it is possible I have a systemic error in how I am coding up these equations.&#43;&#43;   Too large and I get a ringing and subsequent divergence with respect to predictions.  This to my mind is the purpose of the &#34;alpha&#34; parameter (to mute the error signal to bring it back into stable regions).   Hence &#34;sufficiently small&#34; comment by Sutton in his paper.  In conversation with one of the students they mentioned that an &#34;adjustment&#34; factor is in fact standard in this branch of mathematics (so it might be a known &#34;correction&#34; similar to &#43; c in calculus).  I am also aware, that others did not have this type of behavior (and had stable alpha&#39;s).</p>
<p></p>
<p>&gt; But I&#39;m waiting to see the TAs&#39; take on what I said.  Maybe I made some fundamental mistakes in my analysis...</p>
<p></p>
<p>LOL.  Yes.  the hazards of implementing an unknown algorithm, is that maybe the results are invalid, due to error in assumptions. :(  Still a good conversation, is its own reward no? ;)</p>
<p></p>
<p>&#43;&#43; I did not for instance, in HM3 get &#34;exact&#34; matches; but I am unsure if it is because I differed in the # of calls to rand; or if I used a wrong version of the api (for establishing the environment).  But it was close enough, to do well on the homework i.e. there isn&#39;t anything &#34;obviously wrong&#34; so far as I can see with the programming side of it, re: sarsa.</p>",2019-10-07T13:00:00Z,33,Week 10/6 - 10/12,feedback,,jzivtxcbl6964n,k1gfacoacgb721,2019-10-07T13:00:00Z,{},hw2
1858,no,"<p>I&#39;m using equation 4 exactly instead of the incremental form on pg16, and am seeing the same behavior as you. In fact my RMSE is much lower compared to the paper (0.015 ~ 0.050 for me).</p>
<p></p>
<p>I can think of two reasons for this.</p>
<ol><li>The range of alpha values used. In my implementation at least, the algorithm is high sensitive to alpha, especially for lower values of lambdas. </li><li>Convergence measure and threshold. Since the paper did not provide how convergence is determined, I had to make some assumptions. Similar to alphas, I found that different measures of convergences lead to somewhat different results. </li></ol>",2019-09-24T11:34:22Z,35,Week 9/22 - 9/28,followup,,ijctp4ucNy8,k0xri5jvejn6xo,2019-09-24T11:34:22Z,{},project1
1859,no,<p>I found the issue is that I was simply taking the sum of the updates vector instead of the L-1 or L-infinity norm when comparing to epsilon. Hopefully this is helpful if anyone else encounters it.</p>,2019-09-24T22:54:36Z,35,Week 9/22 - 9/28,feedback,,jnjgrn6usm9x,k0yfsy8pcer1z9,2019-09-24T22:54:36Z,{},project1
1860,no,"<p>Michael, out of curiosity, what did you do to calculate the L-1 or L-infinity norm? I&#39;m having problems with convergence on Lambda = 0 for Figure 3... Can&#39;t quite get my epsilon value to stop the loop!</p>",2019-09-25T10:44:27Z,35,Week 9/22 - 9/28,feedback,,jl2bqhkbsux4r,k0z55t23hme37e,2019-09-25T10:44:27Z,{},project1
1861,no,"<p>You could also put in other stopping criteria (say x iterations, etc)...I did this, but seem to always converge before the limit is met.</p>",2019-09-28T03:14:10Z,35,Week 9/22 - 9/28,feedback,,hz3hplpvI9c,k12zeb9ox7521h,2019-09-28T03:14:10Z,{},project1
1862,no,<p>Can someone point me to the  L-1 or L-infinity norm epsilon equations in our lecture or reading material?</p>,2019-09-28T23:21:17Z,35,Week 9/22 - 9/28,followup,,gx3c8l7z7r72zl,k146io7hboi72j,2019-09-28T23:21:17Z,{},project1
1863,no,"<p>Scroll down to &#39;p-norm&#39; here:</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Norm_%28mathematics%29"">https://en.wikipedia.org/wiki/Norm_(mathematics)</a></p>",2019-09-28T23:34:24Z,35,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k146zjjyct97bh,2019-09-28T23:34:24Z,{},project1
1864,no,"<p>All of the graphs that I have posted use the infinity norm. As anonymous states, your method works, but you might also find some more interesting results if you play with the infinity norm and use different epsilons for convergence.</p>
<p></p>
<p>One thing that Sutton claims (without substantiation) is that there is a fixed point in the convergence.</p>",2019-09-24T14:07:15Z,35,Week 9/22 - 9/28,followup,,jc554vxmyuy3pt,k0xwyroco9c5c0,2019-09-24T14:07:15Z,{},project1
1865,stud,"<p><em>&#34;One thing that Sutton claims (without substantiation) is that there is a fixed point in the convergence.&#34;</em></p>
<p><em></em></p>
<p>But that&#39;s obviously true, isn&#39;t it? The update procedure is iterative, basically the equivalent of applying the same function to an initial weights vector over and over again. If we let $$f(w_{\tau}) = w_{\tau} &#43; \Delta w_{\tau}$$ represent the result of applying the update at the $$\tau^{th}$$ iteration to the weights vector at the $$\tau^{th}$$ iteration, then we have it that $$w_{\tau &#43; 1} = f(w_{\tau}) = w_{\tau} &#43; \Delta w_{\tau}$$. </p>
<p></p>
<p>Our convergence criteria (when using the infinity norm) then implies that, <em>if </em>we converge, then we need to see $$\lim_{\tau \rightarrow \infty} w_{\tau &#43; 1}\ - w_{\tau} = 0$$. That means that $$\lim_{\tau \rightarrow \infty} w_{\tau &#43; 1} = \lim_{\tau \rightarrow \infty}w_{\tau}$$. And if <em>that&#39;s</em> true, then in the limit as $$n \rightarrow \infty$$, it needs to be the case that $$ w_{\tau &#43; 1} = f^{n}(w_{\tau}) = w_{\tau} &#43; \Delta w_{\tau} = w_{\tau}$$, where $$f^{n}$$ is the <i>$$n^{th}$$ iterate of $$f$$</i> (i.e., the result of applying $$f$$ to the initial weights vector, taking the updated weight vector that results from <em>that</em> and applying $$f$$ to <em>it</em>, taking the updated weight vector that results from <em>that</em> and applying $$f$$ to <em>it</em>, and so on, $$n$$ times). That implies that $$\Delta w_{\tau} \rightarrow 0$$ as $$n \rightarrow \infty$$. And so the <em>limit vector $$w_{\infty} = \lim_{\tau \rightarrow \infty} w_{\tau}$$ must be a fixed point of the update function, $$f$$ (i.e., </em>$$f(w_{\infty}) = w_{\infty}$$).</p>
<p></p>
<p>I <em> think</em> this is just a general property of dynamical systems of the sort Sutton had in mind when writing the paper. Namely, that <em>if</em> a procedure converges, it must converge to a <em>fixed</em> <em>point</em> of errr...itself, I guess? What <em>else</em> could it converge to?</p>",2019-09-24T14:40:01Z,35,Week 9/22 - 9/28,feedback,a_0,,k0xy4wjm2lx6ll,2019-09-24T14:40:01Z,{},project1
1866,stud,"<p>Btw, I suspect that something even stronger is probably true. Namely, that if the procedure converges to a fixed point, then that fixed point is not <em>only</em> a fixed point of the update procedure, but is actually <em>an attractor</em> for the update procedure (i.e., a fixed point with a <em>nonempty basin of attraction.</em>) </p>
<p></p>
<p>Think about it. If that&#39;s <em>not </em>the case, then every vector near the fixed point is going to get mapped <em>farther</em> away by sucessively applying the update procedure. If that&#39;s <em>the only</em> fixed point for the procedure on a particular training set, then (it seems) we&#39;ll <em>never</em> converge.</p>
<p></p>
<p>Never ever ever!</p>",2019-09-24T14:45:41Z,35,Week 9/22 - 9/28,feedback,a_0,,k0xyc6sxj4jcx,2019-09-24T14:45:41Z,{},project1
1867,no,"<p>Hey, you&#39;re totally giving away my &#34;eureka&#34; moment in my paper!!</p>
<p></p>
<p>Of course it&#39;s obvious, and considerably more profound, but in any science publication you must always provide evidence of your assertions and assumptions. He should have footnoted it or else added a short 3 paragraph reminder about the convergence of the infinity norm. The fact that he did not disclose how he was deliberating the convergence meant that his claim was unsubstantiated.</p>
<p></p>
<p>I spent days of iterating to realize by demonstration that there is an attractor for this problem. It&#39;s clever and frustrating at the same time. I had one iteration that was about 12 hours and it just ended up in the same place as another one that only took 45 minutes.</p>",2019-09-24T20:20:34Z,35,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0yaauonmh92yk,2019-09-24T20:20:34Z,{},project1
1868,stud,"<p><em> &#34;Hey, you&#39;re totally giving away my &#34;eureka&#34; moment in my paper!!&#34;</em></p>
<p><em></em></p>
<p>Sorry, that&#39;s my bad : (</p>
<p></p>
<p><em>&#34;Of course it&#39;s obvious, and considerably more profound, but in any science publication you must always provide evidence of your assertions and assumptions. He should have footnoted it or else added a short 3 paragraph reminder about the convergence of the infinity norm. The fact that he did not disclose how he was deliberating the convergence meant that his claim was unsubstantiated.&#34;</em></p>
<p><em></em></p>
<p>I don&#39;t necessarily disagree. Only, I&#39;d be careful about saying this sort of thing publicly on here. Some of the folks here are evidently part of the &#34;Rich Sutton International Fan Club&#34; and take it <em>real</em> personally when you suggest his work might be anything short of perfect. I&#39;ve run afoul of these characters myself, so be careful! </p>
<p><em></em></p>",2019-09-24T20:38:03Z,35,Week 9/22 - 9/28,feedback,a_0,,k0yaxchmy8a6gr,2019-09-24T20:38:03Z,{},project1
1869,no,"<p>Yeah, I saw that thread. Nobody needs to defend Richard Sutton. His lifetime body of work and contribution stands on its own. All of the soggy old scientists that I&#39;ve known enjoy a good fluff and fold on their work. Good science is all about a strong foundation that supports your suppositions. Rich knows that better than most of us.</p>
<p></p>
<p></p>",2019-09-24T21:08:44Z,35,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0yc0sey3fc51x,2019-09-24T21:08:44Z,{},project1
1870,stud,"<p><em>&#34;Yeah, I saw that thread. Nobody needs to defend Richard Sutton. His lifetime body of work and contribution stands on its own.&#34;</em></p>
<p><em></em></p>
<p>I&#39;m sayin&#39;!</p>",2019-09-24T21:47:27Z,35,Week 9/22 - 9/28,feedback,a_0,,k0ydel6rjgc7j5,2019-09-24T21:47:27Z,{},project1
1871,no,"<p>anonymous ;)  if u&#39;re up for a real-time chat post monday, let me know.</p>",2019-10-06T08:55:55Z,33,Week 10/6 - 10/12,feedback,,jzivtxcbl6964n,k1er4mblmjj6pe,2019-10-06T08:55:55Z,{},project1
1872,stud,"<p><em>&#34;anonymous ;)  if u&#39;re up for a real-time chat post monday, let me know.&#34;</em></p>
<p><em></em></p>
<p>Mayhaps. What would the purpose be? </p>",2019-10-06T11:50:15Z,33,Week 10/6 - 10/12,feedback,a_0,,k1excsw8w115f4,2019-10-06T11:50:15Z,{},project1
1873,no,"<p>&#34;I&#39;m confused because I couldn&#39;t imagine Sutton including this graph without including a table of the best alpha values used in the graph,&#34;</p>
<p></p>
<p>But he&#39;s already provided that in enough detail to give us the general idea - see Figure 4.</p>",2019-09-24T15:16:30Z,60,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k0xzftpvrz56at,2019-09-24T15:16:30Z,{},project1
1874,no,"<p>I also have a confusion about this.</p>
<p></p>
<p>For each lambda and for each alpha combination, we need pick up alpha that has lowest average RMSE for the training sets.</p>
<p></p>
<p>Is that the correct understanding ?</p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>",2019-09-26T00:12:37Z,60,Week 9/22 - 9/28,followup,,jfzaqnqvtQ1m,k0zy14ugtng2wo,2019-09-26T00:12:37Z,{},project1
1875,no,"<p>Yes. That is correct. You are tracing the minimum along the 3d space that defines the error space of the problem. See the 3D images I posted, you will see a wall running along the curve that is figure 5. Its hard to see because of scaling, but it&#39;s blue with a orange outline. &#64;307</p>
<p></p>
<p></p>",2019-09-26T12:26:11Z,60,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k10o8i8nctq70p,2019-09-26T12:26:11Z,{},project1
1876,no,"<p>That&#39;s a very interesting take, I am curious if you have been able to get that to work in practice. </p>
<p></p>
<p>I am asking because I am having trouble understanding how alpha affects Experiment 1. Anything lower than alpha = 0.5 makes the graph completely lose its shape and if alpha &gt; 0.015 I get the following runtime warnings:</p>
<p>P1.py:80: RuntimeWarning: overflow encountered in add</p>
<p>total_delta_wt = total_delta_wt &#43; delta_wt</p>
<p>P1.py:54: RuntimeWarning: invalid value encountered in multiply</p>
<p>  Pt = np.sum(database[i][j][k] * w)</p>
<p>P1.py:59: RuntimeWarning: invalid value encountered in multiply</p>
<p>  Pt_plus1 = np.sum(database[i][j][k&#43;1] * w)</p>
<p>P1.py:85: RuntimeWarning: invalid value encountered in less</p>
<p>  if np.all(abs(total_delta_wt) &lt; convergence_val):</p>
<p> </p>
<p>So does anyone know why alpha = 0.01 seems to be the magic number? Are there other alpha values that work for others in Experiment 1?</p>
<p></p>",2019-09-24T21:17:19Z,42,Week 9/22 - 9/28,followup,,jqmjg3txqw6ji,k0ycbtwbxtf530,2019-09-24T21:17:19Z,{},project1
1877,stud,"<p>Well, for starters, it can&#39;t be the case that the condition from lecture is necessary for convergence in general if you&#39;re seeing convergence with a fixed value of $$\alpha = 0.01$$. I mean, $$\sum_{1}^{\infty}0.01^{2}$$ is clearly divergent.</p>
<p></p>
<p>Have you tried varying $$\alpha$$ <em>jointly</em> with $$\epsilon &gt; 0$$ (i.e., your threshold for convergence)? Intuitively, smaller (constant) values of $$\alpha$$ should &#34;pair with&#34; smaller values of $$\epsilon$$. If your $$\alpha$$ value goes down (it seems to me, at least) that your $$\epsilon$$ value should likewise go down. You&#39;re taking smaller steps with a lower learning rate, and if $$\epsilon$$ is <em>too</em> <em>big</em> for a given $$\alpha$$, those changes might not even register and/or you might converge too soon. That could conceivably &#34;flatten out&#34; the curve here. Consider, for example, the extreme case when $$\alpha &lt;&lt; \epsilon$$. What would you expect to see happen then? </p>",2019-09-24T21:31:17Z,42,Week 9/22 - 9/28,feedback,a_0,,k0yctsr7hn3i6,2019-09-24T21:31:17Z,{},project1
1878,stud,"<p><em>&#34;Anything lower than alpha = 0.5 makes the graph completely lose its shape and if alpha &gt; 0.015 I get the following runtime warnings...&#34;</em></p>
<p><em></em></p>
<p>Wait, I&#39;m confused. Are you getting results like Sutton&#39;s with $$\alpha = 0.01$$ or not? I do get results like Sutton&#39;s with $$\alpha = 0.01$$. </p>",2019-09-24T21:36:10Z,42,Week 9/22 - 9/28,feedback,a_0,,k0yd02rk8k56kv,2019-09-24T21:36:10Z,{},project1
1879,no,"<p>This makes sense. Like you said, if α&lt;&lt;ϵ then the changes may not even register before convergence. </p>
<p></p>
<p>I am getting values like Sutton&#39;s with α=0.01, but I was curious about changing the α to see what would happen. I was surprised to find that there does not seem to be whole lot of wiggle room on α values that give you results like Sutton. </p>",2019-09-24T21:38:21Z,42,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0yd2vwgf0e6ys,2019-09-24T21:38:21Z,{},project1
1880,stud,"<p>I feel that. Especially since Sutton says in the paper something to the effect that as long as $$\alpha$$ is &#34;sufficiently small&#34;, his results follow. To me, that read like there&#39;s supposed to be some threshold value for $$\alpha$$ for which it&#39;s the case that, as long as we&#39;re below that threshold value, his results will pop out. I haven&#39;t found that to be the case, either. Like yourself, I&#39;ve found a rather small range of $$\alpha$$ values work (with $$\alpha = 0.01$$ seeming to work best). </p>
<p></p>
<p>As far as why that might be the case, maybe look at the thread on convergence from this AM that Jacob contributed to? If this process is converging, when it does converge, to an attracting fixed point, then (intuitively) you want values of $$\alpha$$ that ensure that you start out in the basin of attraction for said fixed point. $$\alpha$$ values that are too big <em>or</em> too small might put you outside of that basin. That&#39;s more or less what I&#39;m thinking, as of right now. </p>",2019-09-24T21:46:25Z,42,Week 9/22 - 9/28,feedback,a_0,,k0ydd9fe3004mq,2019-09-24T21:46:25Z,{},project1
1881,no,<p>I&#39;ll check that out. Thanks!</p>,2019-09-24T21:49:31Z,42,Week 9/22 - 9/28,feedback,,jqmjg3txqw6ji,k0ydh8tf3he4no,2019-09-24T21:49:31Z,{},project1
1882,no,"<p>&#64;Alonso The runtime warnings are due to overflows when doing arithmetic.  Lowering $$\alpha$$ will help there.</p>
<p></p>
<p>I think that Anonymous brings up a good point with the interplay of $$\alpha$$ and $$\epsilon.$$  There <em>should</em> only be one fixed point that the algorithm converges to, but if your $$\epsilon$$ is huge and $$\alpha$$ is tiny, then the (small) weight change after one loop of the training set might be misconstrued as convergence.</p>
<p></p>",2019-09-24T21:50:10Z,42,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0ydi2p14jj5kr,2019-09-24T21:50:10Z,{},project1
1883,no,"<p>vahe - this is in effect, the nature of the problem with an alpha &#34;manually adjusting&#34; rewards/utility propogation and claiming that the algorithm works. </p>
<p></p>
<p>it is like saying I can decode an encrypted data stream for sufficiently correct values of the key ;)</p>
<p></p>
<p>reminds me of the issues with variance vs means-adjusted variance.  </p>
<p></p>
<p>I thought the point of the linearized nabla was to address this issue.  only to find out, that it tragically fails and is more akin to an independence statement than a true measure of the scaling required at any particular point in the algorithm.</p>
<p></p>",2019-09-25T00:49:12Z,42,Week 9/22 - 9/28,feedback,,jzivtxcbl6964n,k0yjwbox4n52ai,2019-09-25T00:49:12Z,{},project1
1884,no,"<p>Unless I misunderstand the 2nd experiment, shouldn&#39;t the weight update for Experiment 2 / Figure 4 be after the end of every sequence, not after every training set?</p>",2019-09-25T17:52:41Z,35,Week 9/22 - 9/28,followup,,is4nx55dinr6wk,k0zkgj7ub921oy,2019-09-25T17:52:41Z,{},project1
1885,no,"<p>Yes, that&#39;s correct. I&#39;ll fix my answer.</p>",2019-09-25T17:54:29Z,35,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k0zkiu9c8tu5si,2019-09-25T17:54:29Z,{},project1
1886,no,<p>Solid. I&#39;m getting issues with my graph and was like wait...maybe thats why</p>,2019-09-25T18:10:43Z,35,Week 9/22 - 9/28,feedback,,is4nx55dinr6wk,k0zl3q0nff042w,2019-09-25T18:10:43Z,{},project1
1887,no,"<p>I&#39;m trying to wrap my head around this...</p>
<p></p>
<p>1. We aren&#39;t actually shaping the rewards in temporal difference learning, right?  We&#39;re accepting the rewards, as is, from the MDP.</p>
<p></p>
<p>2. $$\phi$$ looks to be a (stationary) function from the set of states to a real number.  But $$e_t$$ is a non-stationary vector of real numbers.</p>
<p></p>
<p>3. The potential shaping function $$F$$ is added to the nominal MDP reward $$R$$, but $$-\bigtriangledown_wP_{t&#43;1}$$ isn&#39;t added to anything.</p>
<p></p>
<p>I&#39;m trying to decide if this is just a coincidence with the form of the two equations or if there is some kind of connection there.</p>",2019-09-25T08:01:47Z,42,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k0yzcma74jt6ju,2019-09-25T08:01:47Z,{},other
1888,no,"<p>&#34;It&#39;s tricky because in the early stage of exploration, you could easily attach to a not very good state and just keep visiting it until next &#34;important&#34; state comes up. You might want to activate this mechanism later in the process after a bit of exploration.&#34;</p>
<p></p>
<p>This is interesting.  It seems like if this is the case we&#39;re better off trying to find the root cause of why we&#39;re visiting the state more often, if possible.  Going to a state more frequently just because we&#39;ve</p>
<p></p>
<p>a) gone to that state frequently in the past, and</p>
<p></p>
<p>b) have a sense that we&#39;re closer to converging</p>
<p></p>
<p>doesn&#39;t seem to add much value: the agent already values that state highly.  It seems like unless we have some a priori reason to believe that that state is still undervalued (some domain-specific information perhaps), then adding <i>additional</i> positive feedback is like throwing darts.</p>",2019-09-26T00:30:03Z,42,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k0zynjumbu35ye,2019-09-26T00:30:03Z,{},other
1889,no,"<p>&#64;Vahe</p>
<p>What do you mean &#34;lambda = 0.5 in experiment 1&#34;? I read experiment one as we change lambda over the Severn different values of lambda?</p>
<p></p>
<p>Also, when we are supposed to take the RMS of the difference between ideal and measured weights, the weights are a vector and the RMS would produce a vector and our graph would have five plots on it. How do we resolve this? Do we just choose one?</p>",2019-09-25T13:56:45Z,43,Week 9/22 - 9/28,followup,,gx3c8l7z7r72zl,k0zc14cv9u22p0,2019-09-25T13:56:45Z,{},project1
1890,no,"<p>&#34;<em>What do you mean &#34;lambda = 0.5 in experiment 1&#34;? I read experiment one as we change lambda over the Severn different values of lambda?&#34;</em></p>
<p><em></em></p>
<p>Right, $$\lambda$$ is the parameter we&#39;re varying in experiment $$1$$, so $$\lambda=0.5$$ is one value of that parameter, one learning procedure as Sutton puts it, that is being run on the $$100$$ training sets.</p>
<p></p>
<p>RMS stands for root-mean-squared.  What are we taking the &#34;mean&#34; of, in the process of finding the RMS error??</p>",2019-09-25T14:40:37Z,43,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0zdlj62o154ve,2019-09-25T14:40:37Z,{},project1
1891,no,"<p>Correct me if I&#39;m wrong. We use the same weight from one training set and train it on the second training set. Once the weight as converge, we use the same training set on the next training set... until 100 training set right?</p>",2019-09-25T20:06:20Z,43,Week 9/22 - 9/28,followup,,k0bpd3l41jf63x,k0zp8efk5xs31h,2019-09-25T20:06:20Z,{},project1
1892,no,"<p>Take a look at the procedure outlined on page 20 of Sutton&#39;s paper, and/or the caption of his Figure 3 in the erratum.</p>
<p></p>
<p>The different training sets are there to provide distinct samples of the quantity we&#39;re interested in (RMS error as a function of $$\lambda$$).</p>
<p></p>
<p>Having said that, does it actually make a difference if you carry the weight over from one training set to another?  The weight from the previous run will just be the initialization of the weight for the next run, which presumably goes to convergence...</p>",2019-09-25T20:52:38Z,43,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0zqvy9ojsx6ai,2019-09-25T20:52:38Z,{},project1
1893,no,<p>I see! Thanks again Vahe!</p>,2019-09-25T22:26:32Z,43,Week 9/22 - 9/28,feedback,,k0bpd3l41jf63x,k0zu8phfwxy6qc,2019-09-25T22:26:32Z,{},project1
1894,no,<p>No problem :)</p>,2019-09-25T22:33:53Z,43,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0zui5e8w3d54r,2019-09-25T22:33:53Z,{},project1
1895,no,"<p>I think my <strong>key </strong>question is in the <strong>repeated representation until convergence,</strong> it is done on the <strong>same training set. </strong>The purpose of 100 different training set is just to collect standard error and test the algorithm on different settings. Is that correct?</p>",2019-09-26T01:05:57Z,43,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k0zzxpk0rzb7pc,2019-09-26T01:05:57Z,{},project1
1896,no,"<p>&#34;The purpose of 100 different training set is just to collect standard error.&#34;</p>
<p></p>
<p>Full stop.</p>",2019-09-26T02:15:36Z,43,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k102faeua7j6zh,2019-09-26T02:15:36Z,{},project1
1897,no,<p>I see. Since the weight doesn&#39;t changes per sequence. We technically know what all the Ps are. Am i understanding it correctly?</p>,2019-09-25T06:51:12Z,35,Week 9/22 - 9/28,followup,,k0bpd3l41jf63x,k0ywtv40edczw,2019-09-25T06:51:12Z,{},project1
1898,no,"<p>Yes, we do know what all the Ps are, but even if the weight changed per sequence, it wouldn&#39;t matter.  You could make the substitution $$t&#43;1=u$$ and think of $$u$$ as the current time and you wouldn&#39;t be &#34;looking ahead into the future.&#34;</p>",2019-09-25T06:51:47Z,35,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k0ywum09l0235s,2019-09-25T06:51:47Z,{},project1
1899,no,<p>Thanks Vahe!</p>,2019-09-25T06:56:44Z,35,Week 9/22 - 9/28,feedback,,k0bpd3l41jf63x,k0yx0z6qtzg6xz,2019-09-25T06:56:44Z,{},project1
1900,no,"<p>I&#39;m having the same issue with my figures.  I&#39;m not limiting the size of the walks, however I did implement a max=1, min=0 constraint on predictions to force stability.  If I take that out, I end up with rms errors near infinity for lambda=1.  Maybe that has something to do with my issues, maybe not but my figures are identical to yours.  One other thing I was thinking - although the shape of the curve looks strange, it is still reasonably close to Sutton&#39;s results.  Maybe explainable by rounding errors?  </p>",2019-09-25T18:01:01Z,60,Week 9/22 - 9/28,followup,,jl2d544u1wcE,k0zkr8o84mn14i,2019-09-25T18:01:01Z,{},project1
1901,no,"<p>If you&#39;re still getting this issue, check out my updated answer in case you&#39;re doing the same thing :)</p>",2019-09-26T13:45:14Z,60,Week 9/22 - 9/28,feedback,,jl3oi5v7qkSk,k10r260di8i1oa,2019-09-26T13:45:14Z,{},project1
1902,no,<p>Thanks.  That doesn&#39;t appear to be what I&#39;m doing but was worth looking into.  I have an error of (1 - Prediction) for winning sequences and (0-Prediction) for losing sequences.  Seems like that is right unless I&#39;m missing something obvious.  I&#39;ll keep digging.</p>,2019-09-28T21:41:33Z,60,Week 9/22 - 9/28,feedback,,jl2d544u1wcE,k142yef9ewv6au,2019-09-28T21:41:33Z,{},project1
1903,no,"<p>If setting a $$max=1, min=0$$ constraint changes the value of the predictions by a lot compared to what they would have been, is it possible you&#39;re introducing a lot of distortion into your computations, that may lead to reasonable graphs but may be masking the real issue?  In other words, maybe whatever is causing your predictions to be so huge or tiny in the first place is your problem.</p>",2019-09-28T23:09:58Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k14643t2dcm2eo,2019-09-28T23:09:58Z,{},project1
1904,no,"<p>Yea, thats about where Ive been playing. Just seems odd to me that the sequence itself has such a high impact on the graphs. I wouldve thought the graph structure wouldve been more consistent.</p>",2019-09-26T12:20:51Z,60,Week 9/22 - 9/28,followup,,is4nx55dinr6wk,k10o1muap1p3l2,2019-09-26T12:20:51Z,{},project1
1905,no,<p>happened to me few times. i played with random seed value</p>,2019-09-29T14:50:35Z,59,Week 9/29 - 10/5,feedback,,jqmffxz1qtp6yy,k153pqvcck14q7,2019-09-29T14:50:35Z,{},project1
1906,no,<p>Shouldn&#39;t our paper mention what Sutton did in his paper and do comparisons between his results and ours? Is that still stand alone?</p>,2019-09-25T22:56:03Z,60,Week 9/22 - 9/28,followup,,jzj4205g7gd2fw,k0zvao1jnrr5uz,2019-09-25T22:56:03Z,{},project1
1907,no,"<p>You should compare the results, discuss the experiments, discuss the environment, etc.  We provided some pointers/talking points in the the assignment.</p>",2019-09-25T23:32:47Z,60,Week 9/22 - 9/28,feedback,,hz7meu55mi8sd,k0zwlwdeu2m5kg,2019-09-25T23:32:47Z,{},project1
1908,no,<p>Thanks! just making sure my understanding was correct.</p>,2019-09-25T23:34:33Z,60,Week 9/22 - 9/28,feedback,,jzj4205g7gd2fw,k0zwo5zfxeasr,2019-09-25T23:34:33Z,{},project1
1909,no,<p>Also make sure to check &#64;377</p>,2019-09-27T19:06:38Z,60,Week 9/22 - 9/28,followup,,is6lq4mnzu02k5,k12hzc848ey4m6,2019-09-27T19:06:38Z,{},project1
1910,no,Thanks Takahisa. Good advice.<div><br /><div>I already started on getting fig 4 better. Hopefully I should get fig 5 fixed soon.</div></div>,2019-09-27T19:06:56Z,60,Week 9/22 - 9/28,followup,,jfzaqnqvtQ1m,k12hzpqy4hp1lb,2019-09-27T19:06:56Z,{},project1
1911,no,<p>should be more stable.</p>,2019-10-02T17:15:22Z,59,Week 9/29 - 10/5,followup,,jzivtxcbl6964n,k19j7i6xvck3ez,2019-10-02T17:15:22Z,{},project1
1912,no,"<p>thanks a lot for this, I took speed reading classes a long time ago, but it was for books, so this is definitely interesting.</p>
<p>Did you notice an improvement yet?</p>
<p></p>
<p>Btw, for those who spend a lot of time on a computer, the &#39;blue light&#39; coming from the LED&#39;s are detrimental to the eyes.</p>
<p>This application is really good at fixing that problem: <a href=""https://iristech.co/"">https://iristech.co/</a>, which will not only protect the eyes, but allow a better sleep.  </p>",2019-09-27T05:27:24Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k11opsmayrl7ou,2019-09-27T05:27:24Z,{},other
1913,no,<p>Thanks for the response Vahe. I&#39;m also wondering if I&#39;m miscalculating the RMSE. I see Sutton said he averaged the RMSE over 100 which I was a little confused by. I simply took the adjusted projected weights after convergence from each lambda run and took the RMSE in relation to the ideal state values</p>,2019-09-26T03:31:28Z,60,Week 9/22 - 9/28,followup,,j6m1jeidndu6wq,k1054u6qyaw4re,2019-09-26T03:31:28Z,{},project1
1914,no,"<p>Yeah, that&#39;s probably your issue.  If you find the RMSE of the <em>average</em> of your training runs it probably <strong>will </strong>be less than the average of the RMSEs of your training runs!</p>",2019-09-26T03:34:56Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k1059amjh564mt,2019-09-26T03:34:56Z,{},project1
1915,no,<p>Just to make sure I&#39;m understanding you and Sutton correctly.<strong> Before</strong> convergence I should take the RMSE after each training run then average <strong>after</strong> convergence?</p>,2019-09-26T03:45:02Z,60,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k105maj3st72hr,2019-09-26T03:45:02Z,{},project1
1916,no,"<p>Not exactly.</p>
<p></p>
<p>Remember that each time the algorithm runs, it runs to convergence on one training set in isolation.  There are 100 training sets...</p>",2019-09-26T03:47:50Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k105pw2slt15p4,2019-09-26T03:47:50Z,{},project1
1917,no,"<p>Right, 1 training set contains 100 training runs, each run with 10 sequences. The weight update is done after each training set (100 training runs) in experiment 1. So convergence can not be met &#39;in the middle&#39; of a training set. All 100 runs must be completed before checking for convergence. Which is also when the RMSE should be calculated, each time the training set (all 100 runs) are ran?</p>",2019-09-26T04:01:28Z,60,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k1067f0g4pi6ho,2019-09-26T04:01:28Z,{},project1
1918,no,"<p>From Sutton page 20:</p>
<p></p>
<p>&#34;In the first experiment, the weight vector was not updated after each sequence<br />as indicated by (1). Instead, the $$\triangledown$$w&#39;s were accumulated over sequences and only<br />used to update the weight vector after the complete presentation of a training set.<br />Each training set was presented repeatedly to each learning procedure until the<br />procedure no longer produced any signifcant changes in the weight vector.&#34;</p>
<p></p>
<p>Where are you getting the 100 from?</p>
<p></p>
<p>Edit:  I think you should strike the phrase &#34;training run&#34; from your vocabulary.  That may be tripping you up.  We have sequences and training sets only.</p>",2019-09-26T04:21:44Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k106xhrciot1go,2019-09-26T04:21:44Z,{},project1
1919,no,"<p>Sorry I clearly misread that. I am currently updating my weight vector after each training set but I&#39;m mistakenly was not checking for convergence for each individual training set. I was treating the entire combination of 100 training sets as a check for convergence which was incorrect. I must run a training set until convergence, then move to the next training set (there are a total of 100 training sets). After a training set is converged I calculate the RMSE of that training set (there are 100 in total) then I take the average of my RMSEs for all my training sets at that specific lambda value (such as lambda = 0)</p>",2019-09-26T04:42:13Z,60,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k107ntohpde7nh,2019-09-26T04:42:13Z,{},project1
1920,no,<p>Exactly</p>,2019-09-26T05:04:47Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k108guq1f26zi,2019-09-26T05:04:47Z,{},project1
1921,no,"<p>Anyone getting this for figure 3 / experiment 1?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhdgaq99za3um%2Fk11rg8n4fbne%2FScreen_Shot_20190927_at_2.jpg"" alt="""" /></p>
<p></p>
<p>I ~think~ it&#39;s the way I&#39;m generating data. Here&#39;s what I&#39;m doing:</p>
<ol><li>Generated 9948 unique sequences that terminate (half in A, half in G)</li><li>Sorted them to be A termination, G termination, A, G, and so on </li><li>I converge my Δw-vector (ε = 0.001) before moving onto the next 10 sequences</li><li>After convergence, my w-vector is updated (with respect to a certain lambda)</li><li>Ran 100 training sets per lambda which takes about 10min. I could do more, but want to make sure I&#39;m not doing anything glaringly wrong</li></ol>
<p></p>
<p>My main question before experimenting further is <strong>am I doing something that&#39;s blatantly wrong?</strong></p>
<p><strong></strong></p>
<p>I do like how λ &lt; 1 does significantly better than λ = 1, but of course want to avoid confirmation bias.</p>",2019-09-27T06:53:32Z,60,Week 9/22 - 9/28,followup,,jzhdgaq99za3um,k11rskd790c4h4,2019-09-27T06:53:32Z,{},project1
1922,stud,"<p>I know that x is my observation vector and P are my predictions, which is a scalar because of the dot product. I meant that as weight vector dot observation vector at tilmestep t (didn&#39;t finish typing that out). At any point in time, my P&#39;s are scalars.</p>
<p></p>
<p>My gradients are vectors (partial gradient with respect to w basically making it a vector of x&#39;s).</p>
<p></p>
<p></p>",2019-09-26T04:42:32Z,60,Week 9/22 - 9/28,followup,a_0,,k107o8x91to3va,2019-09-26T04:42:32Z,{},project1
1923,stud,"<p>As for the second bullet point - I&#39;m not running to convergence only one one training set. To illustrate more clearly:</p>
<p></p>
<p>* Weight increments added up as we step through the sequence</p>
<p>* After 10 sequences, increments for each sequence were added up. They were then added to the weight vector</p>
<p>* Convergence check based on the L1 norm of the increments of the last 10 sequences is made</p>
<p>* On convergence, generate RMSE using weight vector with desired weights vector (based on reading), - generated through sqrt(sum((ideal vector - weight vector) ** 2) / 5)</p>
<p></p>
<p>This is done against 100 training sets (each having 10 random sequences)</p>",2019-09-26T04:55:40Z,60,Week 9/22 - 9/28,followup,a_0,,k10854dqq20397,2019-09-26T04:55:40Z,{},project1
1924,no,"<p>I think your four bullet points are perfect.  We may have different interpretations on what to do with $$w$$ after we have converged on a single training set.</p>
<p></p>
<p>Edit: I think regardless what you do with $$w$$ you should be fine based on what you wrote.  Are you averaging $$100$$ RMSEs?  Are you calculating RMSE correctly?</p>",2019-09-26T05:03:06Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k108eox5v52w5,2019-09-26T05:03:06Z,{},project1
1925,stud,"Thanks Vahe. I collect an array of RMSE values per each convergence (100 converged 10-sequence sets, so 100 RMSE values) and I take the mean of that array. The RMSE is calculated using my equation above.",2019-09-26T05:18:35Z,60,Week 9/22 - 9/28,feedback,a_0,,k108yldbaom4xf,2019-09-26T05:18:35Z,{},project1
1926,no,<p>Hrmm.  Another possible area to look is your implementation of the weight increment update rule.  I&#39;m not sure in how much more detail we really should go as far as debugging.  Sorry I couldn&#39;t help.</p>,2019-09-26T05:26:13Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k1098f5bx2c1md,2019-09-26T05:26:13Z,{},project1
1927,stud,<p>I figured it out. It was a bug in my convergence code!</p>,2019-09-26T15:41:05Z,60,Week 9/22 - 9/28,feedback,a_0,,k10v75cdaz74fn,2019-09-26T15:41:05Z,{},project1
1928,stud,<p>Could you please add more details with respect to your bug? I also get a u-shape figure and do not know why...</p>,2019-09-27T09:30:19Z,60,Week 9/22 - 9/28,feedback,a_1,,k11xe6see3l7pg,2019-09-27T09:30:19Z,{},project1
1929,stud,<p>Me also sorted. It is because I forget to update the epsilon value.</p>,2019-09-27T10:07:58Z,60,Week 9/22 - 9/28,feedback,a_1,,k11yqlm0zk17a9,2019-09-27T10:07:58Z,{},project1
1930,no,"<p>How and why would you update your epsilon value. <br /><br /></p>
<p></p>
<p>Also, can someone point me to the L-1 or L-infinity norm epsilon equations in our lecture or reading material?</p>",2019-09-28T23:22:45Z,60,Week 9/22 - 9/28,feedback,,gx3c8l7z7r72zl,k146kjh4unh40a,2019-09-28T23:22:45Z,{},project1
1931,no,"<p>I don&#39;t know if they were in the lectures but scroll down to &#39;p-norm&#39; here:</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Norm_%28mathematics%29"">https://en.wikipedia.org/wiki/Norm_(mathematics)</a></p>",2019-09-28T23:33:11Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k146xyp8tzv5mp,2019-09-28T23:33:11Z,{},project1
1932,stud,"<p>After a presentation of a training set, my epsilon is already very small. Because I forgot to reset it to a large value, at the next iteration, this small epsilon is used directly, resulting no updating of weight.</p>",2019-09-29T00:58:28Z,59,Week 9/29 - 10/5,feedback,a_1,,k149zn9kuu67k0,2019-09-29T00:58:28Z,{},project1
1933,no,<p>And the ellipse is your convergence threshold?</p>,2019-09-29T01:22:22Z,59,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k14auda5xog7jf,2019-09-29T01:22:22Z,{},project1
1934,no,"<p>&#64;Vahe</p>
<p></p>
<p>You said &#34;Finally, your second bullet point may be problematic.  You&#39;re running to convergence only on one training set (of 10 sequences) in isolation.&#34;</p>
<p></p>
<p>I read the paper as we were supposed to converge each 100 training sets individually. But your comment above makes it sound like we need to converge overa 100 training sets in tandem so that would mean all 100 training sets would keep getting sent to the learner until convergence as opposed to sending each training set over and over to convergence is that accurate?</p>
<p>Also, during your second conversation with the anonymous classmate above it once again makes it sound like you&#39;re checking for convergence among each training set individually, I guess I&#39;m a bit confused LOL.</p>",2019-09-26T14:14:03Z,60,Week 9/22 - 9/28,followup,,gx3c8l7z7r72zl,k10s37u2mdofu,2019-09-26T14:14:03Z,{},project1
1935,no,"<p>Aaron, from what Vahe stated that I understood (and as my understanding too), we generate an RMSE value for each of the 100 training sets. That means we have to run each training set into convergence, as opposed to trying to converge over the 100 training sets in tandem.</p>",2019-09-26T14:48:53Z,60,Week 9/22 - 9/28,feedback,,jqnardlwW3NE,k10tc0eoxro1ub,2019-09-26T14:48:53Z,{},project1
1936,no,"<p>&#34;<em>I read the paper as we were supposed to converge each 100 training sets individually&#34;</em></p>
<p><em></em></p>
<p><em></em></p>
<p>Exactly.  This is why I used the word &#34;isolation,&#34; to emphasize that we&#39;re converging, separately, to each of the $$100$$ training sets..</p>
<p></p>
<p>&#34;<em>But your comment above makes it sound like we need to converge overa 100 training sets in tandem so that would mean all 100 training sets would keep getting sent to the learner until convergence as opposed to sending each training set over and over to convergence is that accurate</em>?&#34;</p>
<p></p>
<p>No, this is pretty much the opposite of my comment.</p>
<p></p>
<p>&#34;<em>it once again makes it sound like you&#39;re checking for convergence among each training set individually</em>&#34;</p>
<p></p>
<p>Now I&#39;m confused!</p>
<p></p>
<p>I interpret experiment 1 just as Alastair does.</p>",2019-09-26T15:56:36Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k10vr3o9fzh7i5,2019-09-26T15:56:36Z,{},project1
1937,no,"<p>I also had a non-monotically increasing curve, until I did something.</p>
<p>It&#39;s either implementing eligibility traces or restricting the lengths of the sequences, not sure because I haven&#39;t started writing my report and gone through all I did but I&#39;m sure one of these things will fix your issue.  </p>
<p>The max sequence length is a very influential factor at times.  </p>",2019-09-27T04:03:00Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k11lp9b55oe3ak,2019-09-27T04:03:00Z,{},project1
1938,stud,<p>Did Sutton mention in his paper to set a maximum sequence length? And how long would be reasonable? Thx!</p>,2019-09-27T06:00:49Z,60,Week 9/22 - 9/28,feedback,a_1,,k11pwrqfwzc6kn,2019-09-27T06:00:49Z,{},project1
1939,no,"<p>No, but that was mentioned in a OH and I tried, and oh boy it really does make a difference.  </p>
<p>It&#39;s not hard to imagine Sutton limited the sequence lengths to avoid rare but theoretically possible infinite sequences.  </p>
<p></p>
<p>After all, when you&#39;re done fiddling with alpha, there&#39;s not many things to play with after you&#39;ve made it work, at least partially, and then you try to optimize.  </p>
<p></p>
<p>I&#39;ll let you find the best value, it&#39;s pretty straightforward when you do a few tries.  </p>
<p></p>
<p></p>",2019-09-27T06:09:35Z,60,Week 9/22 - 9/28,feedback,,jzh6k6o994a6dh,k11q8125rkf31f,2019-09-27T06:09:35Z,{},project1
1940,no,<p>So are you saying that for $$\lambda=0$$ you&#39;re getting convergence?</p>,2019-09-26T06:09:28Z,60,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k10as0ymstz44a,2019-09-26T06:09:28Z,{},project1
1941,no,Not really. After 20 epoch the weight is still going slowly. The weight values are under 1.  Not like giant scientific number. <div><br /></div><div>Do you see anything wrong with the weight update above? </div>,2019-09-26T06:21:00Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k10b6v6g2o53kk,2019-09-26T06:21:00Z,{},project1
1942,no,<p>Is an epoch one loop through $$10$$ episodes?  Then $$20$$ epochs is not very many.  What do you have $$\alpha$$ set to?</p>,2019-09-26T07:00:09Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k10cl7funu53bk,2019-09-26T07:00:09Z,{},project1
1943,no,1 epoch is over one training set which contains 10 sequences. I set alpha to 0.01 based on other post. ,2019-09-26T16:09:39Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k10w7vu1rqa648,2019-09-26T16:09:39Z,{},project1
1944,no,"I got a similar problem.

Since delta_w is accumulated over sequence(keep summing up in my understanding) and only used to update the weight vector after complete presentation of a training set (pg 20 in Sutton paper), my delta_w keeps glowing to infinity.

Is there any trick to make sure it goes converge? I am pretty sure I am doing some wrong.

Thanks.",2019-09-26T09:05:05Z,60,Week 9/22 - 9/28,followup,,jr7165ipLPTP,k10h1vcjcd97i2,2019-09-26T09:05:05Z,{},project1
1945,no,"<p>There&#39;s a parameter that controls how large the increments are.  Assuming there aren&#39;t other issues in your implementation, you can try adjusting that.</p>",2019-09-26T12:05:10Z,60,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k10nhgmuck432g,2019-09-26T12:05:10Z,{},project1
1946,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhz3hplpvI9c%2Fk12qipcmvs0t%2FAnnotation_20190927_190511.png"" alt="""" /></p>",2019-09-27T23:05:45Z,60,Week 9/22 - 9/28,feedback,,hz3hplpvI9c,k12qiu2ffqk4tx,2019-09-27T23:05:45Z,{},project1
1947,no,How to calculate sum of gradient P? I can only think something could go wrong there. Do you accumulate the sum of gradient P at each step or calculate the sum from scratch at each step? ,2019-09-26T16:12:04Z,60,Week 9/22 - 9/28,followup,,j6ln9puq99s5uv,k10wazekbsf28r,2019-09-26T16:12:04Z,{},project1
1948,no,<p>The gradient P should just be the sum of all the weights at the specific state that you see from time t to k. Where k is the step taken in the sequence </p>,2019-09-26T19:14:15Z,60,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k112ta3hiro1o3,2019-09-26T19:14:15Z,{},project1
1949,no,"<p>Well, I think I found my issue. </p>
<p>In the above step 3. <strong>&#34;Add this to the gradient sum&#34;</strong> &gt;&gt;&gt;&gt; This step is not necessary. I am already tracking all the historical discounted gradient in the loop, so I don&#39;t need to add it back to the gradient sum. Hope this can help someone else too.</p>",2019-09-26T19:59:55Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k114g021mg97kq,2019-09-26T19:59:55Z,{},project1
1950,no,"<p>Zhenning:  look at your delta calc.  P.t - Pt-1, is a terrible syntax.  the actual difference is between estimates in the same time-step (between &#34;new state&#34; and &#34;source state&#34;).</p>
<p></p>
<p>also watch your order on the eligibility trace.  for some reason, it matters.  and also your &#34;final state&#34; valuation.</p>
<p></p>",2019-09-27T00:00:21Z,60,Week 9/22 - 9/28,followup,,jzivtxcbl6964n,k11d171y5jp1f8,2019-09-27T00:00:21Z,{},project1
1951,no,"<p>Thanks</p>
<p></p>",2019-09-27T18:26:41Z,60,Week 9/22 - 9/28,feedback,,j6ln9puq99s5uv,k12gjy4yjll3wy,2019-09-27T18:26:41Z,{},project1
1952,no,<p>Got it!! So any naming rule for the report filename?</p>,2019-09-26T14:17:14Z,60,Week 9/22 - 9/28,followup,,jl2bqebwzel2s,k10s7bcu1gx7df,2019-09-26T14:17:14Z,{},project1
1953,no,<p>see &#64;9</p>,2019-09-26T21:30:23Z,60,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k117oc6x5lq6fw,2019-09-26T21:30:23Z,{},project1
1954,no,"<p>In my opinion, analysis means &#34;detecting an issue and made one or some hypotheses to explain or to handle it&#34;.</p>
<p>The exploration means &#34;validating the hypotheses you built is right or wrong&#34;.</p>
<p></p>
<p>So as long as you implement everything you can get from the paper, then you will find the result is not the same as shown on paper.</p>
<p>Then you will build hypo, explore with your program, and analysis the new result.</p>
<p>It&#39;s recursive and lots of work but the page length is limited. Take that into consideration and you will know how to present your work.</p>",2019-09-26T14:25:32Z,60,Week 9/22 - 9/28,followup,,jl2bqebwzel2s,k10shzbcz51yg,2019-09-26T14:25:32Z,{},project1
1955,no,"<p>Is the goal to show the results are not the same or are the same? I keep seeing tidbits dropped throughout the forums on &#34;graphs may look different but your conclusions should be the same&#34; to &#34;the paper is wrong and your results wont match.&#34; </p>
<p></p>
<p>There seems to be a clear correct answer expected by the instructors for this assignment, but incredibly little in the way of guidance. There is 0 way to know if you are on the right track for this or not. This has made this incredibly frustrating to figure out what were supposed to be figuring out from this assignment. At this point I feel less and less like I&#39;m learning anything and more just adjusting hyperparameters at random to try and get some semblance of a match between my results and the papers, which may or may not be the correct way ahead.</p>",2019-09-26T14:39:15Z,60,Week 9/22 - 9/28,followup,,is4nx55dinr6wk,k10szmcfoue52o,2019-09-26T14:39:15Z,{},project1
1956,no,"<p>I have different opinion. The instructors might know the correct answer but they know the guidance is too little (that&#39;s why the project name is &#34;<strong>Desperately</strong> Seeking Sutton&#34;)</p>
<p>It&#39;s good if you can perfectly replication the figures, but I think instructors will be more interested how you find it. For most case I think, there will be slightly different. I think the minimum requirement is to show the process of how you analyze the problem. It&#39;s meaningless of tuning hyperparameters. You need to find out why it works after tuning something. If not, you still learn tuning these are useless or even make things worse, right?</p>",2019-09-26T14:53:27Z,60,Week 9/22 - 9/28,feedback,,jl2bqebwzel2s,k10thvr7ju41k1,2019-09-26T14:53:27Z,{},project1
1957,no,"<p>Hi &#64;Jacob, thanks for the paper.  It&#39;s very interesting. </p>
<p>I don&#39;t know you, your background, but I like the fact that I&#39;m starting to be able to read RL papers and understand them (after a rough start due to my total ignorance of this field).  </p>
<p>I even found the last recommended paper on rewards modifications with potential functions by Ng &amp; al very enjoyable.  </p>",2019-09-27T04:36:29Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k11mwbgsvwk79k,2019-09-27T04:36:29Z,{},project1
1958,no,"<p>If you don&#39;t do it, your weights will never change because that&#39;s how the weights deltas start spreading, when you get a &#39;1&#39; from the absorbing state G.  Otherwise A and G are indistinguishable.</p>
<p></p>",2019-09-27T04:16:16Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k11m6bmhxst6tk,2019-09-27T04:16:16Z,{},project1
1959,no,<p>&#43;1</p>,2019-09-28T02:41:06Z,60,Week 9/22 - 9/28,feedback,,hz3hplpvI9c,k12y7rxlljp2uo,2019-09-28T02:41:06Z,{},project1
1960,no,<p>I haven&#39;t tried it but I once used Pypy and it was really much faster.</p>,2019-09-27T04:59:28Z,60,Week 9/22 - 9/28,followup,,jzh6k6o994a6dh,k11npuzy54l5eg,2019-09-27T04:59:28Z,{},project1
1961,no,"<p>Just using Numpy, and vector operations, mine runs within a minute or so on i7.  Pretty sure its close to correct, as the graph looks like Sutton&#39;s non-errata version atm.  Try to minimize loops where possible.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhz3hplpvI9c%2Fk12yhljc4qpm%2Frepeated_presentations.png"" alt="""" /></p>",2019-09-28T02:50:42Z,60,Week 9/22 - 9/28,followup,,hz3hplpvI9c,k12yk4lqbau7e2,2019-09-28T02:50:42Z,{},project1
1962,no,<p>thanks. I got it. No naming convention</p>,2019-09-26T21:36:28Z,60,Week 9/22 - 9/28,followup,,jc9nkspumds4ki,k117w6619ww49a,2019-09-26T21:36:28Z,{},project1
1963,no,<p>Very nice. If anyone has previously gone to such trouble they haven&#39;t shared it with the class.</p>,2019-09-27T01:08:56Z,47,Week 9/22 - 9/28,followup,,i4op5p9vfbq5yz,k11fhein68s3qt,2019-09-27T01:08:56Z,{},project1
1964,no,"<p>I really like you guys. Plus, I wanted to make it easier for us to compare our results. I&#39;ll try to finish out the matrix tomorrow. It took an hour to get this far, and those congested data points are hard to separate.</p>
<p></p>
<p>Hopefully some of us include these numbers compared to the computed numbers to compare relative errors...</p>
<p></p>
<p>Oh yeah, these numbers are within 0.05 btw. The eyeball method is not very accurate.</p>",2019-09-27T01:48:28Z,47,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k11gw8vmt2sxq,2019-09-27T01:48:28Z,{},project1
1965,no,"<p>Thank you very much Jacob, it&#39;s very interesting indeed! :-)  </p>",2019-09-27T12:14:46Z,47,Week 9/22 - 9/28,feedback,,jqkxzdmmolGf,k1239nz8yyl1ug,2019-09-27T12:14:46Z,{},project1
1966,no,<p>Nice work!</p>,2019-09-28T23:22:47Z,47,Week 9/22 - 9/28,feedback,,hzoi2qsuCAd,k146klapwkh162,2019-09-28T23:22:47Z,{},project1
1967,no,"<p>As a side note, if you don&#39;t want to clone the repro, I did not have this issue when I just pip installed gym instead (using version 0.14.0). When I created the Taxi-v2 environment, it gave me the expected one when I looked at it.</p>
<p></p>
<p>ref: <a href=""http://gym.openai.com/docs/"">http://gym.openai.com/docs/</a></p>",2019-09-29T19:06:07Z,39,Week 9/29 - 10/5,followup,,ixty1midfufhd,k15cudpcdco15o,2019-09-29T19:06:07Z,{},hw4
1968,no,"<p>&#43;1, downgrading version was easiest for me.</p>",2019-10-01T04:04:58Z,39,Week 9/29 - 10/5,feedback,,jcg0nzvdk8272b,k17bj6zkv4f7k6,2019-10-01T04:04:58Z,{},hw4
1969,no,"<p>thank you so much for this post, i was wondering why I was getting a wrong value for the last one for last 5 hours</p>",2019-10-14T08:57:41Z,37,Week 10/13 - 10/19,followup,,iv2q8g9wyas6yx,k1q6pota1491pc,2019-10-14T08:57:41Z,{},hw4
1970,no,<p>So  do you get overflow errors for $$\alpha=0.2$$ when $$\lambda = 1.0$$ and for $$\alpha=0.4$$ when $$\lambda=0.8$$?  Or did you just not plot them because the error was relatively large?</p>,2019-09-28T03:59:03Z,27,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k13100wvtae1on,2019-09-28T03:59:03Z,{},project1
1971,no,<p>I do get overflow warnings at those values. My understanding of experiment 2 was to update w after each sequence and at the end of a set calculate the rmse. Then average all those RMSEs over all 100 sets</p>,2019-09-28T14:47:53Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k13o6f83jey5m9,2019-09-28T14:47:53Z,{},project1
1972,no,<p>Am I correct in my implementation when I sum the delta_w for each state in a sequence then update the w after the sequence then reset delta_w for the next sequence?</p>,2019-09-28T14:56:40Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k13ohpzw7r31sg,2019-09-28T14:56:40Z,{},project1
1973,no,"<p>Yes, that is how I interpreted it.  I would dig into your weight increments and see how the overflows are happening.  Where are the large values coming from?  I would also try extending $$\alpha$$ to higher values to see if $$\lambda=0.3$$ also diverges, since I would think the same problem happens regardless of $$\lambda$$.  As Don suggested there&#39;s probably a bug somewhere there.</p>",2019-09-28T16:11:45Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k13r6a9i6ti79d,2019-09-28T16:11:45Z,{},project1
1974,no,"<p>The way my code is set up, the next sequence (lets say sequence #2) is using the weights that were updated from sequence #1 in the same set. There&#39;s a moment when the error is just so large that the weight is just increasing very fast and those vastly increased weight values are being used in the next sequence to calculate delta_w. I&#39;m thinking I&#39;m doing something wrong where I&#39;m not resetting w in the proper place</p>",2019-09-28T19:27:48Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k13y6ektddy3of,2019-09-28T19:27:48Z,{},project1
1975,no,"<p>Well, $$w$$ shouldn&#39;t be reset in the middle of a training set, right?  Only at the start.</p>
<p></p>
<p>As for $$\Delta w$$, at that moment where it jumps, see what&#39;s causing the jump.  Is it the sum of gradients?  Or something else?</p>",2019-09-28T20:23:16Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k1405q91e5166j,2019-09-28T20:23:16Z,{},project1
1976,no,"<p>Right I am correct in not resetting w until a new set. I believe I have an incorrect understanding of the gradient summation. My values are blowing up from the summation gradient which current for me is the sum of weight at state during step t times lambda which in the case of lambda =1 is just the sum of weights. I think I have an incorrect understanding of, ∇w*Pk.</p>
<p></p>
<p>Currently I&#39;m just grabbing the weight value of a state at step t which is Pk but I&#39;m not understanding ∇w. Is this the current value of weight at a state minus the previous value of the state weight?</p>",2019-09-28T21:00:30Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k141hmbez8g6vi,2019-09-28T21:00:30Z,{},project1
1977,no,"<p>Yeah, $$\Delta_w\cdot P_k$$ is not what you think it means.</p>
<p></p>
<p>It&#39;s the &#39;gradient vector&#39; of $$P_k$$ with respect to $$w$$, which is a vector, each of whose components is a partial derivative.</p>
<p></p>
<p>Specifically, the first component is</p>
<p></p>
<p>$$\frac{\partial}{\partial w_1} P_k = \frac{\partial}{\partial w_1} x^T\cdot w = \frac{\partial}{\partial w_1} (x_1\cdot w_1 &#43; x_2\cdot w_2 &#43; x_3\cdot w_3 &#43; x_4\cdot w_4 &#43; x_5\cdot w_5) = x_1$$</p>
<p></p>
<p>The other four components of the gradient vector are computed similarly.</p>
<p></p>
<p>In the above, $$x^T$$ is the transpose of the observation vector $$x$$.</p>",2019-09-28T22:59:03Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k145q2xobeu1qy,2019-09-28T22:59:03Z,{},project1
1978,no,<p>Is the observation vector the expected value of the states?</p>,2019-09-28T23:12:08Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k1466w2z6vf5yp,2019-09-28T23:12:08Z,{},project1
1979,no,"<p>No, it&#39;s the current state the agent is in, i.e. $$x_t$$ is the state the agent is in at time $$t$$.</p>
<p></p>
<p>For example, at $$t=1$$ the agent is always in state $$D$$, which is the starting state of the MDP.  Sutton represents state D as the one-hot vector $$[0,0,1,0,0]$$, so in this case, $$x_1 = [0,0,1,0,0]$$.</p>",2019-09-28T23:16:00Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k146bv7ce0n3kc,2019-09-28T23:16:00Z,{},project1
1980,no,"<p>Oh gotcha that&#39;s where the one-hot encoding is coming into play. It&#39;s used to fill in the lambda weights to the proper weight states  which is then multiplied by the error correction</p>
<p></p>
<p>So that x1 in your example will simply be multiplied by the correspond lambda exponent, and the summation is used to include all those different lambda exponents at different weight states through incremental steps</p>",2019-09-28T23:25:11Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k146nogona36jk,2019-09-28T23:25:11Z,{},project1
1981,no,<p>This means the summation is a vector correct?</p>,2019-09-28T23:27:51Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k146r43yc2f2ba,2019-09-28T23:27:51Z,{},project1
1982,no,"<p>Yes! (to your first post)</p>
<p></p>
<p>And yes to your second post too.</p>
<p></p>
<p>And note that $$\Delta_wP_k$$, while looking complex, has a really simple resolution because of the one-hot vector definition for his observations.  If you haven&#39;t already, finish taking the partials I was doing in my above post and form the gradient vector and see what it is.</p>",2019-09-28T23:30:31Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k146uj9o7hu261,2019-09-28T23:30:31Z,{},project1
1983,no,"<p>The rest of your partials depend on where the random walk went for step 2 but if it went to E the partial derivative would be [0, 0, 0, 1, 0]  and the summation would be the previous sum when t=1 &#43; the partial of step 2 multiplied by lambda</p>",2019-09-29T00:01:43Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k147ynoik852e7,2019-09-29T00:01:43Z,{},project1
1984,no,"<p>Your description is right, but I think you may have it the opposite way around.  The <em>earlier</em> time ($$t=1$$ in this case) has the factor of $$\lambda$$.</p>
<p></p>",2019-09-29T01:24:05Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k14awle19yy3rv,2019-09-29T01:24:05Z,{},project1
1985,no,"<p>I think I may have realized my error. With TD lambda we are putting more weight the further back we go from the current step correct?</p>
<p></p>
<p>So if the agent were at t = 4 (step 4), the weight gradient would be:</p>
<p>(λ^3 * x1) &#43; (λ^2 * x2) &#43; (λ^1 * x3) &#43; (x4)</p>",2019-09-29T01:58:30Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k14c4ulcu2n3bi,2019-09-29T01:58:30Z,{},project1
1986,no,"<p>Yup, the earliest observations are decayed the most.</p>",2019-09-29T02:21:48Z,27,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k14cyte7q691zl,2019-09-29T02:21:48Z,{},project1
1987,no,<p>that fixed it! Thank you Vahe!</p>,2019-09-29T03:42:57Z,27,Week 9/22 - 9/28,feedback,,j6m1jeidndu6wq,k14fv5xoi1q320,2019-09-29T03:42:57Z,{},project1
1988,no,"<p>When i try to setup the custom map, i get this error:</p>
<p></p>
<pre>    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(desc=custom_map).unwrapped
AttributeError: module &#39;gym.envs&#39; has no attribute &#39;toy_text&#39;</pre>
<p>Google didn&#39;t have any advice in this regard, so anyone else have this error?</p>
<p></p>
<p>My pip freeze:</p>
<p></p>
<pre>asyncio-nats-client==0.9.2
atari-py==1.2.1
box2d==2.3.2
certifi==2019.6.16
chardet==3.0.4
cloudpickle==1.2.2
cycler==0.10.0
enum34==1.1.6
future==0.17.1
-e git&#43;https://github.com/openai/gym.git&#64;c33cfd8b2cc8cac6c346bc2182cd568ef33b8821#egg=gym
idna==2.8
joblib==0.13.2
kiwisolver==1.1.0
matplotlib==3.1.1
nose==1.3.7
numpy==1.17.0
opencv-python==4.1.1.26
pandas==0.25.0
Pillow==6.1.0
pyglet==1.3.2
pyparsing==2.4.2
python-dateutil==2.8.0
pytz==2019.2
requests==2.22.0
scikit-learn==0.21.3
scipy==1.3.0
six==1.12.0
sklearn==0.0
urllib3==1.24.3
websocket-client==0.56.0
websockets==8.0.2</pre>
<p></p>",2019-09-28T20:02:37Z,27,Week 9/22 - 9/28,followup,,jc554vxmyuy3pt,k13zf68q98d1su,2019-09-28T20:02:37Z,{},hw3
1989,no,"<p>I think this is another change to the latest version of OpenAI Gym. This worked for me:</p>
<p></p>
<pre>env = gym.make(&#39;FrozenLake-v0&#39;, desc=custom_map)
env = env.unwrapped</pre>",2019-09-28T20:12:07Z,27,Week 9/22 - 9/28,feedback,,jzifg1e23c29s,k13zrel88pcoa,2019-09-28T20:12:07Z,{},hw3
1990,no,<p>Thanks Steph! This setup has been my own personal version of FrozenLake.....</p>,2019-09-28T20:15:20Z,27,Week 9/22 - 9/28,feedback,,jc554vxmyuy3pt,k13zviu5rm449h,2019-09-28T20:15:20Z,{},hw3
1991,no,"<p>&#64;Steph, where do you find this <strong>gym.make()</strong> function? Couldn&#39;t find any document on how to make the env. I am looking at the code on GitHub master branch. It&#39;s very confusing. Could you point me to the right document? Thanks. </p>",2019-10-01T01:41:08Z,26,Week 9/29 - 10/5,feedback,,j6ln9puq99s5uv,k176e7ilwk937n,2019-10-01T01:41:08Z,{},hw3
1992,no,"<p>Here&#39;s the documentation: <a href=""http://gym.openai.com/docs/#environments"">http://gym.openai.com/docs/#environments</a></p>
<p></p>
<p>It also explains how to take actions in the environment and get next state / reward :)</p>",2019-10-01T01:46:16Z,26,Week 9/29 - 10/5,feedback,,jzifg1e23c29s,k176kt7xmso7lw,2019-10-01T01:46:16Z,{},hw3
1993,no,"<p>I recently had the issue that if you use &#39;-e&#39; when you download a library from github, it doesn&#39;t end up in the normal directory with all the libraries under .../python3.7&#39; but in another one in your user directory... </p>
<p>I see the &#39;-e&#39; with your download of gym, so maybe that&#39;s what happened, and python has trouble finding it, although it&#39;s funny that if you do import gym.envs.toy_text, then it finds it... </p>",2019-10-02T18:00:15Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19kt7sx28749q,2019-10-02T18:00:15Z,{},hw3
1994,no,"<p>&#64;TA&#39;s</p>
<p></p>
<p>Is it OK to use Steph&#39;s approach?</p>",2019-10-04T04:24:38Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1bmk1bo50i6t6,2019-10-04T04:24:38Z,{},hw3
1995,no,"<p>What you did can work, you just need to:</p>
<p></p>
<pre>import gym.envs.toy_text;</pre>",2019-09-29T02:48:05Z,26,Week 9/29 - 10/5,followup,,jzhl7qlwrpagr,k14dwm9cwqi9v,2019-09-29T02:48:05Z,{},hw3
1996,no,<p>&#43;1</p>,2019-10-03T19:14:59Z,26,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1b2x6jeiak77t,2019-10-03T19:14:59Z,{},hw3
1997,no,"<p>Hi all,</p>
<p></p>
<p>Thank you for the helpful information above. But just to clarify, do we need to copy the frozen lack environment code from Git Hub locally, then run this command above? I ask because I ran the follow command and no frozen lack environment exists?</p>
<p></p>
<pre>from gym import envs<br />print(envs.registry.all())</pre>
<p></p>
<p>Thank you for your help.</p>",2019-10-02T17:50:43Z,26,Week 9/29 - 10/5,followup,,gx3c8l7z7r72zl,k19kgy9tgxj2xe,2019-10-02T17:50:43Z,{},hw3
1998,no,"<p>If you installed gym with <em>pip install gym</em>, then you need to install the environments too, to install the toy_text environment, you can do</p>
<p><em>pip install gym[toy_text].</em> You can install all environments using </p>
<p><em>pip install &#39;gym[all]&#39;</em>, but that requires you to get the mujoco license and install it first.</p>",2019-10-03T03:53:20Z,26,Week 9/29 - 10/5,feedback,,j6pmq1sglzo35i,k1a5zxgkfhw4,2019-10-03T03:53:20Z,{},hw3
1999,no,"<p>thx &#64;Hazel, I tried your command pip install gym[toy-text],and I got a very confusing error message</p>
<p>gym 0.14.0 does not provide the extra &#39;toy_text&#39; although we can see on the line just above that it does ???</p>
<p></p>
<pre>pip3 install gym[toy_text] --user
Requirement already satisfied: gym[toy_text] in /Users/JPB/Library/Python/3.7/lib/python/site-packages (0.14.0)
  WARNING: gym 0.14.0 does not provide the extra &#39;toy_text&#39;</pre>
<p>Any idea why this is happening?</p>",2019-10-03T06:23:08Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1abckpwftt5he,2019-10-03T06:23:08Z,{},hw3
2000,no,"<p>Try putting quotes around ‘gym[toy_text]’</p>
<p>so pip install ‘gym[toy_text]’</p>
<p></p>",2019-10-04T02:07:54Z,26,Week 9/29 - 10/5,feedback,,j6pmq1sglzo35i,k1bho79lggs17p,2019-10-04T02:07:54Z,{},hw3
2001,no,"<p>aaah, ok, thx</p>",2019-10-04T03:01:21Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1bjkxedgts3ty,2019-10-04T03:01:21Z,{},hw3
2002,no,"<p>Hazel/Jean,</p>
<p></p>
<p>This did not work for me:</p>
<p></p>
<pre>python -m pip install &#39;gym[toy_text]&#39;
    Invalid requirement: &#39;&#39;gym[toy_text]&#39;&#39;</pre>
<p>Any suggestions?</p>",2019-10-04T03:48:24Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1bl9g53k1t4rd,2019-10-04T03:48:24Z,{},hw3
2003,no,"<p></p><pre>python -m pip install gym[all]<br /><br /></pre>
<p></p>
<p>Partially worked, as in it at least got me EnvSpec(FrozenLake-v0) and EnvSpec(FrozenLake8x8-v0)</p>
<p></p>",2019-10-04T04:02:08Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1blr3pl8w1o5,2019-10-04T04:02:08Z,{},hw3
2004,no,"<p>Hi Aaron, toy_text was already installed before I tried Hazel&#39;s command.  I&#39;m just surprised I get a warning message telling me that gym does not provide toy_text when it says it&#39;s already installed the line just above.</p>
<p>Anyway, I never installed toy_text specifically, it came with gym.  </p>
<p>I don&#39;t remember how I installed gym though.</p>
<p>Not sure if I used pip install gym or the using the full git path instead, but I know that, in the latter case, sometimes we find installations procedures that put a &#39;-e&#39; before the git path, and that means that the library is not installed in the library directory but simply downloaded in the user&#39;s home directory.  Maybe that&#39;s your problem.  That&#39;s all I can think ok.  It happened to me recently.  I thought I had downloaded a library but instead it was just laying there in ~/src.  </p>",2019-10-04T04:08:20Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1blz2ib6l423v,2019-10-04T04:08:20Z,{},hw3
2005,no,<p></p>,2019-10-04T04:22:11Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1bmgvl8tna1l8,2019-10-04T04:22:11Z,{},hw3
2006,no,"<p>What&#39;s your convergence criteria? If you have some sort of $$\epsilon $$, have you tried setting it lower?</p>",2019-09-28T15:47:26Z,60,Week 9/22 - 9/28,followup,,jl3oi5v7qkSk,k13qb0fy5er4fq,2019-09-28T15:47:26Z,{},project1
2007,no,"<p>I&#39;ve set the convergence criteria to .001 and .0000001 and everything in between so I don&#39;t think that&#39;s it. I&#39;ve also gone from alpha of .03 (larger diverges) to .001.</p>
<p></p>
<p></p>",2019-09-29T03:35:19Z,59,Week 9/29 - 10/5,feedback,,jc7qnuoloyn56p,k14flcszurh76i,2019-09-29T03:35:19Z,{},project1
2008,no,"<p>&#34;<em>it is very consistent when running with different seeds&#34;</em></p>
<p></p>
<p>At roughly what fraction of seeds does this happen?  Before identifying this as a problem I would try at least $$10$$ different seeds and note the percentage where this happens.</p>",2019-09-28T16:16:56Z,60,Week 9/22 - 9/28,followup,,jzfsa4a37jf4aq,k13rcxzl2f83gg,2019-09-28T16:16:56Z,{},project1
2009,no,"<p>Vahe, I took your advice and 3 of 10 seeds I tested do not have this problem. Strange. Is this something you have seen in your own experiments?</p>",2019-09-29T04:06:56Z,59,Week 9/29 - 10/5,feedback,,jc7qnuoloyn56p,k14gq0cmxgx3qp,2019-09-29T04:06:56Z,{},project1
2010,no,"<p>I personally think it is a non-issue, as the standard error of these estimates is &gt;&gt; the differences involved.  Don&#39;t take my word for it though.</p>",2019-09-29T04:15:55Z,59,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k14h1k2nurj647,2019-09-29T04:15:55Z,{},project1
2011,no,"<p>That&#39;s a very good point, Vahe. That makes me feel better about it to put it in that perspective. Thanks!</p>",2019-09-29T04:17:57Z,59,Week 9/29 - 10/5,feedback,,jc7qnuoloyn56p,k14h46cwpeh97,2019-09-29T04:17:57Z,{},project1
2012,no,"<p>You can do either of these, they are equivalent, to show your current commit hash:</p>
<pre>git log -n 1 --format=%H<br />git show -s --format=%H</pre>
<p>The commit hash is a stamp for your whole repo, not just for single file. TA want us to include this as a snapshot-proof of the code when we submit our report.</p>
<p>You should include the hash after you finalize all the changes you want for your code.</p>",2019-09-29T18:54:44Z,59,Week 9/29 - 10/5,followup,,jl5wq8mca7o0,k15cfq0u9mt4d0,2019-09-29T18:54:44Z,{},project1
2013,stud,"<p>Thanks &#64;Quang, I don&#39;t know much about git but I know there&#39;s a SHA (hence a hash) code attached to every version of a file, so even if we change the file after the deadline, they&#39;ll look at the one before the deadline... </p>
<p>Don&#39;t you think they TA&#39;s want the hash code for that very purpose?</p>
<p></p>
<p>But I&#39;ll try your commands, I need to build a minimum knowledge of git (for now).</p>
<div>
<div></div>
</div>",2019-09-29T23:27:20Z,59,Week 9/29 - 10/5,feedback,a_0,,k15m6aqswn27k5,2019-09-29T23:27:20Z,{},project1
2014,no,What do you mean by the length of the sequences ? Should we restrict the length of the random walks we generate ? ,2019-09-29T18:19:09Z,59,Week 9/29 - 10/5,followup,,jqkuetouttn5,k15b5z1c6vz2po,2019-09-29T18:19:09Z,{},project1
2015,no,<p>I can log in now.  Thanks.</p>,2019-09-28T16:50:27Z,60,Week 9/22 - 9/28,followup,,j6lnhjhnzr3y5,k13sk214ca976a,2019-09-28T16:50:27Z,{},hw3
2016,stud,<p>Can you see the assignments ? </p>,2019-09-29T14:06:36Z,59,Week 9/29 - 10/5,feedback,a_0,,k15256v0cy31gk,2019-09-29T14:06:36Z,{},hw3
2017,no,"<p>I don&#39;t have an access to HW3 or HW4 yet</p>
<p></p>",2019-09-29T15:31:26Z,59,Week 9/29 - 10/5,feedback,,jqkxzdmmolGf,k1556ahbg5830l,2019-09-29T15:31:26Z,{},hw3
2018,no,"I can’t get access to HW3 or 4 yet, although I can log in and HW 1 &amp; 2 are there.",2019-09-29T16:52:35Z,59,Week 9/29 - 10/5,feedback,,jzvoaw0pl6326,k1582n8hjdt4eg,2019-09-29T16:52:35Z,{},hw3
2019,no,"<p>I don&#39;t see hw3 and hw4, but I can access https://​rldm.herokuapp.com.</p>",2019-09-29T17:28:03Z,59,Week 9/29 - 10/5,feedback,,j6lnhjhnzr3y5,k159c95wt6v9m,2019-09-29T17:28:03Z,{},hw3
2020,no,<p>How does it open up this weekend and due 2 days later (October 7th for hw3)</p>,2019-09-30T16:43:16Z,59,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k16n6ibdvwj5dq,2019-09-30T16:43:16Z,{},hw3
2021,no,"<p>I guess it was supposed to open the weekend that just passed, the TA&#39;s post was on Saturday</p>",2019-09-30T23:44:22Z,59,Week 9/29 - 10/5,feedback,,jqkxzdmmolGf,k17281ryceam4,2019-09-30T23:44:22Z,{},hw3
2022,no,"<p>fyi, it looks like they are available on herokuapp now. (Monday night).</p>",2019-10-01T02:27:21Z,59,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k1781njmtii2ei,2019-10-01T02:27:21Z,{},hw3
2023,no,"<p>Hello Instructors/TA&#39;s,</p>
<p></p>
<p>I am unable to sign in to: https:// rldm.herokuapp.com</p>
<p></p>
<p>I credentials worked for the last two HW&#39;s but it wont let me in now?!?!</p>",2019-10-05T19:44:22Z,59,Week 9/29 - 10/5,followup,,gx3c8l7z7r72zl,k1dyuo03dse1sg,2019-10-05T19:44:22Z,{},hw3
2024,no,"<p>The link from the HW3 write up does not include the &#34;s&#34; in the &#34;https://&#34;, so add &#34;https://&#34; before the URL for Heroku.</p>",2019-10-06T15:59:14Z,58,Week 10/6 - 10/12,feedback,,gx3c8l7z7r72zl,k1f6905klo43i2,2019-10-06T15:59:14Z,{},hw3
2025,no,<p>I can&#39;t seem to log in either...</p>,2019-10-06T15:11:29Z,58,Week 10/6 - 10/12,followup,,jzlf32odc2k20h,k1f4jlago7d77y,2019-10-06T15:11:29Z,{},hw3
2026,no,"<p>The link from the HW3 write up does not include the &#34;s&#34; in the &#34;https://&#34;, so add &#34;https://&#34; before the URL for Heroku.</p>",2019-10-06T15:59:09Z,58,Week 10/6 - 10/12,feedback,,gx3c8l7z7r72zl,k1f68wi2itt3eu,2019-10-06T15:59:09Z,{},hw3
2027,no,"<p>&#64;Vahe</p>
<p></p>
<p>Thank you for your response.</p>
<p></p>
<p>So what defines an episode in our case? Does episode = one sequence? If so, and a sequence has 8 steps, then we would start at t=1 on the first iteration and t=2 on the second all the way to t&#43;7?</p>
<p></p>
<p>Something like:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fgx3c8l7z7r72zl%2Fk13was6d3pug%2F20190928_113051.jpg"" alt="""" width=""578"" height=""468"" /></p>
<p></p>
<p></p>
<p></p>",2019-09-28T18:37:02Z,35,Week 9/22 - 9/28,followup,,gx3c8l7z7r72zl,k13wd4l7bot43j,2019-09-28T18:37:02Z,{},project1
2028,no,"<p>Yup, episode and sequence are interchangeable.</p>
<p></p>
<p>Your equations look fine except for the exponents on $$\lambda$$.  There should be a $$k$$ there instead of a $$1$$.</p>",2019-09-28T19:15:14Z,35,Week 9/22 - 9/28,feedback,,jzfsa4a37jf4aq,k13xq8gkj2y246,2019-09-28T19:15:14Z,{},project1
2029,no,"<p>&#64;Vahe</p>
<p>This makes sense, but what I am struggling with is where to go from here. I am not sure if I should do the following:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjqs38bj1ako389%2Fk15mp7hwb2u2%2F02.jpg"" alt="""" /></p>
<p></p>
<p>and leave in this format for graphing, or set lambda to a fixed value based on experimentation and sum together. Based on the graphs, it looks like Sutton is mapping a polynomial, but I&#39;m not sure based on all of the posts on Piazza.</p>",2019-09-29T23:47:14Z,34,Week 9/29 - 10/5,feedback,,jqs38bj1ako389,k15mvvmycjy4n6,2019-09-29T23:47:14Z,{},project1
2030,no,"<p>Barry, a couple of things:</p>
<p></p>
<p>1. You&#39;re skipping $$\lambda$$, and going straight from $$1$$ to $$\lambda^2$$ in your final two lines for $$\Delta w_3$$ (it&#39;s correct in the first one).</p>
<p></p>
<p>2. Are you talking about Figures $$3,4,$$ and $$5$$?  If so the value of $$\lambda$$ is specified by Sutton in his experimental procedures - see for example, the first paragraph on page 20 of his paper.</p>",2019-09-30T00:09:02Z,34,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k15nnx13wld3gv,2019-09-30T00:09:02Z,{},project1
2031,no,"<p>Vahe,</p>
<p>Thanks for responding so quickly! Not sure why this is taking me so long to understand.</p>
<p></p>
<p></p>
<p>1. Good catch on my skipping of λ.</p>
<p></p>
<p>2. Yes, figures 3, 4, and 5. Working on Figure 3. The passage you pointed out is exactly where I&#39;m confused, he uses [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1].</p>
<p></p>
<p>So, am I essentially, tracking and updating a vector with these values calculated after each sequence and then update weights vector after each set?</p>",2019-09-30T00:24:48Z,34,Week 9/29 - 10/5,feedback,,jqs38bj1ako389,k15o87jea8p65b,2019-09-30T00:24:48Z,{},project1
2032,no,"<p>Figure 3 has a different procedure than Figures 4, and 5.  The former is the &#34;first experiment&#34; and the latter is the &#34;second experiment&#34; per Sutton&#39;s paper.</p>
<p></p>
<p>Each one has a different protocol for when to update the weight vector.  The rule you listed applies to experiment 1 (update after training set), although you need to keep a running sum of the weight increments throughout the set until that update.</p>",2019-09-30T03:09:35Z,34,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k15u443ux9m37f,2019-09-30T03:09:35Z,{},project1
2033,no,"<p>Thank you, Vahe. I appreciate your help.</p>",2019-09-30T04:53:49Z,34,Week 9/29 - 10/5,feedback,,jqs38bj1ako389,k15xu5v3xkm4gj,2019-09-30T04:53:49Z,{},project1
2034,no,"<p>Looks like you are adding the admissible metric from A*. This $$w_d$$ value is a vector of distances of each state from the zero terminal state? What is the significance of $$d$$? Is this the current time step?</p>
<p></p>
<p>Wouldn&#39;t $$j = d$$ result in the partial being $$x_{i,j} &#43; i$$ ? </p>",2019-09-28T22:23:57Z,60,Week 9/22 - 9/28,followup,,jc554vxmyuy3pt,k144gxoidcx5nc,2019-09-28T22:23:57Z,{},office_hours
2035,no,"<p>Good questions! I&#39;m not intentionally making any parallels to A*, but in a way I&#39;m adding in a heuristic, so maybe there&#39;s some link? I see this more like &#39;feature engineering&#39; from non-RL ML. We know that in this particular game, there&#39;s a known solution where states are more valuable when they&#39;re further from A, so maybe we could add a term like &#34;distance to A&#34;, which just happens to be i. </p>
<p></p>
<p>I introduced $$w_d$$ to be a component of the parameter vector $$w$$, with the idea that it wouldn&#39;t be one of the $$w_j$$ terms with $$j \in [1,n]$$, but a new separate weight. I think that means the partial should drop the $$x_{i,j}$$ terms, but I don&#39;t think I made much of that clear.</p>",2019-09-29T02:26:08Z,59,Week 9/29 - 10/5,feedback,,jqwygbqmHAiE,k14d4e4oe217ot,2019-09-29T02:26:08Z,{},office_hours
2036,no,"<p></p>
<p>$$\frac{\partial}{\partial w_j} \Sigma_{j=1}^n (w_j*x_{t,j}) $$ will always be $$x_{t,d}$$ when $$j=d$$. For that reason, you will be always adding the $$w_d$$ term to the partial derivative at $$j=d$$. That happens because $$\frac{\partial}{\partial w_j} ( w_d*i )$$ at $$j=d$$ is the same as $$\frac{\partial}{\partial w_j} ( w_j*i )$$ which is just $$i$$.</p>
<p></p>
<p>Therefore, you end up with $$x_{t,d} &#43; i$$. Maybe I am misunderstanding what you mean by the $$j$$ subscript above.</p>
<p></p>",2019-09-29T04:54:03Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k14iem1yzly7j,2019-09-29T04:54:03Z,{},office_hours
2037,no,"<p>Cool idea!</p>
<p></p>
<p>I&#39;m wondering, how does this modify the MDP?  Or does it just modify the function approximator ($$w$$).  Since the old $$w$$ already encoded every state of the original MDP, would we be adding any new information by encoding the distance of a state from state A?</p>",2019-09-29T05:29:15Z,59,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k14jnvt042r4sa,2019-09-29T05:29:15Z,{},office_hours
2038,no,"<p>Earlier I was using python 2.7 and changed it to 3 now. This solved my issue with the graph. But, I am not sure how. Does anyone have any idea why it was like that? </p>",2019-09-29T08:35:27Z,59,Week 9/29 - 10/5,followup,,jl2bw1yzg5w1ej,k14qbbl7z3447r,2019-09-29T08:35:27Z,{},project1
2039,no,"<p>Wow! That&#39;s interesting... I have no idea, but I&#39;d like to hear some opinions on this.</p>",2019-09-29T23:22:43Z,59,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k15m0d9mzjh1ds,2019-09-29T23:22:43Z,{},project1
2040,no,"<p>Strange, because limiting the sequences length DOES reduce the error a lot on the right side of the graph... </p>",2019-09-29T23:21:07Z,26,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k15lyb12w9v76g,2019-09-29T23:21:07Z,{},project1
2041,no,"<p>how do you let the gym sample the action space?</p>
<p></p>",2019-09-30T21:44:49Z,26,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k16xyb8f7j335o,2019-09-30T21:44:49Z,{},hw3
2042,no,"<p></p><div>
<div>env.action_space.sample()</div>
</div>",2019-09-30T23:19:16Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k171bs4y8s36nd,2019-09-30T23:19:16Z,{},hw3
2043,no,<p>^^^^^^^^ do not use this ^^^^^^^^^^ this is bad for HW3 ^^^^^^^^^^^^</p>,2019-10-02T22:09:33Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19tptusekyu8,2019-10-02T22:09:33Z,{},hw3
2044,no,<p>:)</p>,2019-10-14T02:26:30Z,24,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1psqmo5dee64i,2019-10-14T02:26:30Z,{},hw3
2045,no,"<p>Second map is of size 25 (SFFFFHFFFFFFFFFFFFFFFFFFG), you can only reshape it as 5x5</p>",2019-09-29T02:02:59Z,26,Week 9/29 - 10/5,followup,,jqkxzdmmolGf,k14cam7mdl43eh,2019-09-29T02:02:59Z,{},hw3
2046,no,"I tried this too, setting custom-map = [‘SFFF’, ‘HFFF’, ‘FFFF’, ‘FFFG’] but when I then call env.render() i get:<div>&lt;0x1b&gt;[41ms&lt;0x1b&gt;[0mFFF</div><div>HFFF</div><div>FFFF</div><div>FFFG]</div><div>Any ideas what I’ve done wrong?</div>",2019-09-29T12:48:40Z,26,Week 9/29 - 10/5,followup,,jzvoaw0pl6326,k14zcyvxr6o15u,2019-09-29T12:48:40Z,{},hw3
2047,no,The parameter is called desc I think.  Not at a computer to check. <p></p>,2019-09-29T13:08:32Z,26,Week 9/29 - 10/5,feedback,,jzhl7qlwrpagr,k1502ixunxs6iv,2019-09-29T13:08:32Z,{},hw3
2048,stud,<p>In windows I am having this issue (<em>&lt;0x1b&gt;[41ms&lt;0x1b&gt;[0m</em> is probably the red cursor). I tried  in linux and It works fine .</p>,2019-09-29T14:44:58Z,26,Week 9/29 - 10/5,feedback,a_0,,k153ijbe8oo5f6,2019-09-29T14:44:58Z,{},hw3
2049,no,<p>I opened it using bash shell on windows and it displays the red highlight of the current state. If you have WinGit installed then you have MINGW and that gives you bash shell.</p>,2019-09-29T15:46:01Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k155p1r7hgd4ic,2019-09-29T15:46:01Z,{},hw3
2050,no,"After playing with the taxi environment too I concluded it’s a display problem (for me using Windows and sublime text), the program works just fine even though the environment renders strangely.  Thanks for your help.",2019-09-29T16:49:57Z,26,Week 9/29 - 10/5,feedback,,jzvoaw0pl6326,k157z9bs1l61iw,2019-09-29T16:49:57Z,{},hw3
2051,no,<p>So what is the rule to format the map? It has to be reshaped to a square? I think this type of conditions should be better specified in HW and project documents.</p>,2019-10-03T08:22:45Z,26,Week 9/29 - 10/5,followup,,jzjb6ojxtuc1d3,k1afmeem60h4el,2019-10-03T08:22:45Z,{},hw3
2052,no,<p>&#43;1</p>,2019-10-03T19:17:16Z,26,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1b3048dbja43d,2019-10-03T19:17:16Z,{},hw3
2053,no,"<p>from <a href=""https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"">https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py</a></p>
<p></p>
<pre>self.desc = desc = np.asarray(desc,dtype=&#39;c&#39;)
self.nrow, self.ncol = nrow, ncol = desc.shape</pre>
<p></p>",2019-10-03T19:20:48Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1b34ni4ma71dw,2019-10-03T19:20:48Z,{},hw3
2054,no,"I don&#39;t see how the instructor marked answer above here answers the initial question. Maybe I am confused. The initial question was regarding if we should reshape the flat string of 25 states into a 5x5 (I think so?). 

The answer above just says that the frozen lake code will take the dimensions of the string array we pass in.",2019-10-04T03:17:23Z,26,Week 9/29 - 10/5,feedback,,ixpwxv7xdgi1u6,k1bk5jmtryr3nq,2019-10-04T03:17:23Z,{},hw3
2055,no,"<p>Those two lines of code, which are from the frozen_lake source code, give a very good clue as to what kind of shape the environment is expecting.  Note the variables &#39;nrow&#39; and &#39;ncol&#39; in the second line.  And the main point of my post is that, instead of relying on what other people think about what the correct format should be, why not look at the actual code and know for sure?</p>
<p></p>
<p>Also, my answer was <em>not</em> marked or approved by an instructor.</p>",2019-10-04T03:39:54Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1bkyifvz8l654,2019-10-04T03:39:54Z,{},hw3
2056,no,"<p>Thank you Vahe. Sorry I haven&#39;t been clear on my question, which slightly sided away from the initial post. I understand that how Fronzenlake take the data format in. I was asking what the rule to convert a string given in HW3 to a what shape of an array. Say I have a 16-size map, it can be 4-by-4 or 2-by-8 or even 1-by-16, which I assume would result in different behaviors. Given all current examples are squares, I probably better stick with it. I&#39;m just feel this type of &#39;assumption&#39; was stated clearly in the HW3 description.</p>",2019-10-06T05:26:21Z,25,Week 10/6 - 10/12,feedback,,jzjb6ojxtuc1d3,k1ejn3kw8dd5ci,2019-10-06T05:26:21Z,{},hw3
2057,no," I thought it was. I see your point now, I was just confused. ",2019-10-04T03:41:55Z,26,Week 9/29 - 10/5,followup,,ixpwxv7xdgi1u6,k1bl13a8fe37m4,2019-10-04T03:41:55Z,{},hw3
2058,no,"<p><md><br />Please check the output of the function ***generate_random_map***, and you will see the map format.<br />&#96;&#96;&#96;Python<br />gym.envs.toy_text.frozen_lake.generate_random_map()<br />&#96;&#96;&#96;<br /></md></p>",2019-10-04T09:43:22Z,26,Week 9/29 - 10/5,followup,,jzj0om7qnbd4yf,k1bxxx25qiv71b,2019-10-04T09:43:22Z,{},hw3
2059,no,"<p>Thank you. I understand this part. My question is does a map has to be refactored into a square? I can make this assumption given the examples in both HW3 and Gym. But it never explicitly stated anywhere else. Maybe I missed something? Say a 16-size map can also be refactored into 2-by-8, right? </p>",2019-10-06T05:14:38Z,25,Week 10/6 - 10/12,feedback,,jzjb6ojxtuc1d3,k1ej81qpk9v13j,2019-10-06T05:14:38Z,{},hw3
2060,no,"<p>No, I don&#39;t think they have to be square at all.  That is pretty cool out-of-the-box thinking.  You *do* need a two-dimensional shape, but one of the dimensions could be $$1$$ and it would still run.  I think for the homework to pass though, you need to make it a square.</p>",2019-10-06T05:32:36Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1ejv4z160o1dp,2019-10-06T05:32:36Z,{},hw3
2061,stud,"<p>Do you mean the maximum length of a walk? I tried to tune that parameter, and it did help my figure 4 but figure 5 starts to look like figure 3. </p>
<p></p>
<p>However, by averaging over 1000 sequences, both of my figure 4 and 5 start to look like Sutton&#39;s. </p>",2019-09-29T02:17:52Z,27,Week 9/22 - 9/28,followup,a_0,,k14ctr1u2zv1jz,2019-09-29T02:17:52Z,{},project1
2062,no,"<p>Yes, that was the one I as referring to. Ah, for me tuning that parameter mainly affected figure 4 and I couldn&#39;t get figure 5 to match Sutton no matter how much I experiment with walk length restrictions.</p>
<p></p>
<p>That last point is interesting. It might suggests the hidden parameter, if he used one, was effective as decreasing variance or making it converge to reasonable values more quickly.</p>",2019-09-29T04:11:27Z,27,Week 9/22 - 9/28,feedback,,jqstjkc1dh1602,k14gvtdgm045tu,2019-09-29T04:11:27Z,{},project1
2063,no,"<p>I think the apparent linearity of figure 4 might suggest that their is an error in your discount of past states across time. Rather than getting the compounding discounts (gamma &#43; gamma^2 &#43; ....) you might be effectively getting just gamma.</p>
<p></p>",2019-09-29T08:57:14Z,59,Week 9/29 - 10/5,followup,,jqstjkc1dh1602,k14r3cjhlm46jo,2019-09-29T08:57:14Z,{},project1
2064,no,"<p>Thanks for the suggestion.</p>
<p>I was thinking something like that too.</p>
<p>I zoomed in on my $$e_{t}$$ calculation for a test sequence and it looks ok to me.</p>
<p></p>
<p>Here&#39;s what I tried</p>
<ul><li>Sequence: [D,C,D,E,F,G] </li><li>lambda: 0.1</li><li>$$e_{t}$$ for each iteration
<ul><li>0: [0. 0. 1. 0. 0.]<br />1: [0. 1. 0.1 0. 0. ]<br />2: [0. 0.1 1.01 0. 0. ]<br />3: [0. 0.01 0.101 1. 0. ]<br />4: [0. 0.001 0.0101 0.1 1. ]</li></ul>
</li></ul>
<p></p>
<p>This looks ok to me - like I&#39;m picking up the additional lambda for each step in the future.</p>
<p>Any ideas?</p>
<p></p>
<p>Thanks again.</p>",2019-09-29T11:37:58Z,59,Week 9/29 - 10/5,feedback,,jzhnj3cj4pl2uy,k14wu1vmb7g6ks,2019-09-29T11:37:58Z,{},project1
2065,no,"<p>I found a bug.</p>
<p></p>
<p>In my head I understood that for experiment 2, we are supposed to calculate $$\Delta w$$ for the sequence, use that result to update $$w^{T}$$, and use the new $$w^{T}$$ as a starting point for the next sequence in the training set.</p>
<p>However, my code was not written as such.</p>
<p>I had it starting each sequence with $$w^{T}=[0.5, 0.5, 0.5, 0.5, 0.5]$$ and then only updating the cumulative total at the end of the training set.</p>
<p></p>
<p>I might have a few more kinks to work out, but it looks a lot closer, so I&#39;m happy.</p>
<p>Thanks for helping me think it through.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhnj3cj4pl2uy%2Fk1541gty3paz%2FScreen_Shot_20190929_at_10.58.15_AM.png"" alt="""" width=""519"" height=""779"" /></p>",2019-09-29T15:05:22Z,59,Week 9/29 - 10/5,feedback,,jzhnj3cj4pl2uy,k1548rvqeas48s,2019-09-29T15:05:22Z,{},project1
2066,no,<p>That&#39;s interesting because I have the opposite problem.  Mine has the characteristic curve but the error values are ~0.11 to ~0.17 range</p>,2019-09-29T17:10:31Z,26,Week 9/29 - 10/5,followup,,idfzuuu9fnI,k158ppsjfgw3rf,2019-09-29T17:10:31Z,{},project1
2067,no,Did you figured out your issue ? I have the same exact issue and it’s making me nuts. Have struggled all day today and yersterday cannot see what my issue ,2019-09-29T18:15:31Z,26,Week 9/29 - 10/5,feedback,,jqkuetouttn5,k15b1ajhw0k7hg,2019-09-29T18:15:31Z,{},project1
2068,no,"<p>Thanks to Daniel Smith help, I figured out its because I calculate RMS wrong (I was using sum instead of mean). After changing that, it&#39;s now to the 0.11-017 range.</p>",2019-09-29T22:22:52Z,26,Week 9/29 - 10/5,feedback,,jzjb6ojxtuc1d3,k15jvdvqcuo43n,2019-09-29T22:22:52Z,{},project1
2069,no,"<p>I&#39;m having the same problem... 0.11-0.17 range, which is incorrect (well, maybe more correct?). Any luck to get into 0.19-0.25 range?</p>",2019-09-30T07:21:27Z,26,Week 9/29 - 10/5,feedback,,jl9m84myFYYZ,k16340ctrfo50c,2019-09-30T07:21:27Z,{},project1
2070,no,"<p>There is this in the writeup:</p>
<p></p>
<p>&#34;<em>improve π while increasing the policy’s greediness.</em>&#34;</p>
<p></p>
<p>Does this mean we decay the epsilon after each test?</p>",2019-09-29T15:37:06Z,25,Week 9/29 - 10/5,followup,,jc554vxmyuy3pt,k155dkocd1n2ue,2019-09-29T15:37:06Z,{},hw3
2071,no,"<p>I didn&#39;t, and I seem to be getting the correct answers now. </p>",2019-09-29T16:39:13Z,25,Week 9/29 - 10/5,feedback,,jl2bq5rf8b67pq,k157lg8iy072u0,2019-09-29T16:39:13Z,{},hw3
2072,no,"<p>Good catch. In order to guarantee convergence epsilon needs to converge in the limit to 0, but for this homework epsilon is fixed.</p>",2019-09-29T18:29:13Z,25,Week 9/29 - 10/5,feedback,,jzlyi4e55bz5kj,k15bix8r12l4l0,2019-09-29T18:29:13Z,{},hw3
2073,no,"<p>So in the test run, the agent is moving around, sometimes doing what Q tells it to do, sometimes slipping on the ice. Should the optimal policy output be what Q is telling it to do, or what the agent is actually doing?</p>",2019-09-29T15:54:16Z,25,Week 9/29 - 10/5,followup,,jl2bq5rf8b67pq,k155znnlobb3b0,2019-09-29T15:54:16Z,{},hw3
2074,no,"<p>The optimal policy is just a pass that for each state, prints the best action per state, based on what you trained.  The solution should be the same size as the map, for each cell you&#39;ll have a best action to take.</p>",2019-09-29T16:23:46Z,25,Week 9/29 - 10/5,feedback,,jzhl7qlwrpagr,k1571l96ng57iw,2019-09-29T16:23:46Z,{},hw3
2075,no,<p>Thank you! I was wayyyy overthinking it.</p>,2019-09-29T16:27:55Z,25,Week 9/29 - 10/5,feedback,,jl2bq5rf8b67pq,k1576x6cme643c,2019-09-29T16:27:55Z,{},hw3
2076,no,<p>I was not able to get the correct answers at first. I tried to debug but I can not figure out where it goes wrong. It turns out that I forgot to set env = env.unwrapped. </p>,2019-09-29T17:29:43Z,25,Week 9/29 - 10/5,followup,,ixqtso0lS12,k159eee4w6r7ac,2019-09-29T17:29:43Z,{},hw3
2077,no,<p>What does unwrapped actually do? Same thing happened to me but am curious what this does.</p>,2019-09-29T18:28:29Z,25,Week 9/29 - 10/5,feedback,,jcg0nzvdk8272b,k15bhyvhy396ku,2019-09-29T18:28:29Z,{},hw3
2078,no,"<p>Based on the information here, it removes all the extra wrappers that surround the environment, and returns you the inner environment. </p>
<p><a href=""https://github.com/openai/gym/tree/master/gym/wrappers"">https://github.com/openai/gym/tree/master/gym/wrappers</a></p>
<p></p>
<p>I was looking at both the wrapped and unwrapped version in the pycharm debugger and it looks like it basically strips some of the functions away; which make since based on that the wrapper just provide extra abilities around the environment.</p>",2019-09-29T18:59:06Z,25,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k15clclmbsw78d,2019-09-29T18:59:06Z,{},hw3
2079,no,<p>The key for me was <br /><br />&#34;2. Your agent&#39;s policy is not the history of actions taken corresponding to an experienced trajectory. It is the function the agent uses to select the action to take at each state. So your output for this homework should be the trained agent&#39;s preferred action at each state (the output should be the same length as the map).&#34;</p>,2019-09-30T19:51:10Z,25,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k16tw5jk1o62lw,2019-09-30T19:51:10Z,{},hw3
2080,no,<p>Thanks both of you. After your answer I re-checked and find my typo.</p>,2019-09-30T04:44:51Z,59,Week 9/29 - 10/5,followup,,jzygktecb0e3i1,k15ximh860o5hg,2019-09-30T04:44:51Z,{},hw3
2081,no,<p>I&#39;m certainly no python expert.  Now I&#39;m concerned whether or not I&#39;m using the correct OpenAI gym for the homeworks too.  Be nice if there was an installation procedure to ensure we did things correctly.</p>,2019-09-30T13:41:30Z,59,Week 9/29 - 10/5,followup,,ixpclzk97jg2fo,k16gorhbv9g6u9,2019-09-30T13:41:30Z,{},project2
2082,no,<p>Lunar lander has not changed in a while. LunarLander-v2 is still current as per the project instructions.</p>,2019-10-01T04:04:22Z,59,Week 9/29 - 10/5,feedback,,jcg0nzvdk8272b,k17biey6y3z6qk,2019-10-01T04:04:22Z,{},project2
2083,no,"<p>Can we use the same version on OpenAI gym for the homeworks and the project?</p>
<p></p>
<p>I have downloaded the latest source and built it. Will it cause issues with HW3 and HW4?</p>",2019-10-02T13:01:06Z,59,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k19a4i9ga492fn,2019-10-02T13:01:06Z,{},project2
2084,no,<p>yes you can. I did. You will have to modify the map for taxi-v3 - there is a piazza post for that. just search for taxi-v3</p>,2019-10-02T18:23:06Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19lmm6g25r7hl,2019-10-02T18:23:06Z,{},project2
2085,no,<p>We&#39;ll probably update the Taxi homework next semester... v3 must have been recently introduced.</p>,2019-10-04T18:38:37Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1ch297nydy43e,2019-10-04T18:38:37Z,{},project2
2086,no,"<p>It may help to look more at the overall algorithm than at the individual update rules. SARSA is based on the transition from a state-action pair to the next state-action pair. The learning comes from State-Action-Reward-State-Action. A&#39; is determined with the epsilon-greedy method, meaning it may end up being a random value used to update Q. In Q-Learning, A&#39; is not explicitly required, and even though A is determined by epsilon-greedy, the A&#39; used to update Q is always the best action possible. </p>
<p></p>
<p>So, in SARSA, the A&#39; used to update Q may be the best action, may be the random action. In Q-Learning, the A&#39; to update Q is always the best action. </p>
<p></p>
<p>At least, this is my intuition, hopefully it is correct/makes sense. </p>",2019-09-30T00:49:47Z,43,Week 9/29 - 10/5,followup,,jl2bq5rf8b67pq,k15p4bs18qp4rh,2019-09-30T00:49:47Z,{},hw3
2087,no,"For HW3, is that ON or OFF policy?",2019-10-04T14:00:34Z,43,Week 9/29 - 10/5,feedback,,j6lg2nbgton2on,k1c74oz3cxs5ef,2019-10-04T14:00:34Z,{},hw3
2088,no,<p>ON policy (SARSA)</p>,2019-10-04T15:02:45Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c9cnhfdnh3l,2019-10-04T15:02:45Z,{},hw3
2089,no,"<p>HW4 =&gt; OFF policy (Q-learning)</p>
<p></p>
<p>We got you guys covered!</p>",2019-10-04T18:37:45Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1ch1578aq130q,2019-10-04T18:37:45Z,{},hw3
2090,stud,LOL,2019-10-05T00:11:09Z,43,Week 9/29 - 10/5,feedback,a_0,,k1csxwhzck35vj,2019-10-05T00:11:09Z,{},hw3
2091,no,"<p>Check silver&#39;s lecture 5: you can see the pseudo-code of SARSA vs <br />Q learning for off policy. <strong>you can see the difference between them is how we select the next action A&#39; at the step where we update Q(S,A)</strong>. The policy we are following is known as the behavior policy while the other policy is known as the target policy. Let behavior policy be epsilon greedy while target to be greedy.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>In my understanding, for the on policy case, given S&#39;, we will select the A&#39; for  Q(S&#39;,A&#39;) using the same policy we are following (for example epsilon greedy, this means the A&#39; given S&#39; may have a chance to not be the best action by definition). This A&#39;will also be the action we will be taking in the next time step. </p>
<p></p>
<p>Whereas for the off-policy case, given S&#39;, we will select the a for max Q(S&#39;,a)  (by definition, max= greedy, which is based on target policy instead of  epsilon greedy). However, the a just chosen for max Q(s&#39;,a) to update Q(s,a) is not the action we will be taking next time step. We will still use the epsilon greedy to select the action next time step. </p>
<p></p>
<p></p>
<p>Also one more source: <a href=""https://stackoverflow.com/questions/6848828/what-is-the-difference-between-q-learning-and-sarsa"">https://stackoverflow.com/questions/6848828/what-is-the-difference-between-q-learning-and-sarsa</a></p>",2019-09-30T10:24:27Z,43,Week 9/29 - 10/5,followup,,jvfpllmsggt7p4,k169nd477w5636,2019-09-30T10:24:27Z,{},hw3
2092,no,"<p>&#34;<strong>[...] the difference between them is how we select the next action [...]&#34;</strong></p>
<p></p>
<p>I kinda disagree this is the difference. It may be a programming difference, but it is not the main difference. They both select an action using [typically] an epsilon-greedy policy. They select the next action the same exact way.</p>
<p></p>
<p>The main difference is that Sarsa takes this action selection into account. Sarsa evaluates the policy, meaning it estimates the action-value function Q, using this selected actions, while Q-learning doesn&#39;t. Q-learning doesn&#39;t care about the action it will take in the next step while estimating the action-value function Q, instead Q-learning &#34;pretends&#34; it will take the greedy action, the action that it estimates will give it the max expected return. But, Q-learning may or may not pick that action on the next step!</p>
<p></p>
<p>So, again, the main difference is not how they select the action, but how they update the Q-function. Sarsa uses the action it will take next to calculate the current, Q-learning pretends it will always take the best action, even if it doesn&#39;t.</p>
<p></p>",2019-10-06T00:53:45Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1e9wjhj6q154z,2019-10-06T00:53:45Z,{},hw3
2093,no,<p>So on-policy/off-policy $$\neq $$ model/model-free?</p>,2019-10-04T03:38:01Z,43,Week 9/29 - 10/5,followup,,gx3c8l7z7r72zl,k1bkw3eabph4tu,2019-10-04T03:38:01Z,{},hw3
2094,no,"<p>No, those are two completely different sets of terms.  All of the algorithms we&#39;re learning now (SARSA, Q-learning) are model-free.</p>
<p></p>
<p>I&#39;m not sure if the terms on-policy/off-policy apply to model-based algorithms, like value iteration and policy iteration.  In those two algorithms, there isn&#39;t a fixed policy that we&#39;re exploring with.</p>",2019-10-04T04:14:02Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1bm6eoa8sdka,2019-10-04T04:14:02Z,{},hw3
2095,no,<p>I think the lecture &#39;Exploring Exploration&#39; puts a bound on this.</p>,2019-09-30T03:06:53Z,43,Week 9/29 - 10/5,followup,,jl3we43d3bp15p,k15u0mq6nw11dx,2019-09-30T03:06:53Z,{},hw4
2096,no,<p>I am having the same issue. it does nothing after clicking on &#34;Sign in&#34; </p>,2019-09-30T08:42:10Z,26,Week 9/29 - 10/5,followup,,jl2gfssgneuU,k165ztda6th4jh,2019-09-30T08:42:10Z,{},hw3
2097,no,"<p>it works when you use the https://​rldm.herokuapp.com  rather then clicking the link from file.</p>
<p></p>
<p>But i dont see assignments are posted.</p>",2019-09-30T08:44:41Z,26,Week 9/29 - 10/5,followup,,jl2gfssgneuU,k16631v8b1u4ep,2019-09-30T08:44:41Z,{},hw3
2098,no,<p>I still don&#39;t see hw3 or hw4 yet.  </p>,2019-09-30T12:48:16Z,26,Week 9/29 - 10/5,feedback,,j6lnhjhnzr3y5,k16esauatzb26h,2019-09-30T12:48:16Z,{},hw3
2099,no,<p>Yes.. It works with this URL. Thanks for sharing. </p>,2019-09-30T13:12:53Z,26,Week 9/29 - 10/5,feedback,,jzg6jh2hn6f43c,k16fnyicdrmev,2019-09-30T13:12:53Z,{},hw3
2100,no,<p>me either</p>,2019-09-30T16:41:14Z,26,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k16n3w8xzjl1wt,2019-09-30T16:41:14Z,{},hw3
2101,no,<p>I can not login to herokuapp with my username and GTID. Others have this problem?</p>,2019-09-30T17:09:10Z,26,Week 9/29 - 10/5,followup,,j6ll2xkiDJf,k16o3tm7x413bl,2019-09-30T17:09:10Z,{},hw3
2102,no,"Yeah can’t login. <div><br /></div><div>When/if I can log in, the problems have yet to be posted </div>",2019-09-30T17:34:53Z,26,Week 9/29 - 10/5,followup,,jlcjoyw4Fsyi,k16p0wlhgx33jl,2019-09-30T17:34:53Z,{},hw3
2103,no,<p>I still cannot login. Please suggest a fix</p>,2019-10-02T01:14:52Z,26,Week 9/29 - 10/5,followup,,jqtnwove8X4N,k18kwao1h1z68i,2019-10-02T01:14:52Z,{},hw3
2104,no,"<p>Are you using this link?</p>
<p></p>
<p><a href=""https://rldm.herokuapp.com"">https://rldm.herokuapp.com</a><a href=""http://rldm.herokuapp.com/student/"" target=""_blank"" rel=""noopener noreferrer""></a></p>
<p></p>
<p>If that doesn&#39;t work please make a private post</p>",2019-10-02T01:16:24Z,26,Week 9/29 - 10/5,feedback,,hz7meu55mi8sd,k18ky9hjpc455a,2019-10-02T01:16:24Z,{},hw3
2105,no,<p>open gym has a frozen lake...</p>,2019-09-30T16:36:19Z,26,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k16mxkicujh7dw,2019-09-30T16:36:19Z,{},hw3
2106,no,<p>sO yOu doN&#39;t nEeD pybox2d</p>,2019-09-30T19:51:55Z,26,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k16tx4qlx1v4vq,2019-09-30T19:51:55Z,{},hw3
2107,no,<p>pybox2d is not required for HW3 but it will be needed for Project 2 so no harm in installing it now.</p>,2019-10-01T00:09:07Z,26,Week 9/29 - 10/5,feedback,,hz7meu55mi8sd,k1733vu0l8v554,2019-10-01T00:09:07Z,{},hw3
2108,no,"<p>Do we need mujoco-py for project 2? It says students can get a free license for persomal projects, but not if more that half the class is using MuJoCo</p>
<table><tbody><tr><td width=""170"">Personal Student</td><td></td><td>Free</td><td></td><td>Available to full-time students for use in personal projects only. The software may not be used as part of employment, in projects receiving financial support, or in class projects where more than half of the class is using MuJoCo. Requires school email address.</td></tr></tbody></table>",2019-10-01T12:30:55Z,26,Week 9/29 - 10/5,followup,,j6pmq1sglzo35i,k17tlu89p2q3zx,2019-10-01T12:30:55Z,{},hw3
2109,no,<p>Mujoco will be necessary for the lunar landar on ubuntu or MacOS. I don&#39;t know that you can use Box2D for those platforms (not sure why you wouldn&#39;t be able to). you need a physics package for lunar lander.</p>,2019-10-01T14:09:04Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k17x42g05sc2a5,2019-10-01T14:09:04Z,{},hw3
2110,no,"<p>Sorry, since when Lunar Lander requires MuJoCo?! I don&#39;t think this is the case. Lunar Lander uses Box2D for rendering, but pretty basic physics equation for calculating motion.</p>
<p></p>
<p>I know they keep improving the environments, but it doesn&#39;t seem to be the case for the Lunar Lander:</p>
<p></p>
<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py"">https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py</a></p>
<p></p>",2019-10-04T18:34:52Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cgxfkqtbl635,2019-10-04T18:34:52Z,{},hw3
2111,no,"<p>Just a note that the only way that I got Lunar Lander to work on MacOS without MuJoCo was downgrading my python to 3.4 and installing Box2D with conda pkg a la: <a href=""https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md#install-pre-built-conda-package-its-easy"">https://github.com/pybox2d/pybox2d/blob/master/INSTALL.md#install-pre-built-conda-package-its-easy</a>, then &#96;pip install gym&#96;</p>
<p></p>",2019-10-09T15:16:01Z,25,Week 10/6 - 10/12,feedback,,jl2bqn4biet72,k1jf0yyp5kc1x2,2019-10-09T15:16:01Z,{},hw3
2112,no,"<p></p>
<p>Jacob Thanks for randint answer.</p>
<p></p>
<p>for square map, what I was asking that in order to create the environment, do we need to create the square map  in order to proceed. or could the problem be done without without converting to square map ( as told in &#64;432)</p>
<p></p>
<p>In other words, is following good enough to create one sample env </p>
<p></p>
<p>env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(&#34;SFFG&#34;).unwrapped</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-09-30T18:35:36Z,25,Week 9/29 - 10/5,followup,,jc9nkspumds4ki,k16r6zjt53soi,2019-09-30T18:35:36Z,{},hw3
2113,no,"<p>yes but instead of &#34;SFFG&#34;, you need to add the custom map.</p>",2019-10-01T00:17:08Z,25,Week 9/29 - 10/5,feedback,,hz7meu55mi8sd,k173e72xgps3ah,2019-10-01T00:17:08Z,{},hw3
2114,no,"<p>Can you elaborate on this a little? I have tried both the &#34;SFFG&#34; and the custom_map in the way Neeraj described and i keep getting a:<br /><br />line 105, in __init__<br />    self.nrow, self.ncol = nrow, ncol = desc.shape<br />ValueError: not enough values to unpack (expected 2, got 0)</p>
<p></p>
<p>when using: env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(custom_map).unwrapped</p>
<p></p>
<p></p>",2019-10-01T13:46:36Z,25,Week 9/29 - 10/5,feedback,,jzttp1ojahj6ju,k17wb6ig68677x,2019-10-01T13:46:36Z,{},hw3
2115,no,"<p>You can look at the source code frozen_lake.py to see why that error is being generated:</p>
<p></p>
<pre>self.desc = desc = np.asarray(desc,dtype=&#39;c&#39;)<br />self.nrow, self.ncol = nrow, ncol = desc.shape</pre>
<p></p>
<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"">https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py</a></p>
<p></p>
<p></p>",2019-10-01T16:50:08Z,25,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k182v770n0v7op,2019-10-01T16:50:08Z,{},hw3
2116,no,"<p>So the answer is . Yes, we need to create square maps from the map provided ( take square root of length to get one side)  and then create env as described in &#64;432. That&#39;s how I solved it.</p>
<p></p>
<p>Following articles helped me a lot in understanding the open AI gym environment and creating custom maps</p>
<p></p>
<p><a href=""https://twice22.github.io/rl-part1/"">https://twice22.github.io/rl-part1/</a></p>
<p><a href=""https://reinforcementlearning4.fun/2019/06/16/gym-tutorial-frozen-lake/"">https://reinforcementlearning4.fun/2019/06/16/gym-tutorial-frozen-lake/</a></p>
<p></p>
<p></p>",2019-10-01T17:20:50Z,25,Week 9/29 - 10/5,feedback,,jc9nkspumds4ki,k183yoe3iu66px,2019-10-01T17:20:50Z,{},hw3
2117,no,"<p>&#64;Vahe <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>I reviewed the frozen lake source code and found that it can take a 1D array costume enviorment. So can we get a little more guidance on what we are suppose to do with the input value of &#34;amap=SFFG&#34;?</p>
<p></p>
<p>Is it safe to assume we always need to transform it into a square?</p>",2019-10-04T19:37:33Z,25,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1cj61uf8i132b,2019-10-04T19:37:33Z,{},hw3
2118,no,"<p>&#34;<em>Is it safe to assume we always need to transform it into a square?&#34;</em></p>
<p><em></em></p>
<p>That&#39;s what I did.</p>",2019-10-04T19:45:46Z,25,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cjgmkd9sd5y5,2019-10-04T19:45:46Z,{},hw3
2119,no,<p>Thank you Vahe!</p>,2019-10-04T19:47:55Z,25,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1cjje5gxlx15i,2019-10-04T19:47:55Z,{},hw3
2120,no,"<p>Yeap, maybe something we should improve in the document and make it very clear that we are passing a grid world. Good point!</p>
<p></p>",2019-10-04T20:34:30Z,25,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cl7abvhn45dq,2019-10-04T20:34:30Z,{},hw3
2121,no,"<p><strong attention=""hyxsfbkeit22m2"">&#64;Alec Feuerstein</strong> maybe you can help us with that?</p>",2019-10-04T20:35:12Z,25,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cl86n8z2d6a8,2019-10-04T20:35:12Z,{},hw3
2122,no,"<p>I have a follow-up question regarding randint(). If I understand correctly, we need to compare a random value with epsilon, which is in range (0, 1). But randint() generate integers. Then random.uniform(0, 1) should be used as instead. Do I misunderstand something here?</p>",2019-09-30T18:45:35Z,25,Week 9/29 - 10/5,followup,,jzg6jh2hn6f43c,k16rjthppdn5oj,2019-09-30T18:45:35Z,{},hw3
2123,no,"<p>which you are doing okay. After deciding to take exploring policy, you need to chose action from action space ( 0:left, 1:right, 2:down, 3:up) .</p>
<p>These actions you chose will be decided by randint which should randomly chose from above action space</p>",2019-09-30T20:24:28Z,25,Week 9/29 - 10/5,feedback,,jc9nkspumds4ki,k16v2z27rlk6rn,2019-09-30T20:24:28Z,{},hw3
2124,no,<p>Thanks for the confirmation. </p>,2019-09-30T21:24:54Z,25,Week 9/29 - 10/5,feedback,,jzg6jh2hn6f43c,k16x8ox2qd076o,2019-09-30T21:24:54Z,{},hw3
2125,no,How does randint( ) can be looked like  for 5X5 map?,2019-10-01T03:49:48Z,25,Week 9/29 - 10/5,feedback,,jzw0xjg6eya610,k17azohcpj81in,2019-10-01T03:49:48Z,{},hw3
2126,no,"<p>no need to create 5x5 map . As I said earlier, you just need to pick only one of 4 values randomly using radint() ie. radint should generate a single number between 0 to 3.</p>",2019-10-01T17:06:30Z,25,Week 9/29 - 10/5,feedback,,jc9nkspumds4ki,k183g8vb38f5ou,2019-10-01T17:06:30Z,{},hw3
2127,no,"<p> ( 0:left, 1:right, 2:down, 3:up)   should be  ( 0:left, 1:down, 2:right, 3:up) .<br /></p>",2019-10-05T18:42:11Z,25,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dwmoyq7pl2dv,2019-10-05T18:42:11Z,{},hw3
2128,no,"<p>Shouldn&#39;t we also use epsilon to choose action, where epsilon probability we pick max and (1-epsilon random)?</p>",2019-09-30T20:41:31Z,26,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k16vowcnzw750r,2019-09-30T20:41:31Z,{},hw3
2129,no,"<p>Yes, but it is used in the evolution. Once it is done, we could use max(Q) as output policy. </p>",2019-09-30T21:35:53Z,26,Week 9/29 - 10/5,feedback,,jzg6jh2hn6f43c,k16xmtnvsga2gm,2019-09-30T21:35:53Z,{},hw3
2130,no,"<p>I am getting following:</p>
<p></p>
<p>v,^,^,^,&lt;,^,^,^,&gt;,&gt;,v,&gt;,v,v,&gt;,&lt;</p>
<p></p>
<p></p>
<p>Not sure what I am doing incorrect. As per example, it should be </p>
<p></p>
<p>^,v,v,&gt;,&lt;,&gt;,&gt;,v,v,v,&gt;,v,&gt;,&gt;,&gt;,&lt;</p>
<p></p>",2019-09-30T20:51:44Z,26,Week 9/29 - 10/5,followup,,jc9nkspumds4ki,k16w21h5nwy7nz,2019-09-30T20:51:44Z,{},hw3
2131,no,<p>Answer given by Rui Yuan saved the day. Thanks</p>,2019-09-30T21:51:14Z,26,Week 9/29 - 10/5,feedback,,jc9nkspumds4ki,k16y6k93j5320,2019-09-30T21:51:14Z,{},hw3
2132,no,<p>what was it?</p>,2019-10-01T02:33:16Z,26,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k17899gxmjecy,2019-10-01T02:33:16Z,{},hw3
2133,no,"<p>This is what I got. Not sure why the first step is down to the Hole.</p>
<p></p>
<p>v,&gt;,&gt;,&gt;,&lt;,&gt;,&gt;,&gt;,v,^,&gt;,v,&gt;,&gt;,^,&lt;</p>
<p></p>",2019-10-01T06:06:32Z,26,Week 9/29 - 10/5,feedback,,j6ln9puq99s5uv,k17fvj0y7d2456,2019-10-01T06:06:32Z,{},hw3
2134,no,<p>Looks like the policy is far from convergence in the current settings. How to debug that I can produce exactly the same policy update as the answers? Not sure if the random seed is set up the same. I set up both env and NumPy seed at the before the first episode. </p>,2019-10-01T06:11:39Z,26,Week 9/29 - 10/5,feedback,,j6ln9puq99s5uv,k17g244e90d67u,2019-10-01T06:11:39Z,{},hw3
2135,no,"<p>question</p>
<p>i am little confuse on how to  print this:</p>
<p></p>
<pre>v,^,^,^,&lt;,^,^,^,&gt;,&gt;,v,&gt;,v,v,&gt;,&lt;</pre>
<p>any tips to do print this after the matrix</p>
<p></p>
<p>thank you</p>",2019-10-03T02:57:43Z,26,Week 9/29 - 10/5,followup,,jqtti9gfnagH,k1a40eghjr51j2,2019-10-03T02:57:43Z,{},hw3
2136,no,"<p>Try np.argmax() to get the best action given the learned Q table, then map actions from [0, 4) to [&#39;&lt;&#39;, &#39;v&#39;, &#39;&gt;&#39;, &#39;^&#39;] (order matters! See the source code of frozen lake environment for the mapping).</p>",2019-10-03T19:23:52Z,26,Week 9/29 - 10/5,feedback,,jzhy4sbxyar5l4,k1b38lq66aacx,2019-10-03T19:23:52Z,{},hw3
2137,no,<p>thank you</p>,2019-10-04T00:18:41Z,26,Week 9/29 - 10/5,feedback,,jqtti9gfnagH,k1bdrqwne74vt,2019-10-04T00:18:41Z,{},hw3
2138,no,"<p>just as a note for others -- after testing, argmax() should be used for selection on both the action selection from the Q table and the action selection within the epsilon greedy policy (if it uses the greedy action selection).</p>",2019-10-04T02:10:56Z,26,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1bhs3gofqy65d,2019-10-04T02:10:56Z,{},hw3
2139,no,<p>I think argmax is fine.</p>,2019-10-04T02:15:05Z,26,Week 9/29 - 10/5,feedback,,jzhy4sbxyar5l4,k1bhxfak9h94rm,2019-10-04T02:15:05Z,{},hw3
2140,no,"<p>correct, i was just noting it needs to be used both in selecting from the final q table and in the epsilon greedy policy function. thank you</p>",2019-10-04T02:15:59Z,26,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1bhyl7ug9u5l3,2019-10-04T02:15:59Z,{},hw3
2141,no,<p>how are you generating epsilon?</p>,2019-10-01T14:22:18Z,26,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k17xl3avpsxpe,2019-10-01T14:22:18Z,{},hw3
2142,no,<p>make it up. try different epsilons.</p>,2019-10-01T16:00:49Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1813s7s18n5du,2019-10-01T16:00:49Z,{},hw3
2143,no,<p>thanks it was due to a bug.</p>,2019-10-01T02:30:11Z,26,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k1785alchrc7dg,2019-10-01T02:30:11Z,{},hw3
2144,no,"<p>Can you please point  where was you  bug on code .I am facing same issue and  have same  Martix at Last column at  0 .</p>
<p></p>
<p>I am terminating after updating Q  .</p>
<p></p>
<p>1) Loop while true</p>
<p>2) update state/reawards/acions </p>
<p>3) update Q</p>
<p>4) if Fall  into H or G  , then break </p>
<p>5) update new states and actions </p>
<p></p>
<p>Plz any pointers what i am doing  wrong ?</p>",2019-10-05T13:26:05Z,26,Week 9/29 - 10/5,feedback,,jzjvnayuw8t2br,k1dlc777ju9116,2019-10-05T13:26:05Z,{},hw3
2145,no,"<p>Hey Angel,<br />I seem to be stuck where you were. Any hints on your bug?</p>",2019-10-05T19:31:21Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dydxia46s2p9,2019-10-05T19:31:21Z,{},hw3
2146,no,"<p>The heroku app won&#39;t let me login? It worked 3 hours ago, but now it returns to the login screen after I enter my login information...</p>",2019-10-01T16:12:45Z,43,Week 9/29 - 10/5,followup,,jc554vxmyuy3pt,k181j4e2ghm2xw,2019-10-01T16:12:45Z,{},hw3
2147,no,"<p>It works for me:</p>
<p><a href=""https://rldm.herokuapp.com/assignment/sarsa/"">https://rldm.herokuapp.com/assignment/sarsa/</a></p>
<p>Make sure you use https instead of http. It does not work with http anymore.</p>",2019-10-01T16:48:03Z,43,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k182sikptqo55s,2019-10-01T16:48:03Z,{},hw3
2148,no,"<p>Jacob, last night I had the same problem, and after about 10 failed attempts, maybe a total of half an hour later, I suddenly got in.  I would enter my login info and it would clear the fields as if I had entered the information incorrectly.</p>",2019-10-01T16:55:58Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1832p6u8r232w,2019-10-01T16:55:58Z,{},hw3
2149,no,<p>https did the trick.</p>,2019-10-01T17:03:44Z,43,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k183cop037629w,2019-10-01T17:03:44Z,{},hw3
2150,no,<p>was having the same problem. Solution was https as well. Thanks for the tip!</p>,2019-10-03T19:38:37Z,43,Week 9/29 - 10/5,feedback,,jzozvpx25to679,k1b3rkuxjrfqe,2019-10-03T19:38:37Z,{},hw3
2151,stud,<p><md><br />Is it possible to post recordings for OH week 6 and 7? Thanks<br /></md></p>,2019-10-05T00:09:56Z,59,Week 9/29 - 10/5,followup,a_0,,k1cswc0q1l62n6,2019-10-05T00:09:56Z,{},logistics
2152,no,Can you please share the recording for Thursday&#39;s OH?,2019-10-05T18:27:48Z,59,Week 9/29 - 10/5,followup,,jctgczyiequ6gr,k1dw471zdck1c5,2019-10-05T18:27:48Z,{},logistics
2153,no,"<p>When available, they will be here:</p>
<p></p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=132"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=132</a></p>
<p></p>
<p></p>",2019-10-06T00:39:14Z,58,Week 10/6 - 10/12,followup,,hyx9thiqa6j4nn,k1e9dv4a21f2y,2019-10-06T00:39:14Z,{},logistics
2154,no,"<p>Interesting.... from &#64;467 I only see version 0.14.0, or at least from the pip install that is what I get. is 0.15.2 the latest compiled source?</p>",2019-10-01T02:33:53Z,59,Week 9/29 - 10/5,followup,,ixty1midfufhd,k178a1llp7t6ot,2019-10-01T02:33:53Z,{},project2
2155,no,"<p>Mine reports the same version 0.14.0, but when I run gym.make(&#39;LunarLander-v2&#39;) I get the following error: AttributeError: &#39;module&#39; object has no attribute &#39;LunarLander&#39;</p>
<p></p>
<p>Any ideas on what I am doing wrong?</p>",2019-10-02T01:07:03Z,59,Week 9/29 - 10/5,followup,,jl2bq5rf8b67pq,k18km89xi6k2k6,2019-10-02T01:07:03Z,{},project2
2156,no,<p>The writeup suggest that you will need to install from source (pip install -e .) - so get the github version and do that.</p>,2019-10-02T17:51:40Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19ki6o2gnt5t4,2019-10-02T17:51:40Z,{},project2
2157,no,"<p>Right, I&#39;ve done that (see below commands). Still get the same error. </p>
<p></p>
<pre>git clone https://github.com/openai/gym.git
cd gym
pip install -e .</pre>",2019-10-02T22:21:53Z,59,Week 9/29 - 10/5,feedback,,jl2bq5rf8b67pq,k19u5ompp1243v,2019-10-02T22:21:53Z,{},project2
2158,no,"<p>Did you install the the &#96;boxe2d&#96; gym/envs?</p>
<pre>pip install -e &#39;.[box2d]&#39;<br /></pre>
<p>They are included. But still, we need to manually install them:</p>
<p><a href=""https://github.com/openai/gym/blob/master/docs/environments.md"">https://github.com/openai/gym/blob/master/docs/environments.md</a></p>
<p></p>",2019-10-02T23:02:43Z,59,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k19vm70n75j44h,2019-10-02T23:02:43Z,{},project2
2159,no,"<p>I&#39;m on windows, and I open the command prompt and run that [box2d] command. Is there a specific directory I should be in before running it? I&#39;ve tried it from the standard Users\Me and from Users\me\gym. I also confirmed I&#39;m on pip version 19.2.3. Regardless, I get the following error:</p>
<p></p>
<p>ERROR: &#39;.[box2d]&#39; is not a valid editable requirement. It should either be a path to a local project or a VCS URL (beginning with svn&#43;, git&#43;, hg&#43;, or bzr&#43;).</p>
<p></p>",2019-10-03T00:47:51Z,59,Week 9/29 - 10/5,feedback,,jl2bq5rf8b67pq,k19zdenyjeu2w8,2019-10-03T00:47:51Z,{},project2
2160,no,"<p>you would be in the source directory for gym.</p>
<p></p>
<p>windows will not install box2d like that. there are compiler errors from box2d. you have to install it manually (see other posts).</p>",2019-10-03T01:24:51Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1a0ozcdnwe4da,2019-10-03T01:24:51Z,{},project2
2161,no,<p>I was having the same problem. See &#64;466 where TA suggests installing from pre-built wheel. This ended up working for me.</p>,2019-10-03T01:34:39Z,59,Week 9/29 - 10/5,feedback,,ixq2z0d02Kn,k1a11l7azfa4yk,2019-10-03T01:34:39Z,{},project2
2162,no,"<p>I did try installing the wheel, and it seemed to install successfully (Box2D‑2.3.2‑cp37‑cp37m‑win32.whl). However, I still get that error, even after restarting. Even trying &#34;pip install Box2D&#34; states &#34;Requirement already satisfied.&#34; Not sure what to try next. HW3 and HW4 worked so smoothly, and I can&#39;t even get the project started.</p>",2019-10-03T23:07:26Z,59,Week 9/29 - 10/5,feedback,,jl2bq5rf8b67pq,k1bb84abz2k1a,2019-10-03T23:07:26Z,{},project2
2163,no,"<p>It may be redundant, but I just want to double check, did you install the gym&#39;s box2d environments after installing Box2D wheel? For windows environment, the command would be:</p>
<pre>pip install -e &#34;.[box2d]&#34;</pre>
<p>The Box2D is just one dependencies of the environment. If you still have issue, can you post the full error message that you got?</p>
<p></p>
<p>Alternatively, Jacob mentioned a way to use gym 0.14.0 (homework version) for project 2, in &#64;467. I haven&#39;t tried it. But maybe that work for you.</p>",2019-10-03T23:52:56Z,59,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k1bcumnjbxi61e,2019-10-03T23:52:56Z,{},project2
2164,no,"<p></p>
<p></p>
<p>Had same issue please try: </p>
<p></p>
<pre>pip3 install gym[box2d]</pre>
<p></p>",2019-10-04T18:35:03Z,59,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1cgxo9zy4u3bo,2019-10-04T18:35:03Z,{},project2
2165,no,"<p>Had to install visual studio, but the pip install -e &#34;.[box2d]&#34; worked, I definitely have box2d. Restarted my computer, still getting the original error. Is there something I need to import other than gym? Something else I might be missing?</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;C:/Users/Mark/PycharmProjects/rlhw3/lunar.py&#34;, line 13, in &lt;module&gt;
    env = gym.make(&#39;LunarLander-v2&#39;)
  File &#34;C:\Users\Mark\PycharmProjects\rlhw3\venv\lib\site-packages\gym\envs\registration.py&#34;, line 156, in make
    return registry.make(id, **kwargs)
  File &#34;C:\Users\Mark\PycharmProjects\rlhw3\venv\lib\site-packages\gym\envs\registration.py&#34;, line 101, in make
    env = spec.make(**kwargs)
  File &#34;C:\Users\Mark\PycharmProjects\rlhw3\venv\lib\site-packages\gym\envs\registration.py&#34;, line 72, in make
    cls = load(self.entry_point)
  File &#34;C:\Users\Mark\PycharmProjects\rlhw3\venv\lib\site-packages\gym\envs\registration.py&#34;, line 18, in load
    fn = getattr(mod, attr_name)
AttributeError: &#39;module&#39; object has no attribute &#39;LunarLander&#39;</pre>
<p></p>",2019-10-06T23:03:11Z,58,Week 10/6 - 10/12,feedback,,jl2bq5rf8b67pq,k1fle78hcn85wv,2019-10-06T23:03:11Z,{},project2
2166,no,<p>I ran into this error and spent over 5 hours debugging this issue... ultimately it was that the process that was running was not using the environment where I installed the box2d... the pain of having multiple virtual environments...</p>,2019-10-06T23:43:52Z,58,Week 10/6 - 10/12,feedback,,ixty1midfufhd,k1fmuiufavl3xy,2019-10-06T23:43:52Z,{},project2
2167,no,"<p>&#64;George I&#39;m not sure I understand what you mean. I&#39;m working in pycharm, is there something I need to add to pycharm to resolve this?</p>",2019-10-07T21:17:18Z,58,Week 10/6 - 10/12,feedback,,jl2bq5rf8b67pq,k1gx1vqdpct6lv,2019-10-07T21:17:18Z,{},project2
2168,no,"<p>doesn&#39;t pycharm have its own installation mechanism? That manual import/install of gym (pip install -e .) maybe is the problem? I don&#39;t know, ... I use the command line and VS code ...</p>",2019-10-08T17:10:47Z,58,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1i3opvofg06ae,2019-10-08T17:10:47Z,{},project2
2169,no,"<p>i was able to get the box 2d stuff with </p>
<pre>pip install box2d-py</pre>
<p></p>",2019-10-08T18:11:01Z,58,Week 10/6 - 10/12,feedback,,idfzuuu9fnI,k1i5u6i5dy53ng,2019-10-08T18:11:01Z,{},project2
2170,no,"<p>Caroline, can you show the code is producing this issue?</p>
<p></p>
<p>LunarLander is not an attribute. It should be in quotes. </p>
<p></p>
<pre>import gym<br />env = gym.make(&#39;LunarLander-v2&#39;)<br />env.reset()</pre>
<p></p>",2019-10-08T21:43:25Z,58,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1idfbxk2ml74d,2019-10-08T21:43:25Z,{},project2
2171,no,"<p>That looks like what I have! The error occurs on the env =... line.</p>
<pre>import gym<br />env = gym.make(&#39;LunarLander-v2&#39;)</pre>
<p></p>",2019-10-08T22:31:14Z,58,Week 10/6 - 10/12,feedback,,jl2bq5rf8b67pq,k1if4tr8kb92e3,2019-10-08T22:31:14Z,{},project2
2172,no,"<p>Ugh, setting up environments is the <strong>worst</strong> part of coding. I figured out Jacob&#39;s suggestion, I tried using the terminal in pycharm instead of the command prompt and the following command did the trick. </p>
<p></p>
<p>pip install box2d-py</p>",2019-10-09T00:32:48Z,58,Week 10/6 - 10/12,feedback,,jl2bq5rf8b67pq,k1ijh5r6onr3i8,2019-10-09T00:32:48Z,{},project2
2173,no,"<p>Sorry, just seeing this... I mean when you define the configuration in pycharm, you tell it what interpreter to use... somehow mine got set to the default and not the proj2 one I created and so none of the stuff I installed worked until I switched it over.</p>
<p></p>
<p>I do find the pycharm terminal works when the package manger doesn&#39;t sometimes... but most of the time they do the same thing for me.</p>",2019-10-10T01:06:03Z,58,Week 10/6 - 10/12,feedback,,ixty1midfufhd,k1k03rp0vlp2a5,2019-10-10T01:06:03Z,{},project2
2174,no,<p>Thank you all! I had the same issues that Caroline had. I can start my project 2 now.</p>,2019-10-12T16:50:15Z,58,Week 10/6 - 10/12,feedback,,jc6xvgjncoey,k1nsppn69du5ol,2019-10-12T16:50:15Z,{},project2
2175,no,"<p>This worked for me:</p>
<pre>pip3 install gym[box2d]</pre>
<p>Thanks Angel Javier Barranco! </p>
<div></div>",2019-10-13T19:45:03Z,57,Week 10/13 - 10/19,feedback,,jzjaw2l4l6m5ns,k1peed644jh3ug,2019-10-13T19:45:03Z,{},project2
2176,no,"<p>Well the instruction says clearly that we need to use gym. I can code up the whole process. But just wonder how to use the gym library properly. Without any document, not sure how the <strong>make(), </strong><b>step() </b>function works. </p>",2019-10-01T04:27:59Z,26,Week 9/29 - 10/5,followup,,j6ln9puq99s5uv,k17ccs18ifu7jk,2019-10-01T04:27:59Z,{},hw3
2177,no,"<p></p>
<h2>Observations</h2>
<p>If we ever want to do better than take random actions at each step, it’d probably be good to actually know what our actions are doing to the environment.</p>
<p>The environment’s <code>step</code> function returns exactly what we need. In fact, <code>step</code> returns four values. These are:</p>
<ul><li><code>observation</code> (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.</li><li><code>reward</code> (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.</li><li><code>done</code> (boolean): whether it’s time to <code>reset</code> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <code>done</code> being <code>True</code> indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)</li><li><code>info</code> (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</li></ul>
<p></p>
<p>from <a href=""https://gym.openai.com/docs/"">https://gym.openai.com/docs/</a></p>
<p></p>
<p>Note that <strong>make() </strong>isn&#39;t necessary for HW3</p>",2019-10-01T05:09:01Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k17dtjqednr5b,2019-10-01T05:09:01Z,{},hw3
2178,no,<p>Got it. Thanks</p>,2019-10-01T06:03:25Z,26,Week 9/29 - 10/5,feedback,,j6ln9puq99s5uv,k17fri6kbhr77z,2019-10-01T06:03:25Z,{},hw3
2179,no,"<p>one can also use desc= with frozen_lake.FrozenLakeEnv(desc=custom_map).unwrapped</p>
<p></p>
<div>
<div></div>
</div>",2019-10-02T17:28:19Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19jo5ecq5k1di,2019-10-02T17:28:19Z,{},hw3
2180,no,"Can you elaborate your 4b in terms of state-action Q values. Suppose your are going from (s,a) to (s’,a’) what do your equations look like.",2019-10-01T03:57:55Z,25,Week 9/29 - 10/5,followup,,jqknbi6c0lHt,k17ba48epvg3qw,2019-10-01T03:57:55Z,{},hw3
2181,no,<p></p>,2019-10-01T13:20:02Z,25,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k17vd0bqqfl7fi,2019-10-01T13:20:02Z,{},hw3
2182,no,"<p>Hey Angel Javier, please don&#39;t share your code in a public post. It&#39;s not encouraged. I have deleted it. </p><p><br /></p><p>My question was to elaborate equations mathematically. </p><br /><p></p><p></p>",2019-10-01T13:42:22Z,25,Week 9/29 - 10/5,feedback,,jqknbi6c0lHt,k17w5qdgop56ur,2019-10-01T13:42:22Z,{},hw3
2183,no,"<p>Sure sorry about that.</p>
<p>I am implementing below equation:<br />$$Q(S,A) = Q(S,A) &#43; \alpha *(R&#43;\gamma*Q(S&#39;,A&#39;)-Q(S,A))$$</p>",2019-10-01T13:56:21Z,25,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k17wnpxjpaf2i0,2019-10-01T13:56:21Z,{},hw3
2184,no,Your equation looks fine. Make sure you follow all other instructions mentioned in HW3 notes. Are you using numpy.random.randint to select actions as mentioned?,2019-10-01T14:01:41Z,25,Week 9/29 - 10/5,feedback,,jqknbi6c0lHt,k17wukopjo4b2,2019-10-01T14:01:41Z,{},hw3
2185,no,"<p>Can I share a snippet?</p>
<p>I am using random.randint with probability of epsilon, and np.argmax for with probability of (1-epsilon)</p>",2019-10-01T14:06:48Z,25,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k17x15ceky66oh,2019-10-01T14:06:48Z,{},hw3
2186,no,"<p>You are supposed to use np.random.randint, not random.randint.</p>
<p></p>
<p>Note: Both are fine implementations, but to reproduce the results mentioned use Numpy version.</p>",2019-10-01T16:19:31Z,25,Week 9/29 - 10/5,feedback,,jqknbi6c0lHt,k181ru1k8893c0,2019-10-01T16:19:31Z,{},hw3
2187,no,"<p>&#64;hey angel, when you say &#34;random.randint with probability of epsilon, and np.argmax for with probability of (1-epsilon)&#34;, do you mean that you use random.random after using randint to decide between randint and argmax? </p>
<p></p>",2019-10-02T15:55:20Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19gckt7qh14ng,2019-10-02T15:55:20Z,{},hw3
2188,no,"<p>Hi there,</p>
<p></p>
<p>I have the exact same sequences of steps as you, and still have not figured out the cause yet. Currently I&#39;m looking at these points. </p>
<p></p>
<p>1) Generating epsilon</p>
<p>Basically I generate integers using np.random.randint(0, 101) between [0, 100], divide by 100 to get a float value up to 2 decimal places</p>
<p></p>
<p>2) Random action</p>
<p>Using np.random.randint(0, 4) to generate [0,3] for the 4 possible actions</p>
<p></p>
<p>3)Policy inference from Q matrix</p>
<p>Basically, I perform the following steps</p>
<ol><li>Reset env</li><li>Start at state 0</li><li>Pick action for current state with the highest value</li><li>Do an env.step(action) function call to get resulting state and update</li><li>Repeat 3 and 4 until env.step() output termination</li></ol>
<p></p>
<p>I&#39;ve tried both setting isSlippery=False/True for the inference part. Neither gives me the correct result.</p>",2019-10-01T14:47:33Z,25,Week 9/29 - 10/5,followup,,ijctp4ucNy8,k17yhk2to5mfw,2019-10-01T14:47:33Z,{},hw3
2189,no,<p>yeah sigh</p>,2019-10-01T15:14:30Z,25,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k17zg7ttq5l38k,2019-10-01T15:14:30Z,{},hw3
2190,no,"<p>The policy is the action you would take if you were in state s. You are not exploring the MDP again to find the policy.</p>
<p></p>
<p>Don&#39;t engage the environment during policy export.</p>",2019-10-01T16:00:13Z,25,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k18130daqx7439,2019-10-01T16:00:13Z,{},hw3
2191,no,"<p>^^ that&#39;s correct, you should explore the environment with the &#34;given epsilon&#34;.</p>",2019-10-01T16:10:54Z,25,Week 9/29 - 10/5,feedback,,jqknbi6c0lHt,k181gqt3s6i7iv,2019-10-01T16:10:54Z,{},hw3
2192,no,"<p>Yup, I realized the HW was about finding the policy and not the best path, and corrected my implementation.</p>
<p></p>
<p>Still not getting the right answer though. </p>",2019-10-02T04:07:10Z,25,Week 9/29 - 10/5,feedback,,ijctp4ucNy8,k18r1v51h9y4sl,2019-10-02T04:07:10Z,{},hw3
2193,no,"<p>Can you elaborate on your &#39;random action&#39;? if you use &#34;np.random.randint(0, 4) to generate [0,3] for the 4 possible actions&#34;, you give them an equal probability... but here we need to give a higher probability to the argmax, so I can&#39;t see how this is feasible with randint(0,4).</p>",2019-10-02T15:52:09Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19g8hfkqsj72g,2019-10-02T15:52:09Z,{},hw3
2194,no,"<p></p>
<p>&#64;Jean-Pierre Bianchi</p>
<p></p>
<p>I&#39;m using np.random.randint(0, 4) to get all actions with equal prob. To my understanding that is the correct way. </p>
<p></p>
<p>What do you mean by &#34;here we need to give a higher probability to the argmax&#34;</p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=506""></a></p>",2019-10-02T16:03:01Z,25,Week 9/29 - 10/5,feedback,,ijctp4ucNy8,k19gmg9s1mh7dk,2019-10-02T16:03:01Z,{},hw3
2195,no,"<p>Hi &#64;zq, well, I meant that we are supposed to implement an epsilon-greedy policy no? Here&#39;s the slide from David Silver&#39;s course.</p>
<p>It shows that we explore all states with a prob $$\epsilon / m$$ and the argmax one with an additional prob $$1-\epsilon$$.  That&#39;s why they give us $$\epsilon$$ after all...</p>
<p>If you use randint(0,4) to pick an action randomly, then you&#39;re fulfilling only the first part, but the argmax policy needs to be picked with a higher probability.  </p>
<p>I can&#39;t see any other way than using random.random() to do that.</p>
<p>But it is possible to do it all with only one random operation.  </p>
<p>I&#39;ve posted about this in various places, so I hope someone will answer soon.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk19j2f4e4ka9%2Fscreenshot_01.jpg"" alt="""" /></p>
<div>
<div></div>
</div>",2019-10-02T17:19:04Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19jc9axw0u14s,2019-10-02T17:19:04Z,{},hw3
2196,no,"<p>Roll the 6 sided dice.</p>
<p></p>
<p>if you roll a 2, then roll the 4 sided dice to pick an action.</p>
<p>if you didn&#39;t roll a 2, then you look at the [s,:] action values and choose the <em>first one</em> with the highest value.</p>",2019-10-02T17:50:52Z,25,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19kh5mvefg4t0,2019-10-02T17:50:52Z,{},hw3
2197,no,<p>&#64;Jacob ... what do a 6 sided has to do with implementing a probability $$\epsilon$$ /4?? </p>,2019-10-02T18:24:05Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19lnvhdb5328r,2019-10-02T18:24:05Z,{},hw3
2198,no,"<p>My comment is a demonstration of how you would decide when to choose a random action versus a greedy action. This is a figurative comment, not a literal comment.</p>",2019-10-02T18:25:46Z,25,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19lq16qoro51z,2019-10-02T18:25:46Z,{},hw3
2199,no,"<p>OK, got it, but the way you do it requires two random actions in most of the cases.  The point I have been trying to make was that that is because we are told to use randint(0,4), which gives the same probability to all states, so we need an additional random.random(), while doing it in another way would need just one.  It matters because we&#39;re told to use seeds, so every &#39;roll of a dice&#39; counts, and that&#39;s why I think I can&#39;t find the same answers as the ones given... </p>",2019-10-02T18:41:07Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19m9sazj6m1od,2019-10-02T18:41:07Z,{},hw3
2200,no,"<p>Yes, you are using two random numbers for the random action selection.</p>
<p></p>
<p>Yes, the seeds matter to the solution to the problem, that&#39;s why you are given the seeds.</p>
<p></p>
<p>If you implement the algorithm correctly, then you will follow the same path as the grader&#39;s path because you will follow the same random number sequence.</p>
<p></p>
<p>If you have seeded the random number generators correctly, then you are not getting the solution because (1) you did not implement the algorithm correctly, or (2) you are exporting the policy incorrectly.</p>",2019-10-02T19:04:41Z,25,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19n43cxved1gd,2019-10-02T19:04:41Z,{},hw3
2201,no,"<p>Thx Jacob, I&#39;m going to have to give another look at the epsilon greediness because this is what I was doing</p>
<p></p>
<pre>rnd = np.random.randint(0,100)<br />return rnd % 4 if  rnd &lt; round(100*epsilon) else np.argmax(q[pos])</pre>
<p>I didn&#39;t see the need to have two random generators... </p>
<p></p>",2019-10-02T19:22:21Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19nqskewhx1al,2019-10-02T19:22:21Z,{},hw3
2202,no,"<p>That&#39;s an efficient way of using random number sequences, but that&#39;s not necessarily the letter of the algorithm. Hopefully you can get it to work for you. Otherwise, do it the less efficient way.....</p>",2019-10-02T21:41:12Z,25,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19spda8x4d6uk,2019-10-02T21:41:12Z,{},hw3
2203,no,"<p>Jean-Pierre, your method of using only a single randomly generated value could be a valid approach, however for this homework the goal is not only to implement the algorithms in a valid way but also to generate the exact policies our agents have generated (which is why we provide the seeds, and some other details). Try using that additional random.random() and see if it works!</p>",2019-10-02T23:19:11Z,25,Week 9/29 - 10/5,feedback,,jzlyi4e55bz5kj,k19w7df2ryh4cd,2019-10-02T23:19:11Z,{},hw3
2204,no,"<p>Thanks, yes, I&#39;ll have to adapt to this exercise.  Also, I just learned from &#64;520 that &#39;there are not always 4 actions&#39;... that&#39;s an even bigger issue I have to fix.  </p>",2019-10-03T03:45:51Z,25,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1a5qat1fc72l4,2019-10-03T03:45:51Z,{},hw3
2205,no,"<p>Not quite for this HW. Environments, in general, not always have 4 actions. It&#39;s a good programming practice to not use &#34;magic variables&#34; such as that one hard-coded in your code.</p>",2019-10-04T10:11:30Z,25,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1byy3od41k7p3,2019-10-04T10:11:30Z,{},hw3
2206,no,"<p>This is a great post. I wish I had seen it before embarking on a trip to find what you&#39;re saying here.</p>
<p></p>
<p>I found that decay is not even necessary, and pure exploration was not optimal either. Close to it though...</p>",2019-10-05T04:51:31Z,39,Week 9/29 - 10/5,followup,,is8ald0uljj3u4,k1d2yg6ip2f6ze,2019-10-05T04:51:31Z,{},hw4
2207,no,"<p>I understand why we are not sharing our work. But I hope the TA can highlight some of the good work done on project 1, so that we can learn from it.</p>
<p>Miguel mentioned about exploring unbalance, bias walk, instead of 50/50 probability walk. I don&#39;t know if anyone follows and exploring this consideration, and what their thought on this.</p>
<p>Also, I haven&#39;t written any conference style paper before. One of my goal for this course is being able to write a good one. And knowing some good writing, and what makes them good, will really help my learning.</p>",2019-10-01T17:03:24Z,26,Week 9/29 - 10/5,followup,,jl5wq8mca7o0,k183c9k7i5m1tw,2019-10-01T17:03:24Z,{},project1
2208,no,<p>Agree entirely - i am here to learn and want more than just a &#34;here is your grade&#34;</p>,2019-10-02T00:54:57Z,26,Week 9/29 - 10/5,feedback,,jzttp1ojahj6ju,k18k6o1ic5e687,2019-10-02T00:54:57Z,{},project1
2209,no,"<p>We&#39;ll try to do that in one of our Study Sessions. This is a very good idea, not sharing the full papers, but highlights. <strong attention=""jl1acpoc4HA9"">&#64;Farrukh Rahman</strong>  <strong attention=""i4i9bi8rFqk"">&#64;Don Jacob</strong>  </p>",2019-10-02T11:40:32Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1978wmo66a4c1,2019-10-02T11:40:32Z,{},project1
2210,no,"<p>Yes, you can use PyTorch for P2. Good choice! ;)</p>",2019-10-02T11:35:31Z,59,Week 9/29 - 10/5,followup,,hyx9thiqa6j4nn,k1972g8yl4u7ca,2019-10-02T11:35:31Z,{},project2
2211,no,"<p>just to add on, is keras allowed? </p>",2019-10-03T02:28:12Z,59,Week 9/29 - 10/5,feedback,,jvfpllmsggt7p4,k1a2yg488gd1tz,2019-10-03T02:28:12Z,{},project2
2212,no,<p>yes keras is allowed. Any library that does autograd is allowed for function approximation. Just be sure as Miguel mentioned the RL portion of the code is your own.</p>,2019-10-03T04:00:53Z,59,Week 9/29 - 10/5,feedback,,jl1acpoc4HA9,k1a69mp9y0u9h,2019-10-03T04:00:53Z,{},project2
2213,no,<p>Thanks! I assume when you mentioned the RL portion that will be setting up the algorithm to learn from experience...etc while using the DL portion as Q function approximation </p>,2019-10-03T05:08:33Z,59,Week 9/29 - 10/5,feedback,,jvfpllmsggt7p4,k1a8onv2ad25cr,2019-10-03T05:08:33Z,{},project2
2214,no,<p>Precisely!</p>,2019-10-03T05:12:15Z,59,Week 9/29 - 10/5,feedback,,jl1acpoc4HA9,k1a8tev6kl05lx,2019-10-03T05:12:15Z,{},project2
2215,no,"<p>Thanks!</p>
<p></p>",2019-10-05T16:11:13Z,59,Week 9/29 - 10/5,feedback,,jzjwcq2u8o7110,k1dr8karpvj68y,2019-10-05T16:11:13Z,{},project2
2216,no,"<p>I noticed one different that could make a difference on the Q value. In the update step, I have </p>
<p>&#96;Q[s, a] &#43;= alpha * (reward &#43; (gamma * Q[s_, a_]) - Q[s, a])&#96;</p>
<p></p>
<p>And then I check if done is true. If it is true, break the loop. </p>
<p>Because the terminating step (H or G) has Q = 0 all the time, so if s_ is terminating step, Q[s_, a_] will always be 0. However, this seems to be wrong. I can see some Q[s_,a_] values are not zero. Not sure why. Can anyone help to explain? Thanks. </p>
<p></p>
<p>If I add the condition to se Q[s_,a_] as 0 for terminating step, I got the following Q value for example 1. Not quite correct but close somehow</p>
<p></p>
<pre>[[0.24645784, 0.27692434, 0.28342901, 0.41416678],
       [0.47783752, 0.50282168, 0.49791094, 0.45964676],
       [0.60342578, 0.59744466, 0.65605313, 0.62485337],
       [0.69693323, 0.69104844, 0.69917969, 0.66651906],
       [0.        , 0.        , 0.        , 0.        ],
       [0.45710856, 0.31695843, 0.55983025, 0.21358786],
       [0.56505278, 0.59889105, 0.69523574, 0.61172254],
       [0.69825349, 0.79462972, 0.73215683, 0.71170136],
       [0.25259085, 0.46395681, 0.31283298, 0.16269544],
       [0.44507453, 0.58782519, 0.62341897, 0.46227745],
       [0.68299647, 0.65825709, 0.71442811, 0.62170113],
       [0.79526316, 0.76536122, 0.80844779, 0.72874745],
       [0.41849942, 0.54543441, 0.47885069, 0.553186  ],
       [0.56817379, 0.63327759, 0.71343271, 0.54454364],
       [0.67276238, 0.88631622, 0.75174712, 0.69535273],
       [0.        , 0.        , 0.        , 0.        ]]</pre>",2019-10-02T01:58:02Z,26,Week 9/29 - 10/5,followup,,j6ln9puq99s5uv,k18mft236bb11e,2019-10-02T01:58:02Z,{},hw3
2217,no,"<p>It is the way you are doing your epsilon greedy. You have to do it correctly using np.random.randint., because the seed is specific to numpy. I managed to get the answer using both np.random.uniform as well as randint. </p>
<p></p>
<p>I was stuck for awhile but i was confident that my sarsa algorithm is correct. As expected, doing epsilon greedy differently results in different results (using the same seed).</p>
<p></p>
<p>Biggest hint: &#64;465</p>",2019-10-02T02:55:10Z,26,Week 9/29 - 10/5,followup,,jvfpllmsggt7p4,k18oh9lo4o06cs,2019-10-02T02:55:10Z,{},hw3
2218,no,"<p>Okay. I got one issue figured out. For the epsilon greedy policy I was running random action with probability of (1- epsilon). This is the opposite of what people use. Now, the results look better. However, I still have some arrows wrong.</p>
<p></p>",2019-10-02T19:40:32Z,26,Week 9/29 - 10/5,feedback,,j6ln9puq99s5uv,k19oe6nlalviw,2019-10-02T19:40:32Z,{},hw3
2219,no,<p></p><pre>I don&#39;t know what you use gym.make() or toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped. The later ended up saving my life. The first one was giving me wrong output even thought my epsilon greedy implementation was correct...</pre>,2019-10-04T00:20:08Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1bdtm2rxyd4c,2019-10-04T00:20:08Z,{},hw3
2220,no,<p>Are you resetting environments before each episode? See my post &#64;546 for a potential problem that causes the problem of getting different results.</p>,2019-10-03T19:20:27Z,26,Week 9/29 - 10/5,followup,,jzhy4sbxyar5l4,k1b347kc9jo105,2019-10-03T19:20:27Z,{},hw3
2221,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhzoi2qsuCAd%2Fk1md86aidnyl%2FScreenshot_from_20191011_114814.png"" alt="""" /></p>
<p>Is this the correct Q(s,a) format we try to replicate?</p>",2019-10-11T16:49:12Z,42,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1md8im7l535zl,2019-10-11T16:49:12Z,{},hw4
2222,no,"<p>Kind of.  What you have pasted above is the Bellman optimality equation for the state-action value function (Q).</p>
<p></p>
<p>In homework 4, we&#39;re implementing a learning algorithm which uses an <em>update rule</em> which is based on that equation.</p>
<p></p>
<p>Check out Sutton and Barto, page 131, for a description of the algorithm, or see below:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk1mi06gm6nr0%2FCapture.PNG"" alt="""" /></p>",2019-10-11T19:02:59Z,42,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mi0k3p25k31j,2019-10-11T19:02:59Z,{},hw4
2223,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>Funny thing is, that comment is technically incorrect.  You really have to go into the code to see:</p>
<p></p>
<pre>if pass_idx &lt; 4 and pss_idx != dest_idx:
    initial_state_distrib[state] &#43;= 1</pre>
<p></p>
<p></p>
<p></p>",2019-10-14T05:19:01Z,41,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1pywhzcnyr48p,2019-10-14T05:19:01Z,{},hw4
2224,no,<p>I&#39;m not sure I follow. The code snippet you are showing is selecting the states for the initial state distribution. I though the original question is asking about the entire state space.</p>,2019-10-14T12:48:59Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1qez5om2vq61f,2019-10-14T12:48:59Z,{},hw4
2225,no,"<p>Since the passenger location can never be the same as the destination (until the game is over), there are actually less than 500 states (4 locations where the destination and passenger are together * 25 taxi locations = 100 unattainable states).</p>
<p>4 of those 100 are possible as the next state, S&#39;, when the game is won (but those 4 will never be updated in our Q table).  I&#39;m sure over the years, many people have wondered why they can never visit 100 * 6 = 600 state-action pairs, after reading that comment in the source code.</p>",2019-10-14T13:58:46Z,41,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1qhgw9hyam373,2019-10-14T13:58:46Z,{},hw4
2226,no,"<p></p><pre>import gym<br />env = gym.make(&#39;Taxi-v2&#39;)<br />env.observation_space.n<br /># shows 500 states, which is correct and what the cited comment states </pre>
<p>5*4*25 = 500, am I missing something?</p>
<p></p>
<p>Again the code snippet you are showing has to do with the initial state distribution, nothing to do with the terminal, or even the way the state space is calculated.</p>
<p></p>
<p>Here are the only terminal transitions:</p>
<p></p>
<pre>In [86]: env.reset()
Out[86]: 204

In [87]: for s in env.P:
    ...:     for a in env.P[s]:
    ...:         for p, sp, r, d in env.P[s][a]:
    ...:             if d:
    ...:                 print(&#39;s={}, a={}, r={}, sp={}, d={}&#39;.format(s, a, r, sp, d))
s=16, a=5, r=20, sp=0, d=True
s=97, a=5, r=20, sp=85, d=True
s=418, a=5, r=20, sp=410, d=True
s=479, a=5, r=20, sp=475, d=True

In [88]: env.reset()
Out[88]: 269

In [89]: for s in env.P:
    ...:     for a in env.P[s]:
    ...:         for p, sp, r, d in env.P[s][a]:
    ...:             if d:
    ...:                 print(&#39;s={}, a={}, r={}, sp={}, d={}&#39;.format(s, a, r, sp, d))
s=16, a=5, r=20, sp=0, d=True
s=97, a=5, r=20, sp=85, d=True
s=418, a=5, r=20, sp=410, d=True
s=479, a=5, r=20, sp=475, d=True</pre>
<p>Where do you get the 100*6?</p>",2019-10-15T20:57:03Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1sbunpg1ob5e1,2019-10-15T20:57:03Z,{},hw4
2227,no,"<p>The initial state assignment prohibits the initial passenger location from being the same as the destination location, right?</p>
<p></p>
<p>Now you tell me how the passenger can EVER be at the same location as the destination.  Tell me how that state can ever occur, at any time t, other than as a terminal state.</p>
<p></p>
<p>Where does the taxi have to be when the above happens?</p>",2019-10-15T23:46:09Z,41,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1shw4btfzu5sh,2019-10-15T23:46:09Z,{},hw4
2228,no,"<p>But a terminal state is not an invalid state, though. That&#39;s part of the environment. It is not &#34;incorrect&#34; or &#34;invalid&#34; technically speaking. Is the state reachable?! Yes, when dropping off a passenger at the expected location. Then, no biggie.</p>
<p></p>
<p>The code snippet you posted above is simply saying: &#34;don&#39;t add a terminal state to the initial state distribution.&#34; There is nothing invalid about that, it is actually the right thing to do.</p>
<p></p>
<p></p>",2019-10-18T22:39:00Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1wptbqht4k79i,2019-10-18T22:39:00Z,{},hw4
2229,no,"<p>96 states are unreachable.  4 are reachable only as terminal states.</p>
<p></p>
<p>The reason this is true is because of the initial state distribution and the rules of Taxi.</p>",2019-10-18T22:46:20Z,41,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1wq2r6p7u126,2019-10-18T22:46:20Z,{},hw4
2230,no,"<p>That&#39;s right!</p>
<p></p>
<p>You are referring to the 96 unreachable states, which are unreachable because out of the 5x5 grid (25 locations) if the passenger is at the destination, the Taxi cannot be at any other location but that same cell. At that point during the game, it&#39;s game over and the taxi cannot be in any other cell corresponding to the passenger at the destination while the taxi is in another cell.</p>
<p></p>
<p>That could be a problem, though, two things: First, thankfully we noticed this issue when we designed the homework in 2016 and removed those problematic state-action pairs (including the 4 final states, of course, which are 0) from the problem set.</p>
<p></p>
<p>Second, honestly, I still don&#39;t see why you say it is because of the initial state distribution. I think it is simply because of the rules of the game, and the way they designed the MDP. The initial state distribution is just preventing the initial states from including any state in which the location of the passenger is the same as the destination (including when the taxi is in any of the other 21 cells). It would be kinda silly to have the taxi pick up the passenger and drop it off at the same location.</p>
<p></p>
<pre>&#34;&#34;&#34;  
    Passenger locations:
    - 0: R(ed)
    - 1: G(reen)
    - 2: Y(ellow)
    - 3: B(lue)
    - 4: in taxi
    
    Destinations:
    - 0: R(ed)
    - 1: G(reen)
    - 2: Y(ellow)
    - 3: B(lue)
&#34;&#34;&#34;
if pass_idx &lt; 4 and pss_idx != dest_idx:
    initial_state_distrib[state] &#43;= 1
</pre>
<p></p>
<p>So, yeah, I think the reason is not the initial state distribution, but the design of the MDP. It seems to me you can easily remove those 96 states, leave the initial state distribution as is, and everything would work the same.</p>
<p></p>",2019-10-19T01:07:32Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1wv4c961uq5t4,2019-10-19T01:07:32Z,{},hw4
2231,no,"<p>I don&#39;t get it. For these Q values, any policy would be acceptable wouldn&#39;t it?</p>
<p></p>
<p>How can we then guarantee &#39;&lt;,&lt;,v,&lt;&#39; as output?</p>",2019-10-02T12:43:05Z,25,Week 9/29 - 10/5,followup,,ijctp4ucNy8,k199hc5nj1245x,2019-10-02T12:43:05Z,{},hw3
2232,no,"<p>argmax() always returns the earliest index in the case of a tie, so argmax([1, 1, 1, 1]) would return 0 (which would indicate moving left). If you print Q out at the end, it will look like all 1s, but (as long as you&#39;re using floating point numbers) if you look at Q in the debugger, you&#39;ll probably find that the numbers aren&#39;t actually all 1s.</p>
<p></p>
<p>Sometimes we may have a row with all the same values, but we&#39;re just trying to replicate results, so as long as we use argmax() the same way they do we should get the same answer. </p>",2019-10-02T14:43:11Z,25,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k19drsm67043bw,2019-10-02T14:43:11Z,{},hw3
2233,stud,"Agree but how I can replicate results, I have seed env the way they described but I got different results it is close to their answer but not the same. Are you able to replicate the same results? ",2019-10-02T14:47:00Z,25,Week 9/29 - 10/5,feedback,a_0,,k19dwp6enwyo6,2019-10-02T14:47:00Z,{},hw3
2234,no,"<p>I can&#39;t find the post, but I heard that we shouldn&#39;t be seeding the environment, only get your random numbers from numpy. I thought I seeded the environment correctly, but when I checked it was the environment giving me different outputs every time. I switched to numpy and it&#39;s consistent (although I&#39;m still struggling to get the correct answer)</p>",2019-10-02T15:29:13Z,25,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k19fezt1i1h6ey,2019-10-02T15:29:13Z,{},hw3
2235,no,"<p>In my case I&#39;m getting the following:</p>
<p></p>
<pre>[[0.99999976 0.99999976 0.99999976 0.99999976]
 [0.9999999  0.9999999  0.9999999  0.9999999 ]
 [0.9999999  0.9999999  0.9999999  0.9999999 ]
 [0.         0.         0.         0.        ]]<br />&lt;&lt;&lt;&lt;</pre>
<p></p>",2019-10-02T15:39:41Z,25,Week 9/29 - 10/5,feedback,,ijctp4ucNy8,k19fsgb4o987e4,2019-10-02T15:39:41Z,{},hw3
2236,no,"<p>I have the same issue here. the Q I got for test 3 was:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjcbkq8bd6l2r%2Fk19q0m267myy%2FCapture.PNG"" alt="""" width=""569"" height=""54"" /></p>
<p>But have problems recreating right output from Q.  I keep getting &#34;&lt;,&lt;,&lt;,&lt;,&lt;,&lt;,v,v,v,v,v,v,v&#34;.</p>
<p></p>
<p>I did set seed for numpy and env and have the env upwrapped. </p>",2019-10-02T20:28:28Z,25,Week 9/29 - 10/5,feedback,,jcbkq8bd6l2r,k19q3tpovf849m,2019-10-02T20:28:28Z,{},hw3
2237,no,<p>np.around should help with the floats precision issue I think</p>,2019-10-02T22:01:56Z,25,Week 9/29 - 10/5,feedback,,jqmfnc46kl26eg,k19tg17f1guiv,2019-10-02T22:01:56Z,{},hw3
2238,no,<p>does no.around make every 0.999... to be 1? I don&#39;t understand if that is the reason I failed to get the correct output.</p>,2019-10-02T22:16:04Z,25,Week 9/29 - 10/5,feedback,,jcbkq8bd6l2r,k19ty7l5oip29b,2019-10-02T22:16:04Z,{},hw3
2239,no,"<p>Sorry, np.around makes every 0.999 to 1, but you&#39;re not supposed to do that.</p>
<p></p>
<p>Remember that you have to output the policy, not the actions. Good luck!</p>",2019-10-03T17:53:34Z,25,Week 9/29 - 10/5,feedback,,jqmfnc46kl26eg,k1b00gvapws452,2019-10-03T17:53:34Z,{},hw3
2240,no,"<p>You should be seeding your environment, but you should not be sampling actions from your environment: &#96;env.action_space.sample()&#96; is a bit of a problem here.</p>",2019-10-04T10:13:46Z,25,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1bz10j9pyj1m1,2019-10-04T10:13:46Z,{},hw3
2241,no,"<p>Here&#39;s a little more on HW4. This graph is just 10k on basic Q - nothing special.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk18ekqs05k6i%2Fhw4shared.png"" alt="""" /></p>",2019-10-01T22:18:14Z,40,Week 9/29 - 10/5,followup,,jc554vxmyuy3pt,k18el4vnmsr6ij,2019-10-01T22:18:14Z,{},hw4
2242,no,<p>Thank you Jacob!</p>,2019-10-02T00:25:41Z,40,Week 9/29 - 10/5,feedback,,jqkxzdmmolGf,k18j516nldc2fl,2019-10-02T00:25:41Z,{},hw4
2243,no,"<p>Very interesting, thanks for sharing.</p>",2019-10-02T11:31:03Z,40,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k196wpjp7mo4p2,2019-10-02T11:31:03Z,{},hw4
2244,no,"<p>Cool!</p>
<p></p>",2019-10-04T00:01:44Z,40,Week 9/29 - 10/5,feedback,,jzygktecb0e3i1,k1bd5xq6w6e15w,2019-10-04T00:01:44Z,{},hw4
2245,no,"<p>I solved HW4. The solution was really to keep it simple. All of the extra variance reduction techniques I used didn&#39;t warm up the Q like it should have. Taxi has loops in it, and those optimal loops can give a good score, but not a great score. If you don&#39;t warm up the Q then you end up becoming a victim of the maximization bias. Here&#39;s is a graph of the solution.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk19gtu0qmv8f%2Fhw4solutionshared.png"" alt="""" /></p>",2019-10-02T16:09:18Z,40,Week 9/29 - 10/5,followup,,jc554vxmyuy3pt,k19gujnugcy29g,2019-10-02T16:09:18Z,{},hw4
2246,no,"<p>Thanks for all the great data visualization.  I think that&#39;s really, really helpful when learning how these algorithms work.</p>
<p></p>
<p>My solution was a bit different I think.  I ran my race first, <em>then</em> warmed up, and it looks like I was able to converge to the optimal q-values in about $$7000$$ episodes.  My mean reward depends on how I compute the mean.  When you say &#34;mean reward should be $$9$$ for it to be considered passing&#34;, is that the mean <em>per-episode</em> after some point?  Or is it the mean over the last certain number of trailing episodes?</p>",2019-10-02T19:37:12Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k19o9wlvvly1c4,2019-10-02T19:37:12Z,{},hw4
2247,no,"<p>The OpenAI &#34;criteria&#34; is to get a mean reward of 9.7 after 100 episodes of querying the model. Not the mean reward from training (the &#34;opposite of good&#34; graph above), as that&#39;s really just a metric for how well you are over-fitting :)</p>",2019-10-02T21:37:32Z,40,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19skn6evmkqo,2019-10-02T21:37:32Z,{},hw4
2248,no,"<p>what WOULD be interesting here is to loop over episodes of warmup until the optimal solution is found, then apply some additional training using the real Q algorithm (not the warmup) followed by the 100 episode test. Hmmmmm</p>",2019-10-02T21:38:36Z,40,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19sm0spfc32ec,2019-10-02T21:38:36Z,{},hw4
2249,no,"<p>With a trailing mean over $$100$$ episodes the best I saw was $$9.55$$.  That was after 300k episodes.  I feel like part of getting a good score there is just the length of time you run it, hoping to get a lucky $$100$$ episodes in a row, no?  </p>
<p></p>
<p>It&#39;s the case that <strong>if</strong> our Q-values really <em>were</em> converged to the optimal Q values, then we&#39;d be completely done right?  So, if, say, you passed Heroku and assumed that those 10 random samples of your state space were an accurate representation that <em>all</em> your Q values were optimal, then what would additional training get you?</p>",2019-10-02T21:47:14Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k19sx4qm6m91ko,2019-10-02T21:47:14Z,{},hw4
2250,no,"<p>That&#39;s the idea, but the optimal Q values does not necessarily reflect the optimal path here. Without using the warmup I was able to get around 9 for the average score, but never got anywhere near the <em>optimal Q values</em>.</p>
<p></p>
<p>Then you have to ask Miguel et al, &#34;what does optimal mean, here?&#34; <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>",2019-10-02T22:06:54Z,40,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19tmeoxh5y3jc,2019-10-02T22:06:54Z,{},hw4
2251,no,"<p>Hmmm. This is an interesting conversation about on-policy and off-policy learning. Q-learning is off-policy, and that basically means it can learn a target policy even by following a behavioral policy. In Q-learning, an optimal action-value function guarantees an optimal path as much as knowing a good diet makes you lose weight. Knowledge != practice.</p>",2019-10-04T10:09:30Z,40,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1byvjg37y9638,2019-10-04T10:09:30Z,{},hw4
2252,no,"<p>Wait, if you know the optimal action-value function, why couldn&#39;t you just argmax it in every state to form an optimal policy?  If I know a good diet, it still might take me months of hard work to follow it and lose weight.  But if I have the optimal action-value function, it just takes me one for loop over the state space to get me *an* optimal policy (there are many for taxi I think).</p>",2019-10-04T13:21:11Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c5q1c767v13f,2019-10-04T13:21:11Z,{},hw4
2253,no,"<p>Besides checking that your $$Q$$-values don&#39;t change, another sanity-check in taxi is making sure you never score less than $$3$$ reward during an episode.  An optimal policy always nets you between $$3$$ and $$15$$ (inclusive) reward depending on how lucky you are with initial taxi placement and pickup and drop-off points.</p>
<p></p>
<p>Even with an optimal policy, you can easily go tens of thousands of episodes without seeing a $$9.0$$ for the mean reward of your trailing 100 episodes, if you get unlucky and rack up a bunch of low rewards.  I have seen one $$9.65$$ up to this point, but typically my high over ~50,000 episodes with a trained agent is between $$9.4$$ and $$9.5$$.  I doubt achieving $$9.7$$ is a necessary condition for solving this problem.</p>",2019-10-04T14:31:46Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c88tc1q372fh,2019-10-04T14:31:46Z,{},hw4
2254,no,"<p>Ha, well what if each time step is a month, then it is the same thing, you still have to follow the optimal path in an episode to get you from the initial to the terminal state.</p>
<p></p>
<p>My point is, you can argmax, but in off-policy learning, you can acquire knowledge despite your behavior. Like learning from reading a book, or watching a lecture. That&#39;s off-policy. That means you can learn &#34;know&#34; to do it regardless of whether you use that knowledge or not. </p>
<p></p>
<p>Quick question, is this environment deterministic or stochastic!? And how does that change things!?</p>",2019-10-04T16:05:37Z,40,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cblhxuxe127y,2019-10-04T16:05:37Z,{},hw4
2255,no,"<p>Taxi is deterministic. We know that action A will always result in s&#39; given s. Whereas, the diet environment you are discussing is stochastic. You don&#39;t know that s&#39; will always result from (s,a).</p>
<p></p>
<p>Taxi doesn&#39;t have to be deterministic. It can model a random failure in the taxicab vehicle which could result in suboptimal policies if you have to include fixup time as a function of distance to the start.</p>
<p></p>
<p>This is a bit like that soccer player robot described in the literature. The one that just sat there touching the ball because it was given a high reward for touching the ball (false correlations). The tax would favor pickups closer to start as a function of probability-of-breakdown.</p>",2019-10-04T16:16:32Z,40,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1cbzjkcxwws0,2019-10-04T16:16:32Z,{},hw4
2256,no,"<p>Yeah, the taxi environment is deterministic, whereas frozen lake was stochastic.</p>
<p></p>
<p>Although in taxi, there is stochasticity between episodes (starting point of taxi and pickup/drop-off points) whereas in frozen lake the map was constant.  This stochasticity between maps in taxi is what led to the subtle &#34;terminal state value bug&#34; caught by Germayne, which we were warned of in the HW4 handout but many of us ignored.</p>
<p></p>
<p><strong>Edit</strong>: &#34;<em>And how does that change things!?&#34;</em></p>
<p></p>
<p>If the environment is stochastic, then I would hope that Q-learning would converge to the expected values of the value function.  In frozen lake, for example, if moving right gets you to the goal and moving south gets you to a hole, then the action &#34;&gt;&#34; sometimes gives you the win and sometimes gives you a loss, but hopefully the Q of that state-action pair reflects this average accurately.  Is this not the case?</p>
<p></p>
<p><strong>Edit2: </strong>Wait a minute!  If the environment is deterministic, $$\alpha$$ <em>doesn&#39;t </em>matter in the long run, because the prediction error gets smaller and smaller, and hence the updates get smaller and smaller, leading to convergence.  BUT, in a stochastic environment, even if your state-action value happened to be at the &#34;converged&#34; expected value, every time you get one of your different stochastic results, the update moves by a huge chunk, determined by $$\alpha$$, and this will NEVER stop, unless somehow you can also reduce $$\alpha...$$.</p>",2019-10-04T17:13:49Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1ce17f05te222,2019-10-04T17:13:49Z,{},hw4
2257,no,"<p>&#34;<em>My point is, you can argmax, but in off-policy learning, you can acquire knowledge despite your behavior.&#34;</em></p>
<p><em></em></p>
<p>Yeah, with on-policy methods like SARSA, you can&#39;t explore <em>and</em> converge to the optimal value function at the same time, right?  Because if you&#39;re $$\epsilon$$-greedy in exploration, you&#39;re forced to be $$\epsilon$$-greedy in the update step as well.  Whereas with off-policy methods, you can explore wherever the heck you want while still having your value function estimates moving in the direction of the optimal value function.</p>
<p></p>
<p>Is that right?</p>",2019-10-04T17:33:09Z,40,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1ceq2vs83v2to,2019-10-04T17:33:09Z,{},hw4
2258,no,"<p>Good insights!</p>
<p></p>
<p>Sarsa converges to the optimal policy that still explores, whatever that means (it means it is not optimal, it is near-optimal). So, if your epsilon is 0.3, then your &#34;optimal policy that still explores&#34; will be pretty crappy.</p>
<p></p>
<p>Time to look up what the heck GLIE means.</p>
<p></p>",2019-10-04T18:29:29Z,40,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cgqif6bi94kt,2019-10-04T18:29:29Z,{},hw4
2259,no,"I&#39;m not sure I understand the answer given here. In the case where every action in one row of the Q table has a value of 1, that doesn&#39;t give me any information about which direction to move, correct? <div><br /></div><div>I&#39;ve found that if I terminate the problem earlier than the given number of runs I get rows in the Q table where not every value is 1, but as I understand it we have to run it the specified number of cycles. If I run more than the specified number, the values stay at 1.</div>",2019-10-01T23:28:05Z,26,Week 9/29 - 10/5,followup,,jl284xdcifz44g,k18h2yfsrze6q4,2019-10-01T23:28:05Z,{},hw3
2260,no,"<p><em> that doesn&#39;t give me any information about which direction to move, correct?</em> <br /></p>
<p></p>
<p>Sure it does. It tells you that any action is equally valued.</p>",2019-10-01T23:33:24Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k18h9sxn1vz3sc,2019-10-01T23:33:24Z,{},hw3
2261,no,"<p>But we still need to output a direction to go, correct? If I have a row of all 1s, I can&#39;t give heroku an answer</p>",2019-10-01T23:40:52Z,26,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k18hjee9bn51s7,2019-10-01T23:40:52Z,{},hw3
2262,no,"<p>The on-policy metric is argmax. So each state has an on-policy action, which would be the argmax of the actions in that state.</p>",2019-10-01T23:42:36Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k18hlml6hib412,2019-10-01T23:42:36Z,{},hw3
2263,no,"<p>Let me try explaining with the shorter example. After the 49553 prescribed episodes for the third example (amap=SFFG) I get the following Q-table: </p>
<p></p>
<pre>[[1. 1. 1. 0.]<br /> [1. 1. 1. 0.]<br /> [1. 1. 1. 0.]<br /> [0. 0. 0. 0.]]</pre>
<p></p>
<p>It&#39;s unclear how to use this Q table to produce the desired output: &lt;,&lt;,v,&lt;. Is there an issue with my code that generated this table, or my understanding of going from the table to &lt;,&lt;,v,&lt; ?</p>
<p></p>",2019-10-01T23:53:03Z,26,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k18hz2ilocg6ei,2019-10-01T23:53:03Z,{},hw3
2264,no,"<p>1. Let&#39;s assume that your $$Q$$ values are correct, which they may not be.</p>
<p></p>
<p>2. Let&#39;s assume that your matrix is mapped to the state space as Row 1 -&gt; top-left corner of grid, Row 2 -&gt; top-right corner, Row 3 -&gt; bottom-left corner, Row 4 -&gt; bottom-right corner</p>
<p></p>
<p>3. Let&#39;s assume that the first element of each row denotes the direction West, the second element South, third element East, and fourth element North</p>
<p></p>
<p>Then, we would find the direction with the highest $$Q$$ value for each state, and write the appropriate character for that direction.  In the case of ties, you should use the same tiebreaker that the grader uses.</p>
<p></p>
<p>4. Let&#39;s assume that you break ties by picking the first element of each row where the maximum $$Q$$ value occurs.</p>
<p></p>
<p>Then, as you have your array written, the output would be: &lt;,&lt;,&lt;,&lt;.</p>
<p></p>
<p><strong>However</strong>, if you print your array out with $$16$$-digit precision, you may see that those $$1.$$&#39;s are not really $$1$$&#39;s, but some floating point number less than $$1$$, which may impact which element in each row is the greatest.</p>",2019-10-02T08:01:43Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k18zfhv68pq26b,2019-10-02T08:01:43Z,{},hw3
2265,no,"<p>Are you using the environment&#39;s action sampling or your own action sampling?</p>
<p></p>
<p>Are you seeding numpy and using numpy&#39;s random functions, or are you using python random instead and forgetting to seed that too?</p>",2019-10-01T22:24:22Z,59,Week 9/29 - 10/5,followup,,jc554vxmyuy3pt,k18et18lb1f1j5,2019-10-01T22:24:22Z,{},hw3
2266,no," I&#39;m using the environment&#39;s random sampling and numpy&#39;s random sampling, and I&#39;m seeding both the environment and numpy.",2019-10-01T23:29:11Z,59,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k18h4dldmjf1fd,2019-10-01T23:29:11Z,{},hw3
2267,no,<p>The environment&#39;s sampling won&#39;t work. It will not respect any of the seeding you are doing.</p>,2019-10-01T23:32:17Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k18h8cyzs81ua,2019-10-01T23:32:17Z,{},hw3
2268,no,"<p>Ok, I switched to just taking a randint from numpy, (0,3) and it seems to be working consistently. Thanks for the help</p>",2019-10-01T23:39:02Z,59,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k18hh1ung6k6q3,2019-10-01T23:39:02Z,{},hw3
2269,no,"<p>I think we added that to the document, correct?</p>",2019-10-02T00:03:27Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k18icg33ir2113,2019-10-02T00:03:27Z,{},hw3
2270,no,"<p>Yes, I see that now. I didn&#39;t understand what it meant exactly on my first readthrough</p>",2019-10-02T14:12:44Z,59,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k19comgz7zvii,2019-10-02T14:12:44Z,{},hw3
2271,no,"<p>Ok, good. Thanks for confirming.</p>",2019-10-04T09:59:19Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1byig0wwwgac,2019-10-04T09:59:19Z,{},hw3
2272,no,"<p>Btw, shouldn&#39;t it be randint(0, 4) to generate integers from [0, 1, 2, 3] for the 4 possible actions?</p>",2019-10-02T11:30:19Z,59,Week 9/29 - 10/5,followup,,ijctp4ucNy8,k196vrl8fcb3f9,2019-10-02T11:30:19Z,{},hw3
2273,no,"<p>Yes, but just do &#96;np.random.randint(env.action_space.n)&#96;.</p>",2019-10-02T11:32:44Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k196yvieaer5s1,2019-10-02T11:32:44Z,{},hw3
2274,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  I&#39;m failing to see the difference between randint(0,4) and np.random.randint(env.action_space.n), or how doing that allows to favor the greedy action $$(1-\epsilon)$$ of the time, unless we use another random number ???</p>
<p></p>
<p>but, besides, I can think of different ways to implement a greedy choice.</p>
<p>I know sarsa converges to the optimal policy so it shouldn&#39;t make a difference, yet I can find the right answer only for the short sequence.  The other 2 are very similar though, so I was thinking maybe the number of iterations is not high enough to erase the last differences between two different implementations.  </p>",2019-10-02T15:15:02Z,59,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k19ewrce3jy1nq,2019-10-02T15:15:02Z,{},hw3
2275,no,"<p>HW3 is not trying to find the optimal policy, just a policy.</p>
<p></p>
<p>The difference between the two is just a reusable code issue: not all environments have 4 actions.</p>",2019-10-03T02:32:26Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1a33wcv1el7jq,2019-10-03T02:32:26Z,{},hw3
2276,no,"<p>Ahhhhhhh... haha, you guys never specifically wrote there were 4 actions now that I re-read the hw... well done.. thx for the insight, not sure I would have ever thought about that.  </p>",2019-10-03T03:47:22Z,59,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1a5s99dijz4om,2019-10-03T03:47:22Z,{},hw3
2277,no,"<p>One sec. all of the environments for HW3 have the same number of actions; HW4, however, is different.</p>",2019-10-04T09:58:59Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1byi0i8bpl47,2019-10-04T09:58:59Z,{},hw3
2278,no,<p>Do we need to do env.reset every episode? Does it reset the seed value?</p>,2019-10-02T22:16:16Z,59,Week 9/29 - 10/5,followup,,jl2egn5k4zo4lp,k19tyglet5l57d,2019-10-02T22:16:16Z,{},hw3
2279,no,<p>env.reset resets your state to the start. The seed is only reset when you re-seed it.</p>,2019-10-02T22:20:29Z,59,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19u3w3fc42w4,2019-10-02T22:20:29Z,{},hw3
2280,no," I have the same problem in &#64;520 I think, hopefully we can both find an answer",2019-10-01T23:30:23Z,26,Week 9/29 - 10/5,followup,,jl284xdcifz44g,k18h5x1z7533h6,2019-10-01T23:30:23Z,{},hw3
2281,no,"<p>I see, and just so I&#39;m totally clear: whether it&#39;s a random action due to epsilon or the action that maximizes return, we simply take the action using env.step(action), and gym returns the state we&#39;re in after that action (which is affected by slipping)?</p>",2019-10-02T03:33:10Z,59,Week 9/29 - 10/5,followup,,jl284xdcifz44g,k18pu5lefad3qd,2019-10-02T03:33:10Z,{},hw3
2282,no,"<p>Exactly, the slipping results from the dynamics of our world (transition matrix). We take an action and the environment decides whether we managed to make it or slipped. </p>",2019-10-02T05:29:43Z,59,Week 9/29 - 10/5,feedback,,jl1acpoc4HA9,k18u00qfpl06zw,2019-10-02T05:29:43Z,{},hw3
2283,no,"<p>I found out that one can disable the slipping by using an arg is_slippery... nice!</p>
<p></p>
<pre>env = gym.make(&#34;FrozenLake-v0&#34;, desc=..., is_slippery=False)</pre>
<p>Could be helpful to debug our code.  </p>
<div>
<div></div>
</div>",2019-10-02T11:44:24Z,59,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k197dvbbpov5j5,2019-10-02T11:44:24Z,{},hw3
2284,no,<p>Good! Maybe post a note separately so people can see it!?</p>,2019-10-04T15:59:04Z,59,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cbd2nv3c84i9,2019-10-04T15:59:04Z,{},hw3
2285,no,<p></p>,2019-10-02T15:30:19Z,26,Week 9/29 - 10/5,followup,,jl284xdcifz44g,k19fgetv6my7oa,2019-10-02T15:30:19Z,{},hw3
2286,stud,"<p>Just checking, are you setting seed for the env?</p>",2019-10-02T15:30:57Z,26,Week 9/29 - 10/5,followup,a_1,,k19fh7jy13pjc,2019-10-02T15:30:57Z,{},hw3
2287,no,"<p></p><div>
<div>
<p>If you mean setting env.seed(seed), then yes. I&#39;m generating all my random numbers through numpy though</p>
<p></p>
</div>
</div>",2019-10-02T15:31:18Z,26,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k19fhntdkuiy9,2019-10-02T15:31:18Z,{},hw3
2288,stud,And you&#39;ve set numpy seed as well?,2019-10-02T17:03:48Z,26,Week 9/29 - 10/5,feedback,a_0,,k19ismd1d842fm,2019-10-02T17:03:48Z,{},hw3
2289,no,Yes,2019-10-02T18:00:10Z,26,Week 9/29 - 10/5,feedback,,jl284xdcifz44g,k19kt3t24xi1xn,2019-10-02T18:00:10Z,{},hw3
2290,no,"<p>From what I remember from David Silver&#39;s video, the epsilon is necessary to not always follow the same, greedy path but to allow the algorithm to explore, so that&#39;s what epsilon accomplishes, but in my understanding, it&#39;s not there to give a &#39;better&#39; optimal if that thing even existed....</p>
<p>But the optimal path is the optimal path anyway imho.</p>
<p></p>",2019-10-02T18:06:41Z,26,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k19l1hfmf4562p,2019-10-02T18:06:41Z,{},hw3
2291,no,<p>Thanks much!</p>,2019-10-02T18:17:01Z,26,Week 9/29 - 10/5,followup,,jzk2u4t3n5o5el,k19lerwfimk6a6,2019-10-02T18:17:01Z,{},hw3
2292,no,"<p>It is also possible to use numpy to copy the string into a byte array and reshape it. FrozenLakeEnv converts its input to a byte array internally anyway.</p>
<p></p>
<pre>import numpy as np<br /><br />def to_array(amap)
    side = int(np.sqrt(len(amap)))
    return np.array(amap, dtype=&#39;c&#39;).reshape((side, side))</pre>
<p></p>",2019-10-04T06:42:23Z,26,Week 9/29 - 10/5,followup,,is8ald0uljj3u4,k1brh6qm98b24v,2019-10-04T06:42:23Z,{},hw3
2293,no,"Alternatively what I did was simply taking the sqrt of the length of the string as n, then reshape directly into (n,n) as an array. This array can be passed directly into desc ",2019-10-04T12:03:55Z,26,Week 9/29 - 10/5,followup,,jvfpllmsggt7p4,k1c2yo0qr5o70f,2019-10-04T12:03:55Z,{},hw3
2294,no,"Thank you, saved some time :-)",2019-10-05T06:28:47Z,26,Week 9/29 - 10/5,followup,,jc5n7oq5zop3e8,k1d6fjkoyqq5hn,2019-10-05T06:28:47Z,{},hw3
2295,no,"<p>I used the &#34;unwrapped&#34; still cannot get the correct answer. really confused now.</p>
<p>I set the seed correctly by using env.seed and np.random.seed, and used np.random.randint(env.action_space.n) to generate the action when np.random.uniform(0,1) &lt; epsilon. </p>
<p>For test 1, I got the Q matrix as:</p>
<p>[[0.20944331 0.48653866 0.49730795 0.68301692]<br />[0.64341951 0.9112584 0.85111285 0.65556764]<br />[0.88837089 0.93115725 0.9167586 0.91419646]<br />[0.93214126 0.92292253 0.94354097 0.93449757]<br />[0. 0. 0. 0. ]<br />[0.25413104 0.47579825 0.92797157 0.81091787]<br />[0.94250322 0.94107199 0.94463412 0.87908423]<br />[0.94330518 0.96351515 0.95250744 0.94962169]<br />[0.53645891 0.89473301 0.29734493 0.67329215]<br />[0.85421854 0.9185382 0.81201688 0.77385265]<br />[0.94364073 0.90618541 0.96586523 0.93488531]<br />[0.95529001 0.97584787 0.96304856 0.96718743]<br />[0.79949079 0.85530704 0.8713067 0.85038178]<br />[0.85332313 0.88474512 0.90352541 0.87641602]<br />[0.9494872 0.95591453 0.96495681 0.95634992]<br />[0. 0. 0. 0. ]]</p>
<p> After getting Q, I took whatever action that gives the max value at each state. But the result I got is: ^,^,v,v,&gt;,v,&gt;,v,v,&gt;,v,&gt;,v,v,&gt;,&gt;.</p>
<p>Anyone could share any thoughts? </p>",2019-10-02T21:56:26Z,26,Week 9/29 - 10/5,followup,,jcbkq8bd6l2r,k19t8yemyt1rv,2019-10-02T21:56:26Z,{},hw3
2296,no,"<p></p><pre>0.64341951 0.9112584 0.85111285 0.65556764</pre>
<p>Why are you exporting &#34;UP&#34; for that state?</p>
<p></p>
<pre>action_s = [&#39;&lt;&#39;,&#39;v&#39;,&#39;&gt;&#39;,&#39;^&#39;]</pre>
<p>The second action would be &#34;v&#34; not &#34;^&#34;, correct?</p>",2019-10-02T22:05:07Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19tk4kj9d0tr,2019-10-02T22:05:07Z,{},hw3
2297,no,"<p>oo, your response made me realize that we were supposed to output the policy, not the path! Thank you!</p>",2019-10-02T22:23:15Z,26,Week 9/29 - 10/5,feedback,,jcbkq8bd6l2r,k19u7fy6ysifv,2019-10-02T22:23:15Z,{},hw3
2298,no,<p>This is what was going wrong with my code. I was trying to output the path. Thanks a lot.</p>,2019-10-04T01:04:29Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k1bfenbef0s5dv,2019-10-04T01:04:29Z,{},hw3
2299,no,"<p>question: what do we do in the case when all values are the same for a state, i.e. [0. 0. 0. 0. ]? or is that the sign of an error that shouldn&#39;t be occuring</p>",2019-10-04T01:18:20Z,26,Week 9/29 - 10/5,feedback,,jqu95q68ljj1pn,k1bfwg1ahy258g,2019-10-04T01:18:20Z,{},hw3
2300,no,<p>Terminal states are expected to have 0s. If any other states have all 0s then it is definitely an implementation bug.</p>,2019-10-04T10:59:24Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k1c0np82l3y37r,2019-10-04T10:59:24Z,{},hw3
2301,no,"<p>argmax takes the index of the first max. But yeah, all zeros shouldn&#39;t be common unless its a terminal state. That&#39;d be &#39;&lt;&#39; on terminal states.</p>",2019-10-04T15:02:34Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c9cf4wgc62hf,2019-10-04T15:02:34Z,{},hw3
2302,no,"<p>I have the same Q values as yours. I have been debugging verifying the seed and nothing. Interestingly, I match the answer for the last test case....</p>",2019-10-03T17:35:19Z,26,Week 9/29 - 10/5,followup,,jl0c61kgrmsY,k1azd0emq9d1t3,2019-10-03T17:35:19Z,{},hw3
2303,no,"<p></p><pre>What am I be doing wrong<br />[[0.18116587 0.58833579 0.50652519 0.70610496]
 [0.55571517 0.74478443 0.87935735 0.56653543]
 [0.882517   0.90130658 0.92283793 0.89331336]
 [0.93622493 0.92898361 0.94016802 0.93763283]
 [0.         0.         0.         0.        ]
 [0.38766766 0.70929993 0.83531693 0.11737437]
 [0.900576   0.90524473 0.93432203 0.91703499]
 [0.93247484 0.93868581 0.95024852 0.93951684]
 [0.51901394 0.8870177  0.40598932 0.67313049]
 [0.85773595 0.90440545 0.84108798 0.81712463]
 [0.912501   0.91931946 0.90751502 0.93076915]
 [0.94077314 0.96213316 0.95430773 0.94876802]
 [0.88264286 0.89215873 0.89019668 0.8981318 ]
 [0.90281552 0.90703892 0.90453071 0.92068735]
 [0.936839   0.95110885 0.9407866  0.94895557]
 [0.         0.         0.         0.        ]]<br /><br />^&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;v&#39;, &#39;v&#39;, &#39;^&#39;, &#39;v&#39;, &#39;^&#39;, &#39;^&#39;, &#39;v&#39;, &#39;&lt;&#39;</pre>
<p></p>",2019-10-03T17:40:30Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1azjoeo9631o7,2019-10-03T17:40:30Z,{},hw3
2304,no,<p>Are you using gym.make() or are you using the suggested solution up above by importing toy_text?</p>,2019-10-03T19:22:26Z,26,Week 9/29 - 10/5,feedback,,idfzuuu9fnI,k1b36r7r1it3ds,2019-10-03T19:22:26Z,{},hw3
2305,no,"<p></p><pre>I am using gym.make()<br /><br />I get a error during execution when i use this code below
toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped
<br /></pre>",2019-10-03T19:29:58Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1b3gg9yc531fx,2019-10-03T19:29:58Z,{},hw3
2306,no,"<p>did you import toy_text?</p>
<pre>from gym.envs import toy_text</pre>
<p></p>",2019-10-03T19:31:18Z,26,Week 9/29 - 10/5,feedback,,idfzuuu9fnI,k1b3i5tduv51sl,2019-10-03T19:31:18Z,{},hw3
2307,no,"<p>I did import that but I just did and I comment of out gym.make().</p>
<p>I am getting error env not defined. checking my code</p>",2019-10-03T19:36:47Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1b3p7wsqhi5it,2019-10-03T19:36:47Z,{},hw3
2308,no,<p>Oh common it worked! I can&#39;t believe this</p>,2019-10-03T19:38:45Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1b3rqzvfflv2,2019-10-03T19:38:45Z,{},hw3
2309,stud,"<p>I got exactly same Q table as yours. However I do use </p>
<p>toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped<br /></p>
<p></p>
<p>Did you get correct answer now?  I am really confused now...  totally no idea how should I debug.</p>",2019-10-03T21:04:59Z,26,Week 9/29 - 10/5,feedback,a_0,,k1b6un22j9p5k0,2019-10-03T21:04:59Z,{},hw3
2310,no,<p>I was getting same Q table when I was not using .unwrapped</p>,2019-10-04T11:00:57Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k1c0poxcdc83t7,2019-10-04T11:00:57Z,{},hw3
2311,no,"<p>The .unwrapped just gets rid of the Time wrapper they add to the frozen lake. Some environments &#34;time out&#34; after ~100 steps and if the frozen lake map you input is very large, you may need more than 100 steps to land in a terminal state. So, .unwrapped may not make any difference in small environments, but it could in large.</p>",2019-10-04T15:05:13Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c9fu1qfx15h0,2019-10-04T15:05:13Z,{},hw3
2312,no,"<p>Re-implemented SARSA a few times, always getting the same policy and value matrix, but it&#39;s not resembling the intended policy nor any of the Q matrices shared on Piazza. Appreciate any insights people may have on what might be off:</p>
<p></p>
<p>FL:</p>
<p>SFFF</p>
<p>HFFF</p>
<p>FFFF</p>
<p>FFFG</p>
<p><br /></p>
<p>[[0.41391731 0.45249536 0.58572891 0.6987519 ]</p>
<p> [0.75000732 0.72338199 0.73547128 0.77697875]</p>
<p> [0.87209776 0.86735463 0.90857729 0.86273895]</p>
<p> [0.95154388 0.94396557 0.95152454 0.94123486]</p>
<p> [0.         0.         0.         0.        ]</p>
<p> [0.63717217 0.71787557 0.86752862 0.38729601]</p>
<p> [0.85675261 0.92019122 0.93686261 0.92326077]</p>
<p> [0.95011189 0.94826008 0.95374743 0.95254996]</p>
<p> [0.61587999 0.77337314 0.42205955 0.54606774]</p>
<p> [0.83875894 0.82671337 0.82544697 0.82402305]</p>
<p> [0.92152498 0.90544008 0.95687758 0.92111495]</p>
<p> [0.9788917  0.96985186 0.96875144 0.96831364]</p>
<p> [0.80683314 0.86702586 0.82697318 0.85007532]</p>
<p> [0.8942945  0.87999381 0.92378154 0.85773469]</p>
<p> [0.94569715 0.96834569 0.94735016 0.94750496]</p>
<p> [0.         0.         0.         0.        ]]</p>
<p>V:</p>
<p>[[0.6987519  0.77697875 0.90857729 0.95154388]</p>
<p> [0.         0.86752862 0.93686261 0.95374743]</p>
<p> [0.77337314 0.83875894 0.95687758 0.9788917 ]</p>
<p> [0.86702586 0.92378154 0.96834569 0.        ]]</p>
<p></p>
<p>Actual:</p>
<p>^^&gt;&lt;</p>
<p>&lt;&gt;&gt;&gt;</p>
<p>v&lt;&gt;&lt;</p>
<p>v&gt;v&lt;</p>
<p></p>
<p>Expected:</p>
<p>^vv&gt;</p>
<p>&lt;&gt;&gt;v</p>
<p>vv&gt;v</p>
<p>&gt;&gt;&gt;&lt;</p>",2019-10-03T21:23:24Z,26,Week 9/29 - 10/5,followup,,isde34zracb1mz,k1b7ibddvc01v2,2019-10-03T21:23:24Z,{},hw3
2313,no,"<p>Since we&#39;re not running to convergence (we&#39;re inputting arbitrary parameters and just trying to match the grader&#39;s output), it&#39;s really hard to look at state values and conclude anything.  Put another way, the random variation from different random seeds could swamp out any meaning from all but the most obvious state value errors.  Looking at your action values, they look *reasonable enough* (actions heading into the hole are discounted, actions near the goal are inflated) where you might be better off taking a different tack.  Maybe make sure you are calling the appropriate number of randomization functions so that there isn&#39;t a seed mismatch.</p>",2019-10-03T21:56:33Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1b8oy9zxt11e5,2019-10-03T21:56:33Z,{},hw3
2314,no,"<p>Thanks, Vahe. If I understand correctly, the first step in each episode requires at least 2 random function calls and at most 4, and every step after the first requires at least 1 call, and at most 2.</p>",2019-10-03T22:08:34Z,26,Week 9/29 - 10/5,feedback,,isde34zracb1mz,k1b94exc467204,2019-10-03T22:08:34Z,{},hw3
2315,no,<p>Solved! In case it helps others: the error was handling terminal states as a special case unnecessarily.</p>,2019-10-03T22:15:17Z,26,Week 9/29 - 10/5,feedback,,isde34zracb1mz,k1b9d1yen5hpx,2019-10-03T22:15:17Z,{},hw3
2316,no,"<p>God. I figured out why. The above issue is due to the gym environment. initially I used the following, which is not well behaving. The max step is 100 steps for one episode. </p>
<pre>gym.make(&#39;FrozenLake-v0&#39;, desc = env_map)</pre>
<p></p>
<p>If I created the env using the following, everything is behaving correctly. Hope this helps other people too, because I saw people mentioning gym.make() function. Please don&#39;t use it!!</p>
<pre>toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped</pre>
<p></p>",2019-10-02T20:17:08Z,26,Week 9/29 - 10/5,followup,,j6ln9puq99s5uv,k19pp8vg7lx1i4,2019-10-02T20:17:08Z,{},hw3
2317,no,"<p>Thanks so much!! You saved me millions of times.</p>
<p></p>",2019-10-02T21:06:27Z,26,Week 9/29 - 10/5,feedback,,jzj6lng3im15y3,k19rgo3tvcr7ot,2019-10-02T21:06:27Z,{},hw3
2318,no,"<p>When I set</p>
<p>env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv().unwrapped</p>
<p></p>
<p>I get the error</p>
<p></p>
<p>AttributeError: module &#39;gym.envs&#39; has no attribute &#39;toy_text&#39;</p>
<p></p>
<p>What do I need to do to get it to work?</p>
<p></p>",2019-10-02T21:06:33Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k19rgss7ltd2b4,2019-10-02T21:06:33Z,{},hw3
2319,no,"<p>you need to add one more line:</p>
<pre>import gym.envs.toy_text;</pre>",2019-10-02T21:10:13Z,26,Week 9/29 - 10/5,feedback,,jcbkq8bd6l2r,k19rlj2obgr3lo,2019-10-02T21:10:13Z,{},hw3
2320,no,"<p>Thanks. This is working, but I am still getting random behavior. Q table is different on every execution. How shall I set the seed for gym?</p>
<p></p>
<p>I am using env.seed(), random.seed() and np.random.seed() with the same value<br /><br /></p>",2019-10-02T21:37:23Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k19skgabgt5je,2019-10-02T21:37:23Z,{},hw3
2321,no,<p>Do not use the action sampling in the env. You must use your own random action sampling.</p>,2019-10-02T21:42:05Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k19sqid94g2bk,2019-10-02T21:42:05Z,{},hw3
2322,no,<p>Thanks. That helps.</p>,2019-10-02T21:53:00Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k19t4jo2jb8270,2019-10-02T21:53:00Z,{},hw3
2323,no,<p>How long have people experienced their hw3 running? The last example in the hw with env &#39;SFFG&#39; was taking longer than 20 min for me</p>,2019-10-03T00:38:51Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k19z1tbec2y6t3,2019-10-03T00:38:51Z,{},hw3
2324,no,<p>typically 5 minutes for all test and heroku problems at the same time.</p>,2019-10-03T01:27:44Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1a0soockz41ep,2019-10-03T01:27:44Z,{},hw3
2325,no,"<p>Ok I got my time issue fixed, now trying to figure out why my sample answers aren&#39;t working.</p>
<p></p>
<p>For the greedy method we use a random action if out random number is &lt; epsilon correct?</p>",2019-10-03T03:16:35Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1a4oo5qpoh23i,2019-10-03T03:16:35Z,{},hw3
2326,no,"<p>Yes, if rand_val &lt; epsilon, take random action. I&#39;m getting reasonable policies but not the same as the HW though. </p>",2019-10-03T03:24:11Z,26,Week 9/29 - 10/5,feedback,,ijctp4ucNy8,k1a4yg1e3pl58q,2019-10-03T03:24:11Z,{},hw3
2327,no,"<p>&#64;Yong</p>
<p></p>
<p>Hi All,</p>
<p></p>
<p>I added:</p>
<p></p>
<pre>import gym.envs.toy_text;</pre>
<p></p>
<p></p>
<p>but I am still having an issue:</p>
<p></p>
<pre>env = toy_text.frozen_lake.FrozenLakeEnv(desc = &#39;SFFFHFFFFFFFFFFG,&#39;).unwrapped<br />NameError: name &#39;toy_text&#39; is not defined</pre>
<p></p>
<p>Any suggestions?</p>",2019-10-04T04:39:49Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1bn3jzgxhn7hx,2019-10-04T04:39:49Z,{},hw3
2328,no,"<p>Isn&#39;t this mentioned in the assignment document?</p>
<p></p>",2019-10-04T15:08:54Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c9kkc09381g7,2019-10-04T15:08:54Z,{},hw3
2329,no,<p>No its the last recorded change for all states</p>,2019-10-03T00:19:00Z,43,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k19ycajkv5514w,2019-10-03T00:19:00Z,{},hw4
2330,no,<p>I can sort of cheat around this by specifying the $$\triangle Q $$ less than some value for a specific state.</p>,2019-10-03T00:20:20Z,43,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k19ye0avc4m2z2,2019-10-03T00:20:20Z,{},hw4
2331,no,<p>But what would it look like to test convergence for all states?</p>,2019-10-03T00:20:46Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k19yekuvbkb3wq,2019-10-03T00:20:46Z,{},hw4
2332,no,"<p>During each episode, we don&#39;t visit every state-action pair, right?  So let&#39;s say that during one episode, the values of all the state-action pairs visited match the last recorded state-action value.  Then the $$\Delta Q$$ for <em>those</em> state-action values is $$0$$.  So then you keep going through episodes until all the $$\Delta Q$$s are zero, right?  That would work in theory I think if you set your initial values of the $$\Delta Q$$s to $$1$$.</p>
<p></p>
<p>But as you saw, it never converged, right?  Maybe that&#39;s because certain state-action pairs were never getting visited again?</p>",2019-10-03T00:29:39Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k19ypzgpyod1p2,2019-10-03T00:29:39Z,{},hw4
2333,no,<p>yeah that&#39;s how I understand it. Any idea around it? Im going to run a very long one where all weight changes converge.</p>,2019-10-03T01:12:15Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1a08rz4zfa7of,2019-10-03T01:12:15Z,{},hw4
2334,no,<p>Make sure you select a suitable learning rate. I was having trouble converging until I lowered mine. Also my convergence critera is (abs(Q_old - Q).max()) has to be less than some small number.</p>,2019-10-03T01:49:04Z,43,Week 9/29 - 10/5,feedback,,jl3we43d3bp15p,k1a1k46agma3cv,2019-10-03T01:49:04Z,{},hw4
2335,no,"<p>I think there&#39;s another complication as well: I don&#39;t think all states are possible.  Can a passenger pick-up location be the same as a drop-off location?</p>
<p></p>
<p>If not, then you are guaranteed to never visit those states.</p>",2019-10-03T01:54:42Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1a1rdh6u1m4s4,2019-10-03T01:54:42Z,{},hw4
2336,no,<p>good point I will amp up the learning rate.</p>,2019-10-03T02:07:24Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1a27p2sjxl23g,2019-10-03T02:07:24Z,{},hw4
2337,no,<p>maybe I exclude those states that don&#39;t ever get hit.</p>,2019-10-03T02:10:35Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1a2bs77czm1ke,2019-10-03T02:10:35Z,{},hw4
2338,no,<p>Neither worked. Any thought from instructors/anyone else?</p>,2019-10-03T14:28:02Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1aso5jnq3j5an,2019-10-03T14:28:02Z,{},hw4
2339,no,"Start with a large $$\alpha$$, require every possible state to be hit at least a certain number of times, then half $$\alpha$$ and require all possible states to be hit a certain number of times again. Keep going until convergence or your laptop melts.<div><br /></div><div>This is an untested theory as of yet.</div>",2019-10-03T14:55:53Z,43,Week 9/29 - 10/5,feedback,,jl3oi5v7qkSk,k1atnz2tdg85l9,2019-10-03T14:55:53Z,{},hw4
2340,no,<p>It worked for me Angel.  Maybe you&#39;re not properly counting the number of valid states.</p>,2019-10-03T15:35:23Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1av2rthmm29a,2019-10-03T15:35:23Z,{},hw4
2341,no,<p>So you are saying if you exclude those states it will work?</p>,2019-10-03T15:47:36Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1avihfh1jn50w,2019-10-03T15:47:36Z,{},hw4
2342,no,"<p>Just an idea: if one sums up all the rewards for each &#39;drive&#39;, wouldn&#39;t that number converge too once we&#39;ve found the optimal q?  Then it could be used a convergence criteria.  </p>",2019-10-03T16:16:31Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1awjo81wn45au,2019-10-03T16:16:31Z,{},hw4
2343,no,"<p>It probably would, but the issue is the policy can converge before the Q-values converge to their optimal values. So you can have Q values that are &#34;good enough&#34; to get the right policy, but that aren&#39;t optimal. </p>",2019-10-03T18:12:17Z,43,Week 9/29 - 10/5,feedback,,jl3we43d3bp15p,k1b0ok1ioon4jk,2019-10-03T18:12:17Z,{},hw4
2344,no,<p>For full convergence (rather than only 1 state) I decided to calculate sum changes in q_table across episodes and if they are under some value for 3000 episodes in a row then I say it converged. Worked well to get all values for HW 4.</p>,2019-10-03T19:52:00Z,43,Week 9/29 - 10/5,feedback,,jzj7y1ofgsro1,k1b48rvcnen4tf,2019-10-03T19:52:00Z,{},hw4
2345,no,"<p>&#64;Shayan yes the policy can converge before the q values, but then why do you base your convergence criteria on $$\Delta$$Q, instead of comparing the previous and new policy? </p>",2019-10-04T10:47:18Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c085711cav5,2019-10-04T10:47:18Z,{},hw4
2346,no,"<p>Also, I tried to see how many states we can reach and to my surprise, I couldn&#39;t go past 80% even with $$\epsilon$$ = 1 ??? </p>
<p>An error is always possible, but picking an action at random and then feeding it to gym is quite straightforward... and the code is pretty much the same as hw3 anyway.  </p>
<p></p>
<p>I tried to let epsilon = 1 until I have reached 80% of states x actions combinations, and then lower epsilon to 0.2 or 0.3, and stop when the policy doesn&#39;t change.</p>
<p>I get to a point where the new policy is exactly the same as the previous iteration, yet I&#39;m far far off the real value... </p>
<p>Has anyone tried this kind of thing? </p>",2019-10-04T10:53:33Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c0g6tswcm56o,2019-10-04T10:53:33Z,{},hw4
2347,no,"<p>&#34;<em>and stop when the policy doesn&#39;t change.&#34;</em></p>
<p></p>
<p>Are you sure the policy isn&#39;t changing any more?  It might stop changing between two successive occurrences of a state-action value, but then it may change again on the third one, which you may not notice unless you&#39;re checking.</p>
<p></p>
<p>I suspect looking for policy changes isn&#39;t the way to go for taxi.  There are, for example, 6 ways to go down two spaces and left two spaces, none of which is any better than the others:</p>
<p>DDLL, DLDL, DLLD, LDDL, LDLD, LLDD, so a bunch of policies are probably close enough in value to keep swapping as &#34;best&#34; policy throughout the learning phase.</p>
<p></p>
<p>&#34;<em>and to my surprise, I couldn&#39;t go past 80% even with ϵ = 1 ???&#34;</em></p>
<p><em></em></p>
<p>Yup, this is exactly my point about 12 posts up.  I explained why this is.</p>",2019-10-04T13:36:34Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c69tsgxyc2bm,2019-10-04T13:36:34Z,{},hw4
2348,no,"<p>Thx for your answer.  Yes, I saw your explanation, yet I was still surprised to see the 80% figure, but I should have been clearer, I was talking about the 3,000 possible (s,a) pairs.  </p>
<p>Anyway, I should have tested more thoroughly if the policy really doesn&#39;t change.  Comparing it to the previous one may not be enough, although if really feels like it when I see the delta shrinking.</p>
<p>I&#39;m going to drop this for now because you&#39;re right, even if I found an optimum policy, there may be another just as good, and their Q values would be different, and I still wouldn&#39;t have the optimum we&#39;re looking for.  </p>
<p>I actually misread the write up and overlook the fact the TA&#39;s wanted the <em>optimal</em> Q values.</p>
<p></p>
<p>I am having a bit of a conundrum with this problem because if the policy converges before Q, why do the TA&#39;s want the optimal Q values.  </p>
<p>Or is it just a general observation that the policy converges first, but people still want the optimum Q? </p>
<p>Or is it because we can never be sure the policy has converged by simply looking at it only? </p>
<p></p>",2019-10-04T14:37:28Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c8g5jkflqhg,2019-10-04T14:37:28Z,{},hw4
2349,no,"<p>&#34;<em>but I should have been clearer, I was talking about the 3,000 possible (s,a) pairs.&#34;</em></p>
<p></p>
<p>There is no difference in the case of taxi.  if $$20\%$$ of the states are unreachable, then $$20\%$$ of the state-action space will also be unreachable since all actions from the unreachable states will be unreachable.</p>",2019-10-04T14:45:02Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c8pvi3fwh1v8,2019-10-04T14:45:02Z,{},hw4
2350,no,"<p><em>&#34;even if I found an optimum policy, there may be another just as good, and their Q values would be different&#34;</em></p>
<p><em></em></p>
<p>I don&#39;t think the Q values of two optimal policies would be different.  That would violate the definition of &#34;optimal.&#34;</p>",2019-10-04T14:47:53Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c8tjk8l8h7m0,2019-10-04T14:47:53Z,{},hw4
2351,no,"<p>&#34;<em>I am having a bit of a conundrum with this problem because if the policy converges before Q, why do the TA&#39;s want the optimal Q values. &#34;</em></p>
<p><em></em></p>
<p>It is usually the case that a policy converges before the state-action values (or state values) converge.  For example, if one action is much, much better than all the others, an agent will figure that out early on even if it doesn&#39;t know precisely <em>how much</em> better it is.</p>
<p></p>
<p>As I just mentioned, in this problem, there are probably many optimal policies, so then how could the TAs check for &#34;the&#34; optimal policy if there are hundreds, or thousands, or millions of them?  I would think that checking for converged Q values would be the best way to assess that the agent has learned.</p>",2019-10-04T14:52:39Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1c8znupu6e54b,2019-10-04T14:52:39Z,{},hw4
2352,no,"<p>Thanks, you&#39;re making many valid points which I need to think about deeper. </p>
<p>This is my first term, and RL is very interesting, yet a bit challenging.  I think I have understood something but then in practice, new aspects come up.  </p>
<p></p>
<p>What do professionals do? Do they stop when they have a very good policy (to save cpu time or it works &#39;well enough&#39;?) or they want the optimal q anyway? </p>",2019-10-04T15:33:13Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1cafu6r2382ol,2019-10-04T15:33:13Z,{},hw4
2353,no,"<p>This is an interesting conversation.</p>
<p></p>
<p>There are two questions you can ask:</p>
<p></p>
<ul><li>How does Q-learning converge?</li><li>How do you guarantee it converges to the optimal values?</li></ul>
<p></p>
<p>Convergence is different than convergence to the optimal values. There are conditions for both.</p>
<p></p>
<p>Trick question: how do you set the learning rate so that Q-learning converges in one time step?</p>
<p></p>",2019-10-04T15:51:44Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cb3nlnuoe2b5,2019-10-04T15:51:44Z,{},hw4
2354,no,"<p>In practice, we cannot find &#34;optimal&#34; policies, we only hope for good enough, use good evaluation criteria, and compare agent performance with human performance. But, once you get into the real world, all convergence guarantees go away.</p>
<p></p>",2019-10-04T15:54:20Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cb6znq28h55r,2019-10-04T15:54:20Z,{},hw4
2355,no,<p>haha you&#39;re aswering questions with more questions... more to think about</p>,2019-10-04T17:00:52Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1cdkjyeijf3oz,2019-10-04T17:00:52Z,{},hw4
2356,no,"<p>&#34;<em>Trick question: how do you set the learning rate so that Q-learning converges in one time step?&#34;</em></p>
<p></p>
<p>I guess if you set $$\alpha=0$$ then Q-learning will converge in one time step to the initial values of your value function.</p>
<p></p>
<p>But if $$\alpha&gt;0$$, then I would think that Q-learning would always be moving in the direction of the optimal values, since your update step will move the current value function in the direction of the sampled return.  The issue with learning rate is that, for higher values, there will be noise in the Q values because of constant adjusting to the latest sampled return, but the mean of the values should always be optimal if the learning rate is low enough, right?</p>",2019-10-04T17:04:22Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cdp247ieid4,2019-10-04T17:04:22Z,{},hw4
2357,no,"<p>Yeah...</p>
<p></p>
<p>So, what does stochastic approximation theory tells us about this issue of moving around following the samples?! What are the sequences of learning rates that would guarantee convergence?!</p>
<p></p>
<p>Sutton 2.5 is an interesting read at this point.</p>
<p></p>
<p>Note, &#34;guarantee convergence&#34;, but not yet to the optimal action-value function. There are a few other conditions to guarantee convergence to the optimal action-value function.</p>
<p></p>",2019-10-04T18:23:26Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cgiqri7jj39p,2019-10-04T18:23:26Z,{},hw4
2358,no,"<p>&#34;<em>So, what does stochastic approximation theory tells us about this issue of moving around following the samples?!&#34;</em></p>
<p></p>
<p>Stochastic approximation theory tells us to reduce $$\alpha$$ over time, but not too quickly:</p>
<p></p>
<p>$$\sum\limits_{n=1}^\infty \alpha_n(a) = \infty,\quad \sum\limits_{n=1}^\infty \alpha_n^2(a) &lt; \infty$$</p>
<p></p>
<p>So... I tried this on the following really simple Frozen Lake map:</p>
<p></p>
<p>S F</p>
<p>H G</p>
<p></p>
<p>with:</p>
<p></p>
<p>$$\gamma=1$$</p>
<p>$$\epsilon=0.1$$</p>
<p>$$\alpha = \max(0, 0.01 - \text{episodeNumber} \cdot 0.0000001)$$</p>
<p></p>
<p>which will cause $$\alpha$$ to decay until episode 100,000 where it hits 0.</p>
<p></p>
<p>It converges within my set tolerance every time, but to slightly different values for different seeds.  The only reason it &#34;converges&#34; is that my $$\alpha$$ gets so close to zero that my algorithm thinks it&#39;s converged when it really isn&#39;t...</p>
<p></p>
<p>I then tried dividing the decay rate of $$\alpha$$ by four and quadrupling the number of episodes to 400k.  But the values are still jumping around for different seeds by roughly the same small amount...</p>",2019-10-04T19:07:58Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1ci404or3j262,2019-10-04T19:07:58Z,{},hw4
2359,no,"<p>&#34;my $$\alpha$$ gets so close to zero that my algorithm thinks it&#39;s converged when it really isn&#39;t&#34;</p>
<p></p>
<p>It is! It converged, just not to the optimal action-value function.</p>
<p></p>
<p>Look up GLIE (for Sarsa):</p>
<p></p>
<ol><li class=""p1"">All state-action pairs must be explored infinitely often.</li><li class=""p2"">The policy must converge on a greedy policy.</li></ol>
<p></p>
<p>For Q-learning, only 1 is a requirement.</p>",2019-10-04T22:02:31Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cochaws8mji,2019-10-04T22:02:31Z,{},hw4
2360,no,"<p>Also, start with a slightly higher $$\alpha$$.</p>",2019-10-04T22:03:53Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1coe8nk97724o,2019-10-04T22:03:53Z,{},hw4
2361,no,"<p>I&#39;m a little confused...</p>
<p></p>
<p>1. GLIE (Greedy in the Limit of Infinite Exploration) seems to involve shrinking $$\epsilon$$, not $$\alpha$$.  In my example above, I&#39;m running an on-policy SARSA agent, so I would like to fix my $$\epsilon$$ (say at $$\epsilon=0.1$$) and just find the optimal value function for <em>that</em> particular $$\epsilon$$-greedy policy.  So the only thing I would be modifying is my learning rate schedule, and what I <em>want </em>to see happen is to find a sequence of schedules for decaying $$\alpha$$ such that every successive schedule I try shrinks the maximum difference between the Q values between runs.  In other words, I want to see evidence that for every slower-decaying $$\alpha$$-schedule that I try, my Q-values seem to be approaching the optimal value function.</p>
<p></p>
<p>2. Fixing $$\epsilon=1$$, I started at $$\alpha=0.2$$ and decayed to zero over 200k time steps, then 400k time steps, then 800k time steps, each time with two seeds.  It <em>does</em> look like the numbers are getting closer and closer together.  But this approach is definitely not as easy as just running something one time and checking for convergence by comparing to some small threshold.  To do this right I would have to keep doubling the length of the $$\alpha$$ decay and keep doing runs and that would take a really long time.</p>
<p></p>
<p>3. But if I <em>need</em> to be GLIE, then maybe my SARSA with fixed policy is not supposed to converge?</p>",2019-10-04T23:28:03Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cregyq1mz2so,2019-10-04T23:28:03Z,{},hw4
2362,no,"<p>Maybe a terminology thing, but the optimal policy is the greedy policy of the optimal value function. There can be many optimal policies, but only one optimal value function. So, perhaps what you are trying to do is evaluating an e-greedy policy?! You can do that, but you&#39;re not finding the optimal value function in that case.</p>
<p> </p>
<p>For Sarsa, you need the GLIE conditions and the stochastic approximation requirements. For Q-Learning, you only need 1 of the 2 conditions under GLIE and the stochastic approximation requirements you outlined above.</p>",2019-10-05T18:13:09Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1dvlde96tm51y,2019-10-05T18:13:09Z,{},hw4
2363,no,"<p>&#34;<em>perhaps what you are trying to do is evaluating an e-greedy policy?!&#34;</em></p>
<p><em></em></p>
<p>Yup, that is exactly what I was trying to do.  I was misusing the term &#34;optimal value function&#34; when what I was really trying to do was converge to the exact value of the e-greedy policy.</p>
<p></p>
<p>So to actually converge to the optimal value function, I need to decay both $$\epsilon$$ and $$\alpha$$, right?  The decaying of $$\epsilon$$ is required for GLIE, and the decaying of $$\alpha$$ is required for the stochastic approximation theorem to hold.</p>
<p></p>
<p>On the other hand, to evaluate the value of the $$\epsilon$$-greedy policy, I just need to decay $$\alpha$$ for stochastic approximation theory to hold, right?</p>",2019-10-05T19:27:59Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1dy9m1ohv86vn,2019-10-05T19:27:59Z,{},hw4
2364,no,<p>Correct.</p>,2019-10-06T00:37:20Z,42,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1e9bf4isa44mz,2019-10-06T00:37:20Z,{},hw4
2365,no,"<p>Starting at a high value and doing an exponential backoff impl for both ϵ and α solved the hw4 for me quickly. While its clear why the exponential backoff is needed, its not clear to me whether there is a relationship between ϵ and α. </p>
<p></p>
<p>- do they have to decay at same rate?</p>
<p>- what happens if you learn aggressively while exploring conservatively? and vice versa</p>
<p></p>
<p></p>",2019-10-07T07:50:40Z,42,Week 10/6 - 10/12,feedback,,jqrr36mqfm8M,k1g48jsnqez5p6,2019-10-07T07:50:40Z,{},hw4
2366,no,"<p>&#64;Anand I started with alpha = 0.1 and never even tried to change it, but I used exponential backoff for $$\epsilon$$ AFTER my $$\Delta$$Q was smaller than 0.1, otherwise it would converge to something else (which would produce a very bad policy ending with negative rewards). </p>
<p>That being said, I didn&#39;t converge exactly to the numbers given, even with a $$\Delta$$Q = $$10^{-7}$$</p>
<p></p>
<p></p>
<p>I&#39;m going to experiment with alpha now.</p>
<p></p>
<p>To answer (tentatively) your second question, I&#39;d say that if you explore conservatively, you may never converge to the optimal Q, but you could still pass (some of) the tests the TA&#39;s have chosen, the ones with &#39;high&#39; positive values.</p>
<p>That too I have to try, but now at least I have 15 values to play with, because, if you read my post &#64;586, I first converged to some less-optimal Q, got all 5 values in the hw right, yet rewards were negative and failed the tests.    </p>
<div>
<div></div>
</div>",2019-10-07T08:14:41Z,42,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1g53fw1v5715d,2019-10-07T08:14:41Z,{},hw4
2367,no,"<p>I tried &#34;what happens if you learn aggressively while exploring conservatively? and vice versa&#34;</p>
<p></p>
<p>Both cases it converges to the values given/expected in hw4. I suspect, for both scenarios all these states are being visited enough to guarantee a good value irrespective of what rate of decay i set.</p>
<p></p>
<p>So I am guessing another variable that matters here is efficiency or how quickly you want to converge. I had 200k episodes.</p>
<p> </p>",2019-10-07T08:29:36Z,42,Week 10/6 - 10/12,feedback,,jqrr36mqfm8M,k1g5mm1c6po2jn,2019-10-07T08:29:36Z,{},hw4
2368,no,<p>&#64;Anand I had 1900 episodes (but I started by loading the q matrix from a pickle I had previously prepared right after the algo had visited 2400 states).  Still 200k is a lot.  </p>,2019-10-07T13:38:00Z,42,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1ggn85k8355u8,2019-10-07T13:38:00Z,{},hw4
2369,no,"<p>As a comparison, I was able to get the right values with ~2100 episodes.</p>
<p></p>
<p>I kept getting the wrong values until I read this from Miguel:</p>
<p></p>
<blockquote>
<p><em>This is an interesting conversation.</em></p>
<p><em> </em></p>
<p><em>There are two questions you can ask:</em></p>
<p><em> </em></p>
<ul><li><em>How does Q-learning converge?</em></li><li><em>How do you guarantee it converges to the optimal values?</em></li></ul>
<p><em> </em></p>
<p><em>Convergence is different than convergence to the optimal values. There are conditions for both.</em></p>
<p><em> </em></p>
<p><em>Trick question: how do you set the learning rate so that Q-learning converges in one time step?</em></p>
</blockquote>
<p><em></em></p>
<p>Beyond that, I thoroughly enjoyed this homework! It was fun experimenting with different learning rates / exploration probabilities (hint...hint). This might also help, something I saw in my own results, if it converges too fast (for me it was &lt;1500 episodes), all your states might not have been properly visited.</p>",2019-10-12T17:58:49Z,42,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1nv5wkjbow5ld,2019-10-12T17:58:49Z,{},hw4
2370,no,"<p>That&#39;s our main purpose, to make the learning process enjoyable. Thanks for the feedback.</p>
<p></p>",2019-10-14T02:23:07Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1psma4d16h2c,2019-10-14T02:23:07Z,{},hw4
2371,no,"<p>I did try both methods for (2)  and neither reproduces the output provided for the examples.</p>
<p></p>
<p>For (1) it isn&#39;t the case that resetting the seed would cause each episode to be identical. The q_table changes over each episode and consequently each episode produces different action selections (based on epsilon-greedy). I implemented (1) both ways and the generated policies are different- similar, but still different for some states.</p>",2019-10-03T01:53:33Z,26,Week 9/29 - 10/5,followup,,jl88dnp8AZzD,k1a1pw87ijs4ah,2019-10-03T01:53:33Z,{},hw3
2372,no,<p>Your right. I was thinking something else.</p>,2019-10-03T02:40:36Z,26,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k1a3eefwm2d3op,2019-10-03T02:40:36Z,{},hw3
2373,no,"<p>For 1), I&#39;d say that I had some trouble finding the way the TA&#39;s had implemented their random generators to reproduce exactly what they did so my code would produce the same result as them.  There are several ways to do it, but only one leads to the results they want.  </p>
<p>Hint: while it is possible to do it with one call to the np.random, the implementation actually does it twice.  </p>
<p>It tooks me hours because, also, I didn&#39;t realize there were not always 4 actions available... </p>",2019-10-03T04:55:54Z,26,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k1a88e656o6k9,2019-10-03T04:55:54Z,{},hw3
2374,no,"<p></p>
<p>Hi,</p>
<p></p>
<p>Would you elaborate on this part - &#34;there were not always 4 actions available&#34;? Do you mean at the edges and corners of the grid world? </p>
<p></p>
<p>My understanding is that the agent does not need to explicitly take these into account, and always consider all 4 actions. </p>
<p></p>
<p>Please correct me if I&#39;m wrong.</p>",2019-10-03T07:39:32Z,26,Week 9/29 - 10/5,feedback,,ijctp4ucNy8,k1ae2thzy5d4h8,2019-10-03T07:39:32Z,{},hw3
2375,no,"<p>See &#64;520 where I discuss the criteria np.random.randint(4) and what Miguel recommends np.random.randint(env.action_space.n) which takes the number of available actions automatically.  </p>
<p>I think there&#39;s probably people who copied that code without realizing that there are cases where not all actions are available, but to be honest I didn&#39;t find interesting to look into that because there&#39;s nothing to learn.</p>
<p>I can imagine that sometimes, the people at openAI have forbidden going up or going left for instance.  </p>
<p>If you read the hw doc, they do not say there are 4 actions ... if Miguel hadn&#39;t told me that, I would still be scratching my head because I was not getting any error message when I was picking actions in a set of 4 while, say, 3 was not allowed.</p>
<p>I guess it was just ignored, but anyway, now you know.  </p>",2019-10-03T09:08:54Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1ah9rb3mrvez,2019-10-03T09:08:54Z,{},hw3
2376,no,"<p>Jean-Pierre,</p>
<p></p>
<p>I&#39;ve checked and I never generate a frozen-lake environment with fewer than 4 available actions for each state. I do realize that some of the other environments in openAI gym have a different number of available actions hence the recommendation from the TA&#39;s. What I am currently doing is:</p>
<p></p>
<p>- calling np.random.randint(env.action_space.n) for the random action selection.</p>
<p>- trying the two variations I mentioned above for the greedy action selection. </p>
<p>- once an action is selected, using env.step(action) to have the environment generate the next state, reward, and done flag.</p>
<p></p>
<p>I take it that you followed a different process? </p>",2019-10-03T12:51:45Z,26,Week 9/29 - 10/5,feedback,,jl88dnp8AZzD,k1ap8cd91od55s,2019-10-03T12:51:45Z,{},hw3
2377,no,"<p>Hi Neeza,</p>
<p>About less than 4 actions, I was just repeating what Miguel said.  I found it strange, but then I changed my greedy generator and it worked so I assumed the reason I succeeded was that I replaced 4 with env.action_state.n, but it&#39;s true that I also did other modifications...</p>
<p></p>
<p>Anyway, what I know for sure is what I said initially about the way you implement your randomness will change the result.</p>
<p>I guess that&#39;s because we are not looking for the optimal policy here, so we need to walk in TA&#39;s footsteps, which means that if you use the random generator differently, you fall out of sync with what they did and that irremediably changes everything after that.  </p>
<p></p>
<p>So, with that in mind, maybe you need to take a closer look if you, say, use the random generator efficiently or if you generate values that you end up not using? </p>
<p></p>
<p>About your issue with multiple max, there are examples in other posts using np.argmax and that&#39;s what I did, so if you tried it and failed, then the reason is prob because you didn&#39;t do the epsilon thing properly.  </p>
<p></p>
<p>Btw, randomly selecting among multiple max&#39;s will also skew your random generator...</p>
<p></p>",2019-10-03T14:40:48Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1at4kula664cs,2019-10-03T14:40:48Z,{},hw3
2378,no,"<p>If there are reduced action spaces for certain states then you would need to account for that in the Q argmax selection as well...</p>
<p></p>
<p>So, I checked and none of the values returned for env.action_space.n are not 4.</p>",2019-10-06T18:42:40Z,25,Week 10/6 - 10/12,feedback,,jzsv9qgvs05p3,k1fc366wl9d5ne,2019-10-06T18:42:40Z,{},hw3
2379,no,"<p>&#64;Kevin yeah, I guess Miguel was talking in general, but you&#39;re right, for this case, there are always 4 actions.</p>",2019-10-06T19:39:54Z,25,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1fe4sb2ncw7bn,2019-10-06T19:39:54Z,{},hw3
2380,no,"<p>For HW3, there are always 4 actions. Other HWs (and environments) do not. </p>
<p></p>
<p>Apologies for the confusion.</p>
<p></p>",2019-10-08T21:40:00Z,25,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1idaxqv97m1zx,2019-10-08T21:40:00Z,{},hw3
2381,no,"<p>Ahhhh.....the pain. I was using random.uniform to generate the test for whether to use random action selection or greedy action selection. I changed it to np.random.uniform(0,1) and things work out. So many wasted hours......</p>",2019-10-03T13:21:03Z,26,Week 9/29 - 10/5,followup,,jl88dnp8AZzD,k1aqa0uyhig37l,2019-10-03T13:21:03Z,{},hw3
2382,no,<p>Thank you for correcting my sudo code Neeza. You correction is what I&#39;m doing in my code and I&#39;m still not matching the results</p>,2019-10-03T14:19:40Z,26,Week 9/29 - 10/5,followup,,j6m1jeidndu6wq,k1asde4stiz165,2019-10-03T14:19:40Z,{},hw3
2383,no,<p>I realized I had been using random.uniform to sample whether to select a random action or to select the greedy action. I changed it to np.random.uniform and things worked out for me.  </p>,2019-10-03T17:01:35Z,26,Week 9/29 - 10/5,feedback,,jl88dnp8AZzD,k1ay5mo23wu59w,2019-10-03T17:01:35Z,{},hw3
2384,no,<p>you&#39;re using np.random.uniform(env.n_states.n)? I can&#39;t remember the exact env.number_of_states call off the top of my head</p>,2019-10-03T17:33:42Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1azax90dv26kb,2019-10-03T17:33:42Z,{},hw3
2385,no,"<p></p>
<p>I&#39;m using np.random.randint(env.action_state.n) to do random action selection just as the TAs suggested.  But you have another sampling step- the step where you generate a random number and see if it is less than epsilon.</p>",2019-10-03T19:30:09Z,26,Week 9/29 - 10/5,feedback,,jl88dnp8AZzD,k1b3gods3k7js,2019-10-03T19:30:09Z,{},hw3
2386,no,"<p>Hey Neeza,</p>
<p></p>
<p>Are you using &#34;np.random.uniform(0, 1)&#34; when deciding to take a random action?</p>",2019-10-05T17:57:38Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dv1enaqjv6yq,2019-10-05T17:57:38Z,{},hw3
2387,no,"<p>Aaron, you deserve a gold medal. I&#39;ve been using</p>
<p></p>
<pre>np.random.randint(0,100)/100.0 or np.random.randint(0,101)/100.0 </pre>
<p>to compare against epsilon but it was not giving me the result that matches the example although it was giving me consistent result every time.</p>
<p></p>
<p>Trying your way of getting random -&gt;</p>
<p></p>
<pre>np.random.uniform(0,1)</pre>
<p>This finally got my answer to print out correctly</p>
<pre>[[0.20944331 0.48653866 0.49730795 0.68301692]
 [0.64341951 0.9112584  0.85111285 0.65556764]
 [0.88837089 0.93115725 0.9167586  0.91419646]
 [0.93214126 0.92292253 0.94354097 0.93449757]
 [0.         0.         0.         0.        ]
 [0.25413104 0.47579825 0.92797157 0.81091787]
 [0.94250322 0.94107199 0.94463412 0.87908423]
 [0.94330518 0.96351515 0.95250744 0.94962169]
 [0.53645891 0.89473301 0.29734493 0.67329215]
 [0.85421854 0.9185382  0.81201688 0.77385265]
 [0.94364073 0.90618541 0.96586523 0.93488531]
 [0.95529001 0.97584787 0.96304856 0.96718743]
 [0.79949079 0.85530704 0.8713067  0.85038178]
 [0.85332313 0.88474512 0.90352541 0.87641602]
 [0.9494872  0.95591453 0.96495681 0.95634992]
 [0.         0.         0.         0.        ]]
</pre>
<div>
<pre>&#39;^,v,v,&gt;,&lt;,&gt;,&gt;,v,v,v,&gt;,v,&gt;,&gt;,&gt;,&lt;&#39;</pre>
<p></p>
<p>My development environment was Jupyter Notebook using Python 3.6.8 and Numpy version 1.17.0</p>
<p></p>
<p></p>
</div>",2019-10-06T05:01:32Z,25,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1eir745i9l12n,2019-10-06T05:01:32Z,{},hw3
2388,stud,"<p>I am stuck at the same point. I implemented as above and for example 1 I get the following Q</p>
<p></p>
<pre>array([[0.25912898, 0.31953118, 0.27125179, 0.48653415],
       [0.4665912 , 0.64710343, 0.71324499, 0.58821987],
       [0.87310506, 0.78164588, 0.90555767, 0.79584453],
       [0.90785282, 0.90124936, 0.91473057, 0.89682017],
       [0.        , 0.        , 0.        , 0.        ],
       [0.32056838, 0.30080575, 0.63825058, 0.50237019],
       [0.84514769, 0.78599549, 0.89378864, 0.71642417],
       [0.90927507, 0.9170328 , 0.93328399, 0.89416426],
       [0.62548435, 0.76848046, 0.56465185, 0.63750564],
       [0.76936329, 0.83261934, 0.57745863, 0.77167042],
       [0.80433701, 0.82262437, 0.93891149, 0.8705824 ],
       [0.91301988, 0.87199476, 0.9592867 , 0.9200788 ],
       [0.70004165, 0.78301838, 0.74979751, 0.73167182],
       [0.79736781, 0.84449423, 0.77552779, 0.82453068],
       [0.93266814, 0.92022944, 0.96151949, 0.92928841],
       [0.        , 0.        , 0.        , 0.        ]])<br />																					<br />[&#39;^&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;V&#39;, &#39;V&#39;, &#39;&gt;&#39;, &#39;&gt;&#39;, &#39;V&#39;, &#39;V&#39;, &#39;&gt;&#39;, &#39;&lt;&#39;]</pre>
<p>I&#39;ve double checked the seeds multiple times and am using the correct functions env.seed() and np.random.seed(). Is there a trick to placing them in in the correct spot? I experimented with a couple and only noticed a difference if I reset the seed for the env every episode.</p>
<p></p>
<p></p>",2019-10-03T23:43:51Z,26,Week 9/29 - 10/5,followup,a_0,,k1bciy0gztk1l1,2019-10-03T23:43:51Z,{},hw3
2389,no,"<p></p><pre>Use this toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped. I had the same output using gym.make() then i switch to 
toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped it worked</pre>
<p></p>",2019-10-04T00:35:57Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1bedy5dhc92wb,2019-10-04T00:35:57Z,{},hw3
2390,stud,"<p>Thanks for the feedback Abdoul. I already am using the toy_text version as follows</p>
<p></p>
<pre>env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(desc=custom_map).unwrapped
</pre>
<p>Then I set the seeds in the next line, and run SARSA algorithm. I am using np.random.uniform() and np.random.randint() for determining if I explore and selecting the action. I think I am setting seeds correctly. Not sure how else to debug.</p>",2019-10-04T01:20:22Z,26,Week 9/29 - 10/5,feedback,a_0,,k1bfz220zc9514,2019-10-04T01:20:22Z,{},hw3
2391,no,<p>How are you breaking out of the loop! I used done to break out the loop. is there a change that you train is stopping early??</p>,2019-10-04T04:22:28Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1bmh98hlsf22a,2019-10-04T04:22:28Z,{},hw3
2392,no,"<p>My small environment is working. My error was thinking the environment was a straight line instead of a 2x2:</p>
<p>[[1. 1. 1. 1.]<br />[1. 1. 1. 1.]<br />[1. 1. 1. 1.]<br />[0. 0. 0. 0.]]<br />[&#39;&lt;&#39;, &#39;&lt;&#39;, &#39;v&#39;, &#39;&lt;&#39;]</p>
<p></p>
<p>Is this the correct way to input the other environments?</p>
<pre>[&#39;SFFF&#39;, &#39;HFFF&#39;, &#39;FFFF&#39;, &#39;FFFG&#39;]</pre>",2019-10-04T12:37:13Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1c45hrktvh6nl,2019-10-04T12:37:13Z,{},hw3
2393,stud,"<p>David, I used a 2d numpy array as input into the environment. You can use env.render() to show the map.</p>
<p></p>
<p>Abdoul, yes I am use the done flag returned from the step function. It is used to break out of a while True loop.</p>",2019-10-04T14:25:44Z,26,Week 9/29 - 10/5,feedback,a_0,,k1c8120870i2k5,2019-10-04T14:25:44Z,{},hw3
2394,stud,<p>I think I got it. There was a mistake in what state I should be sampling the next action from.</p>,2019-10-04T14:47:34Z,26,Week 9/29 - 10/5,feedback,a_0,,k1c8t4ogclw6ys,2019-10-04T14:47:34Z,{},hw3
2395,no,"<p>Did you get it? I had the same issue but I have now completed the homework problems. My issue was that in the next iteration of the while loop, I only carried over the state, not the action.</p>",2019-10-04T18:44:12Z,26,Week 9/29 - 10/5,feedback,,jl2zkad6fs661b,k1ch9gczyoq2mx,2019-10-04T18:44:12Z,{},hw3
2396,no,<p>I had the exact same issue and output problem. The comment about the mistake being to do with the state you are sampling the next action from was spot on. </p>,2019-10-04T19:33:49Z,26,Week 9/29 - 10/5,feedback,,jzozvpx25to679,k1cj1969jjq33s,2019-10-04T19:33:49Z,{},hw3
2397,no,"<p>HI all,</p>
<p></p>
<p>Are you talking about finding the A&#39; in Q(S&#39;,A&#39;)?</p>
<p></p>
<p>I am assuming we use the same greedy test as we used to get A for Q(S,A), but this time we get A&#39; with S&#39; using the same greedy logic. Is this right?</p>
<p></p>
<p></p>
<p>What about resetting the seed on each loop?</p>",2019-10-05T18:04:49Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dvandql4j2fb,2019-10-05T18:04:49Z,{},hw3
2398,no,"<p>Yes I was accidentally choosing the next action from the old state rather then the next state.</p>
<p></p>
<p>I set the seed once at the top and don&#39;t need to reset at each iteration.</p>",2019-10-05T18:34:44Z,26,Week 9/29 - 10/5,feedback,,j6mgaotvo5f6bx,k1dwd4cbp551nk,2019-10-05T18:34:44Z,{},hw3
2399,no,"<p>Thanks Johnny!</p>
<p></p>
<p>After you get A&#39; do you set the next A=A&#39;  for the next loop?</p>",2019-10-05T18:35:51Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dwek6cwme2hs,2019-10-05T18:35:51Z,{},hw3
2400,no,"<p>Yes, that&#39;s correct.</p>",2019-10-05T19:02:13Z,26,Week 9/29 - 10/5,feedback,,jl2zkad6fs661b,k1dxcgnh7qn5xf,2019-10-05T19:02:13Z,{},hw3
2401,no,"<p>edit: for the frozen lake, terminal states are the same. Either the hole or the goal. But for the taxi example, terminal states are not always the same. For example above, we can see in 3 episodes, there are 3 different terminal states. Hence is this the reason why we must add a condition to handle terminal state specially? </p>",2019-10-03T07:40:38Z,43,Week 9/29 - 10/5,followup,,jvfpllmsggt7p4,k1ae48ck40v580,2019-10-03T07:40:38Z,{},hw4
2402,no,<p>That&#39;s a great point!  That <em>is</em> going to have a slight effect on all neighbors to possible terminal states.  And that effect should propagate back!</p>,2019-10-03T08:19:29Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1afi7cgm4q1y4,2019-10-03T08:19:29Z,{},hw4
2403,no,"<p>Looking back at every algorithm we wrote so far, i think we kind of take this for granted because the terminal states are always 0 (since we initialize them that way) and they are never updated. Most importantly, they are always the same states. But from the taxi case, we can see terminal states to be changing. Hence, it seem to be normal to overlook. I think for now, i am convinced we must take care of it by making them 0 until an instructor say otherwise.</p>
<p></p>
<p>Edit: added this to the frozen lake algo and as expected, the results are the same. </p>",2019-10-03T08:27:36Z,43,Week 9/29 - 10/5,feedback,,jvfpllmsggt7p4,k1afsnf4f04hm,2019-10-03T08:27:36Z,{},hw4
2404,no,"<p>Yup.  This made my day.  I actually passed the Heroku tests with this bug in my code.  It&#39;s really subtle, and I am really interested in seeing the performance boost with this fixed. Definitely learned a lesson here.   Thanks!</p>",2019-10-03T08:41:16Z,43,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1aga7kv6uvbe,2019-10-03T08:41:16Z,{},hw4
2405,no,"<p>Calculate your targets as: &#96;reward &#43; gamma * Q[next_state][next_action] * (not done)&#96;</p>
<p></p>
<p>That zeros out the &#34;future&#34; of terminal states. Just know that we are aware of this &#34;issue&#34; and you will not get any of these states in your homework.</p>
<p></p>
<p>But other than that, good thinking, good discovering!!!</p>
<p></p>",2019-10-04T10:41:38Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c00uu1zva16k,2019-10-04T10:41:38Z,{},hw4
2406,no,"<p>Thanks! I have added a ifelse condition for this but what was suggested is much cleaner! </p>
<p></p>
<pre><strong>Just know that we are aware of this &#34;issue&#34; and you will not get any of these states in your homework.</strong></pre>
<p>This explains why i am able to pass the homework even when i did not account this!</p>
<p></p>",2019-10-04T11:23:19Z,43,Week 9/29 - 10/5,feedback,,jvfpllmsggt7p4,k1c1ih1zdol1h,2019-10-04T11:23:19Z,{},hw4
2407,no,"<p>Yeah, but good job thinking through it and bringing it up. It shows you are getting the fundamentals right. Trust me, these little details just make your life much easier on P2.</p>",2019-10-04T14:58:46Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c97j939pa5vw,2019-10-04T14:58:46Z,{},hw4
2408,no,"<p>I think if you initialize your q values to zeros, then you do not need to worry about this problem. </p>
<p></p>
<p>The set of terminal states is fixed, but in one episode, we only encounter one terminal state belonging to that set, and this does not mean that the set is changing. </p>
<p></p>
<p>For example, in episode 1, the passenger is at Y and the destination is at R; in episode 2, the passenger is at B and the destination is at Y; in episode 3, the passenger is at R and the destination is at Y. Then s1 = (x_R, y_R, R, R) is the terminal state of episode 1, s2 = (x_Y, y_Y, Y, Y) is the terminal state of episode 2 and s3 = (x_Y, y_Y, Y, Y) is the terminal state of episode 3. </p>
<p></p>
<p>s2/s3 is never encountered in episode 1, but the set of terminal states is the same. Besides, q(s2,a) is never updated during episode 1 since s2 cannot be encountered. Although s2 is encountered in episode 3, q(s2=s3,a) still remains zero. Therefore, if you initialize your q values to zeros, q(terminal state, a) will remain zero till convergence.</p>",2019-10-13T09:41:47Z,41,Week 10/13 - 10/19,feedback,,jzjw5y0ecos525,k1osukduk7u2lf,2019-10-13T09:41:47Z,{},hw4
2409,no,<p>You&#39;re right.  Note that this thread is outdated and has some wrong information.  See: &#64;641.</p>,2019-10-13T15:41:47Z,41,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1p5pj0q59n31v,2019-10-13T15:41:47Z,{},hw4
2410,no,"<p>I just test &#39;done&#39; before modifying Q, so I exit when the taxi reaches a terminal state without changing Q, so it always stays at zero.  That should take care of it, no?</p>",2019-10-04T11:02:18Z,43,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k1c0rfyc7wh7dd,2019-10-04T11:02:18Z,{},hw4
2411,no,<p>you should be updating the Q value though for the step with done</p>,2019-10-04T11:21:07Z,43,Week 9/29 - 10/5,feedback,,jvfpllmsggt7p4,k1c1fmrfhc96qp,2019-10-04T11:21:07Z,{},hw4
2412,no,"<p>Oh, thx, you&#39;re right! I&#39;m going to put it back where it was with Sarsa, ie after the update of the Q function.  </p>
<p>I don&#39;t quite understand the issue with the terminal states&#39; q values though? </p>
<p>What would happen if we updated them anyway??</p>",2019-10-04T12:33:53Z,43,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c417g0q6069z,2019-10-04T12:33:53Z,{},hw4
2413,no,<p>Correct. You want to handle the states that land in a terminal state. You just don&#39;t want to update the value of a terminal state.</p>,2019-10-04T14:59:59Z,43,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c993gygdk7pk,2019-10-04T14:59:59Z,{},hw4
2414,no,"<p>I was getting a headache figuring how to find the end states for setting them to zero. The way Miguel suggests is the kind of elegance why one enrolls in these courses. For this particular problem though, it is possible to find the right answer starting from all zeros.</p>",2019-10-05T05:10:04Z,43,Week 9/29 - 10/5,followup,,is8ald0uljj3u4,k1d3mbg2alm6cl,2019-10-05T05:10:04Z,{},hw4
2415,no,"<p>Correct, maybe I shouldn&#39;t be as nice next semester and make students initialize the Q-table randomly (which is the &#34;textbook&#34; way)... I think we&#39;ll do that.</p>",2019-10-06T00:34:45Z,42,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1e983u8lpc1ve,2019-10-06T00:34:45Z,{},hw4
2416,no,<p>What do you mean by &#34;the symbols used&#34;? Like our variable names?</p>,2019-10-03T15:46:58Z,26,Week 9/29 - 10/5,followup,,jl284xdcifz44g,k1avhnrnmrsw,2019-10-03T15:46:58Z,{},hw3
2417,no,<p>?? exactly...</p>,2019-10-04T10:36:27Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1bzu6u01rn6vz,2019-10-04T10:36:27Z,{},hw3
2418,no,<p>You watch your epsilons!</p>,2019-10-04T06:33:25Z,26,Week 9/29 - 10/5,followup,,is8ald0uljj3u4,k1br5nepr3m1n3,2019-10-04T06:33:25Z,{},hw3
2419,no,<p>never forget</p>,2019-10-06T03:00:25Z,25,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1eefg1t2lq43l,2019-10-06T03:00:25Z,{},hw3
2420,no,<p>lol</p>,2019-10-07T07:50:33Z,25,Week 10/6 - 10/12,feedback,,iv2q8g9wyas6yx,k1g48edyn675mi,2019-10-07T07:50:33Z,{},hw3
2421,no,<p>haha.</p>,2019-10-08T21:38:23Z,25,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1id8um9q923ca,2019-10-08T21:38:23Z,{},hw3
2422,no,"<p>I&#39;m not sure what you mean Chris. I&#39;m using these exact steps to update Q*</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6m1jeidndu6wq%2Fk1bdatgk4eup%2FScreen_Shot_20191003_at_8.05.17_PM.png"" alt="""" width=""466"" height=""181"" /></p>
<p></p>
<p>Am I incorrect in using these steps then grabbing the argmax() at each state?</p>",2019-10-04T00:04:40Z,26,Week 9/29 - 10/5,followup,,j6m1jeidndu6wq,k1bd9q3r67hej,2019-10-04T00:04:40Z,{},hw3
2423,no,"<p>I followed the same algo but I am not sure why your using</p>
<pre>np.random.random(). Many post stated use something else</pre>",2019-10-04T00:26:05Z,26,Week 9/29 - 10/5,feedback,,jl0c61kgrmsY,k1be193gb0v4tb,2019-10-04T00:26:05Z,{},hw3
2424,no,My mistake I replied to the wrong quotation.,2019-10-04T00:37:59Z,26,Week 9/29 - 10/5,feedback,,i4op5p9vfbq5yz,k1begkfxj6464b,2019-10-04T00:37:59Z,{},hw3
2425,no,"<p>&gt;&gt;&gt; import numpy as np<br />&gt;&gt;&gt; np.random.seed(74168)<br />&gt;&gt;&gt; np.random.random(10)<br />array([0.70898586, 0.10133824, 0.28818245, 0.25796843, 0.78406073,<br />       0.4392945 , 0.9718869 , 0.82584927, 0.67862185, 0.11533311])</p>
<p></p>
<p>numpy==1.17.0</p>
<p></p>
<p></p>",2019-10-04T01:31:39Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1bgdky9guy5kk,2019-10-04T01:31:39Z,{},hw3
2426,no,<p>I was able to solve HW3 and HW4</p>,2019-10-04T01:32:02Z,26,Week 9/29 - 10/5,feedback,,jc554vxmyuy3pt,k1bge2p1b9h64n,2019-10-04T01:32:02Z,{},hw3
2427,no,"<p>Jacob a few questions,</p>
<p></p>
<p>1. did you generate a random decimal every time you needed to select an action?</p>
<p>2. was your epsilon greedy check to take a random action if np.random.random() &lt; epsilon?</p>",2019-10-04T02:14:22Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1bhwi3upop7fp,2019-10-04T02:14:22Z,{},hw3
2428,no,"<p>1. you should.</p>
<p>2. correct. It is epsilon-greedy, meaning be greedy if random is greater than epsilon.</p>
<p></p>",2019-10-04T10:32:53Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1bzplpqjhq5d3,2019-10-04T10:32:53Z,{},hw3
2429,no,"<p>Hi All,</p>
<p></p>
<p>In the assignment notes it states:</p>
<p></p>
<ul><li>Use the provided seed for both Gym and NumPy</li></ul>
<p></p>
<p>I know we can set the numpy seed like this:</p>
<p></p>
<ul><li>np.random.seed(seed)</li></ul>
<p></p>
<p></p>
<p>But how doe we do this for gym? Also, why do we need to set it for gym? Are we not only making a random decision for the greedy logic?</p>",2019-10-04T19:26:11Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1cirflqeqiro,2019-10-04T19:26:11Z,{},hw3
2430,no,"<p>env.seed(seed)</p>
<p></p>
<p>The Frozen Lake environment is stochastic.  This means that it&#39;s going to randomly do things to you, regardless of what <em>you</em> do.  The enviornment seeding controls that randomness.</p>",2019-10-04T19:34:24Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cj2099ezj3vc,2019-10-04T19:34:24Z,{},hw3
2431,no,"<p>Thank you Vahe,</p>
<p></p>
<p>Follow up question, I understand that when &#34;np.random.random() &lt; epsilon&#34; we should be greedy, but does greedy mean take the argmax action and if &#34;np.random.random() <strong>&gt;</strong> epsilon&#34; then don&#39;t be greedy which means we take a random action?</p>",2019-10-04T21:17:50Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1cmr0vy9c3uw,2019-10-04T21:17:50Z,{},hw3
2432,no,"<p>It&#39;s the other way around.  We mean for $$\epsilon$$ to be some <em>small</em> positive number such that when $$\text{np.random.random()} &lt; \epsilon$$ we select a random action, i.e. we&#39;re <i>not </i>greedy.  The rest of the time ($$\text{random number} &gt; \epsilon$$), we take the greedy action, which as you said is the action with the highest state-action value.</p>",2019-10-04T21:50:19Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cnwsirkgc5t1,2019-10-04T21:50:19Z,{},hw3
2433,no,"<p>Here we have to walk in the TA&#39;s footsteps so it&#39;s not only enough to generate the random numbers with their seeds, but also we have to not waste &#39;steps&#39; or we&#39;ll be out of sync.</p>
<p>Are you sure that you are not wasting the randomly generated actions in some cases? </p>
<p>That my was fatal mistake and it all went all right after I fixed that.  </p>",2019-10-04T03:06:58Z,26,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k1bjs5unp0432x,2019-10-04T03:06:58Z,{},hw3
2434,no,"<p>Before we get an action we must generate a random value, that is the only time I use the random function. In addition to generating a random action if we are not being greedy</p>",2019-10-04T05:42:18Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1bpbwmhfmi3u2,2019-10-04T05:42:18Z,{},hw3
2435,no,"<p>the only calls to random stuff are:</p>
<p></p>
<p>1. &#96;np.random.random()&#96; to check for greedy vs. random</p>
<p>2. &#96;np.random.randint(env.action_space.n)&#96; to select a random action only if needed.</p>
<p>3. &#96;env.step&#96; when stepping through the environment.</p>
<p></p>
<p>Setting &#96;np.random.seed&#96; takes care of 1 and 2.</p>
<p>Setting &#96;env.seed&#96; takes care of 3.</p>
<p></p>
<p></p>",2019-10-04T10:35:54Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1bzthhz7vw6lf,2019-10-04T10:35:54Z,{},hw3
2436,no,<p>When is there a random step call to env.step?</p>,2019-10-04T12:11:57Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1c3905y5tdcy,2019-10-04T12:11:57Z,{},hw3
2437,no,"<p>My small environment is working. My error was thinking the environment was a straight line instead of a 2x2:</p>
<p>[[1. 1. 1. 1.]<br />[1. 1. 1. 1.]<br />[1. 1. 1. 1.]<br />[0. 0. 0. 0.]]<br />[&#39;&lt;&#39;, &#39;&lt;&#39;, &#39;v&#39;, &#39;&lt;&#39;]</p>
<p></p>
<p>Is this the correct way to input the other environments?</p>
<pre>[&#39;SFFF&#39;, &#39;HFFF&#39;, &#39;FFFF&#39;, &#39;FFFG&#39;]</pre>",2019-10-04T12:37:23Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1c45puwgbz6qi,2019-10-04T12:37:23Z,{},hw3
2438,no,"<p>&#64;David... when you do env.step(action), the frozen_lake code uses the random generator to decide which state you end up next</p>",2019-10-04T12:38:20Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c46y4eqjy78j,2019-10-04T12:38:20Z,{},hw3
2439,no,"<p>Thank you Jean_Pierre</p>
<p></p>
<p>When running example 2 I get the following output. I think my epsilon check is not correct because it is looking like my agent is not taking many random actions. Does the epsilon greedy check just compare that np.random.random() &lt; epsilon?</p>
<p></p>
<p>[[0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0. ]<br /> [0. 0. 0. 0.12]<br /> [0. 0. 0. 0. ]]<br />[&#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;&lt;&#39;, &#39;^&#39;, &#39;&lt;&#39;]</p>",2019-10-04T12:48:06Z,26,Week 9/29 - 10/5,feedback,,j6m1jeidndu6wq,k1c4jhrdlwc4jo,2019-10-04T12:48:06Z,{},hw3
2440,no,"<p>&#64;David... a few posts above, Miguel says exactly what one has to do to generate the right random sequences.  I honestly can&#39;t tell you more but read carefully what he wrote, especially in 2).</p>",2019-10-04T12:54:31Z,26,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1c4rr2twew1xs,2019-10-04T12:54:31Z,{},hw3
2441,no,"<p>&#64; David and &#64;Jean</p>
<p></p>
<p>I am having the same issue as David mentioned two comments up. I did read Miguel&#39;s post, but nothing is sticking out to me that I do not already have. I am thinking that my number of steps per episode is not high enough??? I currently run through 1000 steps per episode.</p>
<p></p>
<p>What about the done state? Should I only move to the next episode when I reach the goal instead of when I either reach the goal or hit a sink hole?</p>",2019-10-04T22:45:37Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1cpvwjh1uc5g4,2019-10-04T22:45:37Z,{},hw3
2442,no,"<p>You shouldn&#39;t be running any fixed number of steps per episode.  You should let the episode naturally terminate (by landing on a terminal state).  You can accomplish this, for example, with a &#39;while True&#39; loop that you break out of when done.</p>",2019-10-04T23:30:59Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1cri90uurx5qn,2019-10-04T23:30:59Z,{},hw3
2443,no,"<p>&#64;Vahe</p>
<p></p>
<p>I am now using the while loop as suggested, but I am still getting behavior like David above. Any other suggestions?</p>",2019-10-05T16:56:25Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dsuoqtsh7li,2019-10-05T16:56:25Z,{},hw3
2444,no,<p>Is your problem that you&#39;re getting almost all zeros for your Q values?</p>,2019-10-05T17:12:24Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1dtf8g2wf4r7,2019-10-05T17:12:24Z,{},hw3
2445,no,"<p>Not anymore. I am just not getting the same output for example 1, but I am ok for example 3.</p>
<p></p>
<p>For example 1:</p>
<pre>amap =
[[&#39;S&#39; &#39;F&#39; &#39;F&#39; &#39;F&#39;]
 [&#39;H&#39; &#39;F&#39; &#39;F&#39; &#39;F&#39;]
 [&#39;F&#39; &#39;F&#39; &#39;F&#39; &#39;F&#39;]
 [&#39;F&#39; &#39;F&#39; &#39;F&#39; &#39;G&#39;]]
Q =
[[0.41391731 0.45249536 0.58572891 0.6987519 ]
 [0.75000732 0.72338199 0.73547128 0.77697875]
 [0.87209776 0.86735463 0.90857729 0.86273895]
 [0.95154388 0.94396557 0.95152454 0.94123486]
 [0.         0.         0.         0.        ]
 [0.63717217 0.71787557 0.86752862 0.38729601]
 [0.85675261 0.92019122 0.93686261 0.92326077]
 [0.95011189 0.94826008 0.95374743 0.95254996]
 [0.61587999 0.77337314 0.42205955 0.54606774]
 [0.83875894 0.82671337 0.82544697 0.82402305]
 [0.92152498 0.90544008 0.95687758 0.92111495]
 [0.9788917  0.96985186 0.96875144 0.96831364]
 [0.80683314 0.86702586 0.82697318 0.85007532]
 [0.8942945  0.87999381 0.92378154 0.85773469]
 [0.94569715 0.96834569 0.94735016 0.94750496]
 [0.         0.         0.         0.        ]]
Output:  ^,^,&gt;,&lt;,&lt;,&gt;,&gt;,&gt;,v,&lt;,&gt;,&lt;,v,&gt;,v,&lt;</pre>
<p></p>
<p>I think I am having an issues calculating A&#39; = At&#43;1. I use S&#39; with the same greedy logic we already discussed to calumniate A&#39; and on the next loop make S=S&#39; and A=A&#39;.</p>
<p></p>
<p>I run a while loop for each episode until that episode hits a terminal state.</p>
<p></p>",2019-10-05T18:50:59Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dwy0hlvj96c6,2019-10-05T18:50:59Z,{},hw3
2446,no,"<p>I can tell you that your Q values are reasonable enough that it is probably impossible to conclude anything from looking at them.</p>
<p></p>
<p>The first step I would take in debugging this is to assume that your problem is <strong>not</strong> with your algorithm, but rather with your random number generation.  The problem is that changing the seed can completely change all the Q values (try it).</p>
<p></p>
<p>You have to have the same exact seeds for both the environment calls <strong>and</strong> your random number generator calls, as well as having the exact same number of calls as the auto-grader, so that you stay in step with it.</p>",2019-10-05T19:33:38Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1dygva6v6o5v,2019-10-05T19:33:38Z,{},hw3
2447,no,"<p>Hey Vahe,<br /><br />I found my issue! It turns out I was out of sync with the auto-grader because I was not calling the random generator when I hit a terminal state so I was calling it less often. To me it seems like the auto-grader should not call it when we hit a terminal state, e.g. done=true, as well.<br /><br />I have a new problem though, I can no longer signin into https:// rldm.herokuapp.com?!?!?!<br /><br />It worked fine for HW1/2.<br /><br />Do you know who I need to reach out to for this?</p>",2019-10-05T19:59:25Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1dze19vb7k74m,2019-10-05T19:59:25Z,{},hw3
2448,no,<p>It happened to me too.  I tried about 10 times over the course of an hour or so and eventually I got in.</p>,2019-10-05T21:00:28Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1e1kjljm214gt,2019-10-05T21:00:28Z,{},hw3
2449,no,"<p>&#34;<em>To me it seems like the auto-grader should not call it when we hit a terminal state, e.g. done=true, as well.&#34;</em></p>
<p><em></em></p>
<p>That&#39;s a fair point.  I think that, if done your way, you need an extra piece of update code that updates only in the case of terminal states.  If done the grader&#39;s way, you can reuse the old code.</p>",2019-10-05T21:09:10Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1e1vqa5j097gu,2019-10-05T21:09:10Z,{},hw3
2450,no,<p>Thanks Vahe!</p>,2019-10-05T21:11:00Z,26,Week 9/29 - 10/5,feedback,,gx3c8l7z7r72zl,k1e1y346lsb1yw,2019-10-05T21:11:00Z,{},hw3
2451,no,<p>No problem!</p>,2019-10-05T21:11:31Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1e1yr1ly2y2rc,2019-10-05T21:11:31Z,{},hw3
2452,no,"<p>OH, almost forgot.  Someone mentioned that putting https:// before the URL for Heroku may help you get in.</p>",2019-10-05T21:13:10Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1e20uz5clb6py,2019-10-05T21:13:10Z,{},hw3
2453,no,<p>That did it!</p>,2019-10-06T15:57:15Z,25,Week 10/6 - 10/12,feedback,,gx3c8l7z7r72zl,k1f66fw9q2c53f,2019-10-06T15:57:15Z,{},hw3
2454,no,"<p>question regarding the epsilon greedy action choice as it affects the random number generation, should we be running that twice per loop or once and use the predicted action in the next loop?</p>
<p></p>
<p>i.e:</p>
<p><tt>env.reset()</tt></p>
<p><tt>loop</tt></p>
<p><tt>  action = get_action(state)</tt></p>
<p><tt>  ... = env.step(action)</tt></p>
<p><tt>  next_action = get_action(next_state)</tt></p>
<p><tt>  #rest of updates...</tt></p>
<p></p>
<p>or<tt></tt></p>
<p><tt>env.reset()</tt></p>
<p><tt>action = get_action(state)</tt></p>
<p><tt>loop</tt></p>
<p><tt>  ... = env.step(action)</tt></p>
<p><tt>  next_action = get_action(next_state)</tt></p>
<p><tt>  #rest of updates...</tt></p>
<p><tt>  action = next_action</tt></p>
<p> </p>",2019-10-06T17:11:58Z,25,Week 10/6 - 10/12,feedback,,jzsv9qgvs05p3,k1f8uj8dzoltn,2019-10-06T17:11:58Z,{},hw3
2455,no,"<p>In the first version, your current action isn&#39;t based on your previous &#39;next_action&#39;, which violates SARSA, an on-policy algorithm.  In other words, the action you use to update your value function could be different from the action you explore next.</p>
<p></p>
<p>The second one is the correct one.</p>",2019-10-06T18:00:50Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1faldu9ymp1de,2019-10-06T18:00:50Z,{},hw3
2456,no,<p>Thanks.  Saw the assignment at the end just now in the RL book.</p>,2019-10-06T18:19:31Z,25,Week 10/6 - 10/12,feedback,,jzsv9qgvs05p3,k1fb9ejeozx3fv,2019-10-06T18:19:31Z,{},hw3
2457,stud,thanks all! I get my answer correct based on this post!,2019-10-06T19:25:02Z,25,Week 10/6 - 10/12,feedback,a_0,,k1fdlnw0el12pw,2019-10-06T19:25:02Z,{},hw3
2458,no,"<p>the constraint of the usual Q learning is that so far our method (hw3,hw4) utilized a matrix or Q table. If our states are continuous it might be easier to think of using a function to &#39;store&#39; and update Q(s,a) since it not be possible to store large number of states. </p>",2019-10-04T02:53:14Z,29,Week 9/29 - 10/5,followup,,jvfpllmsggt7p4,k1bjahooc7n3cc,2019-10-04T02:53:14Z,{},project2
2459,no,"<p>or even better, learn &#96;Q(s, a)&#96; for all a, given &#96;Q(s)&#96;.</p>",2019-10-04T10:29:16Z,29,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1bzkyb4tvq4dy,2019-10-04T10:29:16Z,{},project2
2460,no,"<p>Are Double DQN or Dueling DQN superior than DQN on this landing game? Are there advantages/disadvantages between libraries such as keras,  PyTorch  or torch? It is my first project using &#34;real&#34; AI libraries, any discussion/suggestion would be helpful. Thanks.</p>",2019-10-04T14:46:37Z,29,Week 9/29 - 10/5,followup,,jzg6jh2hn6f43c,k1c8rwssahz3yq,2019-10-04T14:46:37Z,{},project2
2461,no,"<p>Keras and PyTorch are probable the best libraries. They are easy to use. Keras is probably the easiest, PyTorch is easy and flexible.</p>
<p></p>
<p>Double DQN and Dueling DQN are better. Also, you can implement a Dueling Double DQN, which is the combination of those two improvements.</p>
<p></p>
<p>This is a fun project, enjoy.</p>",2019-10-04T14:54:44Z,29,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1c92c90wdr7o0,2019-10-04T14:54:44Z,{},project2
2462,no,"<p>Thank you very much for the guidance. I found another library called tensorflow, could you guide how it compared with Keras and PyTorch? Thanks again. </p>",2019-10-04T15:07:35Z,29,Week 9/29 - 10/5,feedback,,jzg6jh2hn6f43c,k1c9iv5dwzh3ez,2019-10-04T15:07:35Z,{},project2
2463,no,"<p>Keras can be seen as a high-level wrapper on top of TensorFlow (Keras can be configured to be a high-level wrapper on top of TensorFlow).</p>
<p></p>
<p>I wouldn&#39;t recommend using TensorFlow if this is your first time using a deep learning framework.</p>",2019-10-04T15:47:25Z,29,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cay3vvpaf4l9,2019-10-04T15:47:25Z,{},project2
2464,no,<p>Thanks. Now I know where to start. </p>,2019-10-04T15:56:59Z,29,Week 9/29 - 10/5,feedback,,jzg6jh2hn6f43c,k1cbaepfvecfx,2019-10-04T15:56:59Z,{},project2
2465,no,<p>Good. Enjoy! You are learning very valuable skills.</p>,2019-10-04T15:59:57Z,29,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1cbe7meu1d5bx,2019-10-04T15:59:57Z,{},project2
2466,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  can you give me a tldr of what is the difference between a dueling DQN and using a target network? it seems like some authors use this term interchangeably despite meaning different things.</p>",2019-10-08T21:06:24Z,28,Week 10/6 - 10/12,followup,,jcg0nzvdk8272b,k1ic3q7xpmo1zi,2019-10-08T21:06:24Z,{},project2
2467,no,"<p>Hmm. Interesting... </p>
<p></p>
<p>A target network is basically a technique to avoid &#34;chasing your own tail.&#34; The problem without the target network is the agent is using function approximation for training $$Q(s, a)$$, but to do so, it is using $$Q(sp).max()$$ and that is trouble because changing $$Q(s, a)$$ also changes $$Q(sp).max()$$, so your agent is optimizing against a moving target and it can diverge pretty easily. A target network just freezes the $$Q(sp).max()$$ estimates for a few steps to allow for progress towards a fixed target before letting the estimates update.</p>
<p></p>
<p>A Dueling network is a totally different technique; it is basically a special kind of network that attempts to learn a common $$V(s)$$ so that multiple $$Q(s, a)$$ (basically for all actions $$a$$) can use that $$V(s)$$. Bottom line is, instead of learning the Q-function, we split the $$Q(s, a)$$ into $$V(s)$$ and $$A(s, a)$$, then we can use every experience on $$s$$ to get better estimates for all $$a$$&#39;s.</p>
<p></p>
<p>Hope that makes sense.</p>
<p></p>",2019-10-08T21:34:49Z,28,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1id498kapp4i7,2019-10-08T21:34:49Z,{},project2
2468,stud,I have implemented seeds and random function correctly. I was initially resetting the environment. also resetting the environment and setting it up as first state. After your suggestion i removed initial reset but i am still not getting the answer. Any other suggestion that i should validate ?,2019-10-03T18:52:32Z,26,Week 9/29 - 10/5,followup,a_0,,k1b24biev18594,2019-10-03T18:52:32Z,{},hw3
2469,no,"<p>What do you mean by &#39;setting it up as first state&#39;? Also, I think my point is to <strong>do</strong> initial reset before experiencing episodes, thus I think the part you removed should be there. Other things I can think of, is that if you use Jupyter notebook like me, restart kernel before running experiments (this may or may not affect results, I have not tested yet).</p>",2019-10-03T18:59:37Z,26,Week 9/29 - 10/5,feedback,,jzhy4sbxyar5l4,k1b2deyzf087dn,2019-10-03T18:59:37Z,{},hw3
2470,no,<p></p><pre>Using toy_text.frozen_lake.FrozenLakeEnv(desc = env_map).unwrapped instead of gym.make() made the difference with mine. You could check the way to load your environment</pre>,2019-10-04T00:22:25Z,26,Week 9/29 - 10/5,followup,,jl0c61kgrmsY,k1bdwjuz7mfb4,2019-10-04T00:22:25Z,{},hw3
2471,no,<p>After I got my code to work I tried both the methods of creating the environment and both give identical results.</p>,2019-10-04T10:56:06Z,26,Week 9/29 - 10/5,feedback,,jl2egn5k4zo4lp,k1c0jgjnt691f8,2019-10-04T10:56:06Z,{},hw3
2472,no,"<p>Yeah, Abdoul I had a similar issue, wasted important time on this. I was using gym.make. Scratched my head so much, couldn&#39;t find a single issue apart from it, finally this worked.</p>",2019-10-06T16:16:03Z,25,Week 10/6 - 10/12,feedback,,jzzw8pi7a7t3d4,k1f6umxqcwv567,2019-10-06T16:16:03Z,{},hw3
2473,no,"<p>Let me borrow your post to share. Took me a while to figure out, the actual meaning for the action codes:</p>
<p></p>
<pre>LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3</pre>
<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py"">https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py</a></p>",2019-10-04T21:06:00Z,26,Week 9/29 - 10/5,followup,,jl11i2neyt7G,k1cmbswjg1d18v,2019-10-04T21:06:00Z,{},hw3
2474,no,"<p></p><pre>LEFT, DOWN, RIGHT, UP = range(4)</pre>
<p></p>",2019-10-04T21:57:00Z,26,Week 9/29 - 10/5,feedback,,hyx9thiqa6j4nn,k1co5dxjslj77k,2019-10-04T21:57:00Z,{},hw3
2475,no,"<p>I just ran as many episodes as specified by the example/exercise, and each episode until done.</p>",2019-10-05T04:57:32Z,26,Week 9/29 - 10/5,followup,,is8ald0uljj3u4,k1d366qh57v72e,2019-10-05T04:57:32Z,{},hw3
2476,no,"<p>Thanks, Chris!</p>",2019-10-04T15:34:20Z,44,Week 9/29 - 10/5,followup,,isde34zracb1mz,k1cah9xqw69z,2019-10-04T15:34:20Z,{},project2
2477,no,"<p>This is fascinating, especially Csaba&#39;s explanation.  I&#39;m curious too as to the disadvantages/shortcomings of Advantage Learning.  Convergence problems maybe?</p>",2019-10-04T15:37:29Z,44,Week 9/29 - 10/5,followup,,jzfsa4a37jf4aq,k1calbrcfdd713,2019-10-04T15:37:29Z,{},project2
2478,no,"<p>Update - realized that although Wikipedia appears to have no page for Advantage Learning, the Reinforcement Learning page&#39;s <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning#Comparison_of_reinforcement_learning_algorithms"" target=""_blank"" rel=""noopener noreferrer"">Comparison of RL Algorithms</a> table has an &#39;Operator&#39; column where Q-value is the operator for several algorithms (Q-learning, SARSA, DQN, etc.),  and Advantage is the operator for several as well (A3C, TRPO, PPO, etc., as Chris mentioned).  Also interesting to note the differences in Action Space &amp; State Space columns for most Q-value vs Advantage algorithms. Wikipedia&#39;s RL algs table gives a helpful view of the space.</p>",2019-10-04T18:06:36Z,44,Week 9/29 - 10/5,followup,,isde34zracb1mz,k1cfx3mopky1ee,2019-10-04T18:06:36Z,{},project2
2479,no,<p>Thanks for sharing. It is a nice post. </p>,2019-10-04T18:41:08Z,26,Week 9/29 - 10/5,followup,,jzg6jh2hn6f43c,k1ch5hr3zvi52s,2019-10-04T18:41:08Z,{},hw3
2480,no,"<p>Thanks as well, the visuals really help.</p>",2019-10-05T05:39:13Z,26,Week 9/29 - 10/5,followup,,ixty1midfufhd,k1d4nsqjssb4hx,2019-10-05T05:39:13Z,{},hw3
2481,no,<p>Whats algorithm for function approximation?</p>,2019-10-05T15:48:47Z,59,Week 9/29 - 10/5,followup,,jzj7y1ofgsro1,k1dqfpcoka79d,2019-10-05T15:48:47Z,{},project2
2482,no,"<p>Function approximation covers a large range of algorithms. Basically any algorithm that is trying to estimate the target function from the hypothesis space. So neural networks, decision trees, SVMs, etc are all algorithms that try to approximate the target function we are trying to find, (which of course is the optimal one).</p>",2019-10-05T16:18:04Z,59,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k1drhdeut3j3ca,2019-10-05T16:18:04Z,{},project2
2483,no,"<p>The &#34;generalization&#34; lectures are a good intro. Then, I&#39;d recommend picking up the nature DQN paper.</p>",2019-10-06T00:28:02Z,58,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1e8zgv5nq82v1,2019-10-06T00:28:02Z,{},project2
2484,no,"<p>Is this the DQN paper?</p>
<p><a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a></p>
<p></p>",2019-10-06T18:11:21Z,58,Week 10/6 - 10/12,feedback,,is8ald0uljj3u4,k1faywvvs3y3r,2019-10-06T18:11:21Z,{},project2
2485,no,"<p>This is good.</p>
<p></p>
<p>For answer submissions, do not add any &#39; from your print statement</p>
<p></p>
<p>answer should be</p>
<p></p>
<p>&gt;,&lt;,&gt;,&gt;</p>
<p></p>
<p>like that</p>
<p></p>",2019-10-06T05:08:53Z,25,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1ej0nhzx0v4d2,2019-10-06T05:08:53Z,{},hw3
2486,no,"<p>env.reset() gets you the first state, and things start from there. This simple thing was tripping me up. So see if it applies for you</p>",2019-10-06T04:59:35Z,25,Week 10/6 - 10/12,followup,,jqrr36mqfm8M,k1eioop34zb6vz,2019-10-06T04:59:35Z,{},hw3
2487,no,"<p>maybe what Miguel says at &#64;542 will help you.</p>
<p>he CLEARLY explains what needs to be done</p>",2019-10-06T06:44:32Z,25,Week 10/6 - 10/12,followup,,jzh6k6o994a6dh,k1emfnr492i64z,2019-10-06T06:44:32Z,{},hw3
2488,stud,<p>Aha! Forgot to be NOT greedy at next state. Thanks a lot!</p>,2019-10-05T03:05:54Z,26,Week 9/29 - 10/5,followup,a_0,,k1cz6n77yt4fe,2019-10-05T03:05:54Z,{},hw3
2489,no,"<p>Yeah, I knew exactly which line in your code you had to change, but didn&#39;t want to be too obvious :)</p>",2019-10-05T03:13:14Z,26,Week 9/29 - 10/5,feedback,,jzfsa4a37jf4aq,k1czg2dd8rx5ca,2019-10-05T03:13:14Z,{},hw3
2490,stud,<p>Saved my day!</p>,2019-10-05T03:53:59Z,26,Week 9/29 - 10/5,feedback,a_0,,k1d0wgnvs3w5uz,2019-10-05T03:53:59Z,{},hw3
2491,no,<p>I&#39;m still confused. Could you please elaborate a little bit more? I&#39;m having exactly the same issue.</p>,2019-10-06T11:00:49Z,25,Week 10/6 - 10/12,feedback,,jzjb6ojxtuc1d3,k1evl83wizi1dd,2019-10-06T11:00:49Z,{},hw3
2492,stud,"If you check the definition of Sarsa on page 130, you will see that epsilon-greedy policy is used twice within one Q update. ",2019-10-06T17:14:29Z,25,Week 10/6 - 10/12,feedback,a_0,,k1f8xs23bq11yo,2019-10-06T17:14:29Z,{},hw3
2493,no,"<p>Ok, i was getting all &#39;1&#39;s for the third test case and I thought there was something wrong. But after testing on other it works as intended. So, I guess for test case 3 it&#39;s supposed to behave like that?</p>",2019-10-06T23:05:48Z,25,Week 10/6 - 10/12,feedback,,jzjb6ojxtuc1d3,k1flhk4xb9c5s4,2019-10-06T23:05:48Z,{},hw3
2494,no,"<p>If you print out a high precision, you&#39;ll see that they&#39;re not<em> exactly</em> one, which is why you don&#39;t have &lt;.&lt;.&lt;.&lt;.</p>
<p></p>
<p>But yeah, take a look at the map -- no holes, $$\gamma=1$$, right?</p>",2019-10-07T00:58:26Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1fpif263814u5,2019-10-07T00:58:26Z,{},hw3
2495,no,"<p>&gt; Remember that in SARSA, we must update the value function using the same policy we&#39;re exploring with.</p>
<p></p>
<p>I have been having a weak understanding on this, and on on-policy vs off-policy (even after reading &#64;470 and &#64;590), until I see this. &#64;Vahe, I am curious on how you arrive at this conclusion, if you don&#39;t mind. What was the mid-step info that lead you to this intuition?</p>",2019-10-09T02:03:35Z,25,Week 10/6 - 10/12,followup,,jl5wq8mca7o0,k1impwm1ylo40,2019-10-09T02:03:35Z,{},hw3
2496,no,"<p>Hmm, I guess it comes straight from the text of the algorithm really.  SARSA decides where it&#39;s going to go (chooses an action according to its policy) <em>then</em> updates the current state-action value with a bootstrapped estimate of the value of that future path that it&#39;s chosen to explore.</p>
<p></p>
<p>I guess one thing that helps is to focus on the differences.  Why are there all these different algorithms?  What makes each of them special?  What motivated their invention?  Why use one over another?  This is far from settled in my brain.  I&#39;m sure that after a week of doing something else I&#39;ll have forgotten a lot of what might seem intuitive today, unfortunately.</p>",2019-10-09T02:36:09Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1invs9n230us,2019-10-09T02:36:09Z,{},hw3
2497,no,"<p>When I read the text of the algorithm, I was under the impression picking E-greedy for Q(s&#39;, a&#39;) is just a choice of implementation, without realizing it was actually a keypoint of the method.</p>
<p>I guess I was skimming those too fast without actively questioning like you did. Thanks for answering my question, and for the food for thought! I really appreciate you going to step further to explain your thought.</p>",2019-10-11T00:55:46Z,25,Week 10/6 - 10/12,feedback,,jl5wq8mca7o0,k1lf6ej3wws5h,2019-10-11T00:55:46Z,{},hw3
2498,no,"<p>I think one of the problems with our (this class&#39;) understanding of SARSA is that we did <strong>not</strong> actually do any learning with it; i.e., we didn&#39;t try to converge to a good policy.  HW3 only asked us to run it with given (random) inputs and match it to a grader&#39;s output.</p>
<p></p>
<p>In other words, we didn&#39;t get to <em>play</em> with SARSA to try to make it learn to do something, like even to solve the Frozen Lake problem.  I did that on my own, which helped me gain some intuition for how to play with $$\epsilon$$ to get SARSA to converge, but the homework didn&#39;t make us do that.</p>",2019-10-11T04:05:51Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1llyuk5jor1ue,2019-10-11T04:05:51Z,{},hw3
2499,no,"<p>My hope is that the final will test concepts that, by that point in the course, we should really understand well.  If that is the case, then I can channel all my stress into the projects, homework, and readings, knowing that if I&#39;m diligent with those, the final won&#39;t be too bad.</p>
<p></p>
<p>The alternative is that the final tests esoteric concepts and requires recall of things that aren&#39;t immediately accessible from simply understanding the major concepts well.   If that&#39;s the case, then my approach probably won&#39;t work too well...</p>",2019-10-05T03:11:34Z,59,Week 9/29 - 10/5,followup,,jzfsa4a37jf4aq,k1czdxjs2tb3wm,2019-10-05T03:11:34Z,{},final_exam
2500,no,"<p>I start to imagine which format the exam will be. A written format would be too open and too board. A multiple choice would be easier to test for a wide range of topics, and easier for TAs to grade. I think it could be quiz with calculation involved, like Udacity quiz. Udacity quiz is the most familiar form of confirming knowledge that we had so far into the course.</p>",2019-10-05T04:46:22Z,59,Week 9/29 - 10/5,feedback,,jl5wq8mca7o0,k1d2rua74h9606,2019-10-05T04:46:22Z,{},final_exam
2501,no,"<p>As a note, it was mentioned in office hours 1 that the final will be &#34;true or false&#34; with air quotes, but with the caveat that it is not easy.</p>",2019-10-05T04:51:03Z,59,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k1d2xuko2ip339,2019-10-05T04:51:03Z,{},final_exam
2502,stud,"<p>I wouldn&#39;t bank on the final looking anything like a collection of Udacity quizzes. That was most emphatically <em>not</em> my impression of the midterm/final in ML. </p>
<p></p>
<p>BTW, the <em>average score</em> for the ML final the semseter I took it was right around 50%. </p>",2019-10-05T12:14:48Z,59,Week 9/29 - 10/5,feedback,a_0,,k1disizmc738f,2019-10-05T12:14:48Z,{},final_exam
2503,no,<p>Questions will be True/False with an short explanation as to why it is True/False</p>,2019-10-06T13:36:15Z,58,Week 10/6 - 10/12,feedback,,hz7meu55mi8sd,k1f1548g8e37bd,2019-10-06T13:36:15Z,{},final_exam
2504,no,"<p>1) Closed book likely means no book, no notes, no internet, no scratch paper, no calculator, or extra browser tabs; basically, nothing but your brain :)</p>
<p>Or at least this has been the case is almost every other class that said closed book I&#39;ve taken. Isbell mentioned that we will get everything we need. So if he believes we will need scratch paper, it will be allowed, but I&#39;m guessing not.</p>
<p></p>
<p>2) I&#39;ve found nothing beats me reading\watching it (on paper where I can highlight and underling things), take notes by physically writing them down, and then reviewing my notes (multiple times), and creating &#34;fake exams&#34; out of the lecture quizzes that I can take a month later to see how much I remember things. I also find that hand working out algorithms helps me a lot. However, every person is different.</p>
<p></p>
<p>Just my extra thoughts: </p>
<p> - Isbell had an interesting statement on how he approached the projects and exams in ML. But also swore us to not talk about his exams at all, so there isn&#39;t anything I can say. I&#39;m guessing his approach for this class will be similar; so it will be interesting if he posts a similar FAQ about it as we get closer like he did in ML; although I would expect it after the withdraw date if any.</p>
<p></p>
<p>Some details are in &#64;52 as well.</p>",2019-10-05T04:35:40Z,59,Week 9/29 - 10/5,followup,,ixty1midfufhd,k1d2e2th6zf503,2019-10-05T04:35:40Z,{},final_exam
2505,no,"<p>I have been put off by the format for a while, learning bits from here and there, while having no textbook.  </p>
<p>I tried to take notes while listening to videos, as if I was in a class, but it&#39;s a lot of work and it&#39;s not that easy to study from that, it looks all like more or less the same, just blue text on white paper.</p>
<p></p>
<p>I did that even for D Silver who has lots of very nice slides, so I switched to printing the slides, 6 per page, and taking notes on them, and that made a huge difference, having everything laid out in front of me.</p>
<p>I did that for D S&#39;s lecture 5, about Sarsa and Q-learning, couldn&#39;t happen at a better time, and honestly I still have most of the slides and/or information in my mind.  And it&#39;s so easy to go back to and quickly find what I want.  </p>
<p></p>
<p>So, I decided to create slides from this course videos by taking screenshots, so when I will rewatch them, I&#39;ll have a support to write on, and that&#39;s what I&#39;ll be studying for the final.</p>
<p>However, it takes quite a lot of time to find the right time to take the screenshot when M Littman has his hand away from the text, and sometimes it needs a little photoshopping but, to me, it&#39;s really worth it.  </p>
<p></p>
<p>Here&#39;s an example of what it looks like.  It seems packed but we can always add one white slide per page to leave room for notes.</p>
<p>I started only recently, so I only have AAA, Messing with rewards, Exploring exploration and generalization.  </p>
<p>It&#39;d be great if a few people would help out so we can have the full course quickly and share it among us.  </p>
<p>We could even go further and add slides from Silver or Sutton (I have all the slides for his book that I downloaded from <a href=""http://www-anw.cs.umass.edu/~barto/courses/cs687"">http://www-anw.cs.umass.edu/~barto/courses/cs687</a>) but maybe that&#39;s too much to ask or people could do it invidivually.  </p>
<p></p>
<p>If anyone knows how to easily remove all those greyish areas, on the whole PDF, not slide by slide, let me know.  </p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk1d3lmkj5ap3%2Fscreenshot_16.jpg"" alt="""" /></p>",2019-10-05T05:21:03Z,59,Week 9/29 - 10/5,followup,,jzh6k6o994a6dh,k1d40fkubjw26l,2019-10-05T05:21:03Z,{},final_exam
2506,no,<p>This is actually not a bad shout at all. Since I&#39;m going through the lectures again anyway I&#39;ll try to create some of these. If it goes well I&#39;ll put up the slides somewhere useful.</p>,2019-10-05T10:29:58Z,59,Week 9/29 - 10/5,feedback,,jl3oi5v7qkSk,k1df1ptnhpb3v0,2019-10-05T10:29:58Z,{},final_exam
2507,no,"<p>Hi &#64;Ben, if we could find 5 or 10 people, we could be done in a few days, and then we have a proper support to take notes.  </p>",2019-10-05T11:21:38Z,59,Week 9/29 - 10/5,feedback,,jzh6k6o994a6dh,k1dgw5e3yjq3v1,2019-10-05T11:21:38Z,{},final_exam
2508,no,"<p>Thank you Jean-Pierre Bianchi for showing such a wonderfully simple and painlessly easy way to take notes of the lecture.. In most cases, we could just skip through to the end of the video and screenshot it. There could be some exceptions where some contents were taken away and/or new contents introduced, which we can cover as we are jotting down the notes while watching the video lectures. </p>",2019-10-11T16:42:52Z,58,Week 10/6 - 10/12,feedback,,jc9ssyte5au1hp,k1md0ddwb70615,2019-10-11T16:42:52Z,{},final_exam
2509,no,"<p>I like your take on it.  Physics governs whether a step too close to the edge will lead to a fall off the cliff, and it&#39;s not really subjective.  But what should be assigned as the reward for falling off?  -100?  -10?  0? &#43;5?  LIke you said it&#39;s subjective.</p>
<p></p>
<p>In the framework of MDPs reward is decided ahead of time and provided to us in tandem with the next-state information, so I think it&#39;s lumped with the environment because it doesn&#39;t cost us anything to do that.</p>
<p></p>
<p>Also, the fact that we can &#34;reward-shape&#34; but not &#34;environment-shape&#34; kind of highlights this difference between rewards and the environment - rewards are merely a tool to get the agent to learn some desired behavior.  Environment is the landscape where this behavior is applied.</p>",2019-10-05T04:50:10Z,44,Week 9/29 - 10/5,followup,,jzfsa4a37jf4aq,k1d2wq0b6687fd,2019-10-05T04:50:10Z,{},project2
2510,no,"<p>This is actually a good point. In real-world RL, you commonly use an environment (simulation engine) that simulates the physics of the world. Separately, you have to design a reward function for your agent to learn the task.</p>
<p></p>
<p>I think &#34;in a sense,&#34; that thinking is right.</p>",2019-10-05T18:05:15Z,44,Week 9/29 - 10/5,followup,,hyx9thiqa6j4nn,k1dvb7nxzgqhz,2019-10-05T18:05:15Z,{},project2
2511,no,When I changed from np.random.randint to np.random.random for generating random number for epsilon it worked. Hope this helps.,2019-10-05T06:18:27Z,26,Week 9/29 - 10/5,followup,,jc5n7oq5zop3e8,k1d629hxxe06f6,2019-10-05T06:18:27Z,{},hw3
2512,stud,thanks a lot! np.random.randint wont&#39; return same result every run.,2019-10-05T12:15:52Z,26,Week 9/29 - 10/5,feedback,a_0,,k1ditwf0eu83pk,2019-10-05T12:15:52Z,{},hw3
2513,stud,"thanks. This is what i am doing but still results are wrong. I  am looking at what suggested above  on <a href=""/class/jzh9tkzzxkd7ph?cid=495"">&#64;495</a>. 

Although i am terminating my states .
Can you please point  where was bug  .I am facing same issue and  have same  Martix at Last column at  0 .

I am terminating after updating Q  .

1) Loop while true
2) update state/reawards/acions 
3) update Q
4) if Fall  into H or G  , then break 
5) update new states and actions 

Plz any pointers what i am doing  wrong ?",2019-10-05T13:27:16Z,26,Week 9/29 - 10/5,feedback,a_1,,k1dldprmk8k1k7,2019-10-05T13:27:16Z,{},hw3
2514,stud,"<p>Ah ! Looks like i am not  moving  upwards . Still scathing  my head where the big bug is .</p>
<p></p>",2019-10-05T13:39:40Z,26,Week 9/29 - 10/5,feedback,a_1,,k1dlto4348xt4,2019-10-05T13:39:40Z,{},hw3
2515,no,"<p>I would look at where you select your actions, why would it not select upwards?</p>",2019-10-05T16:09:04Z,26,Week 9/29 - 10/5,feedback,,ixty1midfufhd,k1dr5s7ymvk4pg,2019-10-05T16:09:04Z,{},hw3
2516,no,"Thanks You all  . 2 issues on my code if that helps others :-
1) It was environment.It was wrong . 
2) As mentioned  above i  used   np.random.randint (0,4)  for action and np.random.random for policy comparison with eplison .",2019-10-06T05:41:29Z,25,Week 10/6 - 10/12,feedback,,jzjvnayuw8t2br,k1ek6k3kj101fl,2019-10-06T05:41:29Z,{},hw3
2517,no,Follow up question: do synchronous environments become asynchronous environments by the addition of a &#34;wait&#34; or &#34;do nothing&#34; action?,2019-10-06T02:06:13Z,58,Week 10/6 - 10/12,followup,,jl3oi5v7qkSk,k1echqpvj0n187,2019-10-06T02:06:13Z,{},other
2518,no,<p>It seems like <em>asynchronous </em>environments may become <em>dynamic</em> environments with the addition of a &#34;do nothing&#34; action.</p>,2019-10-06T02:30:00Z,58,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1edcb5k5v62wo,2019-10-06T02:30:00Z,{},other
2519,no,"<p></p>
<p>some posts that helped me with the details</p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=506"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=506</a></p>
<p><br /><a href=""/class/jzh9tkzzxkd7ph?cid=465"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=465</a></p>
<p><br /><a href=""/class/jzh9tkzzxkd7ph?cid=542"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=542</a></p>
<p></p>",2019-10-06T00:22:16Z,25,Week 10/6 - 10/12,followup,,i4t1oo4aALG,k1e8s28xwkb4mb,2019-10-06T00:22:16Z,{},hw3
2520,no,"<p>thanks!!!  I was doing some crazy stuff for initializing the state. After I just used env.reset() to get the first state, all things just worked nicely :) </p>
<p></p>
<p>I was upset this wasn&#39;t documented in gym docs. I was expecting more formal javadoc style </p>
<p></p>
<p>But I found it after I looked again harder..<a href=""http://gym.openai.com/docs/#environments"">http://gym.openai.com/docs/#environments</a>. It  does say in between one of the paragraphs that the reset() returns the initial observation :)</p>
<p></p>
<p></p>
<p></p>",2019-10-06T04:55:29Z,25,Week 10/6 - 10/12,followup,,jqrr36mqfm8M,k1eijexnd483ns,2019-10-06T04:55:29Z,{},hw3
2521,no,"<p>Likely a code bug. I was getting all zeroes initially as well, and I found out that I did not have a &#39;G&#39; in my initial map I was using for testing :). Maybe a silly mistake like that somewhere?</p>",2019-10-06T04:46:08Z,25,Week 10/6 - 10/12,followup,,jqrr36mqfm8M,k1ei7e2sqix4xb,2019-10-06T04:46:08Z,{},hw3
2522,no,<p>It must have been. I restarted the kernel and it worked ok afterward. Still don&#39;t know why haha...</p>,2019-10-06T04:47:56Z,25,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1ei9phe8k33rp,2019-10-06T04:47:56Z,{},hw3
2523,no,"<p>nice! whats the pip module to install for this?</p>
<p></p>
<p>pip install ?</p>",2019-10-06T06:01:06Z,25,Week 10/6 - 10/12,followup,,jqrr36mqfm8M,k1ekvsd011y5bl,2019-10-06T06:01:06Z,{},hw3
2524,no,<p>Yup. pip install colorama</p>,2019-10-06T06:03:45Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1ekz79uul34z7,2019-10-06T06:03:45Z,{},hw3
2525,no,"<p>Question:</p>
<p>When you are using np.random.randint()/100 &lt; epsilon,</p>
<p></p>
<p>Are you using 0,100 or 0,101?</p>
<p></p>
<p></p>",2019-10-06T04:46:30Z,25,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1ei7v2kry62nj,2019-10-06T04:46:30Z,{},hw3
2526,no,"<p></p><p>After 3 hours of debugging, I finally found the culprit. Although many people on Piazza spoke about using randint, it did not produce the result that matched what was on the HW3.pdf.</p>
<p></p>
<p>I&#39;ve been using either version of:</p>
<p></p>
<pre>np.random.randint(0,100)/100.0 or np.random.randint(0,101)/100.0 </pre>
<p>to compare against epsilon but it was not giving me the result that matches the example although it was giving me consistent result every time.</p>
<p> </p>
<p>Trying Aaron&#39;s way of getting random -&gt;</p>
<p> </p>
<pre>np.random.uniform(0,1)</pre>
<p>This finally got my answer to print out correctly</p>
<pre>[[0.20944331 0.48653866 0.49730795 0.68301692]
 [0.64341951 0.9112584  0.85111285 0.65556764]
 [0.88837089 0.93115725 0.9167586  0.91419646]
 [0.93214126 0.92292253 0.94354097 0.93449757]
 [0.         0.         0.         0.        ]
 [0.25413104 0.47579825 0.92797157 0.81091787]
 [0.94250322 0.94107199 0.94463412 0.87908423]
 [0.94330518 0.96351515 0.95250744 0.94962169]
 [0.53645891 0.89473301 0.29734493 0.67329215]
 [0.85421854 0.9185382  0.81201688 0.77385265]
 [0.94364073 0.90618541 0.96586523 0.93488531]
 [0.95529001 0.97584787 0.96304856 0.96718743]
 [0.79949079 0.85530704 0.8713067  0.85038178]
 [0.85332313 0.88474512 0.90352541 0.87641602]
 [0.9494872  0.95591453 0.96495681 0.95634992]
 [0.         0.         0.         0.        ]]
</pre>
<div>
<pre>&#39;^,v,v,&gt;,&lt;,&gt;,&gt;,v,v,v,&gt;,v,&gt;,&gt;,&gt;,&lt;&#39;</pre>
<p> </p>
<p>My development environment was Jupyter Notebook using Python 3.6.8 and Numpy version 1.17.0</p>
<p></p>
<p>Maybe it had something to do with Python version and Numpy versions?</p>
</div>",2019-10-06T05:03:53Z,25,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1eiu7k5is332q,2019-10-06T05:03:53Z,{},hw3
2527,no,<p>Can you please confirm you used random.random() for epsilon comparison and random.uniform for selecting the direction to go?</p>,2019-10-06T18:19:29Z,25,Week 10/6 - 10/12,feedback,,jqud6q7bqlagv,k1fb9cp8brk1fr,2019-10-06T18:19:29Z,{},hw3
2528,no,"<p>The &#39;direction to go&#39; is one of a discrete set of actions (West, South, East, North), so you shouldn&#39;t be using random.random() or random.uniform().</p>",2019-10-06T18:21:59Z,25,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1fbcktrrvsyl,2019-10-06T18:21:59Z,{},hw3
2529,no,"<p>As Vahe mentioned above, np.random.random() or np.random.uniform is used for comparison for epsilon. For the direction, use randint(0,4) for this assignment.</p>
<p></p>
<p>Miguel mentioned that for the next assignment we need to use randint(env.... something that gets the # of actions) as the # of actions available differs from each entry.</p>",2019-10-06T18:51:52Z,25,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1fcf0kn9oz628,2019-10-06T18:51:52Z,{},hw3
2530,no,"<p>Got it. I think I&#39;m getting stuck because of consuming seed. For epsilon comparison I used random.random() and for getting the direction I use np.random.randint()</p>
<p>In my outer loop when I select the action I use the seed once. Within the inner loop I use the seed with env and again for the next action.</p>
<p>So, do I have to consume env seed one more time? And how about done=True?</p>",2019-10-06T20:14:58Z,25,Week 10/6 - 10/12,feedback,,jqud6q7bqlagv,k1ffdvtzbr12n2,2019-10-06T20:14:58Z,{},hw3
2531,no,"<p>I&#39;ve set my environment using the upwrapped suggestion.</p>
<p>Set the seeds for numpy and the environment</p>
<p>Set the Q table to 0&#39;s</p>
<p>For each episode:</p>
<p>-&gt;I have reset my state</p>
<p>-&gt;Checked to see if random.random() &lt; epsilon</p>
<p>--&gt;If it is then I took a random action -<strong>How are you taking a random action?</strong></p>
<p>--&gt;If not then I took an action that has a max Q value for that state</p>
<p>-&gt;After that I looped through each possible states <strong>-What do you mean you loop through each possible states??</strong></p>
<p>--&gt;Checked to see if random.random() &lt; epsilon</p>
<p>---&gt;If it is... do the same as above</p>
<p>---&gt;If not... same thing</p>
<p>-<strong>Assuming the above 3 steps are taken to acquire a&#39;, it is correct.</strong></p>
<p>--&gt;Updated the Q table with Q(s, a) &#43;= alpha * (reward &#43; gamma * Q(s&#39;, a&#39;) - Q(s, a))</p>
<p>--&gt;s = s&#39;</p>
<p>--&gt;a = a&#39;</p>
<p><strong>- terminal state condition?when to break out?</strong></p>
<p><strong></strong></p>
<p>One question that I want to ask here:</p>
<p><strong>Are you at least getting consistently wrong answer? meaning is your output the same each run?</strong></p>",2019-10-06T05:17:44Z,24,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1ejc0xcoge4b5,2019-10-06T05:17:44Z,{},hw3
2532,no,"<p>Thanks for the reply!</p>
<p></p>
<p><strong>How are you taking a random action?</strong></p>
<p>random.randint(env.action_space.n)</p>
<p></p>
<p><strong>What do you mean you loop through each possible states?</strong></p>
<p>The while loop keeps running until the return value from env.step(action) for &#39;done&#39; is True.</p>
<p></p>
<p><strong>Are you at least getting consistently wrong answers?</strong></p>
<p>Yep every time it gives me the same Q table as above. I thought it might of been the seed but it seems like that is doing what its supposed to be doing.</p>",2019-10-06T05:25:19Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1ejls5exls2no,2019-10-06T05:25:19Z,{},hw3
2533,no,"<p>How many times <strong>total</strong> are you calling a random number generator in your code?</p>
<p></p>
<p>And how many of them are in the inner loop, how many in the outer loop, and how many in no loops?</p>",2019-10-06T05:46:20Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1ekcsqg6105j7,2019-10-06T05:46:20Z,{},hw3
2534,no,"<p>Try using the following to compare epsilon</p>
<pre>np.random.uniform(0,1)</pre>",2019-10-06T05:58:39Z,24,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1eksng3mwtii,2019-10-06T05:58:39Z,{},hw3
2535,no,"<p>np.random.random() and np.random.uniform(0,1) are exactly the same</p>",2019-10-06T06:02:46Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1ekxxpyzt34b8,2019-10-06T06:02:46Z,{},hw3
2536,no,"<p>yup, both are same.</p>
<p></p>
<p>I have same problem as John. I get correct answer for example 1 and 40% problems from HW also.</p>
<p></p>
<p>Rest all seem to get &#39;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&#39; as solution.</p>",2019-10-06T06:04:43Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1el0fytelm5vy,2019-10-06T06:04:43Z,{},hw3
2537,no,<p>I feel like this assignment is way too delicate :( </p>,2019-10-06T06:05:40Z,24,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1el1ntb73n6de,2019-10-06T06:05:40Z,{},hw3
2538,no,<p>Try to count how many times random are called per vahe&#39;s suggestion</p>,2019-10-06T06:06:32Z,24,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1el2sg31cm6ym,2019-10-06T06:06:32Z,{},hw3
2539,no,"<p>For example 1 -&gt; 79326 times</p>
<p></p>
<p>Outer loop = 4325</p>
<p>Inner loop = 75001</p>
<p>No loop = 0 ?</p>",2019-10-06T06:15:12Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1eldxorsk92vn,2019-10-06T06:15:12Z,{},hw3
2540,no,"<p>I meant in the code itself, not during runtime.  But we can do it this way too.</p>
<p></p>
<p>For example one, there are 14697 episodes total.  If there were even <em>one</em> call to a random number generator in your outer loop (the one that loops through episodes), wouldn&#39;t there be more than 4325 calls?  If there were two, then there would be $$2\cdot 14697 = 29394$$ calls, just from the outer loop, and many, many more from the inner loop.</p>",2019-10-06T06:35:57Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1em4lzj5t73e,2019-10-06T06:35:57Z,{},hw3
2541,no,"<p>This means</p>
<pre>np.random.uniform(0,1) &gt; epsilon</pre>
<p>mostly for outer loop.</p>
<p></p>
<p>Inner loop are many, many more as you mentioned.</p>",2019-10-06T06:40:13Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1ema38d8qc5mz,2019-10-06T06:40:13Z,{},hw3
2542,no,"<p>Two things:</p>
<p></p>
<p>1. Even if there were only one random call in the outer loop, you would have more than $$4325$$ calls.  Are you using a for-loop or a while-loop for your outer loop?</p>
<p></p>
<p>2. There should be another call to a random function in your outer loop to choose from the Action Space those times when np.random.uniform(0,1) is <em>less than</em> $$\epsilon$$.</p>",2019-10-06T06:48:12Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1emkdeuvk33k,2019-10-06T06:48:12Z,{},hw3
2543,no,"<p>1. Yes, two random calls when random.uniform is less than epsilon makes calls 4325 &#43; 4325 = 8650.</p>
<p>Using a while loop as </p>
<pre>while curr_episode &lt; n_episodes:</pre>
<p></p>
<p>2. Yes, that call is there but only happens when </p>
<pre>np.random.uniform(0,1) &lt; epsilon </pre>",2019-10-06T06:50:20Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1emn3rntbr4ad,2019-10-06T06:50:20Z,{},hw3
2544,no,"<p>So you have two separate calls in the outer while loop -- one to np.random.uniform(0,1) and one to &lt;&gt;?</p>
<p></p>
<p>Then in the inner loop you also have the same two calls?</p>
<p></p>
<p>If that is so, then your calls to random look okay.</p>
<p></p>
<p>It still doesn&#39;t explain the numbers you were getting though...</p>",2019-10-06T06:53:57Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1emrroemm91t0,2019-10-06T06:53:57Z,{},hw3
2545,no,"<p>Yup. So, not sure what&#39;s the issue here. My example 1 produces correct results.</p>
<p></p>
<p>example 2 and 3 not. Let me look more into it.</p>",2019-10-06T06:55:03Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1emt6d3jpj7ki,2019-10-06T06:55:03Z,{},hw3
2546,no,"<p>Oh, never mind.  If your example 1 is correct then it&#39;s definitely not a randomization problem.</p>",2019-10-06T06:55:47Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1emu4n67s33id,2019-10-06T06:55:47Z,{},hw3
2547,no,"<p>The numbers I had to double in both the cases, I guess, for both the calls. But yes, example 1 and some HW3 questions also work but others don&#39;t.</p>",2019-10-06T06:56:54Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1emvkb798l1hm,2019-10-06T06:56:54Z,{},hw3
2548,no,"<p>Focus on example 3 because it&#39;s so simple, only a 2x2 grid.</p>",2019-10-06T06:57:23Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1emw64rw5b4oi,2019-10-06T06:57:23Z,{},hw3
2549,no,<p>It works if episode=1.</p>,2019-10-06T06:59:25Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1emys89dbj33v,2019-10-06T06:59:25Z,{},hw3
2550,no,<p>episode = 1?</p>,2019-10-06T07:00:00Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1emzjoz1oe3p4,2019-10-06T07:00:00Z,{},hw3
2551,no,"<p>yup.</p>
<p></p>
<p>I think the problem is way I create M * N ?</p>
<p></p>
<p>I did 4 * 4 for example 1. </p>
<p></p>
<p>But 5 * 4 for example 2 i.e. I was mixing actions as number of columns in grid vs making the grid a square of N * N.</p>
<p></p>
<p>Looks like, it should be N * N ?</p>
<p></p>",2019-10-06T07:04:48Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1en5pg0ox962h,2019-10-06T07:04:48Z,{},hw3
2552,no,"<p>For the size of the map?  Yeah, it should be square.</p>
<p></p>
<p>But the input string in example 2 is 25 characters long.  That doesn&#39;t factor into anything but 5x5 (and 25x1).</p>",2019-10-06T07:05:59Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1en78hp6v070t,2019-10-06T07:05:59Z,{},hw3
2553,no,"<p>I think that was it. I was making wrong map and passing to desc.</p>
<p></p>
<p>Now I have correct answers for all examples. Thanks for staying in this debugging journey with me tonite :)</p>
<p></p>
<p>Hopefully this helps others also who might have done the same mistake. </p>",2019-10-06T07:07:51Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1en9mk7c871tp,2019-10-06T07:07:51Z,{},hw3
2554,no,"<p>Thanks all for the reply!</p>
<p></p>
<p>&#64;Vahe - I call numpy.random.random() once per episode for the current action to see if its less than epsilon, if it is I call numpy.random.randint()... then during the while loop I&#39;ll call numpy.random.random() to see if its less than epsilon and if it is I call the randint() the code structure (for the random calls) itself is:</p>
<p></p>
<p>For each episode:</p>
<p>     if random() &lt; epsilon -&gt; action = randint</p>
<p>     while not done:</p>
<p>          if random() &lt; epsilon -&gt; new action = randint</p>
<p></p>
<p><strong>Ive also tried reducing the number of random calls, like so:</strong></p>
<p>For each episode:</p>
<p>     action = 3</p>
<p>     while not done:</p>
<p>          if random() &lt; epsilon ... new action = randint</p>
<p></p>
<p>The closest I&#39;ve gotten to the answer was using the first approach.</p>",2019-10-06T15:44:15Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1f5pqofqvf7gu,2019-10-06T15:44:15Z,{},hw3
2555,no,"Few things:<div>1. For initial state, all calculation happens outside the rest of episode one episode </div><div><br /></div><div>2. Inside episode loop, first thing you do is take action found outside episode loop.</div><div>3. New action is being calculated using new state inside loop ?</div><div>4. Random calls are as per what everyone is writing on piazza ?</div><div>5. Try with episode=1 and make sure works as per algorithm in Sutton book.</div><div><br /></div><p></p>",2019-10-06T15:49:02Z,24,Week 10/6 - 10/12,feedback,,jfzaqnqvtQ1m,k1f5vvuirde1gt,2019-10-06T15:49:02Z,{},hw3
2556,no,"<p>&#64;John</p>
<p></p>
<p>Your first approach is the right one.  Your problem may be something other than your randomization seeds not matching the grader&#39;s.</p>",2019-10-06T15:54:23Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1f62rduqtq6re,2019-10-06T15:54:23Z,{},hw3
2557,no,<p>Hmmmm alright thanks for helping!</p>,2019-10-06T15:55:36Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1f64byer0u1db,2019-10-06T15:55:36Z,{},hw3
2558,no,"<p>You&#39;re setting both the env seed and your personal seed, right?</p>",2019-10-06T15:56:47Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1f65ukppp11ai,2019-10-06T15:56:47Z,{},hw3
2559,no,<p>yes sir! After I set the environment as unwrapped and before the &#34;for each episode&#34; loop begins.</p>,2019-10-06T16:02:26Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1f6d46mqoh62f,2019-10-06T16:02:26Z,{},hw3
2560,no,"<p>Thanks for this post. It really helped me solve HW3. I&#39;m pretty surprised that getting the right answer hinges on making the exact right number of calls to a random number generator. </p>
<p></p>
<p>I implemented the important concepts properly, but missed 2 calls to the random number generator in the outer loop. Seems like this assignment should be less dependent on implementation issues that are tangential to the important concepts of SARSA.</p>",2019-10-06T18:41:59Z,24,Week 10/6 - 10/12,feedback,,ixnwq0s4ozg,k1fc2ak5gs4lf,2019-10-06T18:41:59Z,{},hw3
2561,no,"<p>No problem! I unfortunately have not figured it out yet... </p>
<p></p>
<p>Can you help a brother out and explain how you approached it?</p>",2019-10-06T19:16:04Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1fda4q8l5q6u7,2019-10-06T19:16:04Z,{},hw3
2562,no,<p>&#64;John I&#39;m getting the same results. Have you found the reason why it&#39;s generating like that?</p>,2019-10-06T19:42:45Z,24,Week 10/6 - 10/12,feedback,,jqud6q7bqlagv,k1fe8g1lbk05gw,2019-10-06T19:42:45Z,{},hw3
2563,no,"<p>No not yet.</p>
<p></p>
<p>I recently tried changing the new action to the arg max of the Q[new_state] vs Q[state]. </p>
<p></p>
<p>No luck either way.</p>",2019-10-06T20:01:35Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1fewnxc2uu64l,2019-10-06T20:01:35Z,{},hw3
2564,no,"<p>I think it&#39;s about using seeds but I&#39;m still working on it. If I happen to find it, I&#39;ll inform you.</p>
<p></p>",2019-10-06T20:25:43Z,24,Week 10/6 - 10/12,feedback,,jqud6q7bqlagv,k1ffrp56ndq6sk,2019-10-06T20:25:43Z,{},hw3
2565,no,"<p>&#64;Conor, can you please help explain the &#39;2 calls to teh random number generator in the outer loop&#39;? it&#39;s a pain to deal with this ;-( </p>",2019-10-06T20:34:57Z,24,Week 10/6 - 10/12,feedback,,j6ilcw6hxoc77h,k1fg3kbbf863s3,2019-10-06T20:34:57Z,{},hw3
2566,no,<p>&#64;Muzaffer I&#39;ve looked through many examples and it seems having the seed init outside the episode loop is the way to go. If that is what you were referring to that is.</p>,2019-10-06T20:52:11Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1fgpq72lem275,2019-10-06T20:52:11Z,{},hw3
2567,no,"<p>FINALLY!</p>
<p></p>
<p>&#64;Muzaffer there were a few things I realized I did wrong.</p>
<p></p>
<ol><li>The map layout should be NxN (i.e. amap = [&#34;XXX&#34;, &#34;XXX&#34;, &#34;XXX&#34;]</li><li>The next action should be using argmax of Q[next state]</li></ol>
<p></p>
<p>My adjusted version is slightly different than my original code above so hopefully this helps you all. I&#39;ll be out for a few hours but I&#39;ll come back and see if I can help at all.</p>",2019-10-06T21:26:36Z,24,Week 10/6 - 10/12,feedback,,jzkke6iz3cl28t,k1fhxzvlpmq1fz,2019-10-06T21:26:36Z,{},hw3
2568,no,<p>Thanks &#64;John. I&#39;m sure I&#39;m good with layout and argmax - still struggling :(</p>,2019-10-06T21:59:05Z,24,Week 10/6 - 10/12,feedback,,jqud6q7bqlagv,k1fj3rrssd01wu,2019-10-06T21:59:05Z,{},hw3
2569,no,<p>Never mind. I had a bug in my code that is unrelated to any of the piazza discussions. it&#39;s working for me now. thanks for sharing the helpful tips. </p>,2019-10-06T22:06:12Z,24,Week 10/6 - 10/12,feedback,,j6ilcw6hxoc77h,k1fjcwyb8xw57s,2019-10-06T22:06:12Z,{},hw3
2570,no,"<p>Not sure if it helps, I had similar issue as John had. However, my bug was that I did not carry the next action correctly. My $$\epsilon$$-greedy keeps override the action that the model suppose to take, before it actually does that action.</p>
<p>This thread has been really helpful for me to taking the step back and see where things went wrong. Thanks everyone!</p>",2019-10-07T07:15:40Z,24,Week 10/6 - 10/12,feedback,,jl5wq8mca7o0,k1g2zj9y4216x,2019-10-07T07:15:40Z,{},hw3
2571,no,<p>My mapping seems wrong for down vs right?</p>,2019-10-06T05:51:40Z,25,Week 10/6 - 10/12,followup,,jqmj15jbCb55,k1ekjo412m2ug,2019-10-06T05:51:40Z,{},hw3
2572,no,"<p>And also I switched to np.random.uniform(0,1) now. I think I am getting the correct answer now. </p>",2019-10-06T05:52:51Z,25,Week 10/6 - 10/12,feedback,,jqmj15jbCb55,k1ekl6hbyog45j,2019-10-06T05:52:51Z,{},hw3
2573,no,"<p>Yeah. try to avoid random.random(). Use np.random.random() instead, or np.random.uniform(0,1).</p>",2019-10-06T06:11:09Z,25,Week 10/6 - 10/12,feedback,,jl2ggmnlGa55,k1el8ps4o9e29q,2019-10-06T06:11:09Z,{},hw3
2574,no,"<p>Hm, why are you using 3 different random generators?... answer that, and I&#39;m pretty sure you&#39;ll get it right.</p>
<p>Also, read CAREFULLY what Miguel said at &#64;542</p>
<p></p>
<p></p>",2019-10-06T06:36:33Z,25,Week 10/6 - 10/12,followup,,jzh6k6o994a6dh,k1em5dtc4da5vn,2019-10-06T06:36:33Z,{},hw3
2575,no,<p>Got it right! Thanks for the help. :)</p>,2019-10-06T06:42:51Z,25,Week 10/6 - 10/12,feedback,,jqmj15jbCb55,k1emdhlo7jf4yl,2019-10-06T06:42:51Z,{},hw3
2576,no,<p>Yes</p>,2019-10-06T05:56:44Z,43,Week 9/29 - 10/5,followup,,hzoi2qsuCAd,k1ekq6hm1dd6x0,2019-10-06T05:56:44Z,{},hw3
2577,no,<p>Working now. Thanks!</p>,2019-10-06T06:43:36Z,43,Week 9/29 - 10/5,feedback,,jqmj15jbCb55,k1emeg05ffg5ke,2019-10-06T06:43:36Z,{},hw3
2578,no,"<p>the keras potion simply replaces the part where we use a Q table. The entire program still requires your RL logic. (think Hw3/4), computing td error...etc ...etc </p>",2019-10-09T09:08:41Z,58,Week 10/6 - 10/12,followup,,jvfpllmsggt7p4,k1j1wkwnd1i7le,2019-10-09T09:08:41Z,{},project2
2579,no,<p>Exactly.</p>,2019-10-10T00:36:29Z,58,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1jz1qfkhzy6s5,2019-10-10T00:36:29Z,{},project2
2580,no,"<p>thanks &#64;Vahe for your insight, very valuable.  I&#39;m going to experiment more and report it here.  For instance, I haven&#39;t played with alpha at all, but now that I have 15 confirmed values, I hopefully can play more without falling into a trap like in my first try.  </p>
<p>sorry about the 5,000, I did that just before going to bed, it&#39;s obviously 3,000, from the difference between the two matrices (the zeros from the impossible states cancel out so I ignored them)</p>
<p></p>
<p>I am intrigued by the fact that you managed to get all digits right, so I&#39;ll try to replicate that.  It&#39;s quite logical that if you keep $$\epsilon$$ at 1 and try long enough, it has to converge too.  </p>
<p>Someone replied &#34;Interesting. I had the opposite experience — I had to use $\epsilon=1$ (and decreasing learning rate) to get my algorithm to converge to the expected answers.&#34; then removed his answer, but it&#39;s interesting (s)he means that he started with a higher alpha.</p>
<p>Maybe it&#39;s the fact that I kept alpha at 0.1 all the time that got me into trouble in that case.  Maybe when I experiment with that, I&#39;ll understand more clearly the relationship between $$\epsilon$$ and $$\alpha$$.  </p>
<p></p>
<p><em>&#34;It does seem that exploring becomes much more efficient in this problem if there&#39;s a little greediness to it, since I could get a completely converged value function in a fraction of the time with a lower </em><em>ϵ&#34;</em></p>
<p>I didn&#39;t mention it above, but I had to keep $$\epsilon$$ at 1 until $$\Delta$$Q was low (I switch to exponential decay when $$\Delta$$Q gets under 0.1).  If I remove the test on $$\Delta$$Q and start exponential decay right after I have explored the 2400 reachable states at least once, then I converged, even with a very low threshold, but to a different &#39;optimum&#39; (some values were the same, others not).  So, that was a hole the algo couldn&#39;t extract itself from, prob because $$\epsilon$$ was too low by that time to allow exploring out of it.  That too I&#39;m going to test and see which parameter can avoid it.  </p>
<p></p>
<p>After this exercise, I see much more clearly why we get to the optimal policy first, and it&#39;s quite obvious: to get the optimal policy, we just need that the optimal action gets a higher value than the other ones so np.argmax picks that one, while it&#39;s true value can still fluctuate until the algo reaches convergence, as long as THAT action remains max, but it must be the case because $$\Delta$$Q decreases quite smoothly.  I mean, if a state&#39;s argmax is at 5.1 and the other values are lower than, say, 3, and the $$\Delta$$Q&#39;s are, say, &lt; 0.1, then it&#39;s safe to say that the policy won&#39;t change anymore, even if the max Q value may end up at 5.2 or 4.7.  </p>
<p></p>
<p>Another thing to experiment with would be modifying the reward to immediately &#39;blacklist&#39; some moves, or give a bigger reward if the selected action goes in the direction of the customer before picking it up for instance.  </p>
<p></p>
<p>This thing is almost a project! </p>
<div>
<div></div>
</div>",2019-10-07T09:07:27Z,42,Week 10/6 - 10/12,followup,,jzh6k6o994a6dh,k1g6zaywcwj54i,2019-10-07T09:07:27Z,{},hw4
2581,no,"<p>It was me who removed my answer. I made the post and then noticed that the box above was reserved for answers. I meant to post a comment but not an answer. because it was more of my observations to what I did, and I was as puzzled to my observations as you to yours :-D.</p>
<p></p>
<p>I had to keep $$\epsilon=1$$ or very close to it to get it to converge to the expected answer, or otherwise, it converges to something else, which also generated pretty reasonable policies.  Now that you posted details about how you changed $$\epsilon$$ over time, I roughly get why.  I think a smaller constant learning rate works, too, but it is much slower. It took my algo about 1,500 episodes to get to the &#39;optimal&#39; answer. (I used a similar criterion with a threshold of $$10^{-12}$$.)</p>",2019-10-07T12:21:52Z,42,Week 10/6 - 10/12,feedback,,jzhs489b10i78c,k1gdxbghhg226z,2019-10-07T12:21:52Z,{},hw4
2582,no,"<p>&#64;Geoffrey ... 1500 episodes... wow that&#39;s low, I have to try that.  This hw is really interesting.  </p>
<p>Talking about speed, did you notice how much random() slows down the whole thing? </p>
<p>I&#39;m pretty sure that preparing a long random string with actions, and iterating over it would decrease the execution time a lot.</p>",2019-10-07T13:31:58Z,42,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1ggfgx0ura6x9,2019-10-07T13:31:58Z,{},hw4
2583,no,"<p><md><br />I noticed &#96;np.random.randint()&#96; seems very slow but, like you said, I think one can pre-generate an array of random numbers to speed it up.<br /></md></p>",2019-10-07T13:59:37Z,42,Week 10/6 - 10/12,feedback,,jzhs489b10i78c,k1ghf15srdc792,2019-10-07T13:59:37Z,{},hw4
2584,no,"<p>Depends on your definition of a &#39;solution.&#39; If you mean &#39;solved&#39; by the environment&#39;s standards, then it could be a long time or a shorter time depending on your method of solution.</p>
<p></p>
<p>For instance, I am discretizing the space (not doing value or function estimations). I have 1000 episode experiments that produce good results (maybe 50% of the evaluations are scoring &gt; 0, and maybe 1/3rd of those &gt; 200). Those take about 1 minute to train. Then I have longer experiments (50,000, 75,000, 200,000) which don&#39;t do as well and take hours to complete.</p>
<p></p>
<p></p>",2019-10-06T21:37:06Z,43,Week 10/6 - 10/12,followup,,jc554vxmyuy3pt,k1fibicuqha2n,2019-10-06T21:37:06Z,{},project2
2585,no,"<p>Here&#39;s an episode that scored 22.9449385684879. I got this after an hour of training 50,000 episodes using QQ.</p>
<p></p>
<p><video width=""300"" height=""200"" controls=""controls"">
<source src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk1fjktclf1xe%2F50.mp4"" type=""video/mp4"" />

</video></p>",2019-10-06T22:13:25Z,43,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1fjm6ydck3d5,2019-10-06T22:13:25Z,{},project2
2586,no,"<p>Looks good! And yes solved as per the papers description of an average score of 200 over 100 trials. I am also trying discretizing, however I am considering giving up this approach though, did 212k episodes and got less than half over 0. Can I ask how you are discretizing? I tried 10 bins for each of the state dimensions based on manual testing. IE range of x is 1.4 to -1.4 and so on.</p>",2019-10-07T00:11:58Z,43,Week 10/6 - 10/12,feedback,,jzj7y1ofgsro1,k1fnunfr3043mn,2019-10-07T00:11:58Z,{},project2
2587,no,"<p>My position is L/R/TDC. There is no value to the distance metric in the x,y binning.</p>
<p></p>
<p>Attitude is also L/R/TDC. Again no value knowing that you are -.5, 0.5, etc. Either you are off center or on center. You would want to use a continuous attitude space if we were doing a continuous Lunar Lander.</p>
<p></p>
<p>The velocity components need to be binned. They are important because of drift and their connection to the no-op action.</p>
<p></p>
<p>I encoded 3 bits of knowledge into the velocity components (8 bins) and then 12 bits for the spatials and then 1 bit for the legs. leg is either on or off, again don&#39;t care if its left or right leg that is on. 22 bits total. I setup a unit test on the discretizer to make sure numpy could handle the allocation.</p>
<p></p>",2019-10-07T02:40:41Z,43,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1ft5wjp9fi3gn,2019-10-07T02:40:41Z,{},project2
2588,no,"<p>For the last day I&#39;ve been playing with a 14 bit discretizer. This one has a much smaller space that allows me to do dyna on some beefier hardware (not my laptop). It doesn&#39;t run any faster than the higher resolution discretizer I&#39;ve been using (22 bits).</p>
<p></p>
<p>The 14 bit discretizer is just 2 bits for each parameter. X and Y are Left/Right/TDC of either (0,0,0,0) or (bbox) where bbox is derived from inspection of the lunar lander code. Then the vx/vy/vt are either 0 = no movement, 1 = negative movement, 2 = positive movement. The theta is 0 for no rotation, 1 for negative rotation, and 2 for positive rotation.</p>
<p></p>
<p>Low resolution discretization works a little bit, but it takes quite a bit more training. I see very slow progress in learning how to control the descent and then position the lander. It&#39;s particularly sensitive to using (0,0) as the target and often results in psychotic behavior (exit left fast, oscillate left/right on pad, etc.).</p>
<p></p>
<p>I am going to try the MRL paper&#39;s ideas next, making an ensemble-like decision maker that uses a QQ for each action.</p>
<p></p>
<p>Note that I have been using QQ as defined here: <a href=""https://papers.nips.cc/paper/3964-double-q-learning.pdf"">https://papers.nips.cc/paper/3964-double-q-learning.pdf</a></p>
<p></p>",2019-10-08T03:34:50Z,43,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1hajekhm5h2s4,2019-10-08T03:34:50Z,{},project2
2589,no,<p>Is there any guidance how to tune parameters in order to speed up the training? </p>,2019-10-07T19:08:17Z,43,Week 10/6 - 10/12,followup,,jzg6jh2hn6f43c,k1gsfz1in0m7en,2019-10-07T19:08:17Z,{},project2
2590,no,<p>some experience: bigger memory and batch size giving me better results</p>,2019-10-08T12:54:22Z,43,Week 10/6 - 10/12,feedback,,jzj7y1ofgsro1,k1huiyvpnxx53d,2019-10-08T12:54:22Z,{},project2
2591,stud,"<p>Angel, by memory do you mean replay memory i.e, a collection of the last N state transitions pooled over multiple episodes?</p>",2019-10-21T00:30:26Z,41,Week 10/20 - 10/26,feedback,a_0,,k1zoobpgufx3ki,2019-10-21T00:30:26Z,{},project2
2592,no,"<p>That makes sense, thank you Chris.</p>",2019-10-07T00:29:15Z,42,Week 10/6 - 10/12,followup,,is9so9huTMp,k1fogw62zwx62c,2019-10-07T00:29:15Z,{},hw3
2593,no,"<p>Just to follow-up on this and clarify. </p>
<p></p>
<p>Exploration and exploitation are not related in that way to off-policy and on-policy. Just compare Sarsa and Q-learning and notice the action selection step is exactly the same. You can have an on-policy that does more exploration than an off-policy algorithm. Just set the epsilon, on an epsilon-greedy strategy, to, say 0.1, on the on-policy and to 0.01 on the off-policy, and that&#39;s it. The on-policy is doing more exploration.</p>
<p></p>
<p>The only real difference between the two algorithms is the update equation:</p>
<p></p>
<p>Both use the update form of:</p>
<p></p>
<p>$$Q(s, a) \leftarrow Q(s, a) &#43; \alpha \Big[target - Q(s, a)\Big]$$ </p>
<p></p>
<p>Sarsa uses the target:</p>
<p></p>
<p>$$target = R &#43; \gamma Q(s&#39;, a&#39;)$$</p>
<p></p>
<p>Q-learning uses the target:</p>
<p></p>
<p>$$target = R &#43; \gamma \max_a Q(s&#39;, a)$$</p>
<p></p>
<p>Stare at the functions for a while and realize $$\max_a Q(s&#39;, a)$$ is the greedy policy, but in the next step we potentially select an action that is not that max action. So, while we are evaluating the greedy policy, we are acting under [perhaps] an $$\epsilon$$-Greedy policy.</p>
<p></p>
<p>Off-policy just means that: You are evaluating and improving a policy different than the one that generated the data.</p>
<p></p>
<p>All of that out of the way, the main advantage of off-policy methods is you can approximate the optimal policy while following a totally random policy! So, in that sense, perhaps, you could do more exploration than with an on-policy because an on-policy requires you to go GLIE (Greedy in the Limit with Infinite Exploration). That means your policy must converge to the greedy policy to guarrantee convergence.</p>
<p></p>",2019-10-09T13:26:25Z,42,Week 10/6 - 10/12,followup,,hyx9thiqa6j4nn,k1jb40wcydh1oq,2019-10-09T13:26:25Z,{},hw3
2594,no,"<p>Yeah, note how the Q(s&#39;,a) is different from the Q(s&#39;,a&#39;). The (s&#39;,a&#39;) is the on-policy march across the Q space that SARSA does.</p>
<p></p>
<p>In Q, you update (s&#39;,a) but then as Miguel points out, you choose some other on-policy (s&#39;,a&#39;) as the next step.</p>
<p></p>
<p>So pretty much you update your neighbor&#39;s house with a new paint job thinking that your current house will sell better. That&#39;s Q.</p>",2019-10-09T17:26:32Z,42,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1jjou0phh41a2,2019-10-09T17:26:32Z,{},hw3
2595,no,"<p>point of off-policy vs on-policy... is interleaving; you can interleave several policies improvements-updates simultaneously, such that you can improve different policies during different time-steps.</p>
<p></p>
<p>the &#34;move&#34; policy is what generates the data.   but the &#34;update&#34; actually evaluates the effectiveness of an arbitrary policy (&#34;off-policy&#34;) with regard to the generated data.</p>
<p></p>
<p>note: you can have several off-policies being updated... simultaneously.</p>
<p></p>
<p>in essence:</p>
<p></p>
<p>convergence is not affected by stochasticity; that is, if a policy could not be improved when the outcome is different than what the policy dictates; you would live in a &#34;deterministic&#34; only world.  since you must be able to accommodate uncertainty with respect to policy (and reflexively, value); therefore the fact that the move is separate from the eval, does not affect convergence.  </p>
<p></p>
<p>It would be interesting to see/know, if the convergence properties are strictly based off of value improvement (as a &#34;move&#34; system/implicit policy) and thus the convergence benefits for an evaluation policy are implicitly based on this; I suspect highly, that the benefits may &#34;come along&#34; but not for all eval policies.</p>
<p></p>
<p>--</p>
<p></p>
<p>I feel, like these points being specifically called out, would have made it easier to understand the split between off-policy and on-policy.  personally I prefer &#34;move-policy&#34; and &#34;evaluation-policy&#34; as clearer terminology.</p>
<p></p>
<p>the key is that this is &#34;policy&#34;-improvement, not value-improvement.  although fundamentally the same, it matters in this case in that the extra layer of indirection lets you run (or rather improve) multiple policies simultaneously.</p>
<p></p>
<p>this is sort of the benefit of &#34;policy&#34;-improvement rather than value improvement (although value improvement, somewhat ironically, is considered more &#34;computationally-performant&#34; due to the extra calcs involved in policy iteration -- although I suspect the convergence properties of a policy iteration are somewhat susceptible to &#34;convergence-hacking&#34; due to the additional flexibility).</p>
<p></p>
<p>addendum:</p>
<p></p>
<p>additional note: having different on vs off policies lets you compensate for distortion due to frequency of visits.  simple but it works.</p>",2019-10-16T00:58:10Z,41,Week 10/13 - 10/19,followup,,jzivtxcbl6964n,k1skgqih1kxqf,2019-10-16T00:58:10Z,{},hw3
2596,no,"<p>numpy.random.random() and numpy.random.uniform(0,1) return the same result.</p>
<p></p>
<p>if you succeeded with the former and not the latter, it was probably due to other changes that were made in your program</p>",2019-10-07T04:37:16Z,24,Week 10/6 - 10/12,followup,,jqu95q68ljj1pn,k1fxbu8ick63l7,2019-10-07T04:37:16Z,{},hw3
2597,no,"<p>Thanks much for the reply Jasmine! I checked again, they do return the same results. My fault... Sometime PyCharm just gives me &#34;wacky&#34; results, at least hard for me to understand... </p>",2019-10-10T02:37:09Z,24,Week 10/6 - 10/12,feedback,,jznlv5hlsm25uc,k1k3cx1hyob58r,2019-10-10T02:37:09Z,{},hw3
2598,no,"<p>yes, should converge in less than a minute, and my machine is not recent at all</p>",2019-10-10T19:09:57Z,42,Week 10/6 - 10/12,followup,,jzh6k6o994a6dh,k1l2tnt2x8e3j3,2019-10-10T19:09:57Z,{},hw3
2599,no,<p>Thanks. It works with https. </p>,2019-10-07T07:16:27Z,42,Week 10/6 - 10/12,followup,,jl3ol6nb3a5j,k1g30jrjuwpws,2019-10-07T07:16:27Z,{},hw3
2600,stud,"<p>The only thing that I find puzzling in the above response, is that it seems to suggest that SARSA is <em>uniquely</em> capable of continued exploration after deployment. But Q-learning can do this too, right? Unless I&#39;m mistaken, Q-learning with a constant $$\epsilon$$ will continue to explore after deployment, too (though rather less robustly than SARSA, presumably). If we tie the value of $$\epsilon$$ to the number of iterations (e.g. $$\epsilon \rightarrow \epsilon_{\tau} = \frac{\epsilon_{0}}{\tau}$$, then <em>in that case </em>as the number of total iterations goes to $$\infty$$, our Q-learner will explore less and exploit more (alternatively, we could just pre-select an &#34;exploration phase&#34;, where after a certain finite number of iterations, we simply cease exploring altogether and instead exploit whatever knowledge of the environment we&#39;ve thusfar accquired).</p>
<p></p>
<p>But we don&#39;t <em>have</em> to do this, right? We could allow our Q-learner to keep exploring with some small probabiity indefinitely, or could turn exploration on/off if differences in performance with a particular policy due to gradual changes in the environment over time are observed. Would that no longer count as Q-learning? </p>
<p></p>
<p>The other point that seems to be made above is that SARSA learns a more <em>conservative</em> or <em>cautious</em> policy than Q-learning in the case of the cliff-walking example. Is this true in general?</p>",2019-10-07T15:21:04Z,24,Week 10/6 - 10/12,followup,a_0,,k1gkbrrlyo66dl,2019-10-07T15:21:04Z,{},hw3
2601,no,"<p>&#64;Anonymous</p>
<p></p>
<p>Your description of Q-learning isn&#39;t correct. </p>
<p></p>
<p>&#34;<em>and then choose our <em>next</em> action to be whichever action maximizes the value of Q(s′,a),</em>&#34;</p>
<p></p>
<p>We only do this in the update assignment.  Q-learning chooses its next action according to the behavioral policy (the off-policy), e.g. the $$\epsilon$$-greedy one.</p>
<p></p>
<p></p>
<p>&#34;<em>it looks like SARSA can maybe be said to explore more <em>vigorously</em> than Q-learning?&#34;</em></p>
<p></p>
<p>I actually see it the exact opposite way around.  SARSA is constrained to explore <em>less</em> vigorously, since presumably one would want at least a fair amount of greediness in the policy, otherwise the algorithm would perform horribly.  Q-learning, on the other hand, can explore completely randomly and still have a chance at finding the optimal policy.</p>
<p></p>
<p>&#64;Michael</p>
<p></p>
<p>Yeah, that example tripped me up at first too.  It is <em>not</em> at all saying that SARSA is better than Q-learning, just that if Q-learning were <em>forced</em>  to follow a non-optimal policy, then it wouldn&#39;t perform as well as SARSA.  If the environment were stochastic and pushed you off the cliff, then I would expect Q-learning to learn an optimal policy to counter that and still beat SARSA once it switches to 100% greedy.  I think the point of the example was just to illustrate the difference in the mechanics of on-and off-policy.</p>
<p></p>
<p>&#34;<em> if you need an agent that continues to explore stochastically during deployment, and you want it to learn a safer policy to follow in lieu of it&#39;s exploration strategy, SARSA seems better suited there&#34;</em></p>
<p></p>
<p>I&#39;m not even sure if this is true.  The Q-learner knows exactly where that cliff is.  It will have action-values of close to -100 for any action that takes it over it.  I would much rather deploy <em>it</em> with an exploration rule like an $$\epsilon$$-greedy one but weighted by the current action values.  For example, if an action is slightly worse than the current best action, feel free to explore $$\epsilon\%$$ of the time, but if it&#39;s much worse, explore almost never.  In contrast, the SARSA agent takes this highly sub-optimal path because it can&#39;t trust itself, and it will still sometimes get unlucky and fall into the cliff when it $$\epsilon$$&#39;s itself three times in a row.</p>",2019-10-07T15:21:17Z,24,Week 10/6 - 10/12,followup,,jzfsa4a37jf4aq,k1gkc1tc4n96s3,2019-10-07T15:21:17Z,{},hw3
2602,stud,"<p>&#34;I actually see it the exact opposite way around.  SARSA is constrained to explore <em>less</em> vigorously, since presumably one would want at least a fair amount of greediness in the policy, otherwise the algorithm would perform horribly.  Q-learning, on the other hand, can explore completely randomly and still have a chance at finding the optimal policy.&#34;</p>
<p></p>
<p>A fair point. But the thought I had in mind was that for <em>equal </em>values of $$\epsilon$$ SARSA appears to explore more vigorously than Q-learning. SARSA can explore <em>twice</em> per update (it might <em>not</em>, but it <em>can</em>) and Q-learning can&#39;t. Though it does seem true that Q-learning can (must?) use larger values of $$\epsilon$$ than SARSA while still obtaining fair results. </p>
<p></p>
<p>So I think the actual realtionship vis-a-vis differences in exploration strategies is a bit more complicated than <em>either </em> of our remarks suggests</p>",2019-10-07T15:36:17Z,24,Week 10/6 - 10/12,feedback,a_0,,k1gkvc174xk3az,2019-10-07T15:36:17Z,{},hw3
2603,no,"<p>&#64;Vahe, good points. Updated the &#39;student&#39; answer above to reflect this.</p>
<p></p>
<p>&#64;Anonymous,</p>
<p>&#34;SARSA can explore <em>twice</em> per update (it might <em>not</em>, but it <em>can</em>) and Q-learning can&#39;t.&#34;</p>
<p>I thought this was the case too at first glance, but it&#39;s only on the first step in an episode. For all steps after the first, the agent only chooses whether it&#39;s exploring (and if so, only explores) once per step, like Q-learning. I initially implemented this wrong in HW3, but upon closer inspection of the inner loop in <a href=""http://incompleteideas.net/book/first/ebook/node64.html"" target=""_blank"" rel=""noopener noreferrer"">figure6.9</a> there is no additional exploration. The inner loop begins with &#34;Take action a&#34; (decided at the previous iteration).</p>",2019-10-07T15:43:55Z,24,Week 10/6 - 10/12,feedback,,isde332xcka1m0,k1gl559g14m6m3,2019-10-07T15:43:55Z,{},hw3
2604,no,"<p>&#64;Anonymous</p>
<p></p>
<p>&#34;<em> for <em>equal </em>values of ϵ SARSA appears to explore more vigorously than Q-learning&#34;</em></p>
<p></p>
<p>No, they would explore exactly the same in this case.  Q-learning behaves just like SARSA, but at each state, it checks <em>all</em> of the actions available to it, notes down which is the best <em>but does not necessarily take it</em>, then takes the exact same path as the SARSA agent. </p>
<p></p>
<p></p>",2019-10-07T15:50:05Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1gld3eyh2i280,2019-10-07T15:50:05Z,{},hw3
2605,stud,"<p>I&#39;m not sure I agree, but part of this seems like it&#39;s going to come down to semantics.</p>
<p></p>
<p>My agent starts in state $$s$$, and then using an epsilon greedy policy, it decides with probability $$\epsilon$$ to step randomly, and with probability $$1 - \epsilon$$ to choose the (at present) optimal action. </p>
<p></p>
<p>This puts it in a new state, $$s&#39;$$. From there, it again chooses with probability $$\epsilon$$ to step randomly, and probability $$1 - \epsilon$$ to choose the (at present) optimal action. </p>
<p></p>
<p>These choices are indpendent in my implementation. Whether the agent chooses to step radomly the first time around has no impact on whether it chooses to step randomly the second time around. In either case, I&#39;d say that (if it steps randomly), it&#39;s <em>exploring</em>. So you&#39;re right that it <em>seems</em> that way. But I think it seems that way because it <em>is</em> that way. To wit, the second random step it takes doesn&#39;t need to put it in a state it&#39;s seen before. That could land it in a totally novel (to it) state. </p>
<p></p>
<p>But it is true that it won&#39;t actually <em>go</em> to that next state. It will just use the corresponding value in the Q-table to process the update. If you want to say that the second random step doesn&#39;t count as exploration even though it might reveal previously unknown parts of the state space simply because we don&#39;t <em>begin</em> in the state that random step would put us in next time around, I guess that&#39;s fine. I can&#39;t find a perfectly precise definition of &#34;exploration&#34; anywhere anyway (but random steps vs. optimal steps seems to be the essence of it, so far as I can tell). </p>
<p></p>
<p>But that <em>does</em> look like a debate over what the semantics of &#34;explore&#34; should be (in which event, I respectfully opt out, since I don&#39;t think it&#39;s an especially useful debate to have). </p>",2019-10-07T15:58:48Z,24,Week 10/6 - 10/12,feedback,a_0,,k1gloaqxu533yk,2019-10-07T15:58:48Z,{},hw3
2606,no,<p>I don&#39;t think this is a semantic issue.  Both agents only take <em>one</em> random step per iteration.  And they both take the same type of random step if the behavioral policy is $$\epsilon$$-greedy.  So they will both explore equally.</p>,2019-10-07T16:12:27Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1gm5uahbdd4vd,2019-10-07T16:12:27Z,{},hw3
2607,stud,"<p><em>&#34;No, they would explore exactly the same in this case.  Q-learning behaves just like SARSA, but at each state, it checks all of the actions available to it, notes down which is the best but does not necessarily take it, then takes the exact same path as the SARSA agent.&#34;</em></p>
<p></p>
<p>I don&#39;t see how that could be the case (i.e., that with the same $$\epsilon$$ values, they&#39;d be move for move the same).</p>
<p></p>
<p>It seems like there&#39;s at least one more &#39;degree of freedom&#39; in the case of SARSA. To wit, a Q-learner has to make a choice <em>just once</em> per iteration regarding whether to step randomly or optimally. A SARSA agent gets to make this choice twice. </p>
<p></p>
<p>So, let&#39;s assume we have the following setup: We have a SARSA agent, and a Q-learner, and they both use the same value of $$\epsilon$$. Moreoever, for any given iteration, the SARSA agent and the Q-agent <i>behave </i>identically for the first round of action selection. If $$\epsilon_{\tau} &lt; \epsilon$$ for fixed $$\epsilon$$, then a <em>single </em>action is chosen (at random) and <em>both</em> the SARSA agent and the Q-learning agent apply this action to the current state, $$s$$, which we assume to be the <em>same</em> for both agents. This puts them both in some sucessor state $$s&#39;$$. From there, the Q-learner <em>must</em> choose the action which, from $$s&#39;$$, results in the highest value for $$Q(s&#39;, a)$$. The SARSA agent, on the other hand, gets to &#34;go again&#34;. SARSA gets to pick another small $$\epsilon_{\tau}$$ (uniformly at random, let&#39;s say), and if <em>that</em> value is less then $$\epsilon$$, it steps randomly <em>again</em>.</p>
<p></p>
<p>Annnnnnnndddd....</p>
<p></p>
<p>As I write this, I begin to see your point. That &#34;second step&#34; isn&#39;t <em>really</em> a second step, since all we&#39;re doing is updating $$Q(s&#39;, a)$$. We&#39;re not actually <em>going </em>anywhere. Next time around, <em>both</em> agents (SARSA and Q) will be in the same sucessor state $$s&#39;$$. As long as we keep syncing them up like I described above, they&#39;ll (evidently) pass through all of the same states in <em>exactly </em>the same order. Only the individual <em>updates </em>they make to the values in the Q-table will (potentially) be different.</p>
<p></p>
<p>That makes me see more clearly what &#64;Michael was getting at above as well (I think), when he noted that it just &#34;seems&#34; like SARSA (sometimes) explores twice as much (per-iteration) as Q-learning. </p>
<p></p>
<p>I retract my above claim. This isn&#39;t just a semantic debate (well, it <em>sort of</em> is. But Michael&#39;s/Vahe&#39;s semantics for &#34;explore&#34; make more sense.)</p>
<p></p>
<p>: )</p>",2019-10-07T16:22:27Z,24,Week 10/6 - 10/12,feedback,a_0,,k1gmip9mfv832c,2019-10-07T16:22:27Z,{},hw3
2608,no,"<p>Yup exactly!  The way the algorithms are written, it <em>seems</em> like SARSA is moving more than Q-learning, since it&#39;s choosing an action twice in the inner loop.  But it&#39;s actually not, as you pointed out above.  That second choice needs to be done for SARSA to update its Q-table (whereas Q-learning updates its table completely independently of where it&#39;s going), and it remembers that action for the<em> next</em> time around when it actually moves in that direction.</p>",2019-10-07T16:30:00Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1gmsf595h35mp,2019-10-07T16:30:00Z,{},hw3
2609,stud,"<p>&#34;<em>Your description of Q-learning isn&#39;t correct. </em></p>
<p><em> </em></p>
<p><em>&#34;and then choose our next action to be whichever action maximizes the value of Q(s′,a),&#34;</em></p>
<p><em> </em></p>
<p><em>We only do this in the update assignment.  Q-learning chooses its next action according to the behavioral policy (the off-policy), e.g. the ϵ-greedy one.&#34;</em></p>
<p><em></em></p>
<p>Good point! That&#39;s what I meant (but you&#39;re correct that that&#39;s not what I <em>wrote</em>). I&#39;ll fix it now.</p>
<p></p>
<p></p>",2019-10-07T15:23:59Z,24,Week 10/6 - 10/12,followup,a_0,,k1gkfj3urr92f3,2019-10-07T15:23:59Z,{},hw3
2610,no,"<p>My understanding of the difference right now, is that while both algorithms take a &#39;non-optimal move&#39; with probability $$\epsilon$$ the &#39;on-policy&#39; nature of SARSA means it sort of &#39;takes responsibility&#39; for that action, and updates Q(S,A) based on that mistake, while q-learning updates Q(S,A) based on its estimation of the new state&#39;s best action. There are pros and cons of both. The obvious con of SARSA is given in Barto&#39;s example 6.6, where it initially learns a conservative policy away from the cliff to avoid taking responsibility for those random walks off the cliff. The downside of Q-learning seems to be that its estimates of the value of the best action from a state aren&#39;t consistently based on reality like they are in SARSA.</p>",2019-10-08T00:40:23Z,24,Week 10/6 - 10/12,followup,,jqwygbqmHAiE,k1h4b236we07dm,2019-10-08T00:40:23Z,{},hw3
2611,no,"<p>&#34;<em>The downside of Q-learning seems to be that its estimates of the value of the best action from a state aren&#39;t consistently based on reality like they are in SARSA.&#34;</em></p>
<p><em></em></p>
<p>Is this really a downside?  And what exactly is reality?</p>
<p></p>
<p>Q-learning is not necessarily updating the value of the state it just left with the value of the state where it&#39;s going next.  That is true.</p>
<p></p>
<p>It is instead updating the value of the state it just left with something that is closer to the optimal value function than what SARSA is using for its update.  You could look at it as SARSA is sometimes (with probability $$\epsilon$$) ignoring reality and potentially erroneously updating the state it just left with an inferior value estimate, whereas Q-learning always has its head on straight.</p>
<p></p>
<p>And the next time the two algorithms get back to that same state, the &#39;reality&#39; will probably be different for SARSA, but not for the Q-learner - SARSA will, with at least probability $$1-\epsilon$$, actually go in the direction that the Q-learner previously used in <em>its</em> update.</p>",2019-10-08T02:37:26Z,24,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1h8hl987ho2r1,2019-10-08T02:37:26Z,{},hw3
2612,no,"<p>So, I&#39;m definitely speaking loosely rather than rigorously.</p>
<p></p>
<p>The reality I think SARSA does a &#39;better&#39; job of reflecting than Q-learning is that SARSA makes updates based on received rewards, while Q-learning makes updates based on the value-functions evaluation, even when there&#39;s reason to suspect that Q-learning can&#39;t accurately estimate the value of a state/action pair, like early in training.</p>",2019-10-09T01:20:01Z,24,Week 10/6 - 10/12,feedback,,jqwygbqmHAiE,k1il5vq2ewr2tw,2019-10-09T01:20:01Z,{},hw3
2613,no,"<p>The way I like to think about it (disclaimer: i may be wrong):</p>
<ul><li>SARSA is a realist: it interprets its policy the same way it behaves (which is based in reality, i.e. what happened)</li><li>Q-Learning is an optimist: it interprets its policy different than it behaves (it interprets the policy in the best possible light, assuming the best possible outcomes, even though it initially behaves the same way as SARSA would)</li></ul>
<p></p>
<p>That&#39;s why Q-Learning ends up taking &#34;risker&#34; approaches to the cliff problem, because it goes off of the best possible outcomes.</p>",2019-10-08T13:42:59Z,24,Week 10/6 - 10/12,followup,,i4jbttw9ru63ot,k1hw9hjc1q91oz,2019-10-08T13:42:59Z,{},hw3
2614,no,<p>Nice. Thanks for sharing.</p>,2019-10-08T19:32:50Z,34,Week 10/6 - 10/12,followup,,hyx9thiqa6j4nn,k1i8rdxlbqr6fg,2019-10-08T19:32:50Z,{},hw4
2615,no,"<p>After all the reading, finally something &#39;concrete&#39;, I mean, a real life thing, until proj 2 of course.</p>
<p>It&#39;s fair to say this hw has got me excited, first to see it working (the first gif felt like magic), but also because this taxi problem can be extended to many other situations: &#39;start here&#39;, &#39;find the steps to reach this objective&#39;, then &#39;finalize&#39; is very general.</p>
<p>I&#39;m thinking about a robot arm (grab that, drop it there), conversations which are composed of multiple back and forths (say sthg, analyze response, react to it, repeat), or behavioural or organisational strategies (no point doing the second part if first part not succeeded), no? </p>
<p>Going to &#39;meet the customer&#39; can be virtually anything, any type of step towards a goal.  </p>
<p>This has triggered my imagination and curiosity.</p>
<p></p>",2019-10-08T22:12:49Z,34,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1ieh556hon3kg,2019-10-08T22:12:49Z,{},hw4
2616,no,<p>This is why we do what we do.</p>,2019-10-09T12:05:21Z,34,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1j87rvk4565vm,2019-10-09T12:05:21Z,{},hw4
2617,no,<p>thanks :)</p>,2019-10-07T14:52:24Z,34,Week 10/6 - 10/12,followup,,jzivtxcbl6964n,k1gjawf5c6qgg,2019-10-07T14:52:24Z,{},project1
2618,no,<p>Hi! Do you have an idea about when you will release the grades for P1? Just curious :)</p>,2019-10-08T07:17:28Z,58,Week 10/6 - 10/12,followup,,jzjwcq2u8o7110,k1hihphmdso52x,2019-10-08T07:17:28Z,{},logistics
2619,no,<p>It takes us at least a couple of weeks. Working on them now!!</p>,2019-10-08T19:29:16Z,58,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1i8mssvjab27h,2019-10-08T19:29:16Z,{},logistics
2620,no,"<p>Okey, thanks!</p>",2019-10-09T12:48:14Z,58,Week 10/6 - 10/12,feedback,,jzjwcq2u8o7110,k1j9qx10bgs67v,2019-10-09T12:48:14Z,{},logistics
2621,no,"<p>When working on hw3, I accidentally implement Q-learn instead of Sarsa. And I realize that the learning for ex2 did not converge, due to the low $$\epsilon = 0.13$$. The chance for the agent to successfully get to the goal, and make any meaningful update, is 0.13 ^ 16, which is extremely low. So I have 2 questions that I hope we can discuss:</p>
<p>- Is it true that Q-learn requires very very large (infinite) amount of training? Is it meaningful to do infinite training on just finite data?</p>
<p>- What is typical $$\epsilon$$ used for practical Q-learn training, ie: low or high? If we don&#39;t know the size of the state space, what else can we rely on to try low or high $$\epsilon$$?</p>",2019-10-11T00:47:17Z,42,Week 10/6 - 10/12,followup,,jl5wq8mca7o0,k1levh2dvt23rg,2019-10-11T00:47:17Z,{},office_hours
2622,no,"<p>- It requires that all states are visited infinitely often to guarantee convergence to $$Q^*$$, but in practice what we usually care about is $$~pi^*$$ which we can generally solve for more efficiently. Off policy learners in general are not known for being sample efficient.</p>
<p></p>
<p>- In practice Q is generally applied with function approximation with some version of DQN. Many approaches can be used: start high and decay, act randomly for some period of time and then act with fixed epsilon, maintain some small fixed epsilon throughout training, etc.</p>",2019-10-11T15:33:41Z,42,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1majeeucrl5b4,2019-10-11T15:33:41Z,{},office_hours
2623,stud,<p>How do we log in? :o</p>,2019-10-08T06:10:38Z,58,Week 10/6 - 10/12,followup,a_0,,k1hg3roid643e5,2019-10-08T06:10:38Z,{},other
2624,no,"<p></p>
<p><a href=""http://libguides.gatech.edu/ebooks/Safari"">http://libguides.gatech.edu/ebooks/Safari</a></p>",2019-10-08T06:12:55Z,58,Week 10/6 - 10/12,feedback,,jzj72gbzpx37j0,k1hg6pciisu57f,2019-10-08T06:12:55Z,{},other
2625,no,<p>Did you really manage to get in for free? Not a one week trial I mean...</p>,2019-10-10T19:15:58Z,58,Week 10/6 - 10/12,followup,,jzh6k6o994a6dh,k1l31ejrwuf1tk,2019-10-10T19:15:58Z,{},other
2626,no,"Yes, you need to write only your email account(no password) and it will show you the passport authentication page to give you access to your account.",2019-10-10T20:00:05Z,58,Week 10/6 - 10/12,feedback,,jzj72gbzpx37j0,k1l4m59mn8f289,2019-10-10T20:00:05Z,{},other
2627,no,"<p>Thanks, I got in like you said, but it seem they have removed the online trainings.  </p>
<p>I guess they don&#39;t want hordes of non paying people filling them (seat limited for some reason), so I don&#39;t see the point to it honestly.  </p>
<p>When I took the two-week free trial, they were playing all sorts of games, so I&#39;m not surprised.  </p>
<p>I think Lynda, which was bought recently by Linkedin, and integrated into Linkedin Learning, offers a lot more, lots of courses.  </p>",2019-10-11T01:49:26Z,58,Week 10/6 - 10/12,feedback,,jzh6k6o994a6dh,k1lh3eo220o2v0,2019-10-11T01:49:26Z,{},other
2628,stud,"<p>there is no way to download any of this stuff to local e-readers, etc. right? </p>",2019-10-10T22:02:04Z,58,Week 10/6 - 10/12,followup,a_0,,k1l8z0nmefi7bt,2019-10-10T22:02:04Z,{},other
2629,no,"Yes, there is an option in the app.<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj72gbzpx37j0%2Fk1l9frwcgeya%2FScreenshot_20191010171309.png"" /><p></p>",2019-10-10T22:15:32Z,58,Week 10/6 - 10/12,feedback,,jzj72gbzpx37j0,k1l9gbneg9seo,2019-10-10T22:15:32Z,{},other
2630,no,<p>Hmm. So this works but I can’t export to kindle or anything besides web :/ </p>,2019-10-11T19:06:50Z,58,Week 10/6 - 10/12,feedback,,jcg0nzvdk8272b,k1mi5iuu80g1sb,2019-10-11T19:06:50Z,{},other
2631,stud,<p>How&#39;d you take the video?</p>,2019-10-08T13:54:41Z,34,Week 10/6 - 10/12,followup,a_1,,k1hwoj212q95r0,2019-10-08T13:54:41Z,{},project2
2632,stud,"<p></p><pre>from gym.wrappers.monitoring.video_recorder import VideoRecorder<br /><br />video = VideoRecorder(config.env, &#34;{1}/{0}.mp4&#34;.format(episode,outdir))<br />s = env.reset()<br />while in my action loop<br />  env.render()<br />  video.capture_frame()<br />video.close()<br />
</pre>
<p></p>",2019-10-08T16:59:21Z,34,Week 10/6 - 10/12,feedback,a_0,,k1i3a0mwgxu6je,2019-10-08T16:59:21Z,{},project2
2633,stud,"<p>My training data:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj7y1ofgsro1%2Fk1idbq1l3ntg%2FUnknown.png"" alt="""" /></p>
<p>Video of trained episode:<br /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj7y1ofgsro1%2Fk1iday35je3a%2F1.mp4"" alt="""" /></p>",2019-10-08T21:41:12Z,34,Week 10/6 - 10/12,feedback,a_1,,k1idch6sjl1b3,2019-10-08T21:41:12Z,{},project2
2634,stud,"<p>x axis is # of episodes, y axis is reward for that episode</p>",2019-10-08T21:47:07Z,34,Week 10/6 - 10/12,feedback,a_1,,k1idk36ef5t6b2,2019-10-08T21:47:07Z,{},project2
2635,stud,"<p>Nice! Check this out. Look at that -1200 error! That might be a record....</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk1ieg0iixdhs%2Fqqrewardneg1200share.png"" alt="""" /></p>",2019-10-08T22:12:20Z,34,Week 10/6 - 10/12,feedback,a_0,,k1iegi86dk24o7,2019-10-08T22:12:20Z,{},project2
2636,stud,<p>:O</p>,2019-10-09T12:56:06Z,34,Week 10/6 - 10/12,feedback,a_1,,k1ja11kmy9f1l9,2019-10-09T12:56:06Z,{},project2
2637,no,<p>Thanks for the code to record video.</p>,2019-10-27T22:03:23Z,31,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k29ji740kz42iw,2019-10-27T22:03:23Z,{},project2
2638,stud,"<p>I also tried discretizing but I think the state space is too big and the control needed for landing the ship is too fine, I tried 20 bins and had 80GB in memory. I don&#39;t think its worth it after trying DQN (which beat it in 300 episodes).</p>",2019-10-08T13:56:37Z,34,Week 10/6 - 10/12,followup,a_1,,k1hwr0lpsiz7ci,2019-10-08T13:56:37Z,{},project2
2639,stud,"<p>Yeah, I get the whole &#34;let&#39;s solve it&#34; approach and the DQN approach does make it faster (only because of your CUDA processor). I find exploring the discretization problem a little more interesting. There are ways to compress the state space.</p>",2019-10-08T17:02:14Z,34,Week 10/6 - 10/12,feedback,a_0,,k1i3dpzco1222m,2019-10-08T17:02:14Z,{},project2
2640,stud,"<p>Hey Angel, since you&#39;ve already solved it, maybe you&#39;re into NLP? If so, I found the original text version:</p>
<p></p>
<p><a href=""https://www.atariarchives.org/basicgames/showpage.php?page=106"">https://www.atariarchives.org/basicgames/showpage.php?page=106</a></p>
<p></p>
<p>Maybe code up an NLP agent too? :) Might be worth &#43;1 extra credit point ....</p>",2019-10-08T20:25:35Z,34,Week 10/6 - 10/12,feedback,a_0,,k1ian7zcqyc245,2019-10-08T20:25:35Z,{},project2
2641,stud,"<p>Thanks for sharing!! (Also, sorry, accidentally clicked the wrong button when following this post -- didn&#39;t mean to edit anything :( )</p>
<p></p>
<p>What are the reminisce params? Are these for experience replay?</p>",2019-10-08T14:08:46Z,34,Week 10/6 - 10/12,followup,a_2,,k1hx6nhbz823k4,2019-10-08T14:08:46Z,{},project2
2642,stud,"<p>I have a few different ways of doing the replay, which I call &#34;reminiscing.&#34; The negpos is a way for me to do negative or positive reflection in the replay instead of always just keeping a last-visit reminscing session. It still does the random selection of memories that you find in the literature.</p>
<p></p>
<p>The state space takes over 16GB of RAM so I can&#39;t run Dyna on my laptop.I am going to try it on my desktop which has alot more compute capabilities.</p>
<p></p>",2019-10-08T17:05:27Z,34,Week 10/6 - 10/12,feedback,a_0,,k1i3hv1rjr877n,2019-10-08T17:05:27Z,{},project2
2643,stud,"<p>Thank you very much Jacob, very interesting to see the results! Me myself is still in a training phase and haven&#39;t implemented proper caption of video on my Linux box to which I&#39;ve moved last night only.</p>",2019-10-08T19:30:26Z,34,Week 10/6 - 10/12,followup,a_3,,k1i8ob7w1j3ai,2019-10-08T19:30:26Z,{},project2
2644,no,"<p>Best landing  (score 314) from a model trained for 10 minutes or so on a single CPU.</p>
<p></p>
<p><video width=""300"" height=""150"" controls=""controls"">
<source src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhl7qlwrpagr%2Fk1il2rm58tms%2Fopenaigym.video.0.31820.video000000.mp4"" type=""video/mp4"" />
<source />
</video></p>
<p></p>
<p>I monkey patched the renderer so it would show the trajectory and position/score.</p>",2019-10-09T01:24:53Z,34,Week 10/6 - 10/12,followup,,jzhl7qlwrpagr,k1ilc4qgvua68g,2019-10-09T01:24:53Z,{},project2
2645,stud,<p>Thank you Todd!</p>,2019-10-09T01:27:18Z,34,Week 10/6 - 10/12,feedback,a_3,,k1ilf8ebvu3286,2019-10-09T01:27:18Z,{},project2
2646,no,<p>That&#39;s a great landing! Can you train it to do a spin first and then land? :)</p>,2019-10-09T02:00:57Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1immiuimo938d,2019-10-09T02:00:57Z,{},project2
2647,no,"<p>I&#39;ve gotten quite a bit closer to solving LL with just QQ and some variations, all using a discrete state space.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk1lhea6yebny%2Fqqexp1023000_finalshare.png"" alt="""" /></p>",2019-10-11T01:58:20Z,34,Week 10/6 - 10/12,followup,,jc554vxmyuy3pt,k1lheur2io256s,2019-10-11T01:58:20Z,{},project2
2648,no,<p>That&#39;s great Jacob! And only 23k episodes..  I&#39;m also playing around with discretization.  It seems like a fun optimization problem.</p>,2019-10-11T02:40:22Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1liwx56nc951l,2019-10-11T02:40:22Z,{},project2
2649,no,"<p>At 21,000 episodes I get 97% positive landings and my training is a site to see. Getting closer and closer.</p>",2019-10-11T02:44:23Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1lj22ib7pl3xe,2019-10-11T02:44:23Z,{},project2
2650,no,"<p>Jacob,</p>
<p></p>
<p>I know QQ decreases the probability of maximization bias (by a lot), but did you actually see evidence for this bias leading you to use it instead of Q?</p>
<p></p>
<p>I tried both algorithms on discretized CartPole and saw no difference whatsoever.  It&#39;s a lot harder to test for it in Lunar Lander (namely because there are so many other reasons that the agent isn&#39;t converging to a near-optimal policy), but I&#39;m curious as to whether you&#39;re using it, because, well, why not use an algorithm not susceptible to this bias right?  Or whether you actually detected the bias and made the switch.</p>",2019-10-11T15:43:17Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mavqkzvwwiz,2019-10-11T15:43:17Z,{},project2
2651,no,"<p>I did not make a conscious decision to use QQ other than QQ sounds cooler than Q.</p>
<p></p>
<p>Today, though, I did run Q to see how well it acted in one epoch of training (500 episodes). On the same parameters other than algorithm, Q scored 4 positive landings and QQ scored 20. Q&#39;s training was very well defined and behaved exactly how you would expect good training to go. QQ, on the other hand, was all over the place in this epoch. So there was clear overfitting going on in the first epoch with Q versus QQ.</p>",2019-10-11T15:45:15Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1may9l0fer6w7,2019-10-11T15:45:15Z,{},project2
2652,no,"<p>One variance reduction method I use is experience replay, which I call reminiscing. I use all negative, all positive, or a mix of reward values in the memory. What I&#39;ve noticed is that this reminiscing introduces reward bias.</p>
<p></p>
<p>My lander only reminisces after an epoch of training. This usually collects 100 memories to replay randomly. But, those memories are all dated. The rewards are no longer relevant in the current Q state. In fact, they are biased towards a prior Q state that has hopefully been trained-out.</p>
<p></p>
<p>So I am thinking about an exponential decay type aging of those memories where the rewards are adjusted during the epoch of training. That way the bias is reduced. Something smarter might be to evaluate the memories on-policy and use the average reward during replay until the policy diverges, and then use the memory&#39;s reward.</p>
<p></p>",2019-10-11T15:55:44Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1mbbqy7swp62,2019-10-11T15:55:44Z,{},project2
2653,no,"<p>If you repeat that experiment a few times, I wonder if you&#39;ll get the same difference in behavior each time.  It may be that the difference you observed was just variance.</p>
<p></p>
<p>Edit: The above was intended for your second-to-last-post (you sniped me).</p>
<p></p>
<p>Addressing your newest post, have you previously trained on the 100 memories that you&#39;re now reminiscing on?  If so, are you training on them a second time?</p>",2019-10-11T15:56:26Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mbcncd6qr16e,2019-10-11T15:56:26Z,{},project2
2654,no,"<p>So, the reminiscing will not retrain over and over. So long as the epoch is longer than the memory, the churn will push the old memories out. But if we kept the memories across epochs, then it could be useful to degrade them.</p>
<p></p>
<p>I set the evaluation seed to zero each time so that I get the same lander evaluation for each epoch. That way I can reliably compare runs now. Here is a 1000 episode (2 epochs) run using QQ, Q, and Sarsa. All with the same tuning parameters.</p>
<p></p>
<p>There is a little variability in the results because the training lander seed is not set. I think that should be set as well so that the training experience is the same each time.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk1mg9yj8j8ec%2Fqqqsarsacomparisonshare.png"" alt="""" /></p>",2019-10-11T18:14:33Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1mgaacral079a,2019-10-11T18:14:33Z,{},project2
2655,no,"<p>Thanks for sharing this Jacob!</p>
<p></p>
<p>Yeah, it looks pretty clearly like QQ is performing better than Q.</p>",2019-10-11T18:46:25Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mhf97c1zw648,2019-10-11T18:46:25Z,{},project2
2656,no,"<p>Jacob,</p>
<p></p>
<p>Just for curiosity, how long did it take to train 23k episodes? What type hardware setup did you use?</p>
<p></p>
<p>tks</p>",2019-10-16T05:08:18Z,33,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1steezt63i2y0,2019-10-16T05:08:18Z,{},project2
2657,no,"<p>It takes about 5 minutes to run 1000. Then it will get progressively longer to evaluate episodes as the learning rate decreases. So a typical 20k run will take about 5 hours. I have been running 100k episodes that have been going for 4 days non-stop on a Core-i7 at 4Ghz.</p>
<p></p>",2019-10-16T19:21:12Z,33,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1tnv8p9x651kq,2019-10-16T19:21:12Z,{},project2
2658,no,"<p>Jacob,</p>
<p></p>
<p>Thank you for your reply. I&#39;m trying to find out what is going on in my algorithm. I&#39;m doing a DQN and it takes about 20 minutes to run only 500 episodes. My code is vectorized everywhere, so I don&#39;t know why it is taking too long.</p>
<p></p>
<p>If you have any hints, I appreciate that.</p>",2019-10-17T18:25:18Z,33,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1v1b7k33dj406,2019-10-17T18:25:18Z,{},project2
2659,no,"<p>Long training episodes with NN is usually a hyperparameter tuning exercise. Maybe cut your epochs down so that you can see how weights are updating between sets of 50 episodes. Maybe that would give you a hint about where the bottleneck is at.</p>
<p></p>
<p>Long runs are almost always the backprop convergence.</p>",2019-10-17T18:35:34Z,33,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1v1ofa3v6s2ay,2019-10-17T18:35:34Z,{},project2
2660,no,"<p>Hummm!  I was actually using epochs = 1 in the model.fit method.</p>
<p></p>
<p>One question that I have: do we need to call model.fit() after every env.step(action)?  The DQN paper from Nature says so.</p>",2019-10-17T19:25:54Z,33,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1v3h5f4fl27v,2019-10-17T19:25:54Z,{},project2
2661,no,"<p>That I don&#39;t really know for sure. I think some of the NN implementations are doing that every m steps. I suspect you should do it on every step.</p>
<p></p>
<p></p>",2019-10-17T19:51:08Z,33,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1v4dlnh8524jp,2019-10-17T19:51:08Z,{},project2
2662,no,"<p>My guess is that this, like almost everything else, is just another hyperparameter.  I bet that a setting that may work very well for one environment may not work as quite as well for another.</p>
<p></p>
<p>&#64;Danilo 20 minutes for 500 episodes isn&#39;t necessarily slow.  In fact, it could be really fast, depending on the NN architecture you are using, as well as the hyperparameter settings just mentioned, not to mention your computer setup.</p>",2019-10-17T23:30:15Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1vc7ddsl2b6q2,2019-10-17T23:30:15Z,{},project2
2663,no,"<p>Thank you for your feedback as well, Vahe!</p>",2019-10-20T04:35:35Z,33,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1yhzqwnjz51y1,2019-10-20T04:35:35Z,{},project2
2664,no,"<p>How is the score being calculated in the plots above? I have a trained DQN fairy similar to Jacob&#39;s yet my test don&#39;t go above 0....wondering if im miscalculating. I&#39;m taking the moving average over 100 episodes.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6m1jeidndu6wq%2Fk23o6cy0pf6y%2FScreen_Shot_20191023_at_3.24.30_PM.png"" alt="""" width=""257"" height=""189"" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6m1jeidndu6wq%2Fk23o7eflsi3w%2FLunar_Lander_Test.png"" alt="""" width=""287"" height=""191"" /></p>",2019-10-23T19:29:46Z,32,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k23o9890b5o131,2019-10-23T19:29:46Z,{},project2
2665,no,<p>Nevermind I did some more searching and found parameters that solve the environment </p>,2019-10-24T13:00:12Z,32,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k24ps3pp4c7562,2019-10-24T13:00:12Z,{},project2
2666,no,"<p>When you mention increasing the units/layer, are you referring to the <a href=""https://keras.io/layers/core/"" target=""_blank"" rel=""noopener noreferrer"">Dense()</a> method in Keras? I&#39;m getting started on LunarLander now; I tried using the same parameters as CartPole with no luck, so now I&#39;m trying to understand what I need to change</p>",2019-10-13T16:46:47Z,33,Week 10/13 - 10/19,followup,,jl284xdcifz44g,k1p814aocd9230,2019-10-13T16:46:47Z,{},project2
2667,no,"<p>I think I now understand how to calculate Q(s,a), but I&#39;m not clear on how to initialize our Q matrix. How many states do we use to initialize our Q matrix?</p>",2019-10-09T22:14:15Z,34,Week 10/6 - 10/12,followup,,jl284xdcifz44g,k1jtyts5p9c6lx,2019-10-09T22:14:15Z,{},project2
2668,no,"Well because it’s continuous rather then discrete like the Taxi you got two choice. Put the states into bins, aka discretize the continuous ones making it discrete. Or approximate it using a function- popularly by a neural network(this is technique is called DQN).",2019-10-09T22:58:57Z,34,Week 10/6 - 10/12,feedback,,jzj7y1ofgsro1,k1jvkbb7lqj14,2019-10-09T22:58:57Z,{},project2
2669,no,"Right, I&#39;m trying to go functional approximation, I guess what I don&#39;t understand is what does our Q matrix look like when we do functional approximation? Do we have a set number of states? How many? Is it something else entirely?",2019-10-10T02:19:49Z,34,Week 10/6 - 10/12,feedback,,jl284xdcifz44g,k1k2qmy0h1c6v0,2019-10-10T02:19:49Z,{},project2
2670,no,"<p>I believe that no, the Q matrix doesn&#39;t have a set number of states. In fact it&#39;s not a Q <em>matrix</em> at all. Rather it&#39;s a function approximation <em>thing</em> (e.g. neural network) that takes a state and returns a value. The weights are the internal configuration of your <em>thing</em> that you need to update somehow in order to get it to improve.</p>
<p></p>
<p>Sticking with the NN example for now, you might have a NN that takes in your state vector as the input, and returns a Q value as an output. If you have one of these per possible action, your &#34;Q matrix update&#34; equivalent is training the relevant NN to get closer to:</p>
<p></p>
<p>$$Q_a(s) &#43; \alpha(r &#43; \gamma max_{a&#39;} Q_{a&#39;}(s&#39;) - Q_a(s))$$<br /><br />where $$Q_a(s)$$ is the output from the NN associated with action $$a$$ when given input of state $$s$$. This is basically saying to your NN that it needs to get closer to the value approximation for the next state-action pair, which is what tabular Q-learning does.</p>",2019-10-10T06:57:06Z,34,Week 10/6 - 10/12,feedback,,jl3oi5v7qkSk,k1kcn84iha1450,2019-10-10T06:57:06Z,{},project2
2671,no,"<p>Ok, maybe I don&#39;t totally understand exactly what a neural net &#34;is&#34;. If we have a rule to update Q values, doesn&#39;t that imply that we have old Q values and new Q values? And if we have old Q values, don&#39;t they need to be stored in some sort of data structure? </p>",2019-10-10T14:35:18Z,34,Week 10/6 - 10/12,feedback,,jl284xdcifz44g,k1kt0gkc4x45yt,2019-10-10T14:35:18Z,{},project2
2672,no,"<p>a neural net is at simplest a function.</p>
<p></p>
<p>So instead of a table where you look up values and store values, in the neural net you pass the input into the neural net to get Q values. Updates take the form of updating the neural net weights (alters how it gets its Q value).</p>",2019-10-10T15:50:51Z,34,Week 10/6 - 10/12,feedback,,jzj7y1ofgsro1,k1kvpmpgyen1a3,2019-10-10T15:50:51Z,{},project2
2673,stud,Thanks Steph! I appreciate it!,2019-10-09T21:39:25Z,34,Week 10/6 - 10/12,followup,a_0,,k1jsq0zx7di1t0,2019-10-09T21:39:25Z,{},other
2674,stud,"<p>Crystal clear explanation! Thanks, Anonymous!!!</p>",2019-10-10T01:02:49Z,35,Week 10/6 - 10/12,followup,a_0,,k1jzzlu29cg5ek,2019-10-10T01:02:49Z,{},other
2675,no,"<p>Thanks Prof! We are being warned that the outage can last a couple days - maybe even a week. If it does last into Monday here, I will try giving notice as well as record proofs of the power outage as much as possible.</p>
<p></p>
<p>Our work buildings tomorrow are also closed as of 8PM PST today due to the power outage that is happening state-wide and PGE is not giving us a definite time and day as to when we&#39;ll get power back up.</p>",2019-10-10T02:05:32Z,42,Week 10/6 - 10/12,followup,,jqnardlwW3NE,k1k2896rwp33dm,2019-10-10T02:05:32Z,{},hw4
2676,no,<p>Good to know. Let us know in a private post to all instructors once you come back online.</p>,2019-10-14T02:16:53Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1pse9ws99i2bt,2019-10-14T02:16:53Z,{},hw4
2677,no,<p>Hey prof - I was able to submit my assignment without a hitch and the energy shutdowns were not as bad as expected. Thank you!</p>,2019-10-14T02:24:17Z,41,Week 10/13 - 10/19,feedback,,jqnardlwW3NE,k1psnsaztp32v7,2019-10-14T02:24:17Z,{},hw4
2678,no,"<p>Excellent, happy to hear that!</p>",2019-10-14T02:36:45Z,41,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1pt3tbyrvk1bh,2019-10-14T02:36:45Z,{},hw4
2679,no,<p>I was under the impression that late homework was not accepted late -- with or without penalty. So I&#39;m not sure why the &#34;available until&#34; date is ever later than the due date.</p>,2019-10-10T16:52:09Z,58,Week 10/6 - 10/12,followup,,jzqes6i2cl77ot,k1kxwga3v3atq,2019-10-10T16:52:09Z,{},other
2680,no,"<p>you are correct.  even though Canvas says that, the RLDM site will not let you submit after the due date.</p>",2019-10-10T23:56:27Z,58,Week 10/6 - 10/12,feedback,,hz7meu55mi8sd,k1ld23s64s58r,2019-10-10T23:56:27Z,{},other
2681,no,"<p>I don&#39;t see why the nature of the task (episodic vs continuing) would matter here.  The value function doesn&#39;t get reset when an episode terminates, so the variable $$t$$ continues on to $$\infty$$ in episodic tasks just as in continuing tasks.  That $$t$$ will, of course, extend across episodes.</p>",2019-10-10T05:15:16Z,35,Week 10/6 - 10/12,followup,,jzfsa4a37jf4aq,k1k909k54r87dp,2019-10-10T05:15:16Z,{},other
2682,stud,"<p>Well, I think one reason it matters is that for episodic tasks, $$\gamma$$ is often 1.</p>",2019-10-10T05:26:11Z,35,Week 10/6 - 10/12,feedback,a_0,,k1k9eaihccrd,2019-10-10T05:26:11Z,{},other
2683,no,"<p>I see.  This whole proof probably breaks down for that case.  Maybe there is no guarantee that closeness in consecutive value function iterations implies closeness to the optimal value function, if $$\gamma=1$$.  That&#39;s a good insight!</p>",2019-10-10T06:43:54Z,35,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1kc68w2s211ru,2019-10-10T06:43:54Z,{},other
2684,stud,"<p>&#64;tianhang zhu, thanks for the response. Sorry, it takes me a long time to follow up. I am definitely still interested in this. </p>
<p></p>
<p>I understand the infinite horizon case and am talking about finite horizon with $$\gamma=1$$.I am not seeing why $$v_t(s)$$ is undefined when $$\gamma=1$$.  Can you elaborate a bit more? Intuitively, I tried a few distinct cases and it seems I can get $$v_t(s)$$ just fine. </p>
<p></p>
<p>Many thanks!</p>",2019-11-03T05:20:37Z,31,Week 11/3 - 11/9,followup,a_0,,k2ijrkx81if1ze,2019-11-03T05:20:37Z,{},other
2685,stud,"<p>Specifically, when $$\gamma=1$$, I think we still have the Bellman equation:</p>
<p>$$v^*(s) = \max_a q^*(s,a) = \max_a E [R_{t&#43;1}&#43;\max_{a&#39;} q^* (S_{t&#43;1}, a&#39;)|S_t=s, A_t=a ]$$,</p>
<p>where $$q*(s,a) = E[R_{t&#43;1} &#43; \dots &#43;R_{T} | S_t = s, A_t = a]$$</p>",2019-11-03T15:09:30Z,31,Week 11/3 - 11/9,feedback,a_0,,k2j4swg89g03d8,2019-11-03T15:09:30Z,{},other
2686,no,"<p>&#34;v∗(s)=maxaq∗(s,a)=maxaE[Rt&#43;1&#43;maxa′q∗(St&#43;1,a′)|St=s,At=a]&#34; is this correct?</p>
<p>for your definition of q*, I think you meant q∗(s,a)=max_piE[Rt&#43;1&#43;⋯&#43;RT|St=s,At=a]</p>
<p>taking this definition q* is the maximum sum of reward if you have T steps left to go. However, in your recursion, max aE[Rt&#43;1&#43;maxa′q∗(St&#43;1,a′)|St=s,At=a] this is saying that you run for one step and then you take the max sum of reward if you still have T step to go, <strong>which you don&#39;t have,</strong> which is essentially the hard part of the finite horizon. <strong> </strong>To still use DP, you need a separate Q function for each t step left to go sum of rewards.</p>
<p></p>",2019-11-03T18:24:57Z,31,Week 11/3 - 11/9,feedback,,is5gzbotXmz,k2jbs8lcp0f5lq,2019-11-03T18:24:57Z,{},other
2687,stud,"<p>$$\max_\pi [\sum R_i]$$ is a much better way to express what I meant. (I was struggling to express that $$a_2$$ depends on $$a_1$$ and $$S_{t&#43;1}$$ and didn&#39;t figure out a way to write it without recursion.)</p>
<p></p>
<p>I think I see what you are saying: If we not only know $$T&lt;\infty$$ but also know there are exactly $$T$$ steps before completion, we can define $$q(.,T)$$ as the maximum sum of reward if there are $$T$$ steps to go, and can then write a recursion that involves $$q(.,T-1)$$.</p>
<p></p>
<p>But sometimes, we only know $$T&lt;\infty$$ but don&#39;t know from the outset the exact value of $$T$$. There is also the stochastic case, where $$S_t=s$$ makes a transition to different $$S_{t&#43;1}$$&#39;s: $$S_{t&#43;1}^{(1)},S_{t&#43;1}^{(3)},\dots, S_{t&#43;1}^{(M)}$$ based on the transition probability $$p(r,S_{t&#43;1}|S_t, a)$$, and for each of these $$S_{t&#43;1}^{(i)}$$, we end up with a different $$T_i&lt;\infty$$.</p>
<p>In these situatons, I feel the Bellman equation still holds. If we define $$q(s,a)$$ as the maximum reward from state s (if we take action a) till termination. I think the optimal $$q$$ still need to satisfy:</p>
<p>$$
q^{*}(s,a) = \begin{cases}

0 \text{ , when s is terminal} \\

\max_a E[R_{t&#43;1} &#43; \max_{a&#39;} q^{*}(S_{t&#43;1},a&#39;)|S_t=s, A_t=a] \text{ ,otherwise}

\end{cases}

$$</p>
<p></p>
<p>If we have a $$q$$ that does not satisfy the above equation, then we can make improvements to make it better, and thus contracts its optimality. </p>
<p></p>
<p>Do you think $$q*$$ is well defined in this case? Do policy iteration and value iteration work?</p>
<p></p>
<p>Thanks again.</p>",2019-11-03T23:58:44Z,31,Week 11/3 - 11/9,feedback,a_0,,k2jnphpumg76hq,2019-11-03T23:58:44Z,{},other
2688,stud,"<p>I just realize I made a typo above. I meant:</p>
<p></p>
<p>$$

q^{*}(s,a) = \begin{cases}

0 \text{ , when s is terminal} \\

 E[R_{t&#43;1} &#43; \max_{a&#39;} q^{*}(S_{t&#43;1},a&#39;)|S_t=s, A_t=a] \text{ ,otherwise}

\end{cases}


$$</p>
<p></p>
<p></p>",2019-11-04T15:11:07Z,31,Week 11/3 - 11/9,feedback,a_0,,k2kkatx0hsj5u,2019-11-04T15:11:07Z,{},other
2689,no,"<p>to formalize what you say. I think what you are saying is: there exist a pi s.t P(reach terminal state under pi in finite time ) = 1. This is sometimes called indefinite horizon. And yes the recursion still works as normal and the same proof carries over. When gamma = 1 though, if you have extra condition that and non proper policy ( proper policy is the policy where you can terminal with prob 1 under finite time)  have inf cost, then you have contraction under a weighted max norm. Other than that, there is a condition where you can get some contractionness from \sum P(terminal |s,a ), but I couldn&#39;t find it now. But there must be constraint to make the problem guarantee solvable when gamma = 1.</p>",2019-11-04T16:49:06Z,31,Week 11/3 - 11/9,feedback,,is5gzbotXmz,k2knstygh6v4g8,2019-11-04T16:49:06Z,{},other
2690,stud,"<p>Thanks a lot.  Now it makes things much clearer for me. I feel HW1 is one of these examples. It has $$\gamma=1$$ and theoretically, the horizon can be infinite, but it can be bounded by some PAC-style bound $$P(T&lt; N) &gt;1-\delta$$. </p>
<p></p>
<p>I would appreciate it if there are any additional readings you would suggest. Thanks again. </p>
<p></p>",2019-11-04T17:34:03Z,31,Week 11/3 - 11/9,feedback,a_0,,k2kpemzouyz3bv,2019-11-04T17:34:03Z,{},other
2691,no,"<p>if you want to know more about the theoretical analysis of dynamic programming or some approximation type algorithm like Q learning. dimitri bertsekas&#39;s &#34;neuro dynamic programming&#34; is my goto book. For more analysis on MDP, puterman&#39;s markov decision process has a lot. &#34;Bertsekas, D.P., Dynamic Programming and Optimal Control&#34; is the book that we used for the stochastic optimization class here. </p>",2019-11-05T16:46:33Z,31,Week 11/3 - 11/9,feedback,,is5gzbotXmz,k2m35e9ebmk3rw,2019-11-05T16:46:33Z,{},other
2692,no,"<p>For something that is availible online and shorter, I can share you a note for proving properties of the algorithms or properties of the game rigoriously (e.g why a stationary deterministic policy is always good enough instead of say history dependent policy or randomized policy for the inf horizon discounted mdp) on the website of one of our professor here</p>
<p></p>
<p><a href=""https://www2.isye.gatech.edu/~anton/StochOpt.pdf"">https://www2.isye.gatech.edu/~anton/StochOpt.pdf</a></p>",2019-11-05T17:01:49Z,31,Week 11/3 - 11/9,feedback,,is5gzbotXmz,k2m3p1j71qiqw,2019-11-05T17:01:49Z,{},other
2693,stud,<p>Thanks a lot! I really appreciate it. </p>,2019-11-06T05:02:12Z,31,Week 11/3 - 11/9,feedback,a_0,,k2mtfgc1z4v2px,2019-11-06T05:02:12Z,{},other
2694,no,"<p>After installing 0.14.0 I am still getting the following error. I wonder why...</p>
<p></p>
<pre>ImportError: DLL load failed: The specified module could not be found.</pre>",2019-10-11T04:42:01Z,30,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1ln9d0bots4ho,2019-10-11T04:42:01Z,{},hw4
2695,no,"<p>NEVER MIND. something is wrong with my windows.</p>
<p></p>
<p>Works fine in Ubuntu 18.04</p>",2019-10-11T04:51:07Z,30,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1lnl2cgg4hlx,2019-10-11T04:51:07Z,{},hw4
2696,no,<p>Guess windows is only good for playing League of Legends.</p>,2019-10-11T04:51:34Z,30,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1lnln3chl010m,2019-10-11T04:51:34Z,{},hw4
2697,no,"<p>I haven&#39;t started this yet, so I&#39;m also still thinking through this.</p>
<p></p>
<p>With Q-learning, we&#39;re updating with the maximum Q-value at state $$s&#39;$$, which is $$8$$, completely independently of the action we took that got us to $$s&#39;$$, right?</p>
<p></p>
<p>Then $$r &#43; \gamma\cdot8$$ would be the target for the squared error loss, and $$3$$ (a=2, zero-indexed) would be the hypothesis for  the squared error loss, when we perform gradient descent.</p>",2019-10-10T15:50:19Z,34,Week 10/6 - 10/12,followup,,jzfsa4a37jf4aq,k1kvoxi91np1b3,2019-10-10T15:50:19Z,{},project2
2698,no,"<p>The memory part of the RL in DQN is in the network. We are pushing all of that Q memory into the network. That means you need to have a reasonably large network to hold the same memory capacity as the fully described state space. .... Or do you? The state space isn&#39;t quite as big as some people think.</p>
<p></p>
<p>How to access that memory is in the $$\phi(s)$$ much like we use [s,a] indexing. You are replacing that indexing with a function approximation that feeds into the network to produce the $$[a_1, a_2, ..., a_n]$$ values per step.</p>
<p></p>
<p>That&#39;s my take. I won&#39;t look deeply at this until next week.</p>",2019-10-10T18:23:50Z,34,Week 10/6 - 10/12,feedback,,jc554vxmyuy3pt,k1l16d6crh41wg,2019-10-10T18:23:50Z,{},project2
2699,no,"<p>&#64;Vahe I updated my question to put the correct value in $$f_{target}$$.</p>
<p></p>
<p>My question was more, do the other values (1, 2, 4) stay in our $$f_{target}$$, or should it they be updated in some way closer to the (5, 6, 7/8)?</p>",2019-10-11T03:00:15Z,34,Week 10/6 - 10/12,feedback,,jl3oi5v7qkSk,k1ljmha11sp447,2019-10-11T03:00:15Z,{},project2
2700,no,"<p>As I understand it, your target should be a $$r &#43; \gamma\cdot \max Q(s&#39;)$$. In your example, $$\max Q(s&#39;)=8$$ so the target would be $$r &#43; \gamma\cdot 8$$.  This target is used in the squared error loss function to compute gradient descent / backpropagation / whatever is used to update the weights of your neural network.  The other term in the loss function is $$Q(s,2) = 3$$, the Q value of the previous state-action pair.</p>
<p></p>
<p>All those other Q values (1,2,4...5,6,7) don&#39;t matter since they weren&#39;t either of the state-action values in this particular training sample, neither the one you left nor the one you arrived at.</p>
<p></p>
<p>I think I basically repeated my previous post.  So if I&#39;m not answering your question I&#39;m sorry.  I&#39;ll take a look at this post again when I&#39;m a little less tired.</p>",2019-10-11T03:29:55Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1lkomw8nqd6nw,2019-10-11T03:29:55Z,{},project2
2701,no,"<p>I suspect I&#39;m just not explaining myself well (as is unfortunately the case when I don&#39;t understand a thing well).</p>
<p></p>
<p>When you talk of a target being $$r &#43; max Q(s&#39;)$$, that to me looks like a single value, rather than a vector of values (with one value for each action). I&#39;m currently using Keras&#39;s &#96;fit()&#96; method, which takes a vector of outputs to train towards (let&#39;s call it $$v_{target}$$). It&#39;s this vector I&#39;m trying to understand, given that the literature talks of the target being a single value.</p>
<p></p>
<p>I think that by setting the target vector, $$v_{target}$$, as the original output vector ($$f(s)$$), but with only the one value changed as in my post, that will have the correct effect, as the loss function between $$f(s)$$ and $$v_{target}$$ will be zero for all unchanged values (1, 2 and 4 in my original post), and the only value that will change (3 -&gt; 7) will change the weights for that action only.</p>
<p></p>
<p>I&#39;m not sure if that makes much more sense, but I think I&#39;m talking myself into this being right, so I&#39;m going to try a few other things to get it to work (e.g. replay memory).</p>",2019-10-11T04:07:36Z,34,Week 10/6 - 10/12,feedback,,jl3oi5v7qkSk,k1lm13aew3y504,2019-10-11T04:07:36Z,{},project2
2702,no,"<p>The Q values <em>are</em> all scalars.  If you want to turn them into vectors by creating a new vector all of whose values are identical to the previous state&#39;s output, then yes, you&#39;ll get a difference vector with only one nonzero component.  That seems wasteful though.  Are you sure the vectorization isn&#39;t over the <em>mini-batch?</em>  That would make more sense I would think.</p>",2019-10-11T04:19:23Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1lmg8xsrts3vr,2019-10-11T04:19:23Z,{},project2
2703,no,"<p><a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" target=""_blank"" rel=""noopener noreferrer"">This paper</a> definitely has one NN that has one output per action. It kinda makes more sense to me because then you then get to train the same network with each action, rather than having to train different NN for each action, which means that if an action isn&#39;t seen as much, the neural network still gets trained.</p>
<p></p>
<p>That&#39;s just a hypothesis at the moment though, as it might be that discretising the action space is better. It just feels like you&#39;d need to do 4 times as much training to train 4 NNs.</p>",2019-10-11T04:30:59Z,34,Week 10/6 - 10/12,feedback,,jl3oi5v7qkSk,k1lmv5u53bh2hl,2019-10-11T04:30:59Z,{},project2
2704,no,"<p>I think using the vector output for your NN is more efficient, but once you have your vectors $$f$$ and $$f_{target}$$, you can index into both with the actions $$a$$ and $$a&#39;$$ and use the values for the chosen actions only to calculate your loss (as in the DQN pseudocode). </p>",2019-10-11T15:57:28Z,34,Week 10/6 - 10/12,feedback,,jzifg1e23c29s,k1mbdz5yt9q2l9,2019-10-11T15:57:28Z,{},project2
2705,no,"<p>I think the answer is yes for your question. You are doing is correctly. The Q value update is only for 1 action, the rest of 3 actions are the same. However, I am still trying to train my learner. </p>",2019-10-18T23:35:36Z,33,Week 10/13 - 10/19,followup,,j6ln9puq99s5uv,k1wru477ih54x,2019-10-18T23:35:36Z,{},project2
2706,no,"<p>in DQN formula, I did not see alpha or epsilon either. Only see gamma used </p>",2019-10-18T02:34:18Z,57,Week 10/13 - 10/19,followup,,jqrr36mqfm8M,k1vis2enufs22r,2019-10-18T02:34:18Z,{},project2
2707,no,"<p>There are more hyper params then just gamma. For the update rule, that is correct; but the entire RL algorithm has more depending on what you select to do this project with. I think I have at least 5 hyper params with my DQN setup (using the Nature paper).</p>",2019-10-19T01:50:31Z,57,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1wwnmafly33y4,2019-10-19T01:50:31Z,{},project2
2708,no,"<p>There is always an alpha, but when using neural networks, you&#39;re not the one dealing with it - the NN is dealing with it.  You can feed the NN your preferred learning rate through an interface with an <em>optimizer (</em>gradient descent, rmsprop, Adam, etc.)</p>",2019-10-19T04:04:03Z,57,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1x1fcdml0c67j,2019-10-19T04:04:03Z,{},project2
2709,no,"<p>This is super helpful.  I was staring this project down with no idea where to start, but now I feel like I&#39;m going to have enough material to give it a good attempt, and learn a lot to boot!</p>
<p></p>
<p>Very high-quality post there, Chris!  Thank you so much!</p>",2019-10-11T14:56:06Z,24,Week 10/6 - 10/12,followup,,jqmfuaidej9155,k1m9724u7za636,2019-10-11T14:56:06Z,{},project2
2710,stud,<p>I agree! Extremely helpful! Thanks!!!</p>,2019-10-16T20:37:50Z,23,Week 10/13 - 10/19,feedback,a_0,,k1tqlsvu4xc7nq,2019-10-16T20:37:50Z,{},project2
2711,no,<p>Thanks Chris for sharing!</p>,2019-10-11T20:19:12Z,24,Week 10/6 - 10/12,followup,,jfzaqnqvtQ1m,k1mkqkzwpsv14p,2019-10-11T20:19:12Z,{},project2
2712,stud,thanks for sharing!,2019-10-19T13:37:51Z,23,Week 10/13 - 10/19,followup,a_1,,k1xlx91bmo679f,2019-10-19T13:37:51Z,{},project2
2713,no,<p>Good video that explicitly explains DQN&#39;s: https://www.youtube.com/watch?v=wrBUkpiRvCA</p>,2019-10-20T23:09:53Z,22,Week 10/20 - 10/26,followup,,gx3c8l7z7r72zl,k1zlsqr03mh6xn,2019-10-20T23:09:53Z,{},project2
2714,no,<p>Super helpful. Thanks!</p>,2019-10-23T18:24:42Z,22,Week 10/20 - 10/26,feedback,,jc6y8iyomz41me,k23lxjq3b3n61e,2019-10-23T18:24:42Z,{},project2
2715,no,<p>: )</p>,2019-10-29T04:15:24Z,21,Week 10/27 - 11/2,feedback,,gx3c8l7z7r72zl,k2bc8geh1wx7ce,2019-10-29T04:15:24Z,{},project2
2716,no,"<p>Figured it out - I ended up training it over more episodes and increased the learning rate, alpha, after the correct strategy was found. </p>",2019-10-11T15:58:04Z,42,Week 10/6 - 10/12,followup,,jzhdgaq99za3um,k1mber6dlor2ht,2019-10-11T15:58:04Z,{},hw4
2717,no,"<p>I increased alpha after noticing these sparse learning spikes with Q</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhdgaq99za3um%2Fk1mbgwx0p3nh%2FScreen_Shot_20191011_at_11.58.46_AM.png"" alt="""" /></p>",2019-10-11T16:00:07Z,42,Week 10/6 - 10/12,feedback,,jzhdgaq99za3um,k1mbhe6a5lh4xv,2019-10-11T16:00:07Z,{},hw4
2718,no,<p>Good plots... smart way of completing the homework.</p>,2019-10-12T00:42:57Z,42,Week 10/6 - 10/12,feedback,,hyx9thiqa6j4nn,k1mu5r58m2l2n2,2019-10-12T00:42:57Z,{},hw4
2719,no,"<p>Adding to Vahe&#39;s answer, I think the leaderboard considers &#34;solved&#34; to be the start of the 100 consecutive trials. If you click on the top spot on the leaderboard, it shows 85 as &#34;episodes to solve&#34; and 185 as &#34;total episodes&#34;.</p>
<p></p>
<p>It&#39;s always fun to read the comments on that board that say the results were not reproducible even with the exact code :(</p>",2019-10-11T15:56:09Z,34,Week 10/6 - 10/12,followup,,jzifg1e23c29s,k1mbcady23xyx,2019-10-11T15:56:09Z,{},project2
2720,no,"<p>I&#39;m guessing this is because the environment is stochastic, so if you get a lucky randomization seed, you will &#34;solve&#34; it quickly.  If Open AI had more resources, I think it would be cool to have a more standardized, comprehensive, and meaningful test for solving their environments, which would be invariant to things like random seeds.  Having a meaningful leaderboard would encourage more activity from people who are trying to solve these problems, I think.</p>
<p></p>
<p>I noticed the same thing in Taxi, where the nature of the problem makes a perfect policy rake in anywhere between 3 reward and 15 reward per episode.  But with such a wide range in episodal rewards, you could have an optimal policy and easily get a worse average reward than someone with a sub-optimal policy who got a lucky seed, over a measly 100 episodes.</p>",2019-10-11T16:07:53Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mbrdr4e3n6qs,2019-10-11T16:07:53Z,{},project2
2721,no,"<p>True! I think that&#39;s why most papers show results across several random seeds, although the naive version of that comes with its own set of problems (as in the &#34;Deep Reinforcement Learning that Matters&#34; paper).</p>
<p></p>
<p>Anyway, trying out multiple runs for every hyperparameter for project 2 is turning my laptop into a fire hazard :(</p>",2019-10-11T16:21:37Z,34,Week 10/6 - 10/12,feedback,,jzifg1e23c29s,k1mc91oy3cz106,2019-10-11T16:21:37Z,{},project2
2722,no,"<p>So I&#39;m trying to get CartPole to converge, and I noticed this behavior in my results:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl284xdcifz44g%2Fk1mjup0mfl71%2FScreen_Shot_20191011_at_3.51.17_PM.png"" alt="""" width=""582"" height=""764"" /></p>
<p></p>
<p>The algorithm seems like it &#34;figured it out&#34; around trial 36 and there&#39;s a string of perfect episodes with a score of 200. But then after 13 perfect episodes, it starts doing worse. Epsilon should have already decayed by this point, so I don&#39;t think it&#39;s random action choices from epsilon driving this behavior. What factors could cause my algorithm to find what I imagine was the optimal policy, but then stop following it? </p>",2019-10-11T19:57:42Z,34,Week 10/6 - 10/12,followup,,jl284xdcifz44g,k1mjyxm54j850,2019-10-11T19:57:42Z,{},project2
2723,no,"<p>Are you using a neural network?  If so, could it be overfitting?</p>",2019-10-11T20:02:27Z,34,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1mk5175epn5qo,2019-10-11T20:02:27Z,{},project2
2724,no,"I suspect the same thing as Vahe. If your NN has found perfect weights to solve the problem for a number of starting states, then encounters a starting state that it doesn&#39;t solve exactly, it will change the weights of the NN to be better for the new starting state, which will shift your previous &#34;perfect weights&#34;. In theory over time your NN should be able to adapt to have a more robust set of weights that can encompass all starting states.",2019-10-12T01:46:53Z,34,Week 10/6 - 10/12,feedback,,jl3oi5v7qkSk,k1mwfz5qb44401,2019-10-12T01:46:53Z,{},project2
2725,no,"<p>Ok, I&#39;m running some tests right now, but after some research it seems like that&#39;s likely the problem</p>",2019-10-12T14:59:34Z,34,Week 10/6 - 10/12,feedback,,jl284xdcifz44g,k1nordsdvao2z3,2019-10-12T14:59:34Z,{},project2
2726,no,"<p>How did you end up rectifying the apparent overfit, Dalton?</p>",2019-10-13T17:41:20Z,33,Week 10/13 - 10/19,feedback,,jl3we43d3bp15p,k1p9z9cqzmr3jx,2019-10-13T17:41:20Z,{},project2
2727,no,"<p>I did not, although I understand the problem more specifically. I&#39;m working through it in &#64;634</p>",2019-10-13T23:35:51Z,33,Week 10/13 - 10/19,feedback,,jl284xdcifz44g,k1pmn60sp9d1o8,2019-10-13T23:35:51Z,{},project2
2728,no,<p>Ok so this happened to me as well. This wasn&#39;t a neural net issue for me. For Cart Pole I had to ensure I was only making an update to my weights if my time step was &lt;=200. Miguel talks about it in OH yesterday. Vahe also wrote a post &#64;641. If you reach time step 200 Cart Pole says done and that will negative impact a state that most likely is a good state.</p>,2019-10-14T01:53:38Z,33,Week 10/13 - 10/19,feedback,,jc6mqevhagl262,k1prkd4x4ox7bd,2019-10-14T01:53:38Z,{},project2
2729,no,"<p>i just shifted to use anaconda, much easier</p>",2019-10-18T02:43:49Z,57,Week 10/13 - 10/19,followup,,jqrr36mqfm8M,k1vj4b0jw2g7c7,2019-10-18T02:43:49Z,{},project2
2730,no,<p>Ok that makes sense but my question is what exactly is y_j and what does that represent? Is that what we predict based of a sample from D. Does y_j get updated in our initial Q table?</p>,2019-10-11T17:55:16Z,58,Week 10/6 - 10/12,followup,,jc6mqevhagl262,k1mflgvjrv1oc,2019-10-11T17:55:16Z,{},project2
2731,no,"<p>In DQN there is no Q table, Q is represented by a neural network. $$y_j$$ is an NxA array of Q values. It is your existing estimate with the update rule applied to it.</p>",2019-10-11T18:27:29Z,58,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1mgqwgvz2w22w,2019-10-11T18:27:29Z,{},project2
2732,no,<p>so y_j needs to be the same size as Q? I thought y_j was each element in the batch so if you decide to train your batch on 300 memory examples how would that be the same as Q? Q which is represented by a neural net take an input of states (using keras). </p>,2019-10-11T19:02:47Z,58,Week 10/6 - 10/12,feedback,,jc6mqevhagl262,k1mi0aouex84s4,2019-10-11T19:02:47Z,{},project2
2733,no,<p>$$y_j$$ is the target for your neural network - it is the label for your neural network - it is the direction you would like to move your predictions in. Let&#39;s start with how you have structured your Q network - what is the input? what is the output?</p>,2019-10-11T22:08:26Z,58,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1mon24f34o6sx,2019-10-11T22:08:26Z,{},project2
2734,no,<p>the input is the number of states and the output is the number of actions. </p>,2019-10-11T22:17:41Z,58,Week 10/6 - 10/12,feedback,,jc6mqevhagl262,k1moyygi84p93,2019-10-11T22:17:41Z,{},project2
2735,no,<p>Do you mean that is the dimension of the input and output?</p>,2019-10-11T22:28:52Z,58,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1mpdbsk4jx712,2019-10-11T22:28:52Z,{},project2
2736,no,<p>yes input_dim=env.observation_space.shape[0]</p>,2019-10-11T22:29:13Z,58,Week 10/6 - 10/12,feedback,,jc6mqevhagl262,k1mpdsalwwcr2,2019-10-11T22:29:13Z,{},project2
2737,no,<p>So I don&#39;t understand what you mean by &#34;I thought y_j was each element in the batch so if you decide to train your batch on 300 memory examples how would that be the same as Q?&#34;. Let&#39;s assume we&#39;re just training on one example for the sake of simplicity. </p>,2019-10-11T22:35:09Z,58,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1mpleo63yxk,2019-10-11T22:35:09Z,{},project2
2738,no,"<p>right this is where I do not understand y_j. The formula $$y_j - Q(\phi _j,a_j)$$ doesn&#39;t make sense. the $$y_j$$ is a single scalar value whereas $$Q(\phi _j,a_j)$$ output action values such as [.2, .3 , 1.2, .5]. I have tried putting $$y_j$$ into a vector where it is only on for the action that I just took. For example if I just took action 2 my $$y_j$$ would be [0,0, y_j_value, 0]. I can subtract these two, but then this leaves me with the next question of I can&#39;t use  $$y_j - Q(\phi _j,a_j)$$  in my .fit() method. Because .fit() requires inputs that are of the same size as my number of states input_dim=env.observation_space.shape[0].</p>",2019-10-11T22:48:42Z,58,Week 10/6 - 10/12,feedback,,jc6mqevhagl262,k1mq2u95e07ik,2019-10-11T22:48:42Z,{},project2
2739,no,"<p></p><pre>[0,0, y_j_value, 0]</pre>
<p></p>
<p>What you&#39;re saying here is adjust the first action towards 0, the second action towards 0, the third action (the one I took) towards $$y_j$$, and the fourth action towards 0. What you want to say is don&#39;t adjust the actions I didn&#39;t take (I don&#39;t have any information so I wouldn&#39;t know how to adjust them) and adjust only the action I did take.</p>
<p></p>
<p>Fit is where you adjust the weights of your network so that for your input (state vector) the predictions are moved closer to your target (Q value vector).</p>
<p></p>",2019-10-11T23:04:01Z,58,Week 10/6 - 10/12,feedback,,i4op5p9vfbq5yz,k1mqmixx6ma5bn,2019-10-11T23:04:01Z,{},project2
2740,no,"<p>So the &#34;fit&#34; call is when we adjust the weights will use the output of the gradient descent step ? and the fit call to use sample_weights, class_weights to adjust the weight ?</p>",2019-10-14T20:24:20Z,57,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1qv8qjty407ie,2019-10-14T20:24:20Z,{},project2
2741,no,"This will depend on your library. With Keras you can look at the fit function signature (at least the API I&#39;m familiar with) to see it expects inputs and their targets. fit will calculate your loss (based on what loss function you specified when you compiled your model), compute, and apply your gradients (based on the optimizer you specified when you compiled your model). fit is the gradient descent step.",2019-10-14T21:57:08Z,57,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1qyk2hoifd6lo,2019-10-14T21:57:08Z,{},project2
2742,no,"<p>So starting this project very late :). </p>
<p></p>
<p>So looks like y<sub>j</sub> is really the values of the target value for state , action pair (sj, aj) right? </p>
<p>You calculate a new yj based on our q learning formula: Q[s][a] = Q[s][a] &#43; alpha * (r &#43; gamma *(Q[s_prime][max_a_prime]) - Q[s][a])</p>
<p></p>
<p>So then feed this yj to the network using the fit function so it can learn learn the appropriate internal weights</p>
<p></p>
<p>Am I on right directions?</p>",2019-10-18T01:23:10Z,57,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1vg8l4gquc7w,2019-10-18T01:23:10Z,{},project2
2743,no,<p>Yup</p>,2019-10-18T03:15:12Z,57,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1vk8o8c2kf5gp,2019-10-18T03:15:12Z,{},project2
2744,no,"<p>I am debating whether Yj is --&gt; r &#43; gamma *(Q[s_prime][max_a_prime] or is it  --&gt; Q[s][a] &#43; alpha * (r &#43; gamma *(Q[s_prime][max_a_prime]) - Q[s][a])</p>
<p></p>
<p>Since I am training the neural network that the target value for the Q(s,a) is really Yj instead of existing Q(s,a), I am tending towards using the latter formula. </p>
<p></p>
<p>Any feedback?</p>",2019-10-19T01:05:24Z,57,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1wv1lnn8rh45s,2019-10-19T01:05:24Z,{},project2
2745,no,"<p>So, in tabular Q-learning, we use that $$- Q[s][a]$$ to form an error between the old value and target.  And we use that error to get a little closer to the target than our old value was, the distance of movement being determined by our learning rate.</p>
<p></p>
<p>With neural networks, there is no Q-table, but rather <em>weights</em> of the neural network.  The update isn&#39;t an assignment any more - it&#39;s a .fit() call, which means you&#39;re fitting training data to a &#34;label&#34;, or the presumed correct value of that training data.  The error is computed and used <em>within</em> the neural network code to do backpropagation and function minimization.</p>",2019-10-19T04:01:49Z,57,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1x1chaeoi929g,2019-10-19T04:01:49Z,{},project2
2746,no,"<p>isn&#39;t Q(s,a) &#43; alpha * (...) presumed correct value if alpha =1? which means we are controlling the confidence of this presumption using alpha</p>
<p></p>
<p>But I see what you are saying. You want me to use the former formula without the alpha.  Thats what the paper says as well. But I wasn&#39;t convinced why the latter will not work</p>
<p></p>
<p>It will just bring the alpha hyper parameter into play</p>",2019-10-19T04:14:35Z,57,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1x1sw5fusv3i6,2019-10-19T04:14:35Z,{},project2
2747,no,"<p>&#34;<em>isn&#39;t Q(s,a) &#43; alpha * (...) presumed correct value if alpha</em> =1?&#34;</p>
<p></p>
<p>No, if alpha = 1, you have</p>
<p></p>
<p>$$Q(s,a) \leftarrow Q(s,a) &#43; \alpha\cdot \big{(}r &#43; \gamma \max_{a&#39;}Q(s&#39;,a&#39;) - Q(s,a)\big{)}$$</p>
<p></p>
<p>$$= r &#43; \gamma\max_{a&#39;}Q(s&#39;,a&#39;)$$</p>",2019-10-19T04:35:43Z,57,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1x2k2gloh973v,2019-10-19T04:35:43Z,{},project2
2748,no,"<p>&#34;The update isn&#39;t an assignment any more - it&#39;s a .fit() call, which means you&#39;re fitting training data to a &#34;label&#34;, or the presumed correct value of that training data.  &#34;</p>
<p></p>
<p>Ok so one option is to use the discrete actions themselves as the target for the &#39;fit&#39; or another option is to use the &#39;presumed&#39; correct value for the &#39;fit&#39; (Mnih&#39;s paper seems to have used =r &#43; γ max ′Q(s′,a′)). </p>
<p></p>
<p>thanks!</p>
<p></p>",2019-10-19T17:43:08Z,57,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1xuop8j77fle,2019-10-19T17:43:08Z,{},project2
2749,no,"<p>Question.</p>
<p></p>
<p>so Keras seems to handle Q target vs State using fit. But in this case state is BATCHx8 shape and Q target is BATCHx4 shape. (One is state space shape, the latter is action space shape)</p>
<p></p>
<p>how does this work for non Keras packages? Do we select some subset of state? Or what part of state is fed into our loss function? </p>",2019-10-20T01:12:24Z,56,Week 10/20 - 10/26,followup,,jcg0nzvdk8272b,k1yaqgmfiyf3s3,2019-10-20T01:12:24Z,{},project2
2750,no,"<p>The first input to fit() is the input to your NN, which is usually going to be a different shape than your target, i.e. the two vectors&#39; shapes are unrelated.  The important thing is that the shape of your target matches the shape of the output of the NN.  The state is not fed into the loss function - it&#39;s fed into the NN.  Or maybe I&#39;m not understanding your question?</p>",2019-10-20T01:31:35Z,56,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k1ybf403nika,2019-10-20T01:31:35Z,{},project2
2751,no,<p>Where did you get this algorithm? I&#39;ve checked all listed papers and can&#39;t find it. </p>,2019-10-27T15:52:39Z,55,Week 10/27 - 11/2,followup,,jzih0fdt4sn1cq,k2969f9263x7n5,2019-10-27T15:52:39Z,{},project2
2752,no,deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning,2019-10-27T16:31:10Z,55,Week 10/27 - 11/2,feedback,,i4op5p9vfbq5yz,k297my4q9v32qs,2019-10-27T16:31:10Z,{},project2
2753,no,<p>I thought I was going crazy when nothing happened when I hit log in button!</p>,2019-10-11T18:50:27Z,42,Week 10/6 - 10/12,followup,,hzoi2qsuCAd,k1mhkftargk2gy,2019-10-11T18:50:27Z,{},hw4
2754,no,<p>THANK YOU</p>,2019-10-11T19:27:27Z,42,Week 10/6 - 10/12,feedback,,hzoi2qsuCAd,k1miw0v344b42s,2019-10-11T19:27:27Z,{},hw4
2755,no,<p>1 caveat. I spent a bunch of time in cartpole and found that some improvements (w/ DQN) did not translate well to the LL environment space and took a bunch of time to debug. So be sure test incremental changes :)</p>,2019-10-14T01:46:33Z,34,Week 10/13 - 10/19,followup,,jcg0nzvdk8272b,k1prb8z3xv936k,2019-10-14T01:46:33Z,{},project2
2756,no,"<p>Just a clarification... solving the Cart Pole doesn&#39;t give extra credit if you do not solve the Lunar Lander. Students have tried that in the past, I&#39;m not crazy...</p>",2019-10-14T02:13:57Z,34,Week 10/13 - 10/19,followup,,hyx9thiqa6j4nn,k1psahnncei6io,2019-10-14T02:13:57Z,{},project2
2757,no,"<p>I&#39;m a little confused by that approach.  When you say ignore the done flag when updating your action values do you mean only the artificial done flag? Or you ignore all done flags and always update with the regular q-learning update?  The homework says to update with V(St&#43;1) = 0, you didn&#39;t do this?</p>",2019-10-12T18:37:28Z,31,Week 10/6 - 10/12,followup,,jqqssjikI511,k1nwjlwlk3t1wn,2019-10-12T18:37:28Z,{},hw4
2758,no,"<p>Correct, I didn&#39;t do that.</p>
<p></p>
<p>If you distinguish the &#39;artificial done flags&#39; from the true done flags, then you can update (only) in the case of true terminal states with $$V(S_{t&#43;1}) = 0$$ and not introduce any bugs into your algorithm.  However, making this distinction requires you to examine the terminal state that the environment sends you when you simultaneously receive a done flag, and have a conditional branch for the update.</p>
<p></p>
<p>Alternatively, you can initialize all Q values to zero and <em>not</em> ever set $$V(S_{t&#43;1}) = 0$$, and your terminal updates will always be correct, because true terminal states never occur as non-terminal states in Taxi.</p>
<p></p>
<p>Or....you can unwrap your instantiation of Taxi and avoid false done flags, which is what I ended up doing.  You&#39;ll see how slow the first few episodes are as a truly random policy takes a long time to figure out how to win.</p>",2019-10-12T20:28:02Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1o0hstov7g2q0,2019-10-12T20:28:02Z,{},hw4
2759,no,"<p>This is illuminating.  Thank you for posting this Vahe.</p>
<p></p>
<p>I was relying on the &#34;Done&#34; flag in and just running bootstrapped instances for everything I was worth to try to get the right answers.  I now understand why that&#39;s dumb.</p>
<p></p>
<p>Now to rerun my code!</p>",2019-10-13T04:00:45Z,31,Week 10/6 - 10/12,followup,,jqmfuaidej9155,k1ognzvqa2355y,2019-10-13T04:00:45Z,{},hw4
2760,no,"<p>Indeed Vahe, I was wondering what the significance of &#34;unwrapped&#34; was; very informative, clear post.  thank you :)</p>",2019-10-16T01:10:43Z,30,Week 10/13 - 10/19,followup,,jzivtxcbl6964n,k1skwvdgecz6l7,2019-10-16T01:10:43Z,{},hw4
2761,no,"<p>I wouldn&#39;t say it is a bug, but definitely an inconvenience for newcomers. You have no idea how many examples online omit this time wrapper. </p>
<p></p>
<p>As usual, great post!</p>",2019-10-18T22:18:29Z,30,Week 10/13 - 10/19,followup,,hyx9thiqa6j4nn,k1wp2xv9vqc1ct,2019-10-18T22:18:29Z,{},hw4
2762,no,"<p>I&#39;d like confirmation that if it&#39;s needed to change the &#96;done&#96; to 0 when terminating at max-steps. Because the target calculation is dependent on the &#96;done&#96; signal (when done =1, no future value is added). </p>
<p></p>
<p>Is it necessary to make that adjustment instead of taking the env.step() result directly for q&#39; calculation. </p>",2019-10-23T18:42:42Z,29,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k23mkpie9l12ym,2019-10-23T18:42:42Z,{},hw4
2763,no,"<p>I&#39;m confused by what you mean in the bolded section below:</p>
<p></p>
<p>&#34;This same issue exists in CartPole.  The environment-imposed 200 second maximum time for balancing the pole works the same way as the artificial time-limit in Taxi. <strong>By zeroing out the bootstrapped Q update, you are punishing a state-action pair that normally would be proceeding to a non-terminal state by assigning it 0 future rewards.</strong>  You could presumably add time as a state variable yourself as a direct way to handle this issue, but time is <em>not</em> included in the environment&#39;s state definition.&#34;</p>
<p></p>
<p>So as I understand it, we need to communicate to our agent in some way that if the episode terminates with a reward of 200, that&#39;s a good thing. I tried checking the reward if &#34;done&#34; is true, then training the algorithm on that state, action, etc with a reward of 200 (chosen somewhat arbitrarily) in hopes that the algorithm will recognize that that state is good, but this is cause odd behavior in my algorithm (after it gets a perfect 200, it immediately gets 2-3 episodes with scores of 10-20). </p>
<p></p>
<p>I feel like I understand the problem at this point, but I&#39;m not sure how to implement a solution. I&#39;m not sure what you meant by the last line in your note &#64;641, if that was supposed to help &gt;_&lt;</p>",2019-10-12T21:23:16Z,31,Week 10/6 - 10/12,followup,,jl284xdcifz44g,k1o2gt73b84wi,2019-10-12T21:23:16Z,{},project2
2764,no,"<p>What I mean by the line you boldfaced is the following.</p>
<p></p>
<p>When you do your Q-value update, you&#39;re making an assignment to the previous state you were in.   When the episode is terminated because you hit the 200th time step, you receive done = True along with the terminal state.  The vast majority of the time, like 99.99&#43;% of the time, those two states - the one you just left, and the terminal state - are going to be perfectly valid non-terminal CartPole states, with the pole balanced, and the cart somewhere in the middle of the track.</p>
<p></p>
<p>If the target of your update assignment is $$0$$, you&#39;re telling the agent that it was in a bad state - one that expected $$0$$ future rewards.  But that&#39;s not true 99.99&#43;% of the time.  That state was just the unlucky recipient of the done flag.  So don&#39;t send the wrong signal to your agent.  Tell it that it would have kept reaping rewards from that state, if its performance hadn&#39;t been artificially halted.</p>",2019-10-12T21:33:10Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1o2tk3m3so1d7,2019-10-12T21:33:10Z,{},project2
2765,no,"<p>I really appreciate all the responses, but I&#39;m still confused by this line: </p>
<p></p>
<p><strong>Tell it that it would have kept reaping rewards from that state, if its performance hadn&#39;t been artificially halted.</strong></p>
<p></p>
<p>That&#39;s my original question, how do we tell our algorithm that it will continue reaping rewards? I know how to give it a reward in that state, but I don&#39;t know how to tell it that it will continue getting rewards if it stays in that state</p>",2019-10-12T21:56:50Z,31,Week 10/6 - 10/12,feedback,,jl284xdcifz44g,k1o3nzvkqym4wl,2019-10-12T21:56:50Z,{},project2
2766,no,"<p>The way you communicate rewards to your agent in Q learning is via the update rule.</p>
<p></p>
<p>If you update with only $$R $$ as your target, you&#39;re telling it that it stands to reap no more rewards.</p>
<p></p>
<p>The &#39;target&#39; is this thing:  $$R &#43; \gamma \cdot \max_{a&#39;} Q(s&#39;, a&#39;)$$</p>",2019-10-12T22:03:31Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1o3wknj7pb4ei,2019-10-12T22:03:31Z,{},project2
2767,no,"<p>Ok, I think I understand. I&#39;m just having trouble applying it to DQN. I think I have an idea though, I&#39;m going to test changing the &#34;Done&#34; flag to false when updating, if the total reward &gt;= 200. Thanks for all of your help</p>",2019-10-12T22:09:53Z,31,Week 10/6 - 10/12,feedback,,jl284xdcifz44g,k1o44rm2hku64f,2019-10-12T22:09:53Z,{},project2
2768,no,"<p>Yeah, let me know how that works.</p>",2019-10-12T22:10:21Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1o45d682en346,2019-10-12T22:10:21Z,{},project2
2769,no,"<p>No luck. I tried passing &lt;state, action, reward, state&#39;, done&gt; to my agent in most cases, but in the case where the total reward is 200 I passed &lt;state, action, reward, state&#39;, False&gt;. I was hoping this would tell the agent that getting a reward of 200 isn&#39;t bad, but I&#39;m still experiencing the same &#34;collapsing&#34; behavior when I get a few rewards of 200 in a row. </p>",2019-10-13T16:24:52Z,30,Week 10/13 - 10/19,feedback,,jl284xdcifz44g,k1p78xikmzt3hj,2019-10-13T16:24:52Z,{},project2
2770,no,"<p>Wait, you need to still terminate the episode at 200, because the environment terminates the episode.  Meaning, the episode still terminates at t=200.  The weights just need to be updated as if the episode were not done.  Is that what you&#39;re doing?</p>",2019-10-13T17:02:50Z,30,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1p8lqu5sal3wh,2019-10-13T17:02:50Z,{},project2
2771,no,"<p>Correct, I&#39;m still terminating at 200, but I&#39;m passing Done=false to my neural net so that it doesn&#39;t think that&#39;s a terminal state. </p>",2019-10-13T23:34:52Z,30,Week 10/13 - 10/19,feedback,,jl284xdcifz44g,k1pmlx1g3szjm,2019-10-13T23:34:52Z,{},project2
2772,no,<p>How do we initialize states to 0 in DQN? I tried to ask this in &#64;616 and I was more or less told that we don&#39;t initialize our states. Am I missing something?</p>,2019-10-12T21:31:18Z,31,Week 10/6 - 10/12,followup,,jl284xdcifz44g,k1o2r5mcwgz6va,2019-10-12T21:31:18Z,{},project2
2773,no,"<p>My response was generic.  In the case of a neural network, you don&#39;t have a table of Qs that you initialize.  So it might be a little trickier there.  If you&#39;re initializing your NN to random values, then you may have to keep track of the episode time.  If done flag comes on $$t=200$$, update as if the next state isn&#39;t a terminal state.   Otherwise, update with a $$0$$ for the target.  That&#39;s a solution off the top of my head.</p>",2019-10-12T21:35:55Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1o2x3e5qtp73d,2019-10-12T21:35:55Z,{},project2
2774,no,"<p>Dalton,</p>
<p></p>
<p>If you get it to work, and want to try something just slightly harder before moving on to Lunar Lander, change this line in your code</p>
<p></p>
<p>from</p>
<p></p>
<pre>env = gym.make(&#39;CartPole-v0&#39;)</pre>
<p></p>
<p>to</p>
<p></p>
<pre>env = gym.make(&#39;CartPole-v1&#39;)</pre>
<p></p>",2019-10-13T04:41:46Z,31,Week 10/6 - 10/12,feedback,,jzfsa4a37jf4aq,k1oi4q91yq45qg,2019-10-13T04:41:46Z,{},project2
2775,no,<p>Really great paper recommendation. The discussion of each hypothesis really helped my understanding of each algorithm&#39;s strengths and weaknesses.</p>,2019-10-19T16:54:55Z,57,Week 10/13 - 10/19,followup,,isde34zracb1mz,k1xsyokpp6h7ie,2019-10-19T16:54:55Z,{},project2
2776,no,"<p>The notation for this cumulative reward in Sutton &amp; Barto and others (Bertsekas &amp; Tsitstiklis) is $$G$$, in case that helps.</p>",2019-10-13T04:12:22Z,57,Week 10/13 - 10/19,followup,,jqmfuaidej9155,k1oh2x6jixx5w5,2019-10-13T04:12:22Z,{},project2
2777,no,"<p>Return... yeah, we need to fix these wording issues. It confuses people.</p>",2019-10-14T02:36:04Z,57,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1pt2y2ggm5b0,2019-10-14T02:36:04Z,{},project2
2778,no,"<p>I visited the state in question 731k times while running 1M unwrapped episodes that chose exploration 90% of the time w/ a 0.01 learning rate. I counted the frequency of different q-values for the state and put the most frequent into RLDM but it was still incorrect. The q-values generated for examples in the HW description match up to 9 decimal places.</p>
<p></p>
<p>I&#39;m very curious about what I&#39;m doing wrong here.</p>",2019-10-14T04:47:59Z,41,Week 10/13 - 10/19,followup,,ixnwq0s4ozg,k1pxsksticy4v5,2019-10-14T04:47:59Z,{},hw4
2779,no,<p>Here&#39;s a question:  how did you settle on your particular learning rate?</p>,2019-10-14T05:11:46Z,41,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1pyn5prypa6rg,2019-10-14T05:11:46Z,{},hw4
2780,no,"<p>&#64;Conor, did you do any variation such as a decay of your learning rate?  I ended up decaying my $$\epsilon$$ and $$\alpha$$ but I don&#39;t think everyone had to do that.  The idea with this is that I wanted my algorithm to be really opened minded at first but later trust what it had learned earlier.  Also to explore a lot at first but as time went on to exploit more of that trusted knowledge.</p>",2019-10-14T05:14:52Z,41,Week 10/13 - 10/19,feedback,,is9so9huTMp,k1pyr5n6he17oz,2019-10-14T05:14:52Z,{},hw4
2781,no,"<p>Not that this is guaranteed to help you, but I had luck making sure I used version 0.14.0 of gym.  You might try that.</p>
<p></p>
<p>From there, I also ran the full problem like mad, but also initialized to that state by changing the properties of the gym object so that I could get extra information.</p>
<p></p>
<p>YMMV.</p>",2019-10-16T02:05:21Z,41,Week 10/13 - 10/19,followup,,jqmfuaidej9155,k1smv4z4foq3rx,2019-10-16T02:05:21Z,{},hw4
2782,no,"<p>And I just realized this is 2 days too late...</p>
<p></p>
<p>buh.  Sorry, Conor!</p>",2019-10-16T02:08:05Z,41,Week 10/13 - 10/19,feedback,,jqmfuaidej9155,k1smynsdz0y784,2019-10-16T02:08:05Z,{},hw4
2783,no,"<p>On Ubuntu and not sure how similar our issues are, but I had similar difficulty installing box2d-py and it was resolved by switching to Python3. (same as they suggest at the bottom of that github issues discussion, after which people seem to stop asking about the issue).</p>
<p></p>
<p>Someone claimed they also had issues with anaconda&#39;s python 3, but I&#39;m using it fine with box2d-py. I setup a basic environment with anaconda, installed <a href=""https://stackoverflow.com/questions/54252800/python-cant-install-box2d-swig-exe-failed-with-error-code-1"" target=""_blank"" rel=""noopener noreferrer"">swig</a>, then installed box2d-py and it worked fine.</p>",2019-10-13T14:39:15Z,57,Week 10/13 - 10/19,followup,,isde34zracb1mz,k1p3h40l90j24h,2019-10-13T14:39:15Z,{},project2
2784,no,"<p>that would be me.  ;) the issue was: no compilation without swig and I couldn&#39;t install the &#34;msvc&#34; since I am out of hard-drive space and it takes 5 gigs, to get the headers... which were for win10 anyway.  sort of irritating since I already have my OS sdk installed and like 3 different compilers (ming, vc, clion/clang).</p>
<p></p>
<p>and there were some other issues but I can&#39;t remember what they were.  something about copying some lib&#39;s into another lib or enabling a flag to search for DLL&#39;s (for python).</p>
<p></p>
<p>.... if you are experiencing &#34;can&#39;t connect SSL error&#34; that was the bug.   anaconda&#39;s distro has some very stupid defaults set. </p>
<p></p>
<p>swig installed easy though.  and once I fed it the &#34;ideal build kit&#34; looks like it worked. ;)</p>
<p></p>",2019-10-16T01:33:33Z,57,Week 10/13 - 10/19,feedback,,jzivtxcbl6964n,k1slq8rgsxy13h,2019-10-16T01:33:33Z,{},project2
2785,no,"<p>Are you using python 2 or python 3? I was having the same problem on my Mac for python 3 and solved my problem doing:</p>
<p></p>
<pre>sudo pip3 install box2d-py</pre>
<p>hope that helps!</p>",2019-10-13T17:33:50Z,57,Week 10/13 - 10/19,followup,,j6m1jeidndu6wq,k1p9pm1ujjb2z0,2019-10-13T17:33:50Z,{},project2
2786,no,<p>Using python 3. Didnt work. Thanks for the reply.</p>,2019-10-14T08:50:11Z,57,Week 10/13 - 10/19,feedback,,jqt6oket86zV,k1q6g29tmrh55r,2019-10-14T08:50:11Z,{},project2
2787,no,"<p>Thanks all for the replies. I tried multiple things but it didnt work.</p>
<p></p>
<p>Finally this answer worked for me, <a href=""https://stackoverflow.com/questions/52509602/cant-compile-c-program-on-a-mac-after-upgrade-to-mojave/54737537#54737537"">https://stackoverflow.com/questions/52509602/cant-compile-c-program-on-a-mac-after-upgrade-to-mojave/54737537#54737537</a></p>
<p></p>
<p>Please try other solutions too dicussed in the answer, might work for you.</p>
<p></p>",2019-10-14T08:50:18Z,57,Week 10/13 - 10/19,followup,,jqt6oket86zV,k1q6g7eke6h57r,2019-10-14T08:50:18Z,{},project2
2788,no,"<p>sudo pip3 install box2d-py seemed to work (no error message, import sees it etc) but simply resetting the lunar lander env triggered errors.</p>
<p></p>
<p>The only thing that worked was </p>
<p></p>
<pre>pip3 install gym[Box2D] </pre>",2019-10-14T21:31:50Z,57,Week 10/13 - 10/19,followup,,jzh6k6o994a6dh,k1qxnjff518ta,2019-10-14T21:31:50Z,{},project2
2789,no,"<p>I had extremely different run times based on the alpha, epsilon, and decay rates used. When I was experimenting with different values to see how they affected the q table, I had to run the algorithm between 500 and 800,000 episodes depending on what I sent the alpha, epsilon, and decay rates to in order to match the values in the homework. It was interesting to see how the different values effected how quickly the learner found the expected values. Including dyna also adjusted things. It was mentioned in office hours that it could be close to 1 million episodes needed.</p>
<p></p>
<p>Think about what happens when you are decaying the epsilon value as it approaches 0; this means that you will most often just follow the policy that you&#39;ve discovered. If that value gets too small the agent will stop exploring, and the you might not be able to explore every state infinitely often (and it&#39;s hard to do that with only 150 episodes). If your convergence factor is basically looking if the q table has stopped changing, then if you never visit a new state and constantly follow the same path over and over again, sooner then later, your alpha will stop updating those states and you&#39;ll claim you&#39;ve converged.</p>
<p></p>
<p>I would experiment with not allowing your agent to terminate so soon as well as what Vahe suggests.</p>",2019-10-13T21:58:26Z,30,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1pj5wns9eg2zl,2019-10-13T21:58:26Z,{},hw4
2790,stud,"<p>Indeed, when epsilon approaches 0, I was actually doing policy evaluation, and if it approaches 0 too early, I would end up evaluating a sub-optimal policy and then observe convergence.</p>
<p></p>
<p>According to <a href=""/class/jzh9tkzzxkd7ph?cid=653"" target=""_blank"" rel=""noopener noreferrer"">653</a>, I fix my alpha at 1, and initialize epsilon to 1. Interestingly, if epsilon is fixed at 1, my algorithm still converges in around 150 episodes and fail to return optimal q values, but if I decay epsilon with 0.9999, it will reach the correct values. </p>",2019-10-13T22:18:39Z,30,Week 10/13 - 10/19,feedback,a_0,,k1pjvwh7dj81ps,2019-10-13T22:18:39Z,{},hw4
2791,no,"<p>Thanks Ben!</p>
<p></p>
<p>Try CartPole-v1 (500 step cap) and see if it carries over to that without a hitch.</p>",2019-10-13T16:17:32Z,33,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1p6zhpqke53a2,2019-10-13T16:17:32Z,{},project2
2792,no,<p>Thank you Vahe! I was wondering where Miguel got this 200 cap from as I&#39;ve seen 500-step max in my tests.</p>,2019-10-13T16:29:01Z,33,Week 10/13 - 10/19,feedback,,jqkxzdmmolGf,k1p7e9wf5ah5bw,2019-10-13T16:29:01Z,{},project2
2793,no,"<p>:)</p>
<p></p>
<p>The cool kids use v1...</p>",2019-10-14T02:34:44Z,33,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1pt1868bjh6ac,2019-10-14T02:34:44Z,{},project2
2794,no,<p>&#64;Vahe it looks like my agent can&#39;t finish v1 - that is <em>excellent</em> news as it means I have a bunch more things I can try with a much more rapid turnaround (CartPole for me runs several times faster than LunarLander). Thanks for the tip ^_^</p>,2019-10-14T06:46:20Z,33,Week 10/13 - 10/19,feedback,,jl3oi5v7qkSk,k1q20rxhch254e,2019-10-14T06:46:20Z,{},project2
2795,no,<p>What do you mean a <em>single neural network</em>? A single layer?</p>,2019-10-15T01:10:34Z,33,Week 10/13 - 10/19,followup,,is8ald0uljj3u4,k1r5guepod24hx,2019-10-15T01:10:34Z,{},project2
2796,no,"<p>He might mean a single NN.  Some approaches require multiple networks, e.g. <a href=""https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"">https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf</a></p>",2019-10-15T02:29:56Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1r8aws385l63a,2019-10-15T02:29:56Z,{},project2
2797,no,"<p>I do indeed mean a single NN, rather than one per action. I have managed to use some very simple and small structures for my NN though (trying not to give too much hyperparameter info away), so you don&#39;t need to go crazy with the amount of layers or nodes to get a decent performance, at least with CartPole.</p>",2019-10-15T02:35:23Z,33,Week 10/13 - 10/19,feedback,,jl3oi5v7qkSk,k1r8hx1126n6pn,2019-10-15T02:35:23Z,{},project2
2798,no,"<p>Oh, so you&#39;re not keeping a separate target network. Sounds like a good experiment. Thanks!</p>",2019-10-21T01:21:10Z,32,Week 10/20 - 10/26,feedback,,is8ald0uljj3u4,k1zqhkkwqbh7l7,2019-10-21T01:21:10Z,{},project2
2799,no,<p>curious... how long is &#34;a few minutes&#34;? I&#39;m having trouble solving just the CartPole-v0 problem with my DQN and am looking for some direction.</p>,2019-10-21T06:12:32Z,32,Week 10/20 - 10/26,followup,,hbmelkhwx5a5d3,k200w9vt78s6no,2019-10-21T06:12:32Z,{},project2
2800,stud,"<p>I think this is a good place to propose my question as well...</p>
<p></p>
<p>I set alpha=epsilon=1 and run for 3,000 episodes (using for i in range(3000) rather than |q-q_pre|&lt;epsilon), and then I get all answers correct...</p>",2019-10-13T19:31:03Z,27,Week 10/13 - 10/19,followup,a_1,,k1pdwcobz5o1j3,2019-10-13T19:31:03Z,{},hw4
2801,no,"<p>Is your question why this worked?  For $$\alpha$$, see my response above.  For $$\epsilon$$, I think thinking about why that is is probably part of the homework assignment.</p>",2019-10-13T19:41:52Z,27,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1pea9ow69b1jy,2019-10-13T19:41:52Z,{},hw4
2802,stud,"<p>Thank you for your explanation! I understand now why fixing alpha=1 would work. As for epsilon, from <a href=""/class/jzh9tkzzxkd7ph?cid=534"" target=""_blank"" rel=""noopener noreferrer"">534</a>, since Q-learning does not need to satisfy condition 2 of GLIE (for Sarsa):</p>
<p> </p>
<ol><li class=""p1"">All state-action pairs must be explored infinitely often.</li><li class=""p2"">The policy must converge on a greedy policy.</li></ol>
<p> </p>
<p>Does this mean we do not need to decay epsilon so we can choose any fixed epsilon value as long as all state-action pairs are visited enough time during exploration?</p>",2019-10-13T21:58:34Z,27,Week 10/13 - 10/19,feedback,a_1,,k1pj62g8c3y73k,2019-10-13T21:58:34Z,{},hw4
2803,no,<p>That sounds reasonable to me.</p>,2019-10-13T21:59:47Z,27,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1pj7mo4xqf171,2019-10-13T21:59:47Z,{},hw4
2804,stud,"<p>Vahe, thanks for reponding! That makes total sense. I completely forgot reading that detail about Taxi, that it is a deterministic environment. So we don&#39;t have to worry about fading out alpha to prevent estimates being noisy indefinitely because of over-corrections resulting from recent stochastic transitions, correct?</p>
<p></p>
<p>Also, thanks for pointing to the relevant clarifying information about stochastic approximation. For anybody else, you can find and read more about Vahe&#39;s point 2 in section 2.5 of the textbook (starting on page 32).</p>
<p></p>",2019-10-13T20:52:23Z,27,Week 10/13 - 10/19,followup,a_0,,k1pgsyibxhv2hw,2019-10-13T20:52:23Z,{},hw4
2805,no,"<p>Yup, correct!</p>",2019-10-13T20:57:17Z,27,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1pgz9k45n06pb,2019-10-13T20:57:17Z,{},hw4
2806,no,"<p>So weights for Cart Pole would be initialized as follows?</p>
<p></p>
<p>w = [[0 0 0 0],[0 0 0 0]]</p>
<p></p>
<p>with w[0] being weights for action left, and w[1] being weights applied to action right?</p>
<p></p>
<p>And select an action with a linear function approximator being the max dot product of the appropriate weight vector with a feature vector from the last observation, max(w[0] x fv, w[1] x fv)?</p>
<p></p>
<p>The LFA &#34;learns&#34; by adjusting the weight appropriately, and then to change the function approximator, we swap in a NN or other supervised learning method for the FA?</p>",2019-10-14T18:03:05Z,33,Week 10/13 - 10/19,followup,,is4nx55dinr6wk,k1qq73cqw5i7fj,2019-10-14T18:03:05Z,{},project2
2807,no,"<p>As far as I can tell, this is all correct.</p>",2019-10-14T18:31:55Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1qr86cutyo6uz,2019-10-14T18:31:55Z,{},project2
2808,no,<p>correct</p>,2019-10-15T02:35:44Z,33,Week 10/13 - 10/19,feedback,,is5gzbotXmz,k1r8id2f94r778,2019-10-15T02:35:44Z,{},project2
2809,no,"<p>Got it, CartPole now works with SARSA-LFA</p>",2019-10-15T11:54:29Z,33,Week 10/13 - 10/19,feedback,,is4nx55dinr6wk,k1rsgx4q44i2gw,2019-10-15T11:54:29Z,{},project2
2810,no,<p>Great!  I might have to try that if I have time.</p>,2019-10-15T18:13:52Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1s60suks3218u,2019-10-15T18:13:52Z,{},project2
2811,no,<p>That sounds like what I am doing as well.</p>,2019-10-14T01:33:09Z,42,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1pqu0jfomu4xz,2019-10-14T01:33:09Z,{},project2
2812,no,<p>How are you results looking? My NN doesn&#39;t appear to be learning from the training </p>,2019-10-14T01:36:41Z,42,Week 10/13 - 10/19,feedback,,j6m1jeidndu6wq,k1pqyklbypd34d,2019-10-14T01:36:41Z,{},project2
2813,no,"<p>Mine appears to be finally learning, but I spent the last 5 days or so tweaking things. I noticed I had a couple bugs in how I updated weights which caused a lot of head aches. It also takes ~4-5 hours of training; so feedback on my tweaks is slow going; but I am getting 100 run &#34;training&#34; averages in the 250&#43;s which is good; but every once in a while the running average drops back below 200 and sometimes quite a bit... still working through why this occurs and if my hyperparam tweaks can fix is.</p>
<p></p>
<p>Miguel discussed when to do the replay, and potentially having a &#34;warm up&#34; period as well in the study session on Oct 12.</p>",2019-10-14T02:50:18Z,42,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1ptl8mxcc15fp,2019-10-14T02:50:18Z,{},project2
2814,no,"<p>Sounds like what Ive tried as well, but so far using replays has actually made my agent much worse. Not sure why this might be, so for now I&#39;m looking at other approaches.</p>",2019-10-14T07:15:07Z,42,Week 10/13 - 10/19,followup,,jl3oi5v7qkSk,k1q31ser9oo6mg,2019-10-14T07:15:07Z,{},project2
2815,no,"<p>I had the same error on windows 10 using python 3.6</p>
<p></p>
<p>Fix:</p>
<p>1. Download swig and copy files to program directory: <a href=""http://www.swig.org/download.html"">http://www.swig.org/download.html</a></p>
<p>2. Add swig to PATH in environment variables</p>
<p>3. pip install box2d-py</p>
<p></p>",2019-10-14T15:40:24Z,57,Week 10/13 - 10/19,followup,,jzm01jjgolv32u,k1ql3lnpn9h7ji,2019-10-14T15:40:24Z,{},project2
2816,no,"<p>For some reason, I had also to install Visual Studio.</p>",2019-10-15T13:51:24Z,57,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1rwn9xraf51xv,2019-10-15T13:51:24Z,{},project2
2817,no,<p>This could be possible. I already had Visual Studio and all C&#43;&#43; Redistributables</p>,2019-10-16T01:03:43Z,57,Week 10/13 - 10/19,feedback,,jzm01jjgolv32u,k1sknvurgfi4wo,2019-10-16T01:03:43Z,{},project2
2818,no,<p>Thank you! I&#39;ll try the fix and see how it goes~</p>,2019-10-16T01:33:25Z,57,Week 10/13 - 10/19,feedback,,jznmonjuzvi67j,k1slq2x73p71oq,2019-10-16T01:33:25Z,{},project2
2819,no,"<p>&#64;Kurtis,</p>
<p></p>
<p>So we show download swig and install it here: C:\Program Files ? Does this matter?</p>",2019-10-19T18:41:01Z,57,Week 10/13 - 10/19,feedback,,gx3c8l7z7r72zl,k1xwr4pzvog5sl,2019-10-19T18:41:01Z,{},project2
2820,no,"<p>for second step what folder exactly you point path to in windows environment variables? </p>
<p>i.e: root like C:\swig-4.0.1 or other folder in swig directory. </p>",2019-10-20T16:46:14Z,56,Week 10/20 - 10/26,feedback,,jzlwxqswu5m2e7,k1z83d8de1p2ks,2019-10-20T16:46:14Z,{},project2
2821,no,"<p>Hi all,</p>
<p>When reinstalling the box2d-py per the project description, does it matter where we download and build box2d-py on your system? Can it be done anywhere?</p>",2019-10-19T18:25:45Z,57,Week 10/13 - 10/19,followup,,gx3c8l7z7r72zl,k1xw7i38f533yy,2019-10-19T18:25:45Z,{},project2
2822,no,"<p>You&#39;re correct Miguel, I think the term is misleading.</p>
<p></p>
<p><em>&#34;When the RL agent overfits it is replaying a policy that has been overly replayed. This is akin to a &#34;rut&#34; in a path, and the way to fix it was to do more randomized exploration of the more esoteric states.&#34;</em></p>
<p></p>
<p>This statement by me would make someone smarter say, &#34;Well, that&#39;s just not enough exploration, not overfitting.&#34; That would be correct. That&#39;s pretty much what Miguel was saying as well.</p>
<p></p>
<p>The behavior that I saw, though, was convergence to a policy where the policy was not solving the problem consistently. I changed the randomization of the problem to tightly control it so that each experiment was repeatable. After I did that, the rutting that I saw mostly disappeared.</p>
<p></p>
<p>The random influence in the model environment does have an affect on the efficiency of the solution. There is no substitute for a real world application with real world data instead of this fabricated random stuff. Even with a perfect model solution, SpaceX still couldn&#39;t stick all of its landings on the first physical go.</p>
<p></p>",2019-10-21T13:46:59Z,56,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k20h4p0jqcz4fz,2019-10-21T13:46:59Z,{},project2
2823,no,<p>I think I heard of people using OpenCL to use AMD GPUs but it definitely is not as optimized as Cuda is for Nvidia. I only worked with Nvidia GPUs so I am not sure how to go about AMD ones. Sorry!</p>,2019-10-14T19:31:27Z,42,Week 10/13 - 10/19,followup,,hzoi2qsuCAd,k1qtcq3nma66uk,2019-10-14T19:31:27Z,{},project2
2824,no,"<p>&#34;<em>It looks like maybe the neural network is adding some randomness to the experiment. &#34;</em></p>
<p></p>
<p>There is definitely randomness coming from the NN if you are using a library with default settings: the weights will be initialized randomly.</p>",2019-10-15T02:32:48Z,42,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1r8eksfp602as,2019-10-15T02:32:48Z,{},project2
2825,no,"<p>I have avoided the randomness by seeding tensorflow backend and specifying 1 thread for run only, but if you&#39;re running on several cores this definitely not a solution for you. Also the API for tensorflow 2.0 is different from the prev one.</p>",2019-10-15T02:53:31Z,42,Week 10/13 - 10/19,feedback,,jqkxzdmmolGf,k1r958jj34z53i,2019-10-15T02:53:31Z,{},project2
2826,no,<p>I can post these several lines of code here if instructors OK with that</p>,2019-10-15T02:54:27Z,42,Week 10/13 - 10/19,feedback,,jqkxzdmmolGf,k1r96f4ewao6ib,2019-10-15T02:54:27Z,{},project2
2827,no,"<p>Or you can look about the idea here on SO : <a href=""https://stackoverflow.com/a/45234799"" target=""_blank"" rel=""noopener noreferrer"">https://stackoverflow.com/a/45234799</a></p>",2019-10-15T02:56:08Z,42,Week 10/13 - 10/19,feedback,,jqkxzdmmolGf,k1r98lilcez2eu,2019-10-15T02:56:08Z,{},project2
2828,no,"<p>Thanks Vahe and Sergei for the responses; yea. I am seeding the environment and numpy, but didn&#39;t seed the neural network. I guess I just expected the initial weights to be all 0s by default. I&#39;ll look into this.</p>",2019-10-15T04:28:30Z,42,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1rcjdu7nbe7ow,2019-10-15T04:28:30Z,{},project2
2829,no,"<p>Ok, I&#39;ve applied the stuff as mentioned and it seems that it has made the one computer deterministic, but when I run the same code with the same seeds on another computer, they still vary just a little. Hopefully this is good enough.</p>",2019-10-17T01:26:43Z,42,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1u0xbf5cdv1q8,2019-10-17T01:26:43Z,{},project2
2830,no,"<p>Related question: currently I have around 30 logs of previous experiments that I&#39;ve accumulated while looking for optimal hyperparameters, and from which i can produce graphs for the report</p>
<p>Is it OK to include these logs and maybe a separate python script generating charts from these logs to the report into the repo? The format of the logs changed through the time while I was adding/removing hyperparameters, so I would need to normalize them first and use different logs for different charts (like influence of epsilon values on learning rate, then alpha values etc).</p>
<p>Is this approach OK, or we need to generate all charts by our main program? (It would be pity to throw away all these logs - a product of 2-week runs :-) )</p>",2019-10-15T03:10:34Z,42,Week 10/13 - 10/19,followup,,jqkxzdmmolGf,k1r9r5u8phd1ai,2019-10-15T03:10:34Z,{},project2
2831,no,<p>This should be OK assuming the size of your log files is reasonable.</p>,2019-10-15T03:45:11Z,42,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1raznz6cxb7m1,2019-10-15T03:45:11Z,{},project2
2832,no,<p>Thank you Chris for the clarifications!</p>,2019-10-15T03:56:47Z,42,Week 10/13 - 10/19,feedback,,jqkxzdmmolGf,k1rbel1hvj4i,2019-10-15T03:56:47Z,{},project2
2833,no,<p>Thanks for the replies Chris; I&#39;ll work on seeing what I can vectorize. I do have multiple for loops in the code; as well as a while environment is not done loop; and ensure I get enough details into the readme for everything I do.</p>,2019-10-15T04:33:23Z,42,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1rcpnw39uf42y,2019-10-15T04:33:23Z,{},project2
2834,no,"<p>how can your only loop be over episodes with a DQN? the timesteps are dependent on one another, are they not? </p>
<p></p>
<p>I&#39;m also having trouble vectorizing the replay loop... any suggestions on how to fix that? </p>",2019-10-25T04:45:42Z,41,Week 10/20 - 10/26,followup,,hbmelkhwx5a5d3,k25nk0d3cf07c2,2019-10-25T04:45:42Z,{},project2
2835,no,"<p>&#34;<em>how can your only loop be over episodes with a DQN?&#34;</em></p>
<p><em></em></p>
<p>It can&#39;t.  You need, at a minimum, a loop over episodes <em>and</em> a loop over timesteps.  You can&#39;t vectorize the calls to env.step().</p>",2019-10-25T05:08:11Z,41,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k25ocx9ktzj6p3,2019-10-25T05:08:11Z,{},project2
2836,no,<p>This happened to me with a poor combination of params. You might want to look at the resources page for p2 and try starting with some params configured like the papers have as the baseline. </p>,2019-10-16T03:17:36Z,34,Week 10/13 - 10/19,followup,,jcg0nzvdk8272b,k1spg26ur6hmh,2019-10-16T03:17:36Z,{},project2
2837,no,"<p>similar thing is happening to me, i can get one to -80 sma score.</p>
<p>Is the resource page mentioned in p2 (sutton c9) pdf or some other page?</p>",2019-10-18T15:29:00Z,34,Week 10/13 - 10/19,feedback,,i4t1oo4aALG,k1wagchjo5i5hc,2019-10-18T15:29:00Z,{},project2
2838,no,<p>What I&#39;m not understanding is why the &#34;learning&#34; stops abruptly. I&#39;ve seen examples of this and I&#39;ve seen reward plots continuously increase to a positive reward. I think there&#39;s something wrong with the logic in my code. Anyone else have suggestions?</p>,2019-10-18T17:58:28Z,34,Week 10/13 - 10/19,feedback,,j6m1jeidndu6wq,k1wfsk05wuq5vo,2019-10-18T17:58:28Z,{},project2
2839,no,"<p>What&#39;s the mean score it&#39;s settling to?  From the looks of it it&#39;s around -150.  That suggests that it&#39;s not actually learning (much).  The graph looks nice in the sense that there&#39;s this steep fall at the beginning but then it &#34;settles&#34; to a low variance state, but if it&#39;s settling to the value of taking, say, random actions, then it&#39;s not really learning.</p>
<p></p>
<p>You could run an agent that just samples randomly from the action space and plot that graph, and see how it looks compared to this one.</p>",2019-10-18T18:06:14Z,34,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1wg2jaqb4v5eo,2019-10-18T18:06:14Z,{},project2
2840,no,<p>Yes it&#39;s settling around -150. I&#39;m decaying my epsilon and that&#39;s what I think I&#39;m experiencing when it settles at -150. Does this mean my epsilon is decaying too fast?? </p>,2019-10-18T18:16:59Z,34,Week 10/13 - 10/19,feedback,,j6m1jeidndu6wq,k1wggd19cd217h,2019-10-18T18:16:59Z,{},project2
2841,no,<p>Just want to make sure I&#39;m implementing this correctly. Is epsilon only set once then decaying through all episodes? Epsilon isn&#39;t rest every episode right?</p>,2019-10-18T18:33:44Z,34,Week 10/13 - 10/19,feedback,,j6m1jeidndu6wq,k1wh1wp1w7k70f,2019-10-18T18:33:44Z,{},project2
2842,no,"<p>No, what this is telling me is that you are converging to the Q values of some random policy.  I don&#39;t think epsilon is the issue at all.</p>
<p></p>
<p>My guess is that something is wrong in your implementation of the algorithm, irrespective of hyperparameters like epsilon.  My suggestion is to take your exact code, but instead of env.making LunarLander-v2, make CartPole-v0 instead, and then start to debug it if and when it doesn&#39;t work.  CartPole is a much simpler problem, but is the same class of environments as Lunar Lander, and if you coded Lunar Lander in a general way, you should be able to just plop in CartPole without any modification.</p>",2019-10-18T18:49:21Z,34,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1whlzpitxd5ht,2019-10-18T18:49:21Z,{},project2
2843,no,<p>Gotcha ok I&#39;ll try working on the cartpole problem for a little to confirm my implementation. I&#39;m following the pseudo code from the dqn Atari paper and it looks pretty straight forward so I&#39;m wondering what I could be missing</p>,2019-10-18T19:13:00Z,34,Week 10/13 - 10/19,feedback,,j6m1jeidndu6wq,k1wigeshdgw5xr,2019-10-18T19:13:00Z,{},project2
2844,no,"<p>My SARSA-LFA solves both CartPole v0 and v1, although if I increase the episodes sufficiently it&#39;ll oscillate after it solves between low rewards and back to perfect 200/500 scores for v0/v1 respectively. I&#39;m guessing the oscillation is either because of the arbitrary end after 500 steps without including the reward into the weights or the deadly triad, I just haven&#39;t tested accounting for the arbitrary end yet.</p>
<p></p>
<p>Good to know LFA hasn&#39;t solved it yet, I won&#39;t sink more cycles into it.</p>",2019-10-15T18:40:49Z,34,Week 10/13 - 10/19,followup,,is4nx55dinr6wk,k1s6zgk35ug5so,2019-10-15T18:40:49Z,{},project2
2845,no,"<p>Ben,</p>
<p></p>
<p>Thank you for your reply. I&#39;m actually doing that for CartPole, whenever the flag done is True and my total reward is 200, I save this transition experience for memory replay as (s, a, r, s&#39;, is_done) with is_done = False.  Therefore, it computes the target as Q(s,a) = reward &#43; gamma * max Q(s&#39;, a&#39;).</p>
<p></p>
<p>I&#39;ve seen someone commenting on a post to not update the NN weights if the total reward is already 200. I&#39;ll give it a try when I get home.</p>",2019-10-15T15:05:49Z,30,Week 10/13 - 10/19,followup,,jc6xvgjncoey,k1rzaz7lc3x5ok,2019-10-15T15:05:49Z,{},project2
2846,no,"<p>Can someone give some clarification on this &#34;Make sure that when your agent gets a &#34;done&#34; signal for hitting the max step count (i.e. doing really well) you give your function approximator <strong>the value for the next state</strong>, rather than zero, which is what you should give it when it gets a &#34;done&#34; signal because it fell over&#34; ?</p>
<p></p>
<p>I am a bit confused. Thanks.</p>",2019-10-21T05:17:39Z,29,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k1zyxov1pei145,2019-10-21T05:17:39Z,{},project2
2847,no,<p>&#64;641</p>,2019-10-21T05:32:32Z,29,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k1zzguf5vdn2on,2019-10-21T05:32:32Z,{},project2
2848,no,<p>I see. So is this still an issue for the lunar lander environment? </p>,2019-10-22T01:11:31Z,29,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k215l11f4v748y,2019-10-22T01:11:31Z,{},project2
2849,no,"<p>It&#39;s an issue for any episodic environment where a state can be both a non-terminal state, and an <em>artificially created</em> terminal state, but never a real terminal state, where &#39;artificial&#39; and &#39;real&#39; are with respect to the underlying MDP.</p>",2019-10-22T01:46:43Z,29,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k216ua8h66bu0,2019-10-22T01:46:43Z,{},project2
2850,no,"<p>For the reward plot of Lunar lander, I got a plot like this (the total reward for each episode ). It&#39;s very noisy, although the problem is solved with average 100 runs. My question, should we plot the reward at each episode or the average 100 runs of reward at each episode?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6ln9puq99s5uv%2Fk23npukgw9fc%2Freward_plot.png"" alt="""" /></p>",2019-10-23T19:15:37Z,29,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k23nr1fhu4d65b,2019-10-23T19:15:37Z,{},project2
2851,no,"<p>Another related question is that when using pertained agent to repeat 100 trials, do we need to set epsilon_min to a small number like 0.01 or we should totally follow policy based on Q value?</p>",2019-10-23T20:14:26Z,29,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k23puobsw8z3ox,2019-10-23T20:14:26Z,{},project2
2852,no,"<p>Jacob,</p>
<p></p>
<p>I really give you props for sticking with discretization...that is tough.</p>
<p></p>
<p>What&#39;s the x-axis in the heat map, states-action pairs?...Ooh, I just reverse-engineered your discretization resolution.</p>",2019-10-15T19:41:35Z,29,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1s95m5cnxo19v,2019-10-15T19:41:35Z,{},project2
2853,no,"<p>yup, that&#39;s it Vahe! good crypto skills!</p>
<p></p>
<p>The x is the state. It&#39;s the bitwise encoding of a variety of features.</p>
<p></p>
<p>Note some intuition here for you value-function people. The discretization is just F(s) like any other F(s). I happen to keep it all in the Q matrix and I don&#39;t try to iteratively learn sub-sessions like you are doing for the value estimation.</p>
<p></p>
<p>We&#39;re doing the same thing, only mine is not as focused. That&#39;s why mine will take about 75x longer to learn.</p>
<p></p>",2019-10-15T20:31:04Z,29,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1sax8i54nc7mx,2019-10-15T20:31:04Z,{},project2
2854,no,"<p>The same experiment, but with 500 episodes of warmup and just 1000 episodes of training. Note the differences.... This resulted in 70% successful landings with a mean landing score of 210 (overall mean of 122).</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk1sc0pu6pki7%2Fqq_34_70pctheatmapshared.png"" alt="""" /></p>",2019-10-15T21:02:04Z,29,Week 10/13 - 10/19,followup,,jc554vxmyuy3pt,k1sc148748u28t,2019-10-15T21:02:04Z,{},project2
2855,no,"<p>Can you do even more warmup, get even better coverage, then follow that up with a lot of training for any kind of improvement in mean score?</p>",2019-10-15T21:05:43Z,29,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1sc5so6myu1u0,2019-10-15T21:05:43Z,{},project2
2856,no,"<p>yeah, I have explored that too, and there is a level at which the random warmup causes more harm than good.</p>
<p></p>
<p>For instance, I did 2000 episodes of random warmup and the lander did horribly and learned very slow. There is a tuning dependency on the size of the warmup, but I haven&#39;t figured out which parameter would better control that. Maybe a lower epsilon with a slightly higher alpha with a slightly higher alpha decay would do it.</p>
<p></p>
<p>I might try to experiment with that a little bit.</p>
<p></p>
<p>Fortunately, I have been saving my Q dumps after each experiment, so i can pull up the heat maps and see how well they covered the state space. I am working on visualizing the heat map of the MRL, which has 10 agents all learning their own Q states.</p>
<p></p>
<p>(Edit) Note the number of states has changed here. So the top one is a literal cryptographic encoding of the state, whereas the bottom one is a look up table of those cryptograms. It&#39;s important to look at the percent of state space coverage rather than the number along the x axis.</p>
<p></p>",2019-10-15T21:10:21Z,29,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1scbr73hcl7pm,2019-10-15T21:10:21Z,{},project2
2857,no,"<p>Jacob, you are really good at this plot thing... what&#39;s your secret?!</p>",2019-10-19T15:25:37Z,29,Week 10/13 - 10/19,followup,,hyx9thiqa6j4nn,k1xpruaxs0wuu,2019-10-19T15:25:37Z,{},project2
2858,no,"<p>Man, data science is so awesome!</p>",2019-10-19T15:26:05Z,29,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xpsfq37p158,2019-10-19T15:26:05Z,{},project2
2859,no,"<p>My first career was alot of data science with some rather important people in science. I learned alot from them.</p>
<p></p>
<p>But, more importantly, staring at numbers doesn&#39;t help me understand anything. I have to visualize the data in a way that makes me understand that the algorithm is working and the method will work. The heatmap was the only thing that made sense. I am trying to think of a way to do a heatmap with this generalization stuff.</p>
<p></p>
<p>The rest is just photoshop ....</p>",2019-10-21T13:37:21Z,28,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k20gsb8qadf6tl,2019-10-21T13:37:21Z,{},project2
2860,no,<p>I think you mean 10/28 at 8AM EST</p>,2019-10-15T20:53:21Z,57,Week 10/13 - 10/19,followup,,jc554vxmyuy3pt,k1sbpwsqdt2vc,2019-10-15T20:53:21Z,{},logistics
2861,no,<p>you are correct.  Fixed.</p>,2019-10-15T23:45:58Z,57,Week 10/13 - 10/19,feedback,,hz7meu55mi8sd,k1shvvowk0a5yc,2019-10-15T23:45:58Z,{},logistics
2862,no,"<p>Hi,</p>
<p></p>
<p>Would like a simple clarification on the report expectations. For Project 1, we used some room to describe the environment in the report. For Project 2, we don&#39;t have it listed that we need to describe the environment. Instead, we are asked to explain our experiments. Two questions:</p>
<p></p>
<p>(1) Just to confirm, we don&#39;t need to discuss the environment as we did with Proj 1, or if so, just as a brief mention, not a detailed explanation that is our take on it.</p>
<p></p>
<p>(2) For explaining our experiments, are these the experiment runs we do while training the agent to solve the Lunar Lander environment?</p>
<p>I&#39;m not exactly sure what is meant by experiments. Are there other experiments in addition to the following that we are suppose to run? We create the graphs listed, discuss the algorithms used, discuss how hyper params effect things, and then discuss our results in general; along with the other things listed. Mostly confused by the 4th bullet in the Write a Paper section of the assignment (pg 3)</p>
<p></p>
<p>Any additional clarification would be much appreciated.</p>
<p></p>
<p>Thanks,</p>
<p>George</p>",2019-10-17T04:50:13Z,29,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1u870elvcj352,2019-10-17T04:50:13Z,{},office_hours
2863,no,"<p>For using DQN (for project 2):</p>
<p>- For preprocessing with $$ \phi $$, I mainly use it for discretize the continuous space. What else usually goes into to the preprocess?</p>
<p>- For collecting replay memory, how would collecting experience across different episode will have more benefit than collecting experience from same episode? And also, is it implying that capacity for memorized experience is magnitude larger than state space of single episode (since we are collecting experience from multiple episodes)?</p>",2019-10-18T00:49:17Z,29,Week 10/13 - 10/19,followup,,jl5wq8mca7o0,k1vf10yvukd1hw,2019-10-18T00:49:17Z,{},office_hours
2864,no,"<p>In ATARI, for instance, this pre-processing crops and resizes the images to 84x84x3 images, it converts the resized image from color to grayscale, it stacks the 3 most recent previous images (to help the agent infer direction and speed). And I think that&#39;s it. But basically, you do what you need to do to feed the agent minimal information that still does the job. This is where your training time can go from days to hours.</p>
<p></p>
<p>It&#39;s better to collect experience from multiple episodes because that means that the policies will be very different too. That diversity makes updates to the Q-function more stable overall. Note that this is only true for off-policy learning. If you were doing on-policy learning, then it would be the opposite, you must train with recent samples.</p>",2019-10-19T16:01:09Z,29,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xr1j1e33p2xo,2019-10-19T16:01:09Z,{},office_hours
2865,no,"<p>Thank you for clarifying, Miguel!</p>",2019-10-20T03:33:21Z,28,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k1yfrpi5en24cd,2019-10-20T03:33:21Z,{},office_hours
2866,no,"<p>Yw, Quang!</p>",2019-10-20T19:00:59Z,28,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k1zcwng1hs15tp,2019-10-20T19:00:59Z,{},office_hours
2867,no,"<p>When using a replay buffer, does it make sense to initialise it by running the agent randomly (epsilon=1) until the buffer has been filled? Would it be detrimental to do so?</p>",2019-10-19T15:46:39Z,29,Week 10/13 - 10/19,followup,,jl3oi5v7qkSk,k1xqiw68jq962p,2019-10-19T15:46:39Z,{},office_hours
2868,no,"<p>It makes sense to run the agent randomly for some time, but I&#39;m not sure until the buffer has been filled. That&#39;s probably too much if you are using 100k, 1mm, samples. What I see more often is x times the minibatch size. For instance, if your mini-batches are 32 samples, then wait until you have 10*32=320 samples before you start training.</p>
<p></p>
<p>And yeah, filling that up with a totally random agent is not a bad idea. Your agent is dumb early on, anyway.</p>
<p></p>",2019-10-19T15:55:31Z,29,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xquadpw59633,2019-10-19T15:55:31Z,{},office_hours
2869,no,<p>Is there a recording for the office hour? Thanks</p>,2019-10-21T06:15:04Z,28,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k200ziy2a56n4,2019-10-21T06:15:04Z,{},office_hours
2870,no,"<p>All the records for office hour are at &#64;132. Record for this OH is: (copied from that thread)</p>
<p><a href=""https://bluejeans.com/s/94pjx/"">https://bluejeans.com/s/94pjx/</a></p>
<p></p>",2019-10-21T06:30:57Z,28,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k201jyz06x8n5,2019-10-21T06:30:57Z,{},office_hours
2871,no,"<p>Seconded on this.  I know grading projects is no fun (been there, TA&#39;d, done that, got the master&#39;s degree to prove it), but I&#39;d really like to be able to incorporate the feedback I get on the report because I&#39;m worried what I&#39;m doing for project 2 is not satisfactory.</p>",2019-10-16T02:16:56Z,24,Week 10/13 - 10/19,followup,,jqmfuaidej9155,k1sna1g083e1vi,2019-10-16T02:16:56Z,{},project1
2872,no,"When are the grades gonna be released for project 1?
<p></p>",2019-10-19T17:04:45Z,24,Week 10/13 - 10/19,followup,,jl2842t0lky3fc,k1xtbbto70n4rd,2019-10-19T17:04:45Z,{},project1
2873,no,,2019-10-19T17:04:45Z,24,Week 10/13 - 10/19,dupe,,hz7meu55mi8sd,k1xtbbu718m4re,2019-10-19T17:04:45Z,{},project1
2874,stud,"<p>I have the same question, since the comments of project 1 would be helpful for us to avoid pitfall/mistake/misunderstanding on requirement in project 2.</p>
<p>Thanks.</p>",2019-10-19T17:04:45Z,24,Week 10/13 - 10/19,feedback,a_0,,k1xtbbuimu64rg,2019-10-19T17:04:45Z,{},project1
2875,no,<p>I personally would rather have it later than sooner if the choice were between rushing the TAs vs letting them enjoy the experience  (heh).</p>,2019-10-19T17:04:45Z,24,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1xtbbuufz94rh,2019-10-19T17:04:45Z,{},project1
2876,no,"<p>Haha, no worries, we do take our time. But, yeah, I agree with this statement in general! </p>",2019-10-19T17:14:00Z,24,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xtn87mo8h4at,2019-10-19T17:14:00Z,{},project1
2877,no,Thank you,2019-10-16T13:57:12Z,57,Week 10/13 - 10/19,followup,,jqtti9gfnagH,k1tcal3jznb2nv,2019-10-16T13:57:12Z,{},project2
2878,no,"<p>I&#39;ll jump on this too, how do we know what an acceptable capacity for N is? Is that just a hyper parameter we tune until we find good performance? Is there a good rule of thumb on how many is enough based on the environment?</p>
<p></p>
<p>Same question for the minibatch size, is the size of a minibatch another hyper parameter to tune?</p>",2019-10-16T19:31:02Z,33,Week 10/13 - 10/19,followup,,is4nx55dinr6wk,k1to7wfau4p11w,2019-10-16T19:31:02Z,{},project2
2879,no,"<p>Definitely is, for both.</p>",2019-10-16T20:32:45Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1tqf9r390b2b7,2019-10-16T20:32:45Z,{},project2
2880,no,"<p><em>&#34;Do I sample a single experience tuple from the replay memory or do I<strong> sample a &#39;minibatch &lt; N&#39; worth of samples</strong>?&#34;</em></p>
<ul><li>Based on the points above, I guess the 2nd option in my question is the correct one. So, N defines the sample space size from which I select batch size samples to train/update the weights of the NN.</li></ul>
<p></p>",2019-10-16T22:30:20Z,33,Week 10/13 - 10/19,feedback,,jl1b27fpaYkv,k1tumgwtxxe90,2019-10-16T22:30:20Z,{},project2
2881,no,"<p>I understand that there&#39;s no concept of episode in replay memory. </p>
<p>I was thinking epoch is analogous to episode, since there&#39;s an outer episode loop, I thought it would be redundant to have more than 1 epoch in the keras fit?</p>
<p></p>
<p>On second thought, maybe they&#39;re not the same thing, because an epoch will go through the identical dataset multiple times, but each episode may encounter different experiences?</p>",2019-10-16T22:35:38Z,33,Week 10/13 - 10/19,followup,,jl1b27fpaYkv,k1tuta8da6b4uf,2019-10-16T22:35:38Z,{},project2
2882,no,<p>That&#39;s right. Epoch is how many times you perform backprop given the experiences you&#39;ve sampled at each timestep.</p>,2019-10-17T02:28:16Z,33,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1u34gvn9qr5of,2019-10-17T02:28:16Z,{},project2
2883,no,"<p><em>&#34;fit(x=None, y=None, batch_size=None, epochs=1 ...)</em></p>
<p><em>is x=output of the &#39;estimate&#39; NN (current state), y=output of the &#39;target&#39; NN (next state), where output = model.predict(state)&#34;</em></p>
<p></p>
<p>Also I was getting a shape error for x by passing in the output of the &#39;estimate&#39; NN, i.e. literally copying the pseudo code for x. I&#39;m getting around this by passing in the state vector/matrix (if sampling minibatch) for x. The y still works as the output of the target NN.</p>
<p></p>
<p>Read this in &#64;633 to understand this.</p>
<p>&#34;Fit is where you adjust the weights of your network so that for your<strong> input (state vector)</strong> the predictions are moved closer to your<strong> target (Q value vector)</strong>.&#34;</p>
<p></p>
<p></p>",2019-10-16T22:41:43Z,33,Week 10/13 - 10/19,followup,,jl1b27fpaYkv,k1tv1491foz6g2,2019-10-16T22:41:43Z,{},project2
2884,no,"<p>why do we need two NNs?</p>
<p></p>
<p>Why cannot one NN itself suffice?</p>
<p></p>
<p>input layer dim = 8</p>
<p>output layer dim = 4 (representing the value for each action)</p>
<p></p>
<p>Then x represents state and y represents &#39;target&#39; values for each action , and just feed this to the same NN (fit call in case of keras)</p>
<p></p>
<p>Am I simplifying some step?</p>
<p></p>
<p></p>",2019-10-18T02:42:53Z,33,Week 10/13 - 10/19,followup,,jqrr36mqfm8M,k1vj34ahioi6h6,2019-10-18T02:42:53Z,{},project2
2885,no,"<p>We don&#39;t need to use a DQN.  It&#39;s probably more powerful than a vanilla NN, but not necessary.</p>
<p></p>
<p>I actually think it&#39;s cooler to solve this problem with less.  There are going to be like 200 people with DQN solutions, so doing something different, especially if it&#39;s simpler, I think is actually a great idea.</p>
<p></p>
<p>Yeah, your input/output description looks fine.</p>",2019-10-18T02:48:02Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1vj9qk05va5n7,2019-10-18T02:48:02Z,{},project2
2886,no,"<p>yup thanks Vahe, but my question was in context of DQN itself. Do we need two NNs if you are doing DQN? Cant we do DQN with just 1 NN with the input /output the way I described it? </p>",2019-10-18T04:33:05Z,33,Week 10/13 - 10/19,feedback,,jqrr36mqfm8M,k1vn0tg5hw35t1,2019-10-18T04:33:05Z,{},project2
2887,no,"<p>Ahh, I see.  Yes.  If you read the original Deep Mind paper, they only used one NN.  In the follow up, they added a second, &#39;target&#39; NN.  The second NN has a lagged Q-state, which is used to provide the target for the update.  This helps with combating non-stationarity.</p>",2019-10-18T04:36:11Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1vn4tf8pgd10d,2019-10-18T04:36:11Z,{},project2
2888,no,<p>DQN is only 1 neural network. Double DQN is the improvement of DQN using a separate NN </p>,2019-10-18T06:32:03Z,33,Week 10/13 - 10/19,feedback,,jvfpllmsggt7p4,k1vr9tcsuet4ir,2019-10-18T06:32:03Z,{},project2
2889,no,"<p>thanks &#64;Farrukh, I was just trying to understand why so many people talk about DQN  when it seems to me that the strength of DQN is to deal with &#39;sensory inputs&#39;, ie the algorithm has to recognize shapes from the game&#39;s pixels, but that&#39;s not what we&#39;re doing here at all.  That&#39;s why I asked if it wasn&#39;t overkill.  </p>
<p>But, yes, I was thinking of using something else than NN for function approximation, I think I&#39;ll experiment with that first since it seems a good way to better understand what FA is practically.</p>",2019-10-17T00:57:26Z,32,Week 10/13 - 10/19,followup,,jzh6k6o994a6dh,k1tzvnnds4n3g2,2019-10-17T00:57:26Z,{},project2
2890,no,<p>If you remove the convolutional layers and replay memory from DQN what you&#39;re left with is Q learning with the table replaced by a dense neural net - this is the simplest version of Q learning with a neural network and it is capable of solving LL. In this sense even experience replay could be considered &#39;overkill&#39; for P2.</p>,2019-10-17T02:26:15Z,32,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1u31v6idwr28x,2019-10-17T02:26:15Z,{},project2
2891,no,"<p><strong attention=""i4op5p9vfbq5yz"">&#64;Chris Serrano</strong>  I&#39;m caught between a rock and a hard place without NN experience since you said several times that no one had ever made it work with LFA. </p>
<p>Also, even if I could, I feel like the whole point is to get experience with DQN and/or NN here, so I&#39;d just be going for LFA to get the grades while I&#39;m here to really learn, but this project has thrown a curve ball at me. </p>
<p>I just finished a midterm, so I have 10 full job-free days to finish this, but is it really realistic to think I can learn enough about NN, how to implement them in python, then build DQN, then make it work then write the report.  And keep up with the lessons, readings... </p>
<p>And second, if you think it&#39;s worth learning and using NN, then what&#39;s the quickest way? Miguel&#39;s book?</p>",2019-10-18T05:09:10Z,32,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1vob8nf3yx21u,2019-10-18T05:09:10Z,{},project2
2892,no,"<p>Todd Neal mentioned in office hours today that he got LFA with polynomial features to solve Lunar Lander, if you want to give that a try.  Note that that&#39;s linear in the features, though you&#39;ll have to add features that are nonlinear (e.g. polynomial) functions of the state variables.</p>
<p></p>
<p>There are a lot of people implementing NNs; you can look over a bunch of the posts here that talk about them.  Most of the work will be figuring out the interface with the libraries and what all the corresponding hyperparameters mean (which will definitely be a pain, but probably a useful skill to learn), since you don&#39;t have to actually build the NNs or do backpropagation or a function minimization algorithm yourself.</p>",2019-10-18T06:20:08Z,32,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1vquhvjdk449f,2019-10-18T06:20:08Z,{},project2
2893,no,"<p>Thanks &#64;vahe, it&#39;s good advice.   I&#39;ll take your suggestions as a good starting base.  </p>
<p>I&#39;ve seen NN implementations of course, but I hate using things I don&#39;t fully understand, but I guess I&#39;ll have to make an exception with NN, until ML.</p>",2019-10-18T06:52:22Z,32,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1vrzy16frm24u,2019-10-18T06:52:22Z,{},project2
2894,no,"Sure, there&#39;s more than enough time provided you don&#39;t also need to learn calculus, though you can also treat it as a black box optimizer. We&#39;re working with simple feed forward neutral networks - the best way to cement the concepts (for me) was to build a simple network with 2 input nodes, 2 hidden nodes, and a single output node and perform backprop by hand and then build it in code. You can use any neutral net library to verify the correctness of your implementation. I liked the AI a Modern Approach section on it, I think everything you&#39;d need is covered in a dozen pages.",2019-10-18T19:03:07Z,32,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1wi3p1nd93773,2019-10-18T19:03:07Z,{},project2
2895,no,"<p>&#64; Jean-Pierre Bianchi,</p>
<p></p>
<p>If you want a very brief introduction to NN; I thought this series was good to get a high level intuition of how it worked: https://www.3blue1brown.com/neural-networks</p>",2019-10-19T02:57:37Z,32,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1wz1wjf4t63y,2019-10-19T02:57:37Z,{},project2
2896,no,"<p>&#64;George Cox, please call me JP, and thx for the link, there&#39;s so much to learn!</p>",2019-10-19T03:24:50Z,32,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1x00wi0ajh4uc,2019-10-19T03:24:50Z,{},project2
2897,no,"Thanks for the question, it helps me re-think on what i should do.
For project2 I am looking for a faster training time in order to train with more hyper parameters and configuration.
It&#39;s good to learn that DQN is not necessary st. we can build simpler model for faster converge/training.",2019-10-18T11:45:43Z,32,Week 10/13 - 10/19,followup,,jr7165ipLPTP,k1w2h75459m3gq,2019-10-18T11:45:43Z,{},project2
2898,no,"<p>For what it&#39;s worth, I ended up taking a &#39;deeper&#39; look at Miguel&#39;s book &#34;Grokking DRL&#34; and, wow, everything is laid out so much clearer than anywhere I&#39;ve seen so far.  Not a million words but clear concise explanations, nice meaningful graphs.  Honestly, I should have read it before Sutton or watching Silver because it lays down the bases very nicely.  The Deep stuff starts at ch8, p 170, so before that, there 170 pages of RL theory (everything we&#39;ve seen here and there so far, in a chaotic way, laid down properly).  And Ch8 starts with the Cartpole problem, where he goes step by step, of course he uses a NN, but he explains it step by step, the approximation, in clear and visual way, including the code.    </p>
<p>I think this is going to help me do this project.  But I&#39;ll keep reading the first chapters because honestly, it was my worry, how to collect all the bits we&#39;ve been given and prepare for the final, and ch 1-7 are my answer.  </p>
<p>PS: ch 6-7 are still missing, but he said they&#39;ll be released this month.</p>
<div>
<div></div>
</div>
<div>
<div></div>
</div>",2019-10-18T19:04:25Z,32,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1wi5cypea42rk,2019-10-18T19:04:25Z,{},project2
2899,no,"<p>So I was also going the DQN route after the &#34;noone has gotten LFA to work&#34; comment (I asked the original question for thsi), but it sounds like you can use LFA with non-linear features?</p>
<p></p>
<p>Could someone show a practical example of what that means? Currently, for my LFA, I&#39;m just taking the dot product of the actions weight vector and the observation.</p>
<p></p>
<p>How do you generate a non-linear feature? Do we just randomly pick different exponents and apply them to features?</p>",2019-10-18T18:25:50Z,32,Week 10/13 - 10/19,followup,,is4nx55dinr6wk,k1wgrr6bst07ka,2019-10-18T18:25:50Z,{},project2
2900,no,"<p>So, I would bet that using the state space AS IS (dot product of weight and state space features) is NOT doable.  Lunar Lander not only seems to have complex interactions between variables, but also has random maps (Cartpole only has 1 dimension, and the same &#34;map&#34; each time).  So yeah, I think you would drive yourself crazy doing that.</p>
<p></p>
<p>For feature construction, see Sutton and Barto, starting at page 210.</p>",2019-10-18T18:52:58Z,32,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1whqmwwvig5ta,2019-10-18T18:52:58Z,{},project2
2901,no,A student recommended the section of Chapter 9 in Sutton and Barto that covers feature construction.,2019-10-18T19:07:08Z,32,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1wi8vfpwwr5yy,2019-10-18T19:07:08Z,{},project2
2902,no,"<p>Could someone expand on Chapter 9 / pg. 210? I&#39;ve read it countless times now, and still don&#39;t know how to apply what it discusses.</p>
<p></p>
<p>For example, on pg 210, S&#43;B say you can come up with a polynomial feature vector for state s represented by s1 and s2 by combining them in various ways, eg, (1, s1, s2, s1s2), or (1, s1, s2, s1s2, s1^2, s2^2...). </p>
<p></p>
<p>Going through chapter 9, for a polynomial feature construction, how do we combine come up with the combinations? Random selection? Apply Equation 9.17? If its that equation, how do we pick n for c_i_j? From the last comment in that equation box, &#34;these features make up the order-n polynomial basis for dimension k which contains (n&#43;1)^k features. If I assume n=2 so we have an actual polynomial, that means we&#39;ve got 3^8, or 6561 features, which, seems a bit much for this? </p>
<p></p>
<p>What does the capital pi for equation 9.17 mean? Is it projection, as defined by equation 11.4 on page 268? Or is it just the product of all values in the range of the series?</p>
<p></p>
<p>Which brings me back to my original question, could someone demo a practical example of applying the equations described?</p>",2019-10-18T20:27:41Z,32,Week 10/13 - 10/19,feedback,,is4nx55dinr6wk,k1wl4ge3tc051u,2019-10-18T20:27:41Z,{},project2
2903,no,"<p>&#34;<em>If I assume n=2 so we have an actual polynomial, that means we&#39;ve got 3^8, or 6561 features, which, seems a bit much for this? &#34;</em></p>
<p></p>
<p>I think this is one reason why people use NNs :)</p>
<p></p>
<p>But I don&#39;t think that you necessarily have to use all possible second order polynomials.  Let&#39;s say you knew from the underlying physics of the problem that, say, the product of the horizontal velocity and the angular momentum is a very appropriate metric for making a decision, you could just include that one feature as an addition to the original feature (state) space.</p>
<p></p>
<p>I think one of the &#34;bad&#34; things about feature engineering is that a human has to decide which features are important.  It could also be a &#34;good&#34; thing, if you take the view humans have something to add in terms of domain knowledge.  But with NNs with default inputs, you&#39;re letting the NN come up with the function approximation entirely on its own.</p>",2019-10-18T20:38:10Z,32,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1wlhxmfrpg1uk,2019-10-18T20:38:10Z,{},project2
2904,no,"<p>Yeah...</p>
<p></p>
<p>&#34;...a team of Japanese and Hungarian researchers have shown <i>P. polycephalum</i> can solve the <a href=""https://en.wikipedia.org/wiki/Shortest_path_problem"">Shortest path problem</a>. When grown in a maze with oatmeal at two spots, <i>P. polycephalum</i> retracts from everywhere in the maze, except the shortest route connecting the two food sources. When presented with more than two food sources, <i>P. polycephalum</i> apparently solves a more complicated <a href=""https://en.wikipedia.org/wiki/Transportation_problem"">transportation problem</a>. With more than two sources, the amoeba also produces efficient networks. In a 2010 paper, oatflakes were dispersed to represent <a href=""https://en.wikipedia.org/wiki/Tokyo"">Tokyo</a> and 36 surrounding towns. <i>P. polycephalum</i> created a network similar to the existing train system, and &#34;with comparable efficiency, fault tolerance, and cost&#34;. Similar results have been shown based on road networks in the <a href=""https://en.wikipedia.org/wiki/United_Kingdom"">United Kingdom</a> and the <a href=""https://en.wikipedia.org/wiki/Iberian_peninsula"">Iberian peninsula</a> (i.e., <a href=""https://en.wikipedia.org/wiki/Spain"">Spain</a> and <a href=""https://en.wikipedia.org/wiki/Portugal"">Portugal</a>). Some researchers claim that <i>P. polycephalum</i> is even able to solve the <a href=""https://en.wikipedia.org/wiki/NP-hard"">NP-hard</a> <a href=""https://en.wikipedia.org/wiki/Steiner_minimum_tree"">Steiner minimum tree</a> problem&#34;.</p>
<p></p>
<p>Source: <a href=""https://en.wikipedia.org/wiki/Physarum_polycephalum"">https://en.wikipedia.org/wiki/Physarum_polycephalum</a></p>",2019-10-17T03:04:45Z,33,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1u4fdfka1v22m,2019-10-17T03:04:45Z,{},other
2905,no,"<p>&#64;Vahe I work in transportation and I&#39;ve been sending this article to people who do network design for years. </p>
<p></p>
<p>I always get strange reactions.  I don&#39;t know why.</p>
<p></p>
<p>It&#39;s very cool if it can solve the Steiner tree problem, though!</p>",2019-10-17T03:22:01Z,33,Week 10/13 - 10/19,feedback,,jqmfuaidej9155,k1u51l5kazw281,2019-10-17T03:22:01Z,{},other
2906,no,<p>Thanks for that &#64;Vahe.  This thing is amazing and makes you ponder about a few things.  </p>,2019-10-17T12:04:16Z,33,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1unp78wdvcgj,2019-10-17T12:04:16Z,{},other
2907,no,<p>&#64;Tyler WAHA... I can only imagine the face those transportation people make when they see slime basically being as good at their job ... thx for the laugh</p>,2019-10-19T16:25:14Z,33,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1xrwic13oo2hd,2019-10-19T16:25:14Z,{},other
2908,stud,"<p>I saw this in my news feed, too. I initially thought it was a joke for Halloween.</p>",2019-10-17T10:56:36Z,33,Week 10/13 - 10/19,followup,a_0,,k1ula66j50n6ob,2019-10-17T10:56:36Z,{},other
2909,no,"<p>slime mold is intelligent. ;)  there&#39;s a horror-space movie, with a very interesting premise along those lines. </p>",2019-10-18T00:45:27Z,33,Week 10/13 - 10/19,feedback,,jzivtxcbl6964n,k1vew3hliz82bd,2019-10-18T00:45:27Z,{},other
2910,no,"<p>Very interesting, JP. Thanks for sharing.</p>",2019-10-19T15:09:49Z,33,Week 10/13 - 10/19,followup,,hyx9thiqa6j4nn,k1xp7iwh6lw2s8,2019-10-19T15:09:49Z,{},other
2911,no,"<p>Thx <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p>It makes you wonder what is really learning and intelligence.  For instance, if you cover a maze, keep it vertically, and pour water inside, the water will end up pouring from the other side, but we can&#39;t say water learned how to solve a maze. </p>
<p>Or if you take a very complex pipe structure and try to calculate the shortest path between both exits, you simply put a string in there, pull it to the max, and you got your answer.</p>
<p>What I mean is it could be easy to mistake an emergent property of a system for some sort of learning.</p>
<p>Imagine the blob simply sniffing the pieces of food and trying to reach them all at once, it would be as if every side is &#39;pulling&#39; the &#39;slime&#39;s resources (cells) its way rather than producing new ones, which would result in basically creating an optimal biological structure that maximizes the expected return, like pulling on the string minimizes the length.  It&#39;s really interesting but is that really learning?  The researchers must have thought about this though.  </p>
<p></p>
<p>I remember an article with a guy who had an almost empty brain, with just an outer layer of, say, 1cm thick.  And yet he was functional, with barely any limitations.  So many things we don&#39;t know.  </p>",2019-10-19T16:44:37Z,33,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1xslg2d8b7lp,2019-10-19T16:44:37Z,{},other
2912,no,<p>This is so true... very very interesting thoughts!</p>,2019-10-19T16:57:15Z,33,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xt1oajjo2163,2019-10-19T16:57:15Z,{},other
2913,no,"<p>I&#39;m on Mac too (but Mojave). Seems that Catalina is having some issues, but I&#39;ve googled it and found these:</p>
<p><a href=""https://stackoverflow.com/questions/16682156/ld-library-not-found-for-lgsl"">https://stackoverflow.com/questions/16682156/ld-library-not-found-for-lgsl</a><a href=""https://stackoverflow.com/questions/16682156/ld-library-not-found-for-lgsl""></a></p>
<p><a href=""https://discourse.brew.sh/t/mac-os-10-15-catalina-issues/5001"">https://discourse.brew.sh/t/mac-os-10-15-catalina-issues/5001</a></p>",2019-10-17T04:21:15Z,42,Week 10/13 - 10/19,followup,,is6e83bsfvk,k1u75r6oe5358w,2019-10-17T04:21:15Z,{},project2
2914,stud,"<p>Thx. I just move to Catalina. The HW4 was finished on Mojave and after moving to Catalina, the HW4 code cannot run..</p>
<p></p>
<p>I tried the two links but does not seem to work. </p>",2019-10-17T14:01:18Z,42,Week 10/13 - 10/19,feedback,a_0,,k1urvp2x6at6bg,2019-10-17T14:01:18Z,{},project2
2915,no,"<p>It took me a while to figure out you can install box2d from various sources, there seem to be slightly different versions of if with subtle differences like programmers seem to love to do, and I finally found that what you must do is</p>
<p></p>
<pre>pip3 install gym[box2d]</pre>
<p></p>",2019-10-17T23:57:24Z,42,Week 10/13 - 10/19,followup,,jzh6k6o994a6dh,k1vd6anfa877pa,2019-10-17T23:57:24Z,{},project2
2916,stud,<p>Thank you so much! That works!</p>,2019-10-18T03:32:57Z,42,Week 10/13 - 10/19,feedback,a_0,,k1vkvhz4liw7nu,2019-10-18T03:32:57Z,{},project2
2917,no,"<p>Hey Jean,<br />I tried this but I got an error saying I need to install Visual Studio (VS), but I font want to have to pay for VS just to do the project?!</p>
<p></p>
<pre>error: Microsoft Visual C&#43;&#43; 14.0 is required. Get it with &#34;Microsoft Visual C&#43;&#43; Build Tools&#34;: https://visualstudio.microsoft.com/downloads/</pre>
<p>Don&#39;t we get a free Visual Studio install for being a GaTech Student? If so, does anyone have any details they can send my way.</p>",2019-10-19T19:22:40Z,42,Week 10/13 - 10/19,feedback,,gx3c8l7z7r72zl,k1xy8or33wc3k6,2019-10-19T19:22:40Z,{},project2
2918,no,"<p>You can get Visual Studio 2019 community; it is free and includes what you need, plus a lot more. <a href=""https://visualstudio.microsoft.com/downloads/"">https://visualstudio.microsoft.com/downloads/</a><a href=""https://visualstudio.microsoft.com/downloads/""></a></p>
<p></p>
<p>It is actually asking for just Visual c&#43;&#43; 14.0; (which is the 2015 c&#43;&#43; build tools); but it&#39;s easier to install this as it comes with Visual Studio.</p>",2019-10-19T20:05:17Z,42,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1xzri2cmk52ti,2019-10-19T20:05:17Z,{},project2
2919,no,"<p>Thanks George.</p>
<p></p>
<p>I ended up install VS Enterprise 2019 since we get it for free b/c we are GaTech Students: https://support.cc.gatech.edu/resources/downloads</p>",2019-10-19T20:52:32Z,42,Week 10/13 - 10/19,feedback,,gx3c8l7z7r72zl,k1y1g996ffl1q1,2019-10-19T20:52:32Z,{},project2
2920,no,<p>does anyone was able to get gym box2d  to work on windows 10?</p>,2019-10-20T16:27:49Z,41,Week 10/20 - 10/26,feedback,,jzlwxqswu5m2e7,k1z7fp08mtf38g,2019-10-20T16:27:49Z,{},project2
2921,no,<p>I am getting the C&#43;&#43; error above even though I have installed it.</p>,2019-10-21T01:56:44Z,41,Week 10/20 - 10/26,feedback,,j6jxj5zix3746q,k1zrrb7tg5g7bz,2019-10-21T01:56:44Z,{},project2
2922,no,"<p><a href=""https://stackoverflow.com/questions/29846087/microsoft-visual-c-14-0-is-required-unable-to-find-vcvarsall-bat"">https://stackoverflow.com/questions/29846087/microsoft-visual-c-14-0-is-required-unable-to-find-vcvarsall-bat</a></p>
<p></p>
<p>This helped me out.</p>
<p>I did not check Desktop development with C&#43;&#43; under workload</p>",2019-10-21T02:26:07Z,41,Week 10/20 - 10/26,feedback,,j6jxj5zix3746q,k1zst3jliss749,2019-10-21T02:26:07Z,{},project2
2923,stud,"I am also facing this problem. Got:
cc1plus: error: unrecognized command line option &#34;-arch&#34;
cc1plus: warning: command line option &#34;-Wstrict-prototypes&#34; is valid for C/ObjC but not for C&#43;&#43;
error: command &#39;gcc&#39; failed with exit status 1
I am running in anaconda with python 2.7. Does anyone know how to fix it?",2019-10-26T11:13:28Z,41,Week 10/20 - 10/26,followup,a_1,,k27gujasvp227k,2019-10-26T11:13:28Z,{},project2
2924,no,<p>I personally would rather have it later than sooner if the choice was between rushing the TAs vs letting them enjoy the experience  (heh).</p>,2019-10-17T23:34:56Z,57,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1vcdel1qnq40m,2019-10-17T23:34:56Z,{},other
2925,no,<p>Do you have a stack trace? I am also using Keras in Jupyter without issue.</p>,2019-10-18T01:16:45Z,57,Week 10/13 - 10/19,followup,,jcg0nzvdk8272b,k1vg0c09liz66v,2019-10-18T01:16:45Z,{},project2
2926,no,"<p>No. At the end of the last fitting, it got stuck and doesn&#39;t continue the code. </p>",2019-10-18T01:26:57Z,57,Week 10/13 - 10/19,feedback,,j6ln9puq99s5uv,k1vgdgt1cft5x4,2019-10-18T01:26:57Z,{},project2
2927,no,<p>Are you running locally or in the cloud? If locally you will need to find python interpreter logs which is different depending on what OS you use.</p>,2019-10-18T02:23:46Z,57,Week 10/13 - 10/19,feedback,,jcg0nzvdk8272b,k1viej4qr062m,2019-10-18T02:23:46Z,{},project2
2928,no,<p>I am running locally through anaconda. I am not sure about the Python interpreter logs part</p>,2019-10-18T23:18:23Z,57,Week 10/13 - 10/19,feedback,,j6ln9puq99s5uv,k1wr7yw9ayy2vs,2019-10-18T23:18:23Z,{},project2
2929,no,"<p>I am using the following code to make environment.</p>
<p></p>
<pre>from gym.envs import box2d<br />env = box2d.lunar_lander.LunarLander().unwrapped</pre>
<p>I also see other option of using &#96;&#96;&#96;env = gym.make(&#39;LunarLander-v2&#39;)&#96; to make envision. &#96;&#96;&#96;</p>
<p>Not sure if it matters. In homework 3, I found the second way doesn&#39;t work. </p>",2019-10-18T01:34:53Z,57,Week 10/13 - 10/19,followup,,j6ln9puq99s5uv,k1vgnnlq9vf3wu,2019-10-18T01:34:53Z,{},project2
2930,no,<p>Id recommend the 2nd way and see how that goes. Just set seeds and you should be fine.</p>,2019-10-18T02:23:10Z,57,Week 10/13 - 10/19,feedback,,jcg0nzvdk8272b,k1vidr752el6st,2019-10-18T02:23:10Z,{},project2
2931,no,<p>I&#39;ve had Python exit unexpectedly a few times during training. Checking /var/log/syslog it turns out the training process exceeded the memory permitted for a process by the OS and was killed.</p>,2019-10-18T16:08:40Z,57,Week 10/13 - 10/19,followup,,isde34zracb1mz,k1wbvcs8d092mx,2019-10-18T16:08:40Z,{},project2
2932,no,<p>&#43;1 This has happened to me often when I was keeping score and trace histories in memory instead of dumping them to a file on very long training sessions.</p>,2019-10-18T16:13:06Z,57,Week 10/13 - 10/19,feedback,,jc554vxmyuy3pt,k1wc125kvgr5bw,2019-10-18T16:13:06Z,{},project2
2933,no,"<p>When you say score, do you mean total reward for an episode? Even I have 10 episodes, I see the same issue.</p>
<p></p>
<p>For trace, do you mean the memory where holds a certain number of (states, r, next states, action, done)? If you save and read on every step update, will that be too inefficient? </p>",2019-10-18T23:20:48Z,57,Week 10/13 - 10/19,feedback,,j6ln9puq99s5uv,k1wrb2rvrap5k1,2019-10-18T23:20:48Z,{},project2
2934,no,"<p>Solved!  After using 3 different memory profiling packages I found that every time I called model.fit(X,Y) or model.predict(X), Tensorflow2 permanently used another 8MB of memory...</p>
<p></p>
<p>Instead, calling model.train_on_batch(X,Y) and model.predict_on_batch(X), tensorflow memory usage remains below 2% and as a bonus it also trains 300%&#43; faster...</p>
<p></p>
<p>Thanks to NealKelly &amp; Pappy on Slack for pointing out some of the different tensorflow Model methods available.</p>",2019-10-18T23:23:12Z,57,Week 10/13 - 10/19,followup,,isde34zracb1mz,k1wre66cg6f70m,2019-10-18T23:23:12Z,{},project2
2935,no,"<p>I did the above change and still have the issue. is there a slack channel? </p>
<p></p>",2019-10-18T23:52:00Z,57,Week 10/13 - 10/19,feedback,,j6ln9puq99s5uv,k1wsf7fzfmf2fv,2019-10-18T23:52:00Z,{},project2
2936,no,"<p>There seems to be some discussion about this. I am trying the &#96;conda install nomkl&#96; method. It&#39;s taking a long time to install. </p>
<p>https://github.com/dmlc/xgboost/issues/1715</p>",2019-10-19T03:33:10Z,57,Week 10/13 - 10/19,followup,,j6ln9puq99s5uv,k1x0bmq01l758t,2019-10-19T03:33:10Z,{},project2
2937,no,If you are still having this issue I switched from trying to use conda to running it natively and it was instantly fix and much much faster,2019-10-19T06:51:06Z,57,Week 10/13 - 10/19,followup,,ixpwxv7xdgi1u6,k1x7e6hcida6t6,2019-10-19T06:51:06Z,{},project2
2938,no,"<p>Hi Zhenning,</p>
<p>I also have same question #2 from your original post. Did you find an answer on that  yet ?</p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>",2019-10-19T18:05:28Z,57,Week 10/13 - 10/19,followup,,jfzaqnqvtQ1m,k1xvheyc6w44d,2019-10-19T18:05:28Z,{},project2
2939,no,"<p>Well, I don&#39;t have an official answer from the instructors. My understanding is to fee the model.fit() using batch instead of loop. Batch processing should be much efficient. And this is what I plan to use. </p>",2019-10-20T06:16:27Z,56,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k1yllgibmft4ug,2019-10-20T06:16:27Z,{},project2
2940,no,"<p>Yes, batch processing is the way to go. Still working on getting higher score though.</p>",2019-10-20T16:11:46Z,56,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k1z6v1j86ly2kr,2019-10-20T16:11:46Z,{},project2
2941,no,"I was having trouble passing in the entire batch to the fit method, so I was looping through each of the states and calling fit () but I am still struggling with getting things working with it right now. ",2019-10-19T18:09:11Z,57,Week 10/13 - 10/19,followup,,ixpwxv7xdgi1u6,k1xvm6lw4nn278,2019-10-19T18:09:11Z,{},project2
2942,no,<p>Check out my post above about using train_on_batch() instead of fit().  Same goes for predictions.</p>,2019-10-19T18:20:27Z,57,Week 10/13 - 10/19,feedback,,isde34zracb1mz,k1xw0ok6w5m2gh,2019-10-19T18:20:27Z,{},project2
2943,no,"<p>&#34;The current version of Tensorflow installed via pip uses the AVX instruction set at compile time.This means that your CPU needs to support the AVX instruction set. This instruction set is supported from the second generation of Intel Core CPUs (codenamed SandyBridge). You can compile a Tensorflow from the source that does not use the AVX instruction set. Or find an already compiled one on the internet.&#34;</p>
<p></p>
<p><a href=""https://github.com/tensorflow/tensorflow/issues/24548"">https://github.com/tensorflow/tensorflow/issues/24548</a></p>",2019-10-19T00:58:12Z,50,Week 10/13 - 10/19,followup,,jzfsa4a37jf4aq,k1wuscdcs4b3p1,2019-10-19T00:58:12Z,{},project2
2944,no,"<p>&#39;a real pain&#39; just like you said it would be</p>
<p>but thx</p>",2019-10-19T01:21:36Z,50,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1wvmfqot407h9,2019-10-19T01:21:36Z,{},project2
2945,no,"<p>Yeah.  I actually installed CUDA to try using my GPU for training, but it turns out that, for this set of problems, it&#39;s slower than my CPU.  So that was a wasted effort, although it&#39;s cool to have that option in the future.</p>
<p></p>
<p>But if PyTorch works, then you&#39;re set, right?</p>",2019-10-19T01:26:19Z,50,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1wvshngo2o47b,2019-10-19T01:26:19Z,{},project2
2946,no,"<p>Lucky you, I have an AMD RX580 which is a great card but I expect more pain in the future because it&#39;s not Nvidia.</p>
<p>There was another guy who also tried his nvidia and came to the same conclusion.</p>
<p>I&#39;m following Miguel&#39;s book for the moment because it explains things really clearly, a lifesaver for a noob like me, and he uses pytorch so I should be good, yeah, thx, although I&#39;m pretty sure this avx problem too will come bite me some time down the road.</p>
<p></p>
<div>
<div></div>
</div>",2019-10-19T02:07:53Z,50,Week 10/13 - 10/19,feedback,,jzh6k6o994a6dh,k1wx9y3lcfv3xm,2019-10-19T02:07:53Z,{},project2
2947,no,"<p>in the office hours video from yesterday, Chris mentioned that a couple thousand episodes should be 20-30 minutes with updating every 4 steps; another student mentioned 2000 episodes takes about 1 hr; I am seeing similar results as well (2k ~1hr). but have seen variations based on the hyper params.</p>",2019-10-19T03:53:08Z,33,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1x11ao6ymn7nq,2019-10-19T03:53:08Z,{},project2
2948,no,"<p>Thanks. When you say updating every 4 step, do you mean not do model.fit() for each step? In the deep mind paper, it looks like it updates the model every step. I notice one issue is that the epsilon quickly decays to a minimum number because 1 episode can take hundreds of steps. However, 1 episode doesn&#39;t really make the agent learn much. </p>",2019-10-19T05:06:36Z,33,Week 10/13 - 10/19,feedback,,j6ln9puq99s5uv,k1x3nru3tzu7jt,2019-10-19T05:06:36Z,{},project2
2949,no,"<p>I&#39;m not 100% sure, since I don&#39;t do that; but that would be my guess as well. Chris might chime in to clarify.</p>",2019-10-19T05:09:54Z,33,Week 10/13 - 10/19,feedback,,ixty1midfufhd,k1x3s0sqixs29c,2019-10-19T05:09:54Z,{},project2
2950,no,"<p>Zhenning, that&#39;s exactly right, but you can counter that by decaying epsilon more slowly.  In the end, it&#39;s just another hyperparameter you have to tune.</p>",2019-10-19T05:33:06Z,33,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1x4lvaxfz45ky,2019-10-19T05:33:06Z,{},project2
2951,no,<p>I did some profiling and find that most of the time is spent on interacting with the gym environment. So I am not sure how much time can be saved from skipping training the NN. But I am using pretty simple networks. </p>,2019-10-19T16:08:38Z,33,Week 10/13 - 10/19,feedback,,jzhs489b10i78c,k1xrb63k5cy1mh,2019-10-19T16:08:38Z,{},project2
2952,no,"<p>It also semi-depends on your epsilon decay speed, in the sense that the &#34;better&#34; your selection of hyperparmeters and algorithm/modeling, the less episodes agent needs to explore and thus you can decay epsilon faster and solve the environment in less episodes.</p>",2019-10-19T19:03:36Z,33,Week 10/13 - 10/19,feedback,,jzhwdil7ssn443,k1xxk67toce7km,2019-10-19T19:03:36Z,{},project2
2953,no,"<p>Not sure how large the network people choose, I choose around 128 dim for dense layers. It&#39;s take couple of hours to train. I do see the agent is improving. However, the improvement at episode is small. </p>",2019-10-20T20:23:58Z,32,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k1zfvddh47j5eu,2019-10-20T20:23:58Z,{},project2
2954,no,"<p>I&#39;m getting good results with some 3000 episodes, which takes 15 minutes in my machine. I&#39;m using Keras and found that you can fit and predict on the entire batch at once, which makes things faster. Also I use the Theano backend, because Tensorflow was slower and prints many warnings. </p>",2019-10-20T03:31:32Z,33,Week 10/13 - 10/19,followup,,is8ald0uljj3u4,k1yfpdbu55d26k,2019-10-20T03:31:32Z,{},project2
2955,no,"<p>Hey Alejandro, can you point me to a good resource for figuring out how to fit on the entire batch at once?</p>",2019-10-20T18:34:04Z,32,Week 10/20 - 10/26,feedback,,jl3we43d3bp15p,k1zby1ir9tp7fr,2019-10-20T18:34:04Z,{},project2
2956,no,"<p>You can look at Keras document for model.train_on_batch(), or model.fit()</p>",2019-10-20T20:22:20Z,32,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k1zfta4kksn315,2019-10-20T20:22:20Z,{},project2
2957,no,"<p>Again the number of episodes it takes to converge is arbitrary because everyone have different updating schedule. Here is my output. I have been tweaking around the hyper parameter and all they end up pretty similar. I haven&#39;t waited hours to see one agent to converge. But based current improving speed in 10 min, I feel the agent needs improvement definitely. Can someone who solved it under 1 hour comment on this? Thanks. </p>
<pre>episode: 142/500, total time: 5.73 min, score: -34.20, average score: -82.69, steps: 365, epsilon: 0.22<br />episode: 143/500, total time: 6.05 min, score: -5.61, average score: -81.75, steps: 999, epsilon: 0.21<br />episode: 144/500, total time: 6.38 min, score: 4.97, average score: -80.99, steps: 999, epsilon: 0.20<br />episode: 145/500, total time: 6.71 min, score: 55.80, average score: -78.91, steps: 999, epsilon: 0.19<br />episode: 146/500, total time: 7.06 min, score: -53.66, average score: -77.98, steps: 999, epsilon: 0.18<br />episode: 147/500, total time: 7.32 min, score: -201.70, average score: -78.86, steps: 741, epsilon: 0.17<br />episode: 148/500, total time: 7.68 min, score: -45.95, average score: -78.72, steps: 999, epsilon: 0.16<br />episode: 149/500, total time: 8.05 min, score: -9.68, average score: -78.03, steps: 999, epsilon: 0.15<br />episode: 150/500, total time: 8.43 min, score: 26.84, average score: -76.69, steps: 999, epsilon: 0.15<br />episode: 151/500, total time: 8.82 min, score: 19.34, average score: -75.05, steps: 999, epsilon: 0.14<br />episode: 152/500, total time: 9.26 min, score: -28.27, average score: -73.77, steps: 999, epsilon: 0.13<br />episode: 153/500, total time: 9.69 min, score: -38.63, average score: -73.10, steps: 999, epsilon: 0.13<br />episode: 154/500, total time: 10.11 min, score: 8.54, average score: -71.05, steps: 999, epsilon: 0.12 </pre>
<p></p>",2019-10-22T22:02:21Z,32,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k22e9lxi99c7j,2019-10-22T22:02:21Z,{},project2
2958,no,"<p>I think the major issue that, in later episode, the agent is hovering over the landing spot and doesn&#39;t land. They stay here until the max steps. This is making the agent very slow and not learning. </p>",2019-10-22T22:03:56Z,32,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k22ebmlzqmp42h,2019-10-22T22:03:56Z,{},project2
2959,no,"<p>I think it depends on a lot of things, but there was another post that mentioned 6,000 to 10,000 was the number of episodes needed to see 200. I am also in that range.</p>",2019-10-19T05:03:12Z,30,Week 10/13 - 10/19,followup,,ixty1midfufhd,k1x3jepssng4x6,2019-10-19T05:03:12Z,{},project2
2960,no,<p>On my machine 1000 episodes would take approx 5hrs. Is this type of duration what is expected? I do not have a GPU and running the code from a virtual box. I am using tensorflow so I would guess that it is fast...?</p>,2019-10-19T06:13:17Z,30,Week 10/13 - 10/19,followup,,j6ll2xkiDJf,k1x61je35r26jp,2019-10-19T06:13:17Z,{},project2
2961,no,"No, that&#39;s rather slow and a gpu does not help in this case. Be sure that there are no for loops in your update; it should all be vectorized.",2019-10-19T06:15:03Z,30,Week 10/13 - 10/19,feedback,,i4op5p9vfbq5yz,k1x63st6ayb6x8,2019-10-19T06:15:03Z,{},project2
2962,no,"<p>Thanks, It would take me some time to fully vectorize it. For most part it is...</p>
<p>I will run it over night and see if I can hit the 200 mark..</p>",2019-10-19T06:18:33Z,30,Week 10/13 - 10/19,feedback,,j6ll2xkiDJf,k1x68axnazw14b,2019-10-19T06:18:33Z,{},project2
2963,no,<p>Just curious why a GPU doesn&#39;t help in this case. Is it because we&#39;re only training smallish batch sizes at a time?</p>,2019-10-19T15:25:36Z,30,Week 10/13 - 10/19,feedback,,jl1b27fpaYkv,k1xprtq5ph1ku,2019-10-19T15:25:36Z,{},project2
2964,no,"<p>It should help with the training part, but the major bottleneck on a non-vectorized implementation is not the GPU, but the for loops. So, it &#34;doesn&#39;t help&#34; to get a GPU if you have not vectorized your code.</p>
<p></p>
<p>If you are implementing an algorithm such as A3C, then GPUs, perhaps, don&#39;t help at all but for different reasons. In DQN-style methods, you can expect some performance speedups with a GPU.</p>",2019-10-19T15:44:29Z,30,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xqg3lpy3ta,2019-10-19T15:44:29Z,{},project2
2965,no,"<p>I have tried running my DQN using a GPU and a CPU. By the end, I&#39;m sticking with the CPU because it was a little bit faster. I googled an found out when the NN is too small, the overhead of sending information to the GPU and getting it back is not worth.</p>",2019-10-20T04:22:11Z,30,Week 10/13 - 10/19,feedback,,jc6xvgjncoey,k1yhiiuoejo1s1,2019-10-20T04:22:11Z,{},project2
2966,no,"<p>Yeah, my setup is a Mac Book Pro 2019, but I have an external GPU (AMD Radeon Vega 56). I tried running my DQN with the GPU by using PlaidML, but for some reason the performance was far slower than on my CPU. My code is all vectorized.</p>
<p></p>
<p>I&#39;m not sure if the intense slowdown is something wrong with PlaidML or if it is really that slow because the NN is small.</p>",2019-10-20T16:19:11Z,29,Week 10/20 - 10/26,feedback,,jl1b27fpaYkv,k1z74ktfsil2f6,2019-10-20T16:19:11Z,{},project2
2967,no,<p>I have a vectorized code and using a GPU made it worst. Using the cpu only improves by few hours. It took me 20 hours to train my NN. See &#64;704</p>,2019-10-21T05:30:13Z,29,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzdumjxs110v,2019-10-21T05:30:13Z,{},project2
2968,no,<p>20 hours is a bit much for Lunar Lander.</p>,2019-10-26T00:35:16Z,29,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k26u1t1bu742f7,2019-10-26T00:35:16Z,{},project2
2969,no,"<p>After approx 750 episodes, my lunarlander decides to hover for ever and not land. What do you think that is due to? (it seems that the episode forced terminates after approx 1000 steps resulting in episode return that is over - (minus) 1000)</p>",2019-10-20T01:23:19Z,30,Week 10/13 - 10/19,followup,,j6ll2xkiDJf,k1yb4hzeoou63w,2019-10-20T01:23:19Z,{},project2
2970,no,<p>It&#39;s found a local optimum.</p>,2019-10-20T01:27:49Z,30,Week 10/13 - 10/19,feedback,,jzfsa4a37jf4aq,k1ybaa6ik8i1hm,2019-10-20T01:27:49Z,{},project2
2971,no,"<p>With the lunar lander, the negative reward of crashing is high, then is it possible that the lunar lander is afraid of landing and hovers?</p>",2019-10-20T03:30:41Z,30,Week 10/13 - 10/19,feedback,,j6ll2xkiDJf,k1yfoacdl017jy,2019-10-20T03:30:41Z,{},project2
2972,no,"<p>Yes, that&#39;s possible if the agent has only seen negative rewards.</p>",2019-10-20T03:42:08Z,30,Week 10/13 - 10/19,feedback,,jqknbi6c0lHt,k1yg304w8rzcj,2019-10-20T03:42:08Z,{},project2
2973,no,"<p>assuming that the lunar lander is afraid of landing since it received a substantially high negative reward, does it learn by it&#39;s own to overcome it&#39;s fears by doing more and more episodes? or it is time for tuning the hyper parameters? I doubt it would learn from hovering around? what do you think? or your experience has been with the project?</p>",2019-10-21T02:04:34Z,29,Week 10/20 - 10/26,feedback,,j6ll2xkiDJf,k1zs1do92du3lh,2019-10-21T02:04:34Z,{},project2
2974,no,"<p>i guess my DQN is the same. You will have to hope for epsilon to kick in for it to see that landing is actually good. With enough of such experience, it will be able to move out of that local optimum. </p>",2019-10-21T05:28:12Z,29,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzb9u814u54i,2019-10-21T05:28:12Z,{},project2
2975,no,"<p>I have the same issue. I see it hoving over the yellow flags right above the landing spot and doesn&#39;t land. Tried a bunch of parameters. There are so many hyper parameters, not sure how to tune them to achieve a smart agent within a short training time (preferably within one hour)</p>",2019-10-21T06:22:14Z,29,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k2018qrweo773x,2019-10-21T06:22:14Z,{},project2
2976,no,<p>I am also in the same boat. Consider forcing do nothing action after a threshold of runs to make it land. Is it a legit way to solve this issue?</p>,2019-10-21T06:38:11Z,29,Week 10/20 - 10/26,feedback,,jzhrun61eu91l9,k201t96vsf93e7,2019-10-21T06:38:11Z,{},project2
2977,no,"<p>Chung-Kai, I do not think that what you propose is legit. The beauty of RL is that one avoids putting if, else statements in the code based on environment as the agent should learn from doing on its own. One of the TAs may be able to confirm...</p>",2019-10-21T08:07:22Z,29,Week 10/20 - 10/26,feedback,,j6ll2xkiDJf,k204zyad2bl3fy,2019-10-21T08:07:22Z,{},project2
2978,no,"<p>yes, explicitly forcing actions is not be required. You can achieve the same by tuning the right hyperparameters. Think about whether your agent has actually seen landing in its experience? If it did see the landing, whether is it being learned by the agent? if not, what params can be tuned?</p>",2019-10-21T18:01:19Z,29,Week 10/20 - 10/26,feedback,,jqknbi6c0lHt,k20q7rwccx85g9,2019-10-21T18:01:19Z,{},project2
2979,no,"<p>Thanks! I feel that this portion will take extremely long. I&#39;ve solved the problem with DQN and the agent takes about an hour to train. Every varying hyperparm produces another training curve, and thus adds a lot of runtime to execute these experiments.</p>",2019-10-19T15:16:19Z,57,Week 10/13 - 10/19,followup,,jl1b27fpaYkv,k1xpfvunkoyba,2019-10-19T15:16:19Z,{},project2
2980,no,"<p>I agree. But most of your grade comes from analyzing your implementation, not just implementing and solving. Many hyperparameter sets can work, and we want to make sure you understand how to tune your algorithm.</p>
<p></p>
<p>Most people can implement and solve LL, but to truly prove you understand your implementation, your analysis of the problem, algorithms, hyperparameters, etc. is vital.</p>
<p></p>",2019-10-19T15:50:30Z,57,Week 10/13 - 10/19,feedback,,hyx9thiqa6j4nn,k1xqnu3zghx4ve,2019-10-19T15:50:30Z,{},project2
2981,no,"Okay. I think was doing some sort of informal tuning by guessing params based on intuition until I got it working. <div><br /></div><div>I actually missed we needed the hyperparam graphs until the end!</div><div><br /></div><div>Anyway, thanks for the input; I’ll work on running some more of these experiments formally to get some graphs.</div>",2019-10-19T22:20:13Z,57,Week 10/13 - 10/19,feedback,,jl1b27fpaYkv,k1y4l0swg4t72a,2019-10-19T22:20:13Z,{},project2
2982,no,"<p>here a 3D animation of the training times for the QQ learner that I did (which solved the lander problem). This is ms of training time on a single core of an Intel Core i7-7800X (3.5Ghz) with lots of memory. 1000 episodes of training per configuration was performed.  This experiment produced 1.8GB of collateral materials (Q dumps, discretization performance, telemetry).</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk22dwlrgf2kk%2Ftraintime_movie.gif"" alt="""" /></p>",2019-10-22T21:53:20Z,56,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k22dy0eor2h2dr,2019-10-22T21:53:20Z,{},project2
2983,stud,"<p><em>&#34;I am having a very difficult time understanding how neural networks are tied to Q</em> learning.&#34;</p>
<p></p>
<p>They&#39;re (i.e., neural networks) not <em>really</em> &#34;tied to Q learning&#34;. At least, not as such. They&#39;re just one particular way of learning a <em>parametric form </em>of the Q fuction. When Q was tabular (i.e., when we pretty much literally had a big ole&#39; table of dimension $$\mathcal{S} \times \mathcal{A}$$ as our Q-fucntion) we didn&#39;t need to parameterize Q. Actions were discrete, states were discrete, and life was good. </p>
<p></p>
<p>In project 2, we <em>will</em> need to parameterize the Q function (unless you&#39;re discretizing the state space). The parameters (or weights) are used to generate predictions respecting the Q-values of state-action pairs. But how do we figure out what those weight values should be in order to ensure we wind up with generally good results? </p>
<p></p>
<p>That&#39;s where the NN comes in. NN are little more than very powerful <em>general function approximators</em>. I would just treat the NN as a black box (it more or less <em>is</em> anyway). What the NN does is allow us to learn what those weight values should be, (we hope) easily and efficiently.</p>
<p></p>
<p>Now, we can approximate pretty much <em>any</em> function we want with a suitable NN (there are, I think, certain &#34;pathological&#34; cases for which they won&#39;t work, but it shouldn&#39;t matter here). What we&#39;re doing here is using a NN to approximate the Q-function for the lunar lander problem. The Q-function really is <em>just another function</em>, so this in (in principle) is not an unreasonable thing to do. Though there <em>are</em> some differences in the typical assumptions made regarding the learning environment in supervised learning (where NN were first used) and the learning environment in RL (where we&#39;d like to use it) that can cause trouble for us, there are some things you can do here to work around those differences. The recommended paper by Minh, et al. has the details. </p>
<p></p>
<p>If we let $$Q(s, a) \approx f(x, w)$$, where $$x$$ represents whatever quantities we&#39;re actually trying to <em>learn</em> from, then what the NN is doing is allowing us to learn a value (or set of values) for $$w$$ that minimizes <em>some measure of loss</em> with respect to the function $$f$$ (which, again, is the parametric form of our Q-function). That&#39;s it. </p>
<p></p>
<p>So you needn&#39;t even necessarily <em>use</em> a NN here. There are other function approximators that you might use instead. But the basic idea will be more or less the same: you parameterize the Q-function, &#34;plug-in&#34; some suitable function approximator, and use that to tune the parameters of the Q-function until you see the results you&#39;re after. </p>
<p></p>
<p>Hope this helps!</p>",2019-10-20T22:27:40Z,25,Week 10/20 - 10/26,followup,a_0,,k1zkag4tics50s,2019-10-20T22:27:40Z,{},project2
2984,no,"<p>The above answers do a fine job explaining why neural networks are useful in the context of RL and touch on how they work, but if you want a better visual and intuitive understanding of exactly what happens inside a neural network I highly recommend this playground: </p>
<p></p>
<p><a href=""https://playground.tensorflow.org/"">https://playground.tensorflow.org/</a></p>
<p></p>
<p>Spend 10-15 minutes playing with that and look at the different behaviors a neural net is able to accomplish for different problems.</p>
<p></p>
<p>Specifically, see how well you can get a neural network doing on the spiral problem. (One of the more difficult in the playground). Try adding a few more nodes, try adding or removing layers, check out the visualizations of each node and the final outputs.</p>",2019-10-21T14:39:54Z,25,Week 10/20 - 10/26,followup,,isde332xcka1m0,k20j0qxalfy6vw,2019-10-21T14:39:54Z,{},project2
2985,no,"<p>Bonus questions to try:</p>
<p>- what&#39;s the smallest network you can get to work on each problem?</p>
<p>- what&#39;s the largest? Does there appear to be a trade off in learning time?</p>
<p>- does it seem to help more to add more nodes in earlier or later layers for these problems?</p>
<p></p>
<p>Play around and see! I found the neural network playground really helpful for developing various intuitions of how they work.</p>",2019-10-21T14:54:35Z,25,Week 10/20 - 10/26,feedback,,isde332xcka1m0,k20jjn8fr4m3bo,2019-10-21T14:54:35Z,{},project2
2986,no,<p>It is a very nice platform to play around. Thanks for the reference!</p>,2019-10-21T17:02:44Z,25,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k20o4fpi5v06fw,2019-10-21T17:02:44Z,{},project2
2987,no,"<p>Michael, thank you for this...  This is awesome.</p>",2019-10-21T17:17:21Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k20on8l96jf1yr,2019-10-21T17:17:21Z,{},project2
2988,no,<p>Yep this is really cool -- thanks for sharing it.</p>,2019-10-27T03:32:34Z,25,Week 10/20 - 10/26,feedback,,is9so9huTMp,k28fto1wqka22h,2019-10-27T03:32:34Z,{},project2
2989,no,"<p>Thank you all very much for your in-depth answers, I am light-years ahead of where I was in my understanding of how NNs can be used to solve this. However, while I now have a good conceptual grasp of the implementation of NNs to approximate functions, I don&#39;t think I&#39;ll be able to learn the nuts and bolts of tensorflow or keras before the project is due. Does anyone know of any good resources for learning to program with either?</p>
<p></p>
<p>Thanks again!</p>",2019-10-21T15:31:57Z,25,Week 10/20 - 10/26,followup,,jqmjg3txqw6ji,k20kvotzy5j4pg,2019-10-21T15:31:57Z,{},project2
2990,stud,"<p><em>&#34;However, while I now have a good conceptual grasp of the implementation of NNs to approximate functions, I don&#39;t think I&#39;ll be able to learn the nuts and bolts of tensorflow or keras before the project is due. Does anyone know of any good resources for learning to program with either?&#34;</em></p>
<p><em></em></p>
<p>That&#39;s probably true, but I wouldn&#39;t worry about it (i.e., learning the &#34;nuts and bolts of tensorflow or keras&#34;). You shouldn&#39;t need to. We&#39;re supposed to be responsible for writing our own code for the <em>RL portion </em>of the algorithm <em>only</em>. I&#39;ll let instructors correct me if I&#39;m wrong, but that shouldn&#39;t include the code to generate the actual NN. <em>That</em> code you should just be able to grab from the web. You should no doubt <em>credit</em> the author of said code, but otherwise you should just be able to use it to approximate <em>your</em> function (which you should be defining for yourself, based on the particular problem you&#39;ve been asked to solve.)</p>
<p></p>
<p>There are <em>tons</em> of examples of working networks that use Keras with tensorflow as a backend out there. There are even examples of networks being applied to <em>other</em> RL problems.  I would take a look at those resources (which can, in general, be found with a quick Google search), for starters. For example, there are tons of Medium articles which will walk you through using Keras to apply a NN to the problem of approximating the Q-function on <em>other</em> RL problems. I would start there.</p>
<p></p>
<p>Btw, Keras is essentially a &#34;wrapper&#34; for Tf, from what I understand. Tf (and to a lesser extent, pyTorch, which is also a wrapper for Tf) is really <em>low-level</em>. You&#39;d be hard pressed to learn it on the fly. That&#39;s one reason why Keras is probably the way to go here. It provides a library of high-level methods for constructing and training NNs. So if you&#39;re using Keras (I am), you can pretty much ignore the nuts and bolts of Tf. You just need to install Tf (which can be a bit of a headache, or at least was for me), import it (only once per session/console if you&#39;re using an iPython console/Spyder, otherwise it throws errors because it creates &#39;singletons&#39; every time you import it, and that is evidently bad if it happens more than once per session), and then you can pretty much use Keras exclusively. Keras will manage all the nuts and bolts stuff for you, so you won&#39;t have to do much with Tf directly (I tried to, at first. Then failed. Then failed some more. Now I&#39;m using Keras to avoid having to think about Tf directly). </p>",2019-10-21T15:55:06Z,25,Week 10/20 - 10/26,feedback,a_0,,k20lpgib2tv2y2,2019-10-21T15:55:06Z,{},project2
2991,no,<p>Oh interesting! If this is true it would help out significantly. Can any TAs confirm?</p>,2019-10-21T16:52:58Z,25,Week 10/20 - 10/26,feedback,,jqmjg3txqw6ji,k20nrvfybvn11a,2019-10-21T16:52:58Z,{},project2
2992,stud,"<p>I would like clarifiaction on this too, come to think of it. But my remarks above were based largely on the instructor responses in &#64;584 and &#64;697. In particular, I&#39;m not sure why it would be okay to use someone else&#39;s implementation of k-means and not ok to use/adapt someone else&#39;s NN. The latter has about as much to do with the RL portion of the algorithm as the former does. </p>",2019-10-21T17:10:56Z,25,Week 10/20 - 10/26,feedback,a_0,,k20oezqh915wi,2019-10-21T17:10:56Z,{},project2
2993,stud,<p>&#64;512 likewise contains an instructor response which seems conistent with the above. </p>,2019-10-21T17:20:51Z,25,Week 10/20 - 10/26,feedback,a_0,,k20orqww4cq6yb,2019-10-21T17:20:51Z,{},project2
2994,no,"<p>Someone gave me this <a href=""https://www.3blue1brown.com/neural-networks"" target=""_blank"" rel=""noopener noreferrer"">link</a>, and it made all the difference.  </p>
<p>The graphics are insane, they must have spent weeks to produce those videos, but they clearly show how NN work, how gradients are used to update the weights (not that it matters here, it&#39;s all done by pytorch or tensorflow, but it helps in terms of understanding what&#39;s under the hood).  </p>
<p>I was in the same boat as you but all you got to do is find some code that builds and update a NN, that&#39;s allowed, and focus on the RL part, ie fine-tuning the parameters, the decays etc.  </p>
<p>Miguel gave us once some <a href=""https://github.com/mimoralea/applied-reinforcement-learning/tree/master/notebooks"" target=""_blank"" rel=""noopener noreferrer"">jupyter notebooks</a>.  Number 6 has a q-learning algo with function approximaton using NN &#43; memory replay.  </p>
<div>
<div></div>
</div>",2019-10-21T23:23:39Z,25,Week 10/20 - 10/26,followup,,jzh6k6o994a6dh,k211qb0nf4n1w5,2019-10-21T23:23:39Z,{},project2
2995,no,"<p>Hi Jean,</p>
<p>Did Miguel personally give these jupyter notebooks for us to refer ? Is there a post for this ? </p>
<p></p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>
<p></p>",2019-10-22T06:33:22Z,25,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k21h2x3s9a960z,2019-10-22T06:33:22Z,{},project2
2996,no,"<p>Hi &#64;Anurag, he mentions them in &#64;202.  Do a search on mimoralea, and you&#39;ll find more occurences.  He also has notebooks for his book on github.com/mimoralea/gdrl.  </p>",2019-10-22T09:32:11Z,25,Week 10/20 - 10/26,feedback,,jzh6k6o994a6dh,k21ngvzde9l5hd,2019-10-22T09:32:11Z,{},project2
2997,no,<p>3Blue1Brown is awesome! I watched all his NN videos before trying to implement them.</p>,2019-10-23T21:23:08Z,25,Week 10/20 - 10/26,feedback,,jl2zkad6fs661b,k23sb0n1sch4i5,2019-10-23T21:23:08Z,{},project2
2998,no,"<p>I want to follow up on the example that Miguel gave us on Cartpole. If we have the states and actions:</p>
<p>$$s = [ 0.01158656, -0.04601671, 0.02787309, 0.03230222],  a = [0, 1]$$</p>
<p>We could also have the action as an extra dimension for the Q-network, ie:</p>
<p>$$ [0.01158656,−0.04601671,0.02787309,0.03230222, 0])→[34.1351] $$</p>
<p>$$ [0.01158656,−0.04601671,0.02787309,0.03230222, 1])→[−14.113]$$</p>
<p></p>
<p>However, as noted in Mihn 2015, they choose to have the Q-network outputs all possible actions, in one run through the network, in order to be efficient with forward passing (not sure if they mean for training or just for matrix calculating). So we have what Miguel give us:</p>
<p>$$[0.01158656,−0.04601671,0.02787309,0.03230222])→[34.1351,−14.113]$$</p>
<p></p>
<p>But, for the training process, we only take one action of the same time. For example we are at state $$s$$, take action 0, and receive 2 reward. So our training target is now $$[36.1351, -14.113]$$. If we train to fit our model to that target, not only we calibrating toward new receive value $$36.1351$$, we also calibrate toward $$-14.113$$, which could be an unknown or random value. So my question is if that is what we intend to happen, and trust that over time we will converge to the true value (reward values)? In the training process, there is no other parameter to tell the network to give more weight to new received value.</p>",2019-10-22T01:29:58Z,25,Week 10/20 - 10/26,followup,,jl5wq8mca7o0,k2168qmsd7p24b,2019-10-22T01:29:58Z,{},project2
2999,no,"<p>No, that is <em>not</em> what we want to happen.  We only want to update <em>one</em> Q value, the one for the particular state-action pair that the transition started at, just like in the regular Q-learning algorithm.</p>",2019-10-22T01:43:47Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k216qinc3k4452,2019-10-22T01:43:47Z,{},project2
3000,no,"<p>Thanks Vahe, this is the part I try to understand and try to implement correctly. In the all-actions architecture, the prediction coming out of the Q-network will have the form of a vector with size of n_actions. How do we separate the training to fit on just one action, instead of all the actions?</p>
<p></p>
<p>And on the opposite thought, can we argue this is actually what we want? We want to update our network weight base on the new info we just receive, but also in respect to the old info we already have. We wouldn&#39;t want to overfit our weight just to the new info.</p>
<p></p>
<p>I am trying to see which one makes more sense.</p>",2019-10-22T02:15:56Z,25,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k217vv822obuz,2019-10-22T02:15:56Z,{},project2
3001,no,"<p>Let&#39;s say that we were originally in state $$S$$.  We chose an action, $$A_0$$, according to our policy (usually $$\epsilon$$-greedy), from a choice of $$4$$ actions: $$A_0, A_1, A_2,$$ and $$A_3$$.  Now we take a step in our environment by executing action $$A_0$$.  Our environment then tells us that we are now in state $$S&#39;$$.</p>
<p></p>
<p>What do we do now?</p>
<p></p>
<p>What we want to do is update the Q-value of the pair $$(S,A_0)$$, the state we were just in and the action we just took, to a new, improved value based on the Q-learning update rule.  How do we do that with a neural network?  We need to first find the target value $$\Big{(}\text{arg}\max_{a&#39;}Q(S&#39;,a&#39;)\Big{)}$$, then we need to use that value to update $$Q(S,A_0).$$</p>
<p></p>
<p>If we used that entire vector, as you suggested, then we would be not only updating $$Q(S,A_0)$$, but we&#39;d also be updating $$Q(S,A_1), Q(S,A_2),$$ and $$Q(S,A_3)$$.  Those are three state-action pairs that we did not take.  It&#39;s possible that those other actions don&#39;t even lead to state $$S&#39;$$!  How can we use a Q-value from state $$S&#39;$$ to update them?</p>
<p></p>
<p>&#34;<em>How do we separate the training to fit on just one action, instead of all the actions?&#34;</em></p>
<p></p>
<p>If we are taking the $$\text{arg}\max$$ as in the Q-learning algorithm, then our target won&#39;t be a vector, it will just be a scalar.   How can we form a vector out of that scalar, such that when we do our .fit(), we don&#39;t adjust the weights of $$Q(S,A_1), Q(S,A_2),$$ and $$Q(S,A_3)$$?</p>",2019-10-22T02:56:45Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k219ccda4v4en,2019-10-22T02:56:45Z,{},project2
3002,no,"<p><em>How can we form a vector out of that scalar, such that when we do our .fit(), we don&#39;t adjust the weights of Q(S,A1),Q(S,A2), and Q(S,A3)?</em></p>
<p><em></em></p>
<p>This is the magic sauce that still doesn&#39;t make sense for me, I think I am in a writers block :/ </p>",2019-10-27T09:28:23Z,25,Week 10/20 - 10/26,feedback,,is5x0kzrzjpg7,k28sj8xyccj3o6,2019-10-27T09:28:23Z,{},project2
3003,no,"<p>You&#39;re in state $$S$$, take action $$A$$, and get sent to state $$S&#39;$$.  You can access the vector $$Q(S)$$ from your neural network, which includes $$Q$$-values not just for $$A$$, but for the other $$3$$ actions that were possible.</p>
<p>If the target vector you form is different from $$Q(S)$$ only in the component corresponding to $$A$$, then the loss that is computed by .fit() for the other components will be zero.  So we just need to form that target vector - identical to $$Q(S)$$ in all components but the component corresponding to $$A$$.</p>",2019-10-27T15:03:56Z,24,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k294irvzzd0zc,2019-10-27T15:03:56Z,{},project2
3004,no,"<p>Yep, cheers mate I figured it out after taking a nap and having a coffee!</p>",2019-10-27T17:52:08Z,24,Week 10/27 - 11/2,feedback,,is5x0kzrzjpg7,k29aj2l3i2j2fq,2019-10-27T17:52:08Z,{},project2
3005,no,<p>Great!</p>,2019-10-27T17:58:58Z,24,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k29arv72d9u28g,2019-10-27T17:58:58Z,{},project2
3006,no,<p>Good video that explicitly explains DQN&#39;s: https://www.youtube.com/watch?v=wrBUkpiRvCA</p>,2019-10-22T15:03:05Z,25,Week 10/20 - 10/26,followup,,gx3c8l7z7r72zl,k21zaezf57b3td,2019-10-22T15:03:05Z,{},project2
3007,no,"<p>My training process sounds similar to yours. When you finally get a well trained agent, usually on how many steps it will land? I find my agent is already pretty good at ~200 episode, however, it still takes 3-5 minutes to train and each episode takes about 500 steps. </p>",2019-10-21T05:14:41Z,26,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k1zytw16pzi61f,2019-10-21T05:14:41Z,{},project2
3008,no,"<p>It seems like others are able to train their agents very quickly. But not for me. I noticed that keras is not utilizing my cpu completely as well. As for my trained agent, my single network DQN performs on 6/ 100 new games as follows: </p>
<p></p>
<p>&#96;&#96;&#96;</p>
<p>Episode 0 | Total steps 609 |Total reward 226.67468085754115<br /> 1%|▊ | 1/100 [00:01&lt;02:22, 1.44s/it]<br /> start episode</p>
<p>Episode 1 | Total steps 519 |Total reward 174.24563061439932<br /> 2%|█▋ | 2/100 [00:02&lt;02:04, 1.27s/it]<br /> start episode</p>
<p>Episode 2 | Total steps 307 |Total reward 273.3856742873806<br /> 3%|██▍ | 3/100 [00:02&lt;01:39, 1.02s/it]<br /> start episode</p>
<p>Episode 3 | Total steps 999 |Total reward 139.18746284220225<br /> 4%|███▎ | 4/100 [00:04&lt;01:59, 1.24s/it]<br /> start episode</p>
<p>Episode 4 | Total steps 472 |Total reward 189.40083828086443<br /> 5%|████ | 5/100 [00:05&lt;01:44, 1.10s/it]<br /> start episode</p>
<p>Episode 5 | Total steps 554 |Total reward 260.1954034074119<br /> 6%|████▉ | 6/100 [00:06&lt;01:37, 1.03s/it]<br /> start episode</p>
<p>Episode 6 | Total steps 260 |Total reward 279.24363674074664<br /> 7%|█████▋ | 7/100 [00:06&lt;01:16, 1.22it/s]<br /> start episode</p>
<p>&#96;&#96;&#96;</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-10-21T05:21:22Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zz2hfwvjk4lu,2019-10-21T05:21:22Z,{},project2
3009,no,"<p>Our input is our state, so it is a dimension size of 8. Even with hundreds of units, the NN only has N*8 &#43; N number of parameters on its first HL for example. So its actually small </p>",2019-10-21T05:23:15Z,26,Week 10/20 - 10/26,followup,,jvfpllmsggt7p4,k1zz4wftcja6ej,2019-10-21T05:23:15Z,{},project2
3010,no,"<p>That&#39;s not right.  It has N*8 &#43; N*N &#43; N*4 parameters if you have two hidden layers with N units each, 8 inputs, and 4 outputs.</p>",2019-10-21T05:27:28Z,26,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k1zzab66w9xqa,2019-10-21T05:27:28Z,{},project2
3011,no,<p>edit:  i am speaking only on a single layer. The first dense layer is a N*8 &#43; N (weight &#43; bias)  # of param </p>,2019-10-21T05:31:48Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzfwdi5tf4t9,2019-10-21T05:31:48Z,{},project2
3012,no,"Note: I am no Keras expert so YMMV.

This happened to me. I got fed up with it and swapped over to PyTorch where I can make sure GPUs are being utilized. TF2.0 has a bug right now with evaluation not unrolling on multiple device setups (ex: <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver"" target=""_blank"" rel=""noopener noreferrer"">this resolver</a>). When fit is called many times through every step you effectively lose all the perf gains of the resolver. So I was getting zero perf improvement with Keras &#43; GPU/TPU until I dropped down into TF directly to make sure everything was being executed on graph. See this:
<a href=""https://www.tensorflow.org/guide/distributed_training#onedevicestrategy"">https://www.tensorflow.org/guide/distributed_training#onedevicestrategy</a> . While this does work, it is a huge hassle and makes for weird debugging. Looking back, I would have not spent the time on this had I known PyTorch was effectively doing the same thing but with way less headache (imo).

For comparison [both for 1k episodes]
Keras &#43; GPU -&gt; 2.5 hours
PyTorch optimized -&gt; 17 min

Also: Did you follow these steps? <a href=""https://www.tensorflow.org/install/gpu"">https://www.tensorflow.org/install/gpu</a> windows may have some specific setup outside the norm, esp with cuda drivers.",2019-10-21T05:28:15Z,26,Week 10/20 - 10/26,followup,,jcg0nzvdk8272b,k1zzbbzj1d7567,2019-10-21T05:28:15Z,{},project2
3013,no,<p>i am doing train on batch instead of fit in my code :/</p>,2019-10-21T05:31:22Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzfc7simd49x,2019-10-21T05:31:22Z,{},project2
3014,no,"Hmm, but you are still calling fit? or train_on_batch? Even when you pass a batch size, if you do this every step it will hit this issue. But if you are only calling fit every (BATCH) number of steps you might be ok.",2019-10-21T05:34:02Z,26,Week 10/20 - 10/26,feedback,,jcg0nzvdk8272b,k1zzir92m2n7f8,2019-10-21T05:34:02Z,{},project2
3015,no,"<p>no. I do not use fit. train_on_batch is what i do. But its still 20 hours </p>
<p></p>
<p>Edit: you said it took you 2.5 hours for 1000 episodes. Are you updating your NN every step? </p>",2019-10-21T05:35:19Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzkf1atxxt2,2019-10-21T05:35:19Z,{},project2
3016,no,<p>For what it&#39;s worth my 5-year-old desktop takes about two hours to run 1000 episodes of DQN on its CPU with the update logic fully vectorized. (Keras/Tensorflow)</p>,2019-10-21T05:36:59Z,26,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k1zzmk2uhdv7ez,2019-10-21T05:36:59Z,{},project2
3017,no,"<p style=""text-align:left"">it is really weird. My update logic is vectorized as well. Only for loop is looping for the environment.... What OS are you running it on?</p>",2019-10-21T05:38:51Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzoytvynx657,2019-10-21T05:38:51Z,{},project2
3018,no,Yea that was with calling fit every step which is not ideal. I am running on Linux and OSX with same results.,2019-10-21T05:39:01Z,26,Week 10/20 - 10/26,feedback,,jcg0nzvdk8272b,k1zzp6iu3eo1p8,2019-10-21T05:39:01Z,{},project2
3019,no,<p>Yes seems like people with linux / Mac are doing fine. Not sure if it is a windows 10 thing</p>,2019-10-21T05:40:22Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzqwu3vlf7ob,2019-10-21T05:40:22Z,{},project2
3020,no,Maybe try on a AWS/GCE instance for a few min? But yea getting Keras to make use of resources is not as straightforward as the docs make it seem haha.,2019-10-21T05:41:28Z,26,Week 10/20 - 10/26,feedback,,jcg0nzvdk8272b,k1zzsbx9ud63zi,2019-10-21T05:41:28Z,{},project2
3021,no,<p>Windows 10.  Did you try rendering to watch an episode?  And try not activating the memory replay / update logic and see if it&#39;s zippy.</p>,2019-10-21T05:43:36Z,26,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k1zzv2ak7y93f9,2019-10-21T05:43:36Z,{},project2
3022,no,<p>thanks for the suggestions. I just want to find out if other users have the same issues (especially on windows) </p>,2019-10-21T05:44:08Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k1zzvqw8qrgt7,2019-10-21T05:44:08Z,{},project2
3023,no,"<p>Germayne,</p>
<p></p>
<p>I&#39;m not sure if you are still running into this problem. I can train my DQN with memory replay in 1h or so for 2000 epsides in windows 10 using CPU only.</p>
<p></p>
<p>I would recommend to use a timer between some methods to find out where the process is taking too long. I would guess is the fit method for Keras/Tensorflow. Maybe your minibatch is too big... Try reducing the minibatch size and check the impact.</p>",2019-10-21T15:15:13Z,26,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k20ka5zya156mt,2019-10-21T15:15:13Z,{},project2
3024,no,"<p>&#64;danilo  Glad to see you are able to train in a short time. Some question for you,</p>
<p>1) on average, how many steps do you need for each episode?</p>
<p>2) Do you update nn on each step?</p>
<p>3) how big is your memory size?</p>
<p>Thanks. </p>",2019-10-22T01:00:36Z,26,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k2156z80ljc5oo,2019-10-22T01:00:36Z,{},project2
3025,no,https://github.com/rkern/line_profiler great tool btw. &#64;profile any function you need and you will find your bottleneck.,2019-10-22T04:59:17Z,26,Week 10/20 - 10/26,feedback,,jcg0nzvdk8272b,k21dpx8vctbkd,2019-10-22T04:59:17Z,{},project2
3026,no,<p>Thank you all for the suggestions. Switching to train_on_batch() significantly sped up my training process when using Keras with a Tensorflow backend on MacOS. Would recommend smaller batch sizes as well to speed up training.</p>,2019-10-22T08:11:31Z,26,Week 10/20 - 10/26,feedback,,jnjgrn6usm9x,k21kl52tgu62tp,2019-10-22T08:11:31Z,{},project2
3027,no,"Why is it not ideal to update NN after every step? It’s expensive, but this is what the paper implies AFAICT. <p></p>",2019-10-23T21:16:21Z,26,Week 10/20 - 10/26,feedback,,i4jbttw9ru63ot,k23s2axtzrp7iz,2019-10-23T21:16:21Z,{},project2
3028,no,"<p>Yup, you&#39;re right.  This ends up being another hyperparameter.  You can deviate from the paper and potentially get worse (or better) results.</p>",2019-10-23T22:52:59Z,26,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23vik89yru7hi,2019-10-23T22:52:59Z,{},project2
3029,no,"<p>You might want to check the number of steps your agent is taking each episode.</p>
<p></p>
<p>On my partially vectorized implementation, it takes &lt; 1min to cover the first 100 episodes but 10 times as much time to cover episodes 101 - 200. I realized that my agent was getting good at hovering and hence subsequent episodes got dragged longer. </p>",2019-10-21T13:00:48Z,26,Week 10/20 - 10/26,followup,,ijctp4ucNy8,k20fhaxq2lz4a3,2019-10-21T13:00:48Z,{},project2
3030,no,"<p>From my previous log, it takes 1 sec for me from episode 1-200? Then towards the back, each episode of training takes about 3-5 mins </p>",2019-10-22T03:36:13Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k21ar3ixk6h6gd,2019-10-22T03:36:13Z,{},project2
3031,no,"<p>The episodes in the beginning, albeit short, still take on the order of 100 time steps.  That means that a max-length episode should only take 10 times as long as a short episode (so 10 seconds).</p>
<p></p>
<p>This sounds like one of two things.  Either a computer-specific problem.  Or... you don&#39;t start your memory replay for the first 200 episodes, and therefore aren&#39;t doing the whole update loop when the episodes are fast.  Then as soon as the update part of your code kicks in, things slow down to a crawl.</p>",2019-10-22T03:42:27Z,26,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k21az43k6fd3nv,2019-10-22T03:42:27Z,{},project2
3032,no,"<p>After tried many hyper parameters, I still don&#39;t have a good agent converging within one hour of training. Here is something from my log. </p>
<pre>episode: 269/500, score: -98.26, average score: -79.22, steps: 999, epsilon: 0.30
episode: 270/500, score: -38.79, average score: -79.48, steps: 999, epsilon: 0.29
episode: 271/500, score: -56.07, average score: -79.66, steps: 999, epsilon: 0.28
episode: 272/500, score: -41.47, average score: -77.55, steps: 999, epsilon: 0.28
episode: 273/500, score: 38.37, average score: -76.29, steps: 999, epsilon: 0.27
episode: 274/500, score: -3.89, average score: -75.71, steps: 999, epsilon: 0.26
episode: 275/500, score: -37.01, average score: -75.13, steps: 999, epsilon: 0.26
episode: 276/500, score: -83.30, average score: -75.29, steps: 999, epsilon: 0.25</pre>
<p>Just a couple of question for discussion.</p>
<p>1. If the epsilon is very small, like 0.01 (epsilon_min), will that mean the agent is unlikely to explore and will use Q function to find the best action. At that point, should my train close to convergence? </p>
<p>2. In the average score (latest 100 runs), the increasing is very slow (as you can see above). Do you think it&#39;s because the agent is not actively learning? I saw that the agent end up hovering over the landing spot until max steps. I am decaying the epsilon very slowly. But it doesn&#39;t seem to improve the agent learning. </p>
<p></p>
<p>I spent hours of training and couldn&#39;t find the right hyper parameters. I adjusted neural network size (layer, # of neurons), learning rate, epsilon decaying speed, weight updating every N steps. I would appreciate if someone can provide a guidance to diagnose the problem so that I can adjust the hyper parameters in the right direction. Otherwise, it seems very difficult to find a good agent especially each training taking hours/days. Thanks</p>
<p></p>",2019-10-22T17:36:44Z,26,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k224s0qlf2v3yr,2019-10-22T17:36:44Z,{},project2
3033,no,"Note that many people take 400&#43; episodes to converge. Your loss looks consistent with what others have, as long as average is decreasing over every 5-10 episodes you should be fine. But maybe try tweaking alpha? Remember a larger alpha = less train time but less likely to find true optima. &amp; vice versa

The network does not need to be very complicated and generally smaller networks will solve LL just fine.",2019-10-22T18:02:25Z,26,Week 10/20 - 10/26,feedback,,jcg0nzvdk8272b,k225p1hgvab1zp,2019-10-22T18:02:25Z,{},project2
3034,no,"<p>Thanks for the input. I tried to tune learning rate as well and it seems to too many hyper parameters to tune. The episode number is not the key here. because if I use a small network (2 layer with less than 100 neurons), it can quickly run each episode. However, the learning is not fast enough. I am using a larger network and hopefully to get the agent learn faster and converge in a shorter time. </p>",2019-10-22T21:29:44Z,26,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k22d3n9jbi451,2019-10-22T21:29:44Z,{},project2
3035,no,"<p>looks close to my single network DQN. I am still at -ve at episode 300&#43;. you got to let you train further. As long as your average reward is improving it is correct. eventually positive numbers will come in. I let mine train overnight. </p>
<p></p>
<p>learning rate should ideally be small. you need to take small steps </p>",2019-10-23T10:46:18Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k235k1rcawt5iu,2019-10-23T10:46:18Z,{},project2
3036,no,"<p>Having similar issues as well even though my agent managed to solve cartpolev0 and cartpolev1 in only 50 episodes or so. My network only has 2 hidden layers of 32 neurons each. </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fijctp4ucNy8%2Fk2388n9wsjae%2Ftest1.JPG"" alt="""" /></p>",2019-10-23T12:01:34Z,26,Week 10/20 - 10/26,feedback,,ijctp4ucNy8,k2388u7e6if2lx,2019-10-23T12:01:34Z,{},project2
3037,no,<p>try to make your NN bigger. :) (i.e more units) </p>,2019-10-24T05:36:10Z,26,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k249x25mlh097,2019-10-24T05:36:10Z,{},project2
3038,no,"<p>Thanks, I will give it a look. However, looking at related comments in piazza, it seems like NN size is not a very strong factor for Lunar Lander.</p>",2019-10-24T13:07:01Z,26,Week 10/20 - 10/26,feedback,,ijctp4ucNy8,k24q0uopvh46ev,2019-10-24T13:07:01Z,{},project2
3039,no,"<p>It is likely not a strong factor, however, it needs to be sufficiently complex to solve the algorithm. The exact meaning of sufficiently is part of the report.</p>",2019-10-25T01:33:14Z,26,Week 10/20 - 10/26,feedback,,ixty1midfufhd,k25goidr75i613,2019-10-25T01:33:14Z,{},project2
3040,no,"<p>Question about graph 2 for project 2: </p>
<p></p>
<p>Assignment says make a graph of the reward per trial for 100 trials using your trained agent.  Should this graph be similar to graph 1 where it&#39;s just the reward received for each trial or should it be the average over 100 trials at each trial?  If it&#39;s the latter, then it&#39;s basically just a graph of a straight line...</p>",2019-10-23T12:53:52Z,56,Week 10/20 - 10/26,followup,,jqqssjikI511,k23a43nr7tk75f,2019-10-23T12:53:52Z,{},office_hours
3041,no,"<p>The former, but don&#39;t feel limited. Feel free to make it as informative as possible. Maybe the average isn&#39;t interesting but the variance is, etc.</p>",2019-10-25T01:09:42Z,56,Week 10/20 - 10/26,feedback,,i4op5p9vfbq5yz,k25fu8moo4q531,2019-10-25T01:09:42Z,{},office_hours
3042,no,"<p>I hope we can discuss follow things (mostly about DQN):</p>
<p>- What is the overall effect of the activation method on each layer? What difference will it make on the network, if I use a linear activation function vs exponential activation function? What different does it make to place them on the hidden layer vs output layer?</p>
<p>- Hypothetically, if we want a &#34;tap out&#34; feature, ie: if the agent sees a state too strange or unfamiliar with, it should try not to engage into it rather than approximate it. How would that idea could be implement? (not necessary in context of project 2)</p>",2019-10-24T19:51:20Z,56,Week 10/20 - 10/26,followup,,jl5wq8mca7o0,k254gtenj0q6kt,2019-10-24T19:51:20Z,{},office_hours
3043,no,<p>Answered around 6:10</p>,2019-10-25T01:17:26Z,56,Week 10/20 - 10/26,feedback,,i4op5p9vfbq5yz,k25g46ai81h1l2,2019-10-25T01:17:26Z,{},office_hours
3044,no,<p>I have to output that number to give you an exact answer but I saw it while rendering my training. It would be mid-air then terminate to the next episode. It occurred about 3/50 times of me observing </p>,2019-10-22T00:15:54Z,29,Week 10/20 - 10/26,followup,,j6m1jeidndu6wq,k213lhwqt84y3,2019-10-22T00:15:54Z,{},project2
3045,no,"<p>Yeah, that&#39;s normal.  There&#39;s a 1000 time-step limit.  You could probably get rid of that by &#34;unwrapping&#34; the environment when you instantiate it, but I don&#39;t think you need to.  You&#39;ll receive a Done signal when you a) land b) crash c) exit the legal grid area or d) hit 1000 time steps.</p>",2019-10-22T00:18:18Z,29,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k213okoyghx5de,2019-10-22T00:18:18Z,{},project2
3046,no,"Also a trick you can do is at every iteration evaluate your agent using the greedy policy (epsilon=0), append the total reward into an array. If your agent meets the threshold you can eval again under 100 trials to test the stability of your agent. If the avg reward over the 100 meets the goal you can stop training... This 1 &#43; 100 is to avoid running eval(100) at every iteration which may become slow...",2019-10-22T03:31:44Z,33,Week 10/20 - 10/26,followup,,is6e83bsfvk,k21alc4l1426d9,2019-10-22T03:31:44Z,{},project2
3047,no,"<p>What I do is just store the rewards for all episodes into an array, then compute <em>all</em> of the trailing means, over your entire run, <em>after</em> training is over.  If even one 100-episode trailing mean was over the threshold, then you&#39;ve solved it (regardless of what happened afterward, e.g. your agent over-trained and got worse, or you never had a good solution to begin with and got lucky).</p>",2019-10-22T03:36:57Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k21as1sw92u3q5,2019-10-22T03:36:57Z,{},project2
3048,no,"<p>Thanks Vahe/Guilherme. </p>
<p></p>
<p>Here&#39;s what I do:</p>
<p>1. create a deque of maxlen=100 for episode rewards.</p>
<p>2. Calculate mean at every episode and add to this episode rewards deque. So, I get average of last 100 runs.</p>
<p>3. Check if mean of this episode rewards (last 100 episodes) is &gt; 200 and solve if it is.</p>
<p></p>
<p></p>
<p>Does this sound correct ? You mentioned above that you calculate trailing means after training is over. Do you consider training over on your first 200 score ? And start calculating the trailing means for next 100 episodes to be all greater than 200 ?</p>
<p></p>",2019-10-22T04:47:00Z,33,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k21da4qrowr3yr,2019-10-22T04:47:00Z,{},project2
3049,no,"<p>I&#39;ll assume you&#39;re talking about Lunar Lander, which has a requirement of 200 for solving it.  Deciding when to end training depends on what your objective is.  If you just want to record a &#34;solve,&#34; then I don&#39;t see why you couldn&#39;t just end training right there, as you say.  But there may be other things to train for rather than just getting that solve.  For instance, you may want to build an agent that&#39;s good enough to repeatedly solve the environment, over and over.  Or you may want to build an agent that averages something higher than 200.  It all depends on what you&#39;re trying to do.</p>",2019-10-22T05:01:42Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k21dt1kdqi33fb,2019-10-22T05:01:42Z,{},project2
3050,no,"<p>Appreciate your prompt responses here !</p>
<p></p>
<p>At this point, I am only talking about Lunar Lander :)</p>
<p></p>
<p>I want to initially record a valid &#39;solve&#39; and have project requirements accomplished. I can work on making LL real-world ready later ;)</p>
<p></p>
<p>I was just trying to find out if I reach first 200, can I say optimal Q function is achieved and I can stop NN training ? And just next 100 without training to score average of 200.</p>
<p></p>",2019-10-22T05:07:30Z,33,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k21e0hsh5sm3bl,2019-10-22T05:07:30Z,{},project2
3051,no,"<p>You don&#39;t need to stop training to satisfy the project requirements.  You could record your first 200, continue to train, and even if your agent never records a trailing mean of 200 ever again, you&#39;ve still solved Lunar Lander because of that one trailing mean of 200 that you already observed.  Continuing, or ending, training, doesn&#39;t change that.</p>
<p></p>
<p>And as you pointed out, just because an agent received a 200, doesn&#39;t mean it will continue to receive 200s - stopping training doesn&#39;t make your agent magically better. </p>
<p></p>
<p>If you did end training right as you received that 200, and handed the weights of your neural network to someone else, and he runs your agent, he may not see that 200 if your agent was a marginal performer.  So again, there is no real reason, in my opinion, to stop training as soon as you hit your first 200.</p>",2019-10-22T05:21:47Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k21eiurvjmd1ez,2019-10-22T05:21:47Z,{},project2
3052,no,"<p>Got it.</p>
<p></p>
<p>So, I can say the problem is solved on first score of 200 on average of 100 consecutive runs and may be run another N runs to still continue getting score over 200 to make the agent &#39;perform&#39; better.</p>",2019-10-22T05:41:13Z,33,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k21f7v2ogfv52r,2019-10-22T05:41:13Z,{},project2
3053,no,<p>I think what Vahe is describing is same as stopping the moment you see a 200 score for last 100 episodes rewards average. </p>,2019-10-24T00:22:37Z,33,Week 10/20 - 10/26,feedback,,jqrr36mqfm8M,k23ypue35ul57c,2019-10-24T00:22:37Z,{},project2
3054,no,"<p>Yeah, I think we&#39;re all saying the same thing.</p>
<p></p>
<p>Although, now that I&#39;ve been reading some of the TAs&#39; posts, it seems that we&#39;re supposed to stop training and stop exploring when doing the official 100-consecutive-run test.  So I may train, save the weights of my neural network, then reload them and do the official test separately...  I&#39;m not sure if my original approach (just train and record a &gt; 200 run while training) would count, though I really think it should.</p>",2019-10-24T00:27:24Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23yvzbuiy8f4,2019-10-24T00:27:24Z,{},project2
3055,no,"<p>Yea, the TAs mentioned this in a few posts and in office hours as well. I also noticed my &#34;training moving average&#34; didn&#39;t always correspond to my &#34;evaluation moving average&#34; after turning off the learning and epsilon... and caused some more tweaking. Miguel mentioned that every so many episodes that he would stop the learning and run evaluations with epsilon set to 0 to use as check points for where the agent is. Ref &#64;718, &#64;755, and office hours Oct 12.</p>",2019-10-24T01:25:43Z,33,Week 10/20 - 10/26,feedback,,ixty1midfufhd,k240yz9ah32626,2019-10-24T01:25:43Z,{},project2
3056,no,<p>Thanks George.  I&#39;ll play around with that.</p>,2019-10-24T01:52:26Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k241xcbqh0317,2019-10-24T01:52:26Z,{},project2
3057,no,<p>Thanks George. You saved the day today!</p>,2019-10-24T07:06:51Z,33,Week 10/20 - 10/26,feedback,,jfzaqnqvtQ1m,k24d5ooorzt1d2,2019-10-24T07:06:51Z,{},project2
3058,no,"<p>Do a grid search over your parameters to see how they influence the answer. Consider how much exploration you are doing in the problem and adjust your exploration parameters. Start with no epsilon or alpha decay and see what happens. The decay complicates the problem until you understand the intuition about why you would do decay.</p>
<p></p>
<p>My generalization solution is highly sensitive to exploration at the start. If I start the sim with final hyperparam conditions (low alpha, low epsilon), then I get a horrible solution (mean &lt; -400). If I give it good start conditions, then it starts to hit solutions that are 200&#43; ....</p>
<p></p>
<p></p>",2019-10-22T13:56:49Z,33,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k21wx6x4iuw3fh,2019-10-22T13:56:49Z,{},project2
3059,no,"<p>This exact situation floored me for about a week. My first recommendation is that you vectorise EVERYTHING POSSIBLE so that you can run more experiments in less time. I went with the &#34;copy the Atari paper closely and then try to scale back to as simple a model as possible&#34; approach, but I&#39;m not convinced it was the best approach to be honest.</p>",2019-10-22T14:15:31Z,33,Week 10/20 - 10/26,followup,,jl3oi5v7qkSk,k21xl913het2ci,2019-10-22T14:15:31Z,{},project2
3060,no,"<p>If you got cart-pole to work, I would say you are close. Using the same hyper parameters of cart-pole with lunar lander, gave me horrible results, averaging to -400 even at about 5000 episodes</p>
<p></p>
<p>So to solve Lunar Lander, focus on tweaking the hyper parameters. Small changes here had a big impact</p>
<p></p>
<p>Note: I did not have to make major structural changes to get Lunar Lander to work </p>",2019-10-22T15:52:55Z,33,Week 10/20 - 10/26,followup,,jqrr36mqfm8M,k2212hvkgv14n1,2019-10-22T15:52:55Z,{},project2
3061,no,<p>I&#39;ve vectorized everything and there is a clear difference but I&#39;m still taking out 20 min for 300 episodes. How long is it taking everyone to get through training and for how many episodes? Are you adjusting your batch size? Are you decaying for each step or each episode? </p>,2019-10-22T16:42:42Z,33,Week 10/20 - 10/26,followup,,j6m1jeidndu6wq,k222uiozztr5t3,2019-10-22T16:42:42Z,{},project2
3062,no,"<p>CartPole i took about 10-15 mins or less to converge, maybe 600 episodes. </p>
<p></p>
<p>Lunar Lander takes about 3700 episodes, maybe close to 200 to 240 mins</p>",2019-10-22T17:02:32Z,33,Week 10/20 - 10/26,feedback,,jqrr36mqfm8M,k223k112rpw70j,2019-10-22T17:02:32Z,{},project2
3063,no,"<p>My generalizer (4xSGD with a shaper) takes 5 minutes to run 500 episodes. It took 12 minutes to run 1500 episodes with about 20% good landings. The 200-240 minutes metric is solid. I think my generalizer would take about 5 hours of training to get a solid 200 average.</p>
<p></p>
<p>Convergence criteria is a key component of your &#34;20 minutes for 300 episodes.&#34; If you relax the convergence then you will get faster results, but less accurate. Maybe less accurate is better at first?? Maybe that&#39;s a tuning parameter ...</p>",2019-10-22T17:18:27Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k2244icqonz277,2019-10-22T17:18:27Z,{},project2
3064,no,<p>My cart-pole took about 30 episodes to converge. Not sure if that is alarming or not. Jacob how are you updating your rewards for the actions you took? I&#39;ve vectorized everything but I need to index by the actions I took to update. Also by 4xSGD are you saying you did 4 epochs for a fit?</p>,2019-10-22T17:23:06Z,33,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k224ah06jto4ar,2019-10-22T17:23:06Z,{},project2
3065,no,"<p>I run 4 SGDs, one per action. I am using sklearn&#39;s intrinsic SGDRegressor functions, so I leave the vectorization up to sklearn.</p>
<p></p>
<p>I have a shaper that converts the (monster) tuple into a (nice) tuple of more interesting data. The shaper is mostly irrelevant from what i can tell.</p>
<p></p>
<p>so when I do the &#34;weight updates,&#34; I am just pushing the value targets into the SGD to do partial fits. The SGD does its own updates that it sees fit to match the target Q value that I want.</p>
<p></p>
<p>I use RBFs for my estimator. So the final &#34;learning input&#34; is RBFs(scalerizer(shaper(phi)))</p>
<p></p>
<p>Then my SGD Q-state just predicts on that RBF output on a per-action basis, or over all of the actions.</p>
<p></p>",2019-10-22T17:39:01Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k224uy0og722vv,2019-10-22T17:39:01Z,{},project2
3066,no,<p>what&#39;s the best way to vectorize the solution for a DQN? I&#39;m new to python and trying to squeeze more performance out of my algorithm. </p>,2019-10-25T04:10:12Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k25mad3vxbt261,2019-10-25T04:10:12Z,{},project2
3067,no,"<p>Vectorization is basically the method of removing a &#39;for&#39; in your calculations by using vectors in your math. I would say I&#39;m good at explaining it but Andrew Ng is much better:</p>
<p></p>
<p><a href=""https://www.youtube.com/watch?v=qsIrQi0fzbY&amp;list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&amp;index=17"">https://www.youtube.com/watch?v=qsIrQi0fzbY&amp;list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&amp;index=17</a></p>
<p></p>
<p>Hope this helps</p>",2019-10-25T22:25:00Z,33,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k26pea4lx0m1ex,2019-10-25T22:25:00Z,{},project2
3068,no,"<p>Not if you are doing $$\epsilon$$-greedy action selection. You need to evaluate using greedy only, otherwise you are still evaluating on random actions, which is not your learning model.</p>",2019-10-22T14:23:04Z,41,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k21xuybmit45wc,2019-10-22T14:23:04Z,{},project2
3069,no,"<p>But <em>why</em> do I need to evaluate it using greedy only? If my agent does well enough with ϵ &gt; 0, then why would I need to run different trials?</p>",2019-10-22T14:25:44Z,41,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k21xydny2svjp,2019-10-22T14:25:44Z,{},project2
3070,no,"<p>Because your agent&#39;s learning model is ONLY to use the Q(s,a) state, not random. We train the Q(s,a) state using random actions to ensure that we explore all possible states. That way, when the evaluation occurs, and it has a weird state, we have a higher likelihood of having trained on that state...</p>",2019-10-22T14:29:12Z,41,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k21y2u7uwy95v2,2019-10-22T14:29:12Z,{},project2
3071,no,"<p>I&#39;m not sure I agree. My understanding is that in adversarial games, having some randomness in an agent&#39;s model is actually beneficial, as it stops the opponent from being able to calculate your next move. I would likewise argue that my agent does have randomness in its learning model, as it always gives it a small chance of stumbling across a better path, which is beneficial.</p>",2019-10-22T14:35:44Z,41,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k21yb8xu18z49k,2019-10-22T14:35:44Z,{},project2
3072,no,<p>That sounds like something you can argue in your paper.</p>,2019-10-22T14:38:28Z,41,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k21yer4gmxr1nk,2019-10-22T14:38:28Z,{},project2
3073,no,"<p>There are two different points here I think.  One is whether you&#39;re allowed to have exploration as fart of a &#39;final policy&#39; that would be deployed.  I don&#39;t see why not.  If you were deploying an agent that performed better, or sufficiently, while exploring, then you should be allowed to deploy it.</p>
<p></p>
<p>The other point is whether you are still learning when you deploy your agent.  You can have an $$\epsilon$$-greedy policy, but be doing no updates (hence no more learning). </p>
<p></p>
<p>These two things are independent. I think the latter is what is meant by a &#34;trained agent.&#34;  Even then though, it may be permissible to submit charts while a suitably trained agent is still being trained.  I don&#39;t know.</p>",2019-10-22T14:49:01Z,41,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k21ysc2sxxm9q,2019-10-22T14:49:01Z,{},project2
3074,no,"<p>Huh, I actually hadn&#39;t even considered the &#34;stopped learning&#34; version of trained ($$\alpha = 0$$ rather than $$\epsilon = 0$$). I&#39;m hoping agents that are still learning can be considered trained as it simplified graphs somewhat...</p>",2019-10-22T14:54:57Z,41,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k21yzywhvso1pu,2019-10-22T14:54:57Z,{},project2
3075,no,"<p>These questions were also discussed in the Oct 12th office hours. The following two points were my takeaways:</p>
<p>- evaluate the agent with epsilon set to 0;</p>
<p>- should not be learning when evaluating.</p>
<p></p>
<p>These two points to me meant that the epsilon for the greedy policy should be 0, (i.e. always using the policy), and we should not update our learner by learning (i.e. adjusting network weights), when we are evaluating. For me, this meant that my action selection used 0 epsilon, and I never called fit on my neural network during the evaluation runs.</p>
<p></p>
<p>The instructor&#39;s answer to this post says you can do either way, as long as it&#39;s justified, and since this is the most recent post, likely overrides the office hours mention.</p>
<p></p>",2019-10-24T01:02:17Z,41,Week 10/20 - 10/26,feedback,,ixty1midfufhd,k2404u7cpej4e1,2019-10-24T01:02:17Z,{},project2
3076,no,"<p>let me clarify this, sorry for any confusion. There exist problems where evaluating the agent with epsilon not set to 0 is ok. The same for continuous learning. Justifications should be in context to the problem, simplying graphs is not a valid justification. So the question becomes, which is best (or is there a best) for this type of problem and why?</p>",2019-10-24T05:11:10Z,41,Week 10/20 - 10/26,feedback,,jl1acpoc4HA9,k2490x5mjapc,2019-10-24T05:11:10Z,{},project2
3077,no,"<p>What?? So I can&#39;t just say &#34;Farrukh said it was ok, so I left it on.&#34; ?? ahhhh now I have to rewrite my paper again!</p>",2019-10-24T15:00:54Z,41,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k24u3b8p3gs4lf,2019-10-24T15:00:54Z,{},project2
3078,no,"<p>I did a lot of tweaking of the agent parameters experimenting with Cart-Pole. Since, Lunar Lander takes at least 3 hrs to converge for me, I am not sure, I can redo all that with Lunar Lander</p>
<p></p>
<p>While majority of the analysis will come from Lunar Lander, can some of my analysis use Cart-Pole? </p>",2019-10-22T15:57:03Z,33,Week 10/20 - 10/26,followup,,jqrr36mqfm8M,k2217tlgbau442,2019-10-22T15:57:03Z,{},project2
3079,no,"<p>Hi Anad, I can&#39;t speak on what you can/can&#39;t since its a TA answer. What I may suggest is to: (while on CartPole) </p>
<p>1) Can run these hyperparameter combinations in a consistent way so that you collect data.</p>
<p>2) Ensure you have the infra to collect data (episode rewards, steps, etc) and be able to plot charts about it.</p>
<p></p>
<p>Then you have some tooling enough to analyze results and build a report. Another thing I&#39;d try is to tune parameters to speed up Lunar Lander training, with some parameter combination and lucky seeds I was able to train an agent within 6 mins. Obviously this was the best of all cases, but in general I&#39;ve seen convergence within 1 hour. In my experiments I&#39;ve specified a time limit for training. If the agent doesn&#39;t converge say within 1 hour training is stopped and it moves on to the next experiment (eg. the next alpha / epsilon value).</p>
<p></p>",2019-10-22T17:58:57Z,33,Week 10/20 - 10/26,feedback,,is6e83bsfvk,k225kkzomlb36l,2019-10-22T17:58:57Z,{},project2
3080,no,<p>Can someone clarify what <strong>lambda </strong>refers to in this project? I couldn&#39;t find it. I am using DQN. Thanks</p>,2019-10-23T23:08:05Z,33,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k23w1zm8y8p549,2019-10-23T23:08:05Z,{},project2
3081,no,"<p>It doesn&#39;t relate to this problem unless you are doing a variant of eligibility tracing or TD. For instance, $$Q(\sigma,\lambda)$$.</p>",2019-10-23T23:14:33Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23waauxlvv2qx,2019-10-23T23:14:33Z,{},project2
3082,no,<p>I see. thanks. </p>,2019-10-23T23:50:20Z,33,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k23xkbu5vmy58s,2019-10-23T23:50:20Z,{},project2
3083,no,"<p>Thanks for the confirmation. 250 is a high score, but there can still have a few bad landings in 100 tests, so the trained model is actually useless if we are considering a practical application. </p>
<p></p>
<p>Could we say it is a limitation of neural networks? <strong>Is it&#39;s accuracy a hard limitation</strong>, even if we have millions of episodes? Randomness in environment introduces difficulties, but if there are large enough training examples, does neural network have the potential to achieve 99% rate of good landing? It is supposed to get better estimation with more episodes since more randomness is taking into consideration and randomness may be averaged gradually. </p>
<p></p>
<p>Neural networks is advertised in its great applicability in various applications. I would like to know if it is limited in this case (or in all application cases), or RL is not the best choice here to work with a neural network. Thanks.</p>",2019-10-22T18:07:40Z,56,Week 10/20 - 10/26,followup,,jzg6jh2hn6f43c,k225vszsjyr3r1,2019-10-22T18:07:40Z,{},project2
3084,no,"<p>&#34;<em>but if there are large enough training examples, does neural network have the potential to achieve 99% rate of good landing?</em>&#34;</p>
<p></p>
<p>This is a good question.  I also wonder what level of stability is possible with Lunar Lander.  I don&#39;t know if anyone can answer your question with authority, but I think there&#39;s a good chance that $$99\%$$ <em>is</em> possible, and maybe even $$99.9\%$$ or $$100\%$$.  But I think that the engineering task gets much much harder for every little bit of extra stability you might want.</p>
<p></p>
<p>But note that stability does not necessarily correlate with high score!  You can land $$100\%$$ of the time and average 230, or land $$98\%$$ of the time and average 270.  Which would you prefer? </p>",2019-10-22T18:31:44Z,56,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k226qr6sq1o3h7,2019-10-22T18:31:44Z,{},project2
3085,no,<p>Thanks! for quick reply.</p>,2019-10-22T19:15:18Z,24,Week 10/20 - 10/26,followup,,jl0c61kgrmsY,k228ary6xsk3u3,2019-10-22T19:15:18Z,{},project1
3086,no,"<p>Could one of the TAs please clarify how this policy effectively assesses ability to internalize feedback? We don&#39;t get to resubmit the paper with corrections based on the comments, so we&#39;re not really being given the opportunity to demonstrate our ability to incorporate feedback. We also don&#39;t get to see why we&#39;ve lost points, so we don&#39;t have any idea which of the comments provided resulted in lost points. </p>
<p></p>
<p>As posted in &#64;727 and &#64;729, &#34;A regrade request is only valid if it includes an explanation of where the grader made an error.&#34; and &#34;Do not ask questions like &#34;How many points did I lose for X?&#34; because they will not be answered.&#34; Students then are left with no idea where they lost the points they did, limiting ability to improve for future papers. Are the bulk of points lost because of bad graphs? Poor analysis? How does the student know where to focus their efforts to improve for the next paper? One major lesson from this class is our learning algorithms have to be told what and where they made mistakes so they can properly improve. Students here don&#39;t get that consideration.</p>
<p></p>
<p>Without knowing where and why points were cut effectively turns regrade request into a gamble, which I imagine effectively reduces regrade requests. If the intent is to reduce requests, fine, but I&#39;d rather have that put out up front rather than get told the justification is it&#39;s to assessing our feedback internalization ability, because that explanation feels entirely disingenuous. I realize grading hundreds of papers is a tremendous amount of work, but as a student paying for the opportunity to learn, the lack of clear feedback is disheartening and demoralizing. Students shouldn&#39;t be afraid to ask for clarification.</p>
<p></p>
<p></p>",2019-10-23T15:35:50Z,24,Week 10/20 - 10/26,followup,,is4nx55dinr6wk,k23fwe5bneu15v,2019-10-23T15:35:50Z,{},project1
3087,no,"<p>I agree, penalizing students for a regrade request that didn&#39;t earn them a certain number of points only makes sense if we know how many points we lost based on the comments.</p>
<p></p>
<p>I believe there&#39;s a grading error on one of my graphs, but I can&#39;t submit a regrade request. If that error only lost me 3 points, the regrade request will fail to get me 5 points, so I&#39;ll lose 10 points. </p>
<p></p>
<p>If the regrade penalty is contingent on receiving a certain number of points and not finding grading errors, then we should be aware of how many points we lost for each comment. Can we have access to the rubric used for grading?</p>",2019-10-23T16:36:02Z,24,Week 10/20 - 10/26,feedback,,jl284xdcifz44g,k23i1t07pra74i,2019-10-23T16:36:02Z,{},project1
3088,no,"somehow I knew the lack of a Rubric would cause problems but I didn&#39;t expect it to cost me so heavily for focusing on the wrong aspects of the project. I clearly addressed each of the points laid out in the assignment, however it seems to be with the wrong prioritisation. There might be reasons for not releasing a Rubrik, but from the students perspective, I feel it is important to understand where to put your learning attention and to feel like you&#39;re answering the right questions with the right prioritisation. I do appreciate the effort that goes into marking the assignments but the annotations I received do not feel adequate to explain away the amount of points I lost.
However if I request a regrade, I could instead end up failing the assignment even if there are just a few points to gain. It doesn&#39;t seem fair to me especially since I&#39;ve learnt nothing further upon reading my annotations which should count to my 10 points.",2019-10-23T17:42:01Z,24,Week 10/20 - 10/26,feedback,,jqmfmz2fbs55xs,k23keo3be8i1mh,2019-10-23T17:42:01Z,{},project1
3089,no,"<p>I asked for better feedback just to learn, not even requesting a regrade, and my exact 3 sentences of comments was put in quotes back to me. I understand there are only a handful of TAs for several hundred students but i guess expectation that we would have access to higher level instructors would be a core part of this master&#39;s program. I don&#39;t really blame them individually, it is just seriously disappointing.</p>",2019-10-23T17:46:57Z,24,Week 10/20 - 10/26,feedback,,jzttp1ojahj6ju,k23kl04thshn1,2019-10-23T17:46:57Z,{},project1
3090,no,"<p>So you guys know, this has been an evolving thing. We used to grade that way: &#34;figure 3 is 0.2 below expected -2,&#34; &#34;analysis didn&#39;t mention episode length -3,&#34; &#34;didn&#39;t include a graph of random walk -4.&#34;</p>
<p></p>
<p>There are multiple issues with this approach. First, students then feel the need to have our grading rubric for writing the document, E.g., &#34;you didn&#39;t mention explicitly I had to describe the random walk.&#34; Rubrics are bad for learning. We have noticed that with rubrics, students stop thinking out of the box, and follow each section step-by-step (fewer students go above and beyond.) On the other hand, the uncertainty when not having rubrics is more like the real-world, and a better opportunity for learning. You have to explore your thoughts and intuitions, and nothing beats that.</p>
<p></p>
<p>Anyway, if you were on this side, perhaps you&#39;d understand. I don&#39;t believe learning is about following rubrics; there&#39;s more to it than that. I&#39;m not even sure how helpful it is to &#34;internalize feedback,&#34; either. Of course, each course teaches a specific way. You may agree or disagree, like or dislike the method, but I can assure your learning is one of our main goals. If it were to &#34;limit the amount of work,&#34; we would not use annotated comments on the PDF, that&#39;s for sure. We&#39;d perhaps use automated grading tools, or peer grading (which we have tried in previous semesters.)</p>
<p></p>
<p>In any case, feel free to give us your feedback; we do like ideas to come our way. Many improvements we have made have come from student feedback.</p>
<p></p>
<p></p>
<p></p>",2019-10-23T18:19:14Z,24,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23lqius3qu534,2019-10-23T18:19:14Z,{},project1
3091,no,<p>The only reason I feel that I need a rubric is because a successful regrade is contingent on getting back a certain number of points. I understand what you&#39;re saying about rubrics; I don&#39;t feel that we need a rubric so much as the current regrade policy depends on a rubric. If you remove the point requirement from the regrade policy then we don&#39;t need a rubric. </p>,2019-10-23T18:27:03Z,24,Week 10/20 - 10/26,feedback,,jl284xdcifz44g,k23m0kw4tr6618,2019-10-23T18:27:03Z,{},project1
3092,no,"<p>Your concern seems valid. Though, my expectation is you received enough feedback and PDF annotations to give you solid ideas on where the points were lost. FWIW, not many rubric sections are worth more than 10 points, so you&#39;d have to challenge multiple comments to make it work.</p>
<p></p>",2019-10-23T18:32:14Z,24,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23m78uijn77a7,2019-10-23T18:32:14Z,{},project1
3093,no,"<p>So this means that, even if TAs made a grading mistake, there&#39;s no way to fix the grade unless they made multiple mistakes? Because any individual mistake won&#39;t meet the 10 point threshold?</p>",2019-10-23T18:37:00Z,24,Week 10/20 - 10/26,feedback,,jl284xdcifz44g,k23mdddifff6g7,2019-10-23T18:37:00Z,{},project1
3094,no,"<p>Anyway, if you want a regrade or clarification just follow the process as outlined.</p>",2019-10-23T18:55:58Z,24,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23n1rgofwv36x,2019-10-23T18:55:58Z,{},project1
3095,no,"<p>My biggest hang up is threatening a 10 point loss if you don&#39;t score enough points on the regrade and framing that additional point deduction as a lesson to teach us to internalize feedback. It&#39;s equivalent to saying accept what score you got and be happy with it, because it&#39;s too much of a risk to ask for a regrade. Considering the average score was a 71, and assuming the median was in the same ballpark, half the class is, according to the posted overall course grading criteria, averaging a C in the course. At a 71, asking for a regrade and not getting 5 points drops you to a 61, which now means you&#39;re failing.</p>
<p></p>
<p>Now I&#39;m willing to wager that the actual RL grading scheme will work like ML, which if I understand correctly, essentially means grades will be determined based on how the class does, ie, some normal bell curve based on the class average, which likely means students who scored at the median will probably come out with at least a B. But that isn&#39;t what&#39;s posted on the syllabus. And when students are paying out of pocket with the understanding that you need a &gt;90 for an A and they just earned a 50 or 60 on a paper, they&#39;re probably going to drop. Which now means lost time and money for a course that in reality, they&#39;re probably doing OK in. </p>
<p></p>
<p>I&#39;ve been on the other side of the fence having built a class curriculum and trying to build rubrics for graded events. I&#39;m well aware of the challenges in putting them together, and the significant drawbacks in how students limit their exploration of a topic by focusing solely on rubric criteria. But expectation management matters too. So here&#39;s what I&#39;d recommend:</p>
<p></p>
<p>1- If the policy isn&#39;t intended to dissuade regrade requests because of the workload involved, then change the regrade policy so there isn&#39;t a threat of a point deduction. If a regrade genuinely doesn&#39;t earn more points, then let the original grade stand. If the intent really is to lighten the workload on grading, than change the TA/Student ratio so the workload is manageable. And then still get rid of the 10 point penalization.</p>
<p></p>
<p>2- Be transparent about how overall grading actually works. If students are still going to be successful (ie, earn As/Bs) although they have an average in the 60s, just be up front about it. Nothing is more frustrating than trying to guess if you&#39;re doing ok or not, especially when time and money are on the line. Alternatively, better align grades to how students are actually doing. If folks are getting As with averages in the 60s, why not just give those students 90s to begin with instead of playing grading games?</p>
<p></p>",2019-10-23T20:03:44Z,24,Week 10/20 - 10/26,feedback,,is4nx55dinr6wk,k23pgwkpdqg5du,2019-10-23T20:03:44Z,{},project1
3096,no,<p>&#43;10</p>,2019-10-24T07:07:23Z,24,Week 10/20 - 10/26,feedback,,jqu95q68ljj1pn,k24d6d3e7956lu,2019-10-24T07:07:23Z,{},project1
3097,no,"<p>I agree, the 10 point penalty associated with a regrade doesn&#39;t seem necessary. If the TAs feel that a regrade request provides insufficient detail or the points are not deserved, they can simply let the previous grade stand. At the <strong>very</strong> least, remove the 5 point threshold and say that &#34;if a student&#39;s grade does not increase after a regrade, they will lose X points&#34; because <span style=""text-decoration:line-through"">Miles</span> Miguel said above that it would take multiple mistakes by the grader in order to meet this threshold. In the event that the grader only made one mistake, there&#39;s no way to get it corrected.</p>",2019-10-24T14:17:05Z,24,Week 10/20 - 10/26,feedback,,jl284xdcifz44g,k24siygrmbxyf,2019-10-24T14:17:05Z,{},project1
3098,no,"<p>First, my name is not Miles. Second, I explicitly said: &#34;[...] <em><strong>not many</strong></em> rubric sections [...]&#34; The conclusion of &#34;multiple mistakes&#34; is totally yours, Dalton.</p>
<p></p>
<p></p>",2019-10-24T16:18:13Z,24,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k24wuqjjfgz6d3,2019-10-24T16:18:13Z,{},project1
3099,no,"I don&#39;t believe the TAs are at fault, they are just following a hidden rubric that I&#39;m sure is reasonable. I do believe that one can not be expected to know the hidden weights of importance assigned to questions posed in the project sheet, answering a vague question will always lead to vague results. I&#39;ve had many a project in my academic years without a clear rubric and I do feel like I learn less and am less sure about my competence with these projects.

I am not going to request a regrade because it feels too risky, I may end up failing instead. I have to say it seems a little unorthodox to assign marks to a project for self-reflection in the rubric of the project itself, usually that counts towards a self-assessment after the fact. It does have the effect of silencing students who feel they were graded incorrectly.

I believe in learning from mistakes and moving on so what I do want to do now is make it harder to make the same mistake in the future. Question for the TAs: can I layout each question posed in the project sheet in bold and have my answer underneath? I think it would be comforting to know that my answers clearly address each question, this might also be helpful for self-reflection later on. I know there are some marks in the rubric for formatting but would you consider this a break from what would be considered acceptable formatting?",2019-10-25T11:04:45Z,24,Week 10/20 - 10/26,feedback,,jqmfmz2fbs55xs,k2613h297l43pj,2019-10-25T11:04:45Z,{},project1
3100,no,"<p>Just a suggestion to instructors and TAs ...</p>
<p></p>
<p>Project 3 description has a declaration &#34;Your grade will largely be based upon your report and analysis&#34;. This declaration should be put in project 1 and 2&#39;s problem description too. </p>
<p></p>
<p>It is not wrong to score the work with TA&#39;s opinion or satisfaction on the analysis, but at least, instructors should let the students know &#34;Your grade will largely be based upon your report and analysis&#34;. Otherwise, as a student, I would assume I have completed the project very well and I do not need to write down the additional analysis that I learned. </p>",2019-10-31T18:58:29Z,23,Week 10/27 - 11/2,feedback,,jzg6jh2hn6f43c,k2f2ntj4i2vjo,2019-10-31T18:58:29Z,{},project1
3101,no,<p>But my 4 questions were good questions!!</p>,2019-10-23T03:21:57Z,42,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k22polsywjy3fu,2019-10-23T03:21:57Z,{},project2
3102,no,"<p>Haha were you really the one who submitted four?</p>
<p></p>
<p>Edit: I thought you were joking but it occurred to me that you&#39;re probably not.  That stinks...</p>",2019-10-23T04:11:14Z,42,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k22rfzxq4j249h,2019-10-23T04:11:14Z,{},project2
3103,no,"<p>I am the one! I should read instructions more thoroughly, but I was at the end of setting up an experiment.... oh well, I was more interested in the answer to my questions instead of the points.</p>",2019-10-23T12:29:07Z,42,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k2398a1dxig3c9,2019-10-23T12:29:07Z,{},project2
3104,no,"<p>I think there&#39;s a very good chance the TAs will ask them regardless of the points, if they&#39;re good questions.</p>",2019-10-23T14:47:55Z,42,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23e6rx7om836c,2019-10-23T14:47:55Z,{},project2
3105,no,"<p>I also really want to ask more questions. I predict that we would have less than 30 questions submitted (out of 300 students), so submitting more questions could benefit everyone in a way that we can have larger pool of interest. I thought about &#34;lobbying&#34; people on Slack to ask my questions. It would be a win-win for both of us.</p>",2019-10-23T14:51:42Z,42,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k23ebn2z6ru77g,2019-10-23T14:51:42Z,{},project2
3106,no,"<p>You could also ask them on Quora, if you don&#39;t get an answer here.</p>",2019-10-23T14:59:27Z,42,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23ell8viyc598,2019-10-23T14:59:27Z,{},project2
3107,no,"<p>I guess that I should have phrased my questions as a single, multipart, question. I need to hone-down my CNN journalist skills...</p>",2019-10-23T16:07:21Z,42,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23h0xc38m873u,2019-10-23T16:07:21Z,{},project2
3108,no,"<p>LOL, Jacob... I was like... you really are into P2, aren&#39;t you?</p>
<p></p>
<p>Quang, that&#39;s not a bad idea. How do you suggest we do that?! Maybe creating a poll that students can edit and people can vote multiple times?</p>
<p></p>
<p>I welcome ideas for handling multiple questions (unless that makes me review and edit 300 questions.)</p>
<p></p>",2019-10-23T16:51:53Z,42,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23im6vnfqo56u,2019-10-23T16:51:53Z,{},project2
3109,no,"<p>I did some space work in the past, so LL is interesting to me. I am also interested in game play, but not this Atari stuff. Let&#39;s try to make a Mines of Moiria agent that defeats the Balrog with the lowest possible character level. That kind of decision making is tough.</p>
<p></p>
<p></p>",2019-10-23T16:56:40Z,42,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23iscezeaj1ya,2019-10-23T16:56:40Z,{},project2
3110,no,<p>&#64;Jacob I also think that kind of optimization would be super-fun!</p>,2019-10-23T17:03:23Z,42,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23j0z8uxz14ob,2019-10-23T17:03:23Z,{},project2
3111,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  about lobbying people? Yeah I like that too. That would get me to know more people in the class, in a way that would benefit both of us. I did not actually do that, since I was away on the weekend.</p>
<p></p>
<p>If you are asking about letting people ask multiple questions. I think we can have a scoring system for each question. First question can have 10 pts, next question has 75% the previous. So Jacob can ask 10 questions, but his 10th will be at the bottom of the list. But if no one interested, his 10th question will still get enough attention. If you ask people to send one question per email, this can be done with a Python script to sort through timestamp without requiring you manually going through the email.</p>
<p></p>
<p>For this round, I think an additional poll that students can edit and vote multiple times would be interesting. It would save you time, but can also be more engaging.</p>",2019-10-23T17:40:53Z,42,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k23kd7d9f558,2019-10-23T17:40:53Z,{},project2
3112,no,"<p>Get the poll started and let&#39;s see if it works... I&#39;ll pin it and announce something later tonight or tomorrow.</p>
<p></p>
<p>Quick clarification, the extra credit will come only from this list, but questions may come from either!</p>
<p></p>",2019-10-23T18:22:18Z,42,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23luh58ebi14z,2019-10-23T18:22:18Z,{},project2
3113,no,"<p>I was under the interpretation you would set the student poll, but reading this again I think you mean some of us (students), or specifically me, should do that, and you will help pin it. If this is the case, sorry for not follow through with this discussion.</p>",2019-10-30T02:04:00Z,41,Week 10/27 - 11/2,feedback,,jl5wq8mca7o0,k2cmzbjfjme6vl,2019-10-30T02:04:00Z,{},project2
3114,no,"<p>No worries, all good. Enjoy the session tomorrow.</p>",2019-10-30T03:18:10Z,41,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2cpmp9hdid2rk,2019-10-30T03:18:10Z,{},project2
3115,no,"<p>Just curious, how will you break the tie here ? :)</p>",2019-10-23T18:51:34Z,42,Week 10/20 - 10/26,followup,,jfzaqnqvtQ1m,k23mw3g9gh84tq,2019-10-23T18:51:34Z,{},project2
3116,no,<p>Time breaks any ties!</p>,2019-10-23T18:56:20Z,42,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k23n28slnsv3t0,2019-10-23T18:56:20Z,{},project2
3117,no,"<p>thanks! reading up all the 5 page pdfs of 400&#43; students is a lot of work!</p>
<p></p>
<p>Will you be releasing statistics of the entire class for project1, median or percentiles?</p>",2019-10-23T02:43:17Z,23,Week 10/20 - 10/26,followup,,jqrr36mqfm8M,k22oavo69tq1bn,2019-10-23T02:43:17Z,{},project1
3118,no,"<p>It&#39;s not entirely intuitive, but you can click the small button to the right of the comments button on the grades to see the class mean and a box plot</p>",2019-10-23T02:55:43Z,23,Week 10/20 - 10/26,feedback,,is4nx55dinr6wk,k22oqvm4culh0,2019-10-23T02:55:43Z,{},project1
3119,no,"<p>hmm it gives you mean, median will be ideal :)</p>",2019-10-23T04:40:58Z,23,Week 10/20 - 10/26,feedback,,jqrr36mqfm8M,k22si80mxfr6um,2019-10-23T04:40:58Z,{},project1
3120,no,<p>median will be ideal :)</p>,2019-10-24T02:52:13Z,23,Week 10/20 - 10/26,feedback,,gx3c8l7z7r72zl,k24427nsz0s3y7,2019-10-24T02:52:13Z,{},project1
3121,no,"<p>&#34;Also look for annotations on your PDF that you submitted as another place for feedback.&#34; I don&#39;t see any annotation in my report, if I click on &#34;View Feedback&#34; button in Canvas. Is it the correct way of seeing it?</p>",2019-10-23T17:48:18Z,23,Week 10/20 - 10/26,followup,,is6e83bsfvk,k23kmqowzqk271,2019-10-23T17:48:18Z,{},project1
3122,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong> I am in the same boat and have been looking for these annotation that are supposed to be the clear feedback. I even downloaded the &#34;annotated paper for Project 1&#34; which has nothing on it, which means that the only feedback on the entire paper is the couple of sentences in the comment.</p>",2019-10-23T20:30:10Z,23,Week 10/20 - 10/26,feedback,,jzttp1ojahj6ju,k23qewlc625iy,2019-10-23T20:30:10Z,{},project1
3123,stud,<p>&#43;1. Perhaps something needs to be enabled on the graders end to show it?</p>,2019-10-23T21:19:54Z,23,Week 10/20 - 10/26,feedback,a_0,,k23s6vak96r582,2019-10-23T21:19:54Z,{},project1
3124,no,"<p>I see it, its mostly one line comment for me, so easy to miss. Its also visible from the grades link, same place where you see the &#39;mean&#39; score</p>",2019-10-23T23:29:48Z,23,Week 10/20 - 10/26,feedback,,jqrr36mqfm8M,k23wtwxegby4vy,2019-10-23T23:29:48Z,{},project1
3125,no,<p>You can check the annotations on Assignment &gt; Project 1 &gt; Submission Details &gt; View Feedback</p>,2019-10-24T02:52:11Z,23,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k244269u8vf3wg,2019-10-24T02:52:11Z,{},project1
3126,no,"<p>Right, exactly what I did. Marking this topic as unresolved, hoping that some TA can clarify. Thanks</p>",2019-10-24T17:20:11Z,23,Week 10/20 - 10/26,feedback,,is6e83bsfvk,k24z2flirz82oj,2019-10-24T17:20:11Z,{},project1
3127,no,<p>The steps Danilo mentioned are correct.  If you have questions/clarifications about feedback please make a private post to all instructors with questions.</p>,2019-10-25T00:35:38Z,23,Week 10/20 - 10/26,feedback,,hz7meu55mi8sd,k25emfqj7u2407,2019-10-25T00:35:38Z,{},project1
3128,no,"<p><strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong> That is literally what i did to download the &#34;annotated paper for project 1&#34; and it has nothing in it. Can you please take a look? It sounds like i am not the only one and it would be fantastic if on of the TAs looked into this.</p>",2019-10-25T14:16:52Z,23,Week 10/20 - 10/26,feedback,,jzttp1ojahj6ju,k267yjooqc13tu,2019-10-25T14:16:52Z,{},project1
3129,no,"<p>Yes we are looking into.  As mentioned above, make a private post to all instructors and we will handle it there.</p>",2019-10-25T14:48:39Z,23,Week 10/20 - 10/26,feedback,,hz7meu55mi8sd,k2693et4cbe28m,2019-10-25T14:48:39Z,{},project1
3130,no,Is it a similar policy to CS 7641?,2019-10-23T01:02:02Z,23,Week 10/20 - 10/26,followup,,i4jbttw9ru63ot,k22koo3imac50z,2019-10-23T01:02:02Z,{},project1
3131,no,In spirit if not in detail. <p></p>,2019-10-23T11:22:05Z,23,Week 10/20 - 10/26,feedback,,gph3si6rKEb,k236u21ipm6p9,2019-10-23T11:22:05Z,{},project1
3132,no,<p>:-)</p>,2019-11-27T23:33:22Z,18,Week 11/24 - 11/30,feedback,,jqkxzdmmolGf,k3hxdbcapo06sx,2019-11-27T23:33:22Z,{},project1
3133,no,<p>Edited the question for (hopefully) more clarity.</p>,2019-10-23T01:08:24Z,23,Week 10/20 - 10/26,followup,,jl3oi5v7qkSk,k22kwurc3mw1hl,2019-10-23T01:08:24Z,{},project1
3134,no,<p>Could you help us to evaluate the grades? It might help the decision to drop or not the class. </p>,2019-10-23T01:21:29Z,23,Week 10/20 - 10/26,followup,,jzih0fdt4sn1cq,k22ldozfn7u85,2019-10-23T01:21:29Z,{},project1
3135,no,I found the office hours provided some help in understanding this. It&#39;s only a short conversation around a half hour in. The short answer is that there isn&#39;t a curve.,2019-10-23T10:59:27Z,23,Week 10/20 - 10/26,followup,,jh701barLeiB,k2360yesb1z79o,2019-10-23T10:59:27Z,{},project1
3136,no,Curve is a technical term. I do not force a normal distribution. <div><br /></div><div>Scores are not adjusted in any way. Grade cutoffs usually follow naturally from the data. Aim for whatever you think A work is and you’ll probably be ok. Check GPA distributions from past terms if you need to make yourself feel a sense of lowered entropy. </div>,2019-10-23T11:21:44Z,23,Week 10/20 - 10/26,feedback,,gph3si6rKEb,k236tm3nafz6jt,2019-10-23T11:21:44Z,{},project1
3137,no,"<p>Ah, I think I see where I failed to make myself clear. The scores are not adjusted; the grade cutoff are. Thank you.</p>
<p></p>
<p>Out of interest, where can we see GPA distributions from past terms?</p>",2019-10-23T11:27:53Z,23,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k2371iowp8l34k,2019-10-23T11:27:53Z,{},project1
3138,no,"<p><a href=""https://tableau.gatech.edu/t/EDM/views/LITEGradeDistributionReport/GradeDistribution?:embed=y&amp;:showAppBanner=false&amp;:display_count=no&amp;:showVizHome=no&amp;:render=false&amp;:embed=y&amp;:showVizHome=n&amp;:tabs=n&amp;:toolbar=n&amp;:apiID=host0"">https://tableau.gatech.edu/t/EDM/views/LITEGradeDistributionReport/GradeDistribution?:embed=y&amp;:showAppBanner=false&amp;:display_count=no&amp;:showVizHome=no&amp;:render=false&amp;:embed=y&amp;:showVizHome=n&amp;:tabs=n&amp;:toolbar=n&amp;:apiID=host0</a></p>
<p></p>",2019-10-23T19:04:04Z,23,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k23nc6313173i0,2019-10-23T19:04:04Z,{},project1
3139,no,<p>That link is magic! I have no idea how you found it in the first place but thank you David! :D</p>,2019-10-24T12:55:21Z,23,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k24pluotf5n25v,2019-10-24T12:55:21Z,{},project1
3140,no,"<p>No problem Ben! Idk if you already have this link as well but the OMSCS students keep record of their experience with courses on this site:</p>
<p></p>
<p><a href=""https://omscentral.com"">https://omscentral.com</a></p>
<p></p>
<p>Hope this helps!</p>",2019-10-24T13:03:04Z,23,Week 10/20 - 10/26,feedback,,j6m1jeidndu6wq,k24pvs8ma0z8c,2019-10-24T13:03:04Z,{},project1
3141,no,"<p>SGD has been slow to learn for me. I would suggest using RMSprop as the Neural-Q paper suggests. They did SGD and RMSprop and others, and are much more experienced that most of us. The consensus of NN people is to use Adam though, as it is faster.</p>
<p></p>
<p>If I was to do NN, I would do Adam. But I am not going down that rabbit hole for P2.</p>",2019-10-23T03:25:38Z,25,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k22ptcan2kv4p5,2019-10-23T03:25:38Z,{},project2
3142,stud,Any specific disadvantage of using Adam? It worked fine in my case.,2019-10-23T03:35:13Z,25,Week 10/20 - 10/26,feedback,a_1,,k22q5o4hmw53fe,2019-10-23T03:35:13Z,{},project2
3143,no,"<p>None that I&#39;ve read about, but that doesn&#39;t say much. Andrew Ng suggests using Adam in his Coursera courses. Adam is faster than most, but at what cost? I don&#39;t really know.</p>",2019-10-23T12:25:40Z,25,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k2393u5op0k102,2019-10-23T12:25:40Z,{},project2
3144,no,"<p>I think Farrukh points out a possible disadvantage above.  Adam will be much, much faster, since it adapts the learning rate for you, but possibly may not find quite as good of an optimum.  Also, it&#39;s probably worth noting that when the Atari work was being done, Adam may not have been in widespread use yet, so if that work were being done today, they may have used Adam instead of RMSprop.</p>",2019-10-23T14:22:17Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23d9sj07ho3a1,2019-10-23T14:22:17Z,{},project2
3145,no,"<p>Adam was mentioned several times during OH, I&#39;ve found RMSprop is pretty flaky, but maybe you&#39;ll have a better luck. Basically experimenting with all optimizers/loss functions/hyperparams permutations will take forever, so just use whatever works first for you in your smaller scale experiments (as per bootcamp lecture 6 advise)</p>",2019-10-23T15:12:28Z,25,Week 10/20 - 10/26,feedback,,jqkxzdmmolGf,k23f2cd7yll2fd,2019-10-23T15:12:28Z,{},project2
3146,stud,"Thanks, Vahe. That gives me a good baseline to target and confirms that convergence without any explicit punishment is possible.",2019-10-23T07:26:09Z,25,Week 10/20 - 10/26,followup,a_0,,k22yen4pkcq4ev,2019-10-23T07:26:09Z,{},project2
3147,no,<p>I got in episode 0 ;) :)</p>,2019-10-24T01:10:30Z,25,Week 10/20 - 10/26,followup,,jzivtxcbl6964n,k240feq984i1hj,2019-10-24T01:10:30Z,{},project2
3148,no,<p>Oh I got 60 on the 1st project. I have something below C. Feel very discouraged to continue. Should I drop the class? is there a point to continue?</p>,2019-10-23T15:53:11Z,24,Week 10/20 - 10/26,followup,,jzih0fdt4sn1cq,k23gipju8597da,2019-10-23T15:53:11Z,{},other
3149,no,"<p>I think if you feel like you&#39;re learning a lot in the class, then dropping would be a mistake.</p>
<p></p>
<p>I wouldn&#39;t go by the above numbers.  Take a look here (from OMSCentral - they didn&#39;t have RL grades but they had ML grades):</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk23iqdup7ht6%2FMLGradeDistr.PNG"" alt="""" /></p>
<p></p>
<p>If two-thirds of the class is getting an A and a quarter of the class is getting a B, then being close to the mean pretty much guarantees you won&#39;t get a C, and probably means you have a good shot at an A.</p>",2019-10-23T16:57:01Z,24,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23issikjz93cz,2019-10-23T16:57:01Z,{},other
3150,no,"<p>I second this.  I didn&#39;t go that well on P1 either, my grade was 77. As Vahe pointed, RL seems to look like ML in terms of grades (numbers) and the overall grade(letters). Homeworks should move our grades higher :) Hopefully P2 grades will be better. I feel P1 was hard as there were many unknowns on replicating an old paper.</p>
<p></p>
<p>See the the distribution below, 72.6% of students scored between A-B with the majority scoring A. So the distribution is pretty skewed towards high grades. Of course the is a survivorship bias as ~22% have dropped. But we are just 1.5 months from the end of semester and a lot to learn!</p>
<p> </p>
<p>Source: <a href=""https://tableau.gatech.edu/t/EDM/views/LITEGradeDistributionReport/GradeDistribution?:embed=y&amp;:showAppBanner=false&amp;:display_count=no&amp;:showVizHome=no&amp;:render=false&amp;:embed=y&amp;:showVizHome=n&amp;:tabs=n&amp;:toolbar=n&amp;:apiID=host0"">https://tableau.gatech.edu/t/EDM/views/LITEGradeDistributionReport/GradeDistribution?:embed=y&amp;:showAppBanner=false&amp;:display_count=no&amp;:showVizHome=no&amp;:render=false&amp;:embed=y&amp;:showVizHome=n&amp;:tabs=n&amp;:toolbar=n&amp;:apiID=host0</a><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fis6e83bsfvk%2Fk23jsens5ueq%2FScreen_Shot_20191023_at_10.24.20_AM.png"" alt="""" /></p>",2019-10-23T17:32:21Z,24,Week 10/20 - 10/26,feedback,,is6e83bsfvk,k23k27zqnw763h,2019-10-23T17:32:21Z,{},other
3151,no,"<p>Is the goal of withdrawing to get an A later in lieu of a B today? Instead, I would suggest that you stay, do poorly, apologize to the teaching staff, and then retake it again to get your A. Stay in the class and continue to learn the material so you can do better when you have to retake it.</p>
<p></p>
<p>Should you get the A you want, then you haven&#39;t wasted a semester. If you fully withdraw now, then you have wasted all of this time up until now.</p>",2019-10-23T17:46:26Z,24,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23kkcazkbf6lq,2019-10-23T17:46:26Z,{},other
3152,no,"<p>Honestly I&#39;ve spent so much time on the class and getting 60 is super discouraging, although I submitted the paper a day later. So it&#39;s my fault. I don&#39;t want to fail and at this point it feels like there is a good chance I would. I would love to get B though, but I&#39;m not confident I can get to that level. At this point even C looks out of reach. So I just don&#39;t want to continue and fail. Anyway I&#39;m going to continue as this is my first semester and I need to get 2 Bs in the first year. I guess I don&#39;t have much leverage. This is a really good lesson for me as I&#39;ve learnt there is no much a margin for errors.</p>",2019-10-23T19:50:27Z,24,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k23oztscffc56,2019-10-23T19:50:27Z,{},other
3153,no,"<p>If you submitted your paper a day late, then you didn&#39;t get 60/100, you got 60/80, and would have gotten 80/100.  I don&#39;t think that should be discouraging at all.  The quality of the work is so much more important than the actual grade, and in your case, the lower grade is due to a non-content related penalty.</p>
<p></p>
<p>I also don&#39;t think you&#39;re interpreting the above tables correctly.  Project 1 is just 15% of the overall grade and 92% of the people who stick it out in this class get either an A or a B.</p>",2019-10-23T20:03:08Z,24,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23pg4ojnod48r,2019-10-23T20:03:08Z,{},other
3154,no,"<p>Honestly if you got that grade with late penalty as a first semester, I think you are pretty good! This is my 8th class and I&#39;ve also taken ML previously, just as a comparison...</p>",2019-10-23T20:19:02Z,24,Week 10/20 - 10/26,feedback,,is6e83bsfvk,k23q0ktjbf82cu,2019-10-23T20:19:02Z,{},other
3155,no,"<p>Thank you, guys, I&#39;ll continue. Hopefully I&#39;ll do better on the 2nd project. Writing papers is hard for me as this is my first time studying in an US school. Also this is a very interesting class and I feel I could&#39;ve gotten more from it if I spent more time on it. So that&#39;s why I wanted to drop and study more which might be silly. Appreciate your help and encouragement! </p>",2019-10-23T20:29:24Z,24,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k23qdwyq4f73yb,2019-10-23T20:29:24Z,{},other
3156,no,"<p>Aida, my first class was ML (7641). I took that and RL in Spring 2018. I dropped RL to focus on ML, and my performance was horrible. I hadn&#39;t been to school in 24 years and I had no python skills. I got a B. Stick with it, reach out to your classmates, and follow piazza. You will do fine, and if you get a C, you can always do it again for that A. I got a C in AI this year (Spring 2019). Did it matter? nope. I used everything from that class in ML4T and RL because I still understood all of it, just had alot of bad luck that semester with work and vacation timings.</p>",2019-10-23T22:01:19Z,24,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23to4tc9kh55y,2019-10-23T22:01:19Z,{},other
3157,no,"<p>Does anyone know if these grades are the actual numbers for the class, or just an aggregation of scores from those who put in their information on OMSCentral? If it&#39;s the latter then beware slightly of the bias from those who actually fill in OMSCentral reviews.</p>",2019-10-23T23:53:31Z,24,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k23xoeuut751k3,2019-10-23T23:53:31Z,{},other
3158,no,"<p>I believe they are pulled from the statistics provided here, so they should be accurate statistics of the entire class.</p>
<p>https://tableau.gatech.edu/t/EDM/views/LITEGradeDistributionReport/GradeDistribution?:embed=y&amp;:showAppBanner=false&amp;:display_count=no&amp;:showVizHome=no&amp;:render=false&amp;:embed=y&amp;:showVizHome=n&amp;:tabs=n&amp;:toolbar=n&amp;:apiID=host0</p>
<p></p>
<p>I think that the grading scheme is likekly something like mean score and above you get an A, and mean score - 1 standard deviation (?) is the range for a B. Though based on the stats for ML maybe people even slightly below the mean get an A.</p>",2019-10-23T23:59:15Z,24,Week 10/20 - 10/26,feedback,,jccyemecUB8q,k23xvs6g5o51os,2019-10-23T23:59:15Z,{},other
3159,no,"<p>Thank you, Jacob. You are one of the most knowledgeable people here. Your story is very encouraging. I&#39;ll stick with the class and will try to do better on the remaining assignments. At the end of the day, it&#39;s about learning and not grades :-)</p>",2019-10-24T00:06:15Z,24,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k23y4s3hiy470b,2019-10-24T00:06:15Z,{},other
3160,stud,<p>Awesome - yes I was wondering if I could use double DQN. I&#39;ll edit my question to reflect that. </p>,2019-10-23T17:57:52Z,25,Week 10/20 - 10/26,followup,a_0,,k23kz1rwdnw14b,2019-10-23T17:57:52Z,{},project2
3161,no,"<p>Keep in mind that for this project, we need to also graph the rewards themselves per episode, in addition to the means per 100 episodes, which means we have to keep the reward vector anyway until the very end.</p>
<p></p>
<p>But yeah, for just the mean, we can incrementally compute a new mean every episode without carrying over more than just the old mean, and a count of the number of episodes.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk23hrliqekta%2FCapture.PNG"" alt="""" /></p>",2019-10-23T16:26:59Z,25,Week 10/20 - 10/26,followup,,jzfsa4a37jf4aq,k23hq5pqw007mh,2019-10-23T16:26:59Z,{},project2
3162,no,"<p>Yup, that&#39;s correct.</p>
<p></p>
<p>If you try to graph out 100,000 episodes of training, though, it&#39;s just a brick of color. So you do epochs, and in those epochs, you can do the incremental mean for X training episodes (like 1000 or 500).</p>
<p></p>
<p>What you might find yourself doing is dumping your telemetry to a file. You might want to know if you hit the 200 point target, so running a mean like this would give you that chance without having to keep your telemetry in memory. Once you get that 200 point target you might want to save your state so you can replay it later and prove to Tim that you really did the work.</p>",2019-10-23T16:32:26Z,25,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23hx6n562x1lz,2019-10-23T16:32:26Z,{},project2
3163,no,"<p>This worked. I re-ran my 100,000 episode job in 30,000 episodes.</p>
<p></p>
<p>Mean score= 251.7162393492896<br />Max score= 290.32629749481725<br />Min score= -211.32632397863097<br />Std score= 56.18051841870291<br />Mean positive score= 259.0986457406073<br />Max positive score= 290.32629749481725<br />Min positive score= 192.6464327960607<br />Std positive score= 16.918222416470883<br />% positive landings= 98.0</p>
<p></p>
<p>Did it in 13,000 episodes and 2.5 hours of training.</p>
<p></p>
<p>This is just straight up QQ learning with a discretized state. Wow.</p>
<p></p>
<p></p>",2019-10-23T21:23:14Z,32,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k23sb4zupor2kr,2019-10-23T21:23:14Z,{},project2
3164,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk23td3cd5jlx%2Fqqexp21finalshare.png"" alt="""" /></p>",2019-10-23T21:52:49Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23td75ixq32n,2019-10-23T21:52:49Z,{},project2
3165,no,<p>That&#39;s awesome Jacob!!</p>,2019-10-23T22:20:27Z,32,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23ucql1h261n4,2019-10-23T22:20:27Z,{},project2
3166,no,<p>Thanks Vahe!</p>,2019-10-23T22:29:24Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23uo8yzqi31hs,2019-10-23T22:29:24Z,{},project2
3167,no,"<p>I&#39;ll bet you that -211 was something stupid too, like it almost landed but the ground terrain was really steep and it just barely slipped and tilted head first into the ground.  It looks like your lander is extraordinarily consistent, which is something I noticed seems to be true with the tabular (discretized) approaches to these problems but not nearly as true with the nonlinear function approximators.</p>",2019-10-23T22:35:16Z,32,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23uvs1u7ux5mm,2019-10-23T22:35:16Z,{},project2
3168,no,"<p>Yeah, so I setup a trigger to record video if the training was &gt; 200. It never triggered. The training was always consistently 100 - 150. So that speaks well about overfitting and fitting the policy for the discretized method.</p>
<p></p>
<p>You&#39;re likely correct about the landing. I am going to do some consistency checks to see how well this configuration repeats its performance and try to capture some video.</p>
<p></p>
<p>I looked at the heatmaps around the time when the learning transitioned from total failure to hinting at success. Have to say that nothing obvious stands out about the Q map. Learning is definitely a subtle change in the Q distribution.</p>",2019-10-23T23:02:33Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k23vuvhdv4w5ok,2019-10-23T23:02:33Z,{},project2
3169,no,"<p>This is interesting, thanks for sharing. I&#39;d expect smoother surfaces for both grid searches: e.g. do the apparent catastrophic drops in performance really exist at those areas or were those unlucky runs? Regardless, the general trend is visible and provides helpful insights.</p>
<p></p>
<p>Metrics for evaluating training stability is my primary emphasis as well. Sharpe Ratio&#39;s an interesting idea, though it seems important where you choose to stop measuring it. In investing there&#39;s an expectation you could look at the SR at almost any point and it&#39;d provide a sense of risk adjusted returns. In RL environments like LunarLander, the reward plateaus and the longer you record after that the more you lose the upward trajectory.</p>
<p></p>
<p>It might be interesting to see the SR stopping immediately after the first result that gets a score above 230 or so, or stopping immediately after the first 100 eps avg above 200 (was that the stopping condition here?).</p>
<p></p>
<p>I&#39;m also interested in stability of a learned model after it &#34;solves&#34; an environment. e.g. it could be interesting to compare the SR looking only at the next N episodes after a model averages over 200 for 100 eps to see if different training methods arrive at more robust learned conditions.</p>
<p></p>
<p>Overall, thought provoking results, thanks again for sharing.</p>
<p></p>
<p>Edit - related: this 2018 paper on <a href=""https://arxiv.org/abs/1812.02648"" target=""_blank"" rel=""noopener noreferrer"">Deep RL and the Deadly Triad</a> emphasizes training stability and uses some interesting evaluation metrics as well. Highly recommend checking out section 4.1 on pg 5, and figures 2-3 on pg 6 if time permits. I found it pretty helpful.</p>",2019-10-24T15:45:41Z,32,Week 10/20 - 10/26,followup,,isde332xcka1m0,k24vowrerih59g,2019-10-24T15:45:41Z,{},project2
3170,no,"<p>There does seem to be a paucity of metrics for RL methods. Anything that we can make up that makes sense will help!</p>
<p></p>
<p>I ran my SR grid search using 1000 episodes of training. As you mention, if this SR was a true metric of volatility in training, then starting the calculation at some episode X until X&#43;1000 should demonstrate similar behavior. That would be a good test to run.</p>
<p></p>
<p></p>",2019-10-24T16:13:06Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k24wo6bfxgf3gm,2019-10-24T16:13:06Z,{},project2
3171,no,<p>Agreed. Love the idea of applying a Sharpe Ratio-like metric in RL evaluation. There could be a paper there!  I&#39;d definitely read it. :-)</p>,2019-10-24T16:18:27Z,32,Week 10/20 - 10/26,feedback,,isde332xcka1m0,k24wv1dhwky1on,2019-10-24T16:18:27Z,{},project2
3172,no,"<p>Related: I remember learning about an alternative to Sharpe Ratio in ML4T called the <a href=""https://blog.sutherlandresearch.com/index.php/2017/05/27/alternative-to-the-sharpe-ratio/"" target=""_blank"" rel=""noopener noreferrer"">Sortino ratio</a> which could also be interesting in evaluating RL.</p>
<p></p>
<p>Excerpt:</p>
<ul><li>&#34;The Sortino ratio is like the Sharpe ratio but differs in that it takes account of the downside deviation of the investment as opposed to the standard deviation [...] The Sortino ratio in effect removes the Sharpe ratio’s penalty on positive returns and focuses instead on the risk that concerns investors the most, which is volatility associated with negative returns.&#34;</li></ul>
<p></p>
<p>Calculated: <strong>(Mean Portfolio Return) / Standard Deviation of Negative Portfolio Returns</strong></p>
<p></p>
<p>When agent performance increases, we might not be concerned about that.</p>",2019-10-24T16:29:10Z,32,Week 10/20 - 10/26,feedback,,isde332xcka1m0,k24x8u6qlyfad,2019-10-24T16:29:10Z,{},project2
3173,no,"<p>Exactly, the Sortino Ratio could be an interesting online metric that inputs into your NN or value estimator. This could be a biasing parameter that selects for more optimal paths, like an eligibility trace for NNs....</p>",2019-10-24T16:49:43Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k24xz9c725p5gw,2019-10-24T16:49:43Z,{},project2
3174,no,<p>There&#39;s definitely a research topic here on &#34;Lessons Learned in Financial Predictions Applied to RL&#34; :)</p>,2019-10-24T16:50:21Z,32,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k24y02awhwu707,2019-10-24T16:50:21Z,{},project2
3175,no,"<p>I&#39;ll add a few points here:</p>
<p></p>
<p>TLDR: As you might notice, the usually max norm contraction property doesn&#39;t hold anymore. because we rely on gamma &lt; 1 on that. However, you can use a weighted max norm(just another norm) to contract and you have convergence as usually. (with some assumptions below of course) </p>
<p></p>
<p>====</p>
<p>From a theoretic point of view, what you described is a &#34;stochastic shortest path&#34; problem by &#34;neuro dynamic programming&#34; - Bertsekas. which is &#34; you have a 0 cost terminal state and with prob 1, the agent will get there. </p>
<p></p>
<p>Then adding some spice of minor assumption: there exist a proper policy, improper policy has inf cost. proper policy just means that it has a positive chance to go to terminal state (basically proper policy don&#39;t inf loop, if inf loop, it must be a bad policy(inf cost)). </p>
<p></p>
<p>then you can always find a weight w s.t weighted max norm with that w will give you convergence you looking for. Neat!</p>
<p></p>",2019-10-24T15:38:34Z,25,Week 10/20 - 10/26,followup,,is5gzbotXmz,k24vfrien8k4ur,2019-10-24T15:38:34Z,{},project2
3176,no,<p>I scanned around online but couldn&#39;t locate the proof. But it&#39;s on pg. 248-249 &#34;neuro dynamic programming&#34; - Bertsekas</p>,2019-10-24T15:40:51Z,25,Week 10/20 - 10/26,feedback,,is5gzbotXmz,k24viox019p7z,2019-10-24T15:40:51Z,{},project2
3177,no,<p>Thank you. I&#39;ll take a look on it.</p>,2019-10-24T18:28:29Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k251ia3dipv4sp,2019-10-24T18:28:29Z,{},project2
3178,no,"<p>Tianhang zhu,</p>
<p>I was thinking on an intuitive way to understand the behavior of gamma=1.0 for an environment for with rewards on the terminal states only.</p>
<p>If we think on Q-learning method with gamma=1.0, all the state-actions that leads to the reward will have the same value (equals to the reward) after several iterations. At this point the agent will not be able to decide what action to take to maximize Q, because one of the actions may go back to the previous state which also has the same Q value than the state ahead (closer to the terminal state).</p>
<p>So, the agent could get stuck going back and forth between states with same Q-values and never reach the terminal state (if the environment is timed, otherwise it will eventually).</p>
<p>Does it make sense?</p>",2019-10-24T19:21:22Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k253e9z818p5nv,2019-10-24T19:21:22Z,{},project2
3179,no,"<p>That does make sense.  Setting $$\gamma$$ to less than $$1$$ &#34;rushes&#34; the agent, i.e. it tells the agent that there&#39;s a penalty for taking too long.  Right?</p>",2019-10-24T19:35:52Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k253wxkpyy14v8,2019-10-24T19:35:52Z,{},project2
3180,no,"<p>I would translate &#34;rushes&#34; the agent to actually points the agent towards states where Q-values are higher (but I think we are sharing the same opinion).</p>
<p></p>
<p>So, if gamma=1.0, all the Q-values on the path to the reward would have the same value (=reward). Therefore, if there is an action that takes the agent back to a previous state and another action to go forward to state closer to the reward, the agent will not be able to decide which one to take it.</p>",2019-10-24T19:51:33Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k254h3eyuyf2a0,2019-10-24T19:51:33Z,{},project2
3181,no,"<p>Nice thinking! Given the setup is that there is only one terminal state with R(s*, a*, s) = r when s is the terminal state and R = 0 otherwise and MDP is connected everywhere. I do agree that the Q table after infinite amount of samples will contain same values at all entries. And then you will most likely get a inf loop policy from it which get reward 0 in the end of time. </p>
<p></p>
<p>Firstly, I think this does violate one of the assumptions of the theorem that I posted. My bad. Since it assumes every improper policy gets -inf reward so that it&#39;s never chosen. </p>
<p></p>
<p>Second, from theoretical stand point, I don&#39;t think it will have asymptotic convergence to a optimal policy as I mentioned above, it will very likely have inf loop policy based on Qinf.</p>
<p></p>
<p>Third, from a practical stand point though. Given finite data, finite time updates, I think it will work(get a optimal policy without inf loop) because of the learning rate. Think about it, Q learning is closing the gap from current Q and Q*(or Qinf) exponential fast because of the learning rate. So for example, given just one episode arriving at the goal state, what does Q function looks like after this episode, it will have one Q(s,a) entry positive and everything else 0. what about you go through this same episode again and again? you will get a Q(s,a) path with increasing values to the goal state (this is because Q learning is only propogating signal back one step each time). This statement is true for all episode data. However, with a small probability, you might still get a inf loop policy???  I don&#39;t know an example that you get a inf loop in this case. But I guess it&#39;s a good practice to think about it a bit. And if you get any answer, please post! </p>",2019-10-25T00:34:07Z,25,Week 10/20 - 10/26,feedback,,is5gzbotXmz,k25ekgyawy11ve,2019-10-25T00:34:07Z,{},project2
3182,no,"<p>Ok... My head is trying to process what you and Vahe just mentioned here and in the next thread.</p>
<p></p>
<p>I ended up getting the following chart for alpha=0.999 brown curve (which I consider very close to 1). The agent learns in about 750 episodes and indeed scores well with a trained model at this stage. But if we keep the training, it starts degrading.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk2697q5zhhs9%2Fgamma_2.png"" alt="""" /></p>
<p></p>
<p>I haven&#39;t gotten to a point to fully understand this behavior yet.</p>",2019-10-25T16:01:06Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k26bokr1jb837p,2019-10-25T16:01:06Z,{},project2
3183,no,"<p>Wow Danilo, this is super-interesting.</p>
<p></p>
<p>The problem only starts once the agent has learned to land (converged to a near-optimal policy).  When $$\gamma=0.999$$ the agent learns exactly what to do, but then stops doing it and goes berserk.</p>
<p></p>
<p>If $$\gamma$$ is too low, the agent converges to a sub-optimal solution.  $$\gamma\in[0.985 - 0.995]$$ seems to be the sweet spot where the policy it converges to is good, <em>and</em> it maintains that policy over time.</p>
<p></p>
<p>I don&#39;t know your hyperparameter settings, but I wonder if these curves might change if some of the other hyperparameters change, for example a different epsilon decay, or a different size experience replay buffer, or a different batch size.  Too many combinations to try out...</p>",2019-10-25T17:01:57Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26duu7c14w1j,2019-10-25T17:01:57Z,{},project2
3184,no,<p>What does the loss for each episode look like?</p>,2019-10-25T17:31:37Z,25,Week 10/20 - 10/26,feedback,,jzlyi4e55bz5kj,k26ewznqqty782,2019-10-25T17:31:37Z,{},project2
3185,no,<p>That&#39;s a very clever analysis Danilo! If I only had another week .....</p>,2019-10-25T18:24:58Z,25,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k26gtle5lk7dj,2019-10-25T18:24:58Z,{},project2
3186,no,"<p><strong attention=""jzlyi4e55bz5kj"">&#64;Jonathan Hirscher</strong>  </p>
<p></p>
<p>Why can&#39;t the reward (y-axis) be used as a proxy for the loss?</p>",2019-10-25T18:30:16Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26h0etl5k36sv,2019-10-25T18:30:16Z,{},project2
3187,no,"<p>Jonathan,</p>
<p>I will need to re-run it at home to capture the loss. I didn&#39;t save that.</p>
<p></p>
<p>Jacob,</p>
<p>I wish I had 1 week more and also more pages to do the report. Keeping five pages only is difficult. </p>",2019-10-25T18:30:28Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k26h0o0qodu7pm,2019-10-25T18:30:28Z,{},project2
3188,no,so this is lunar lander not the case what we discuss about right. how many seeds did you average. is it DQN. dqn is quite unstable and sometimes behave like brown curve. try some other seeds. but it does seem to have a optimal gamma though for each env. it might be the case that 0.9999 is higher than the optimal one.,2019-10-25T19:46:59Z,25,Week 10/20 - 10/26,feedback,,is5gzbotXmz,k26jr2t8tw71yt,2019-10-25T19:46:59Z,{},project2
3189,no,"<p>Tianhang,</p>
<p></p>
<p>this is luna lander. I&#39;m using DQN with memory replay, but unfortunately I haven&#39;t done an average for different seeds because it takes too long to run each experiment (about 1h for 2000 episodes). </p>",2019-10-25T19:59:07Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k26k6oka1fw4j4,2019-10-25T19:59:07Z,{},project2
3190,no,"<p>Right, for Lunar Lander reward is given at more than just the terminal state.</p>
<p></p>
<p>Vahe, based on the experiences you are learning with (say in a replay buffer with a fixed size), it is possible for the loss to be decreasing while your reward is also decreasing. This could be related to issues with exploration, the learning rate, catastrophic forgetting, as a few examples. I&#39;m wondering if loss is actually decreasing during the training, which for this high gamma it may not be.</p>",2019-10-25T20:01:04Z,25,Week 10/20 - 10/26,feedback,,jzlyi4e55bz5kj,k26k96stxin124,2019-10-25T20:01:04Z,{},project2
3191,no,"<p>Thanks for the explanation Jonathan!</p>
<p></p>
<p>So if loss were decreasing but reward were also decreasing, that seems to suggest that we are overfitting to past memories that don&#39;t generalize well.  It seems like, ideally, we would want our replay buffer to be filled with memories that d<em>o</em> cause our neural network to generalize well.  And I guess catastrophic forgetting is related to shifting that mix of memories in a negative way?</p>",2019-10-25T20:14:57Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26kr1owgd17mn,2019-10-25T20:14:57Z,{},project2
3192,no,"<p>One nice statistic to collect is, in addition to the sum of rewards, the sum of discounted reward from initial states to the terminal state.</p>
<p></p>
<p>It&#39;s very interesting to see how at times, those numbers are very different (at least more than we intuitively think).</p>
<p></p>",2019-10-26T00:27:23Z,25,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k26trof3cyqwc,2019-10-26T00:27:23Z,{},project2
3193,no,"<p>Jonathan and Miguel,</p>
<p></p>
<p>I started the question for rewards only at the terminal states and somehow I got confused and started talking about Luna Lander. But I still think it is a good discussion.</p>
<p></p>
<p>Going back to Jonathan&#39;s question: &#34;What does the loss for each episode look like?&#34;. </p>
<p>How do I compute this loss? Is it the accumulation of all the losses for each step taken during a episode?</p>
<p></p>
<p>I will try what Miguel is suggesting.</p>",2019-10-26T13:29:50Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k27lpwhc90q6m6,2019-10-26T13:29:50Z,{},project2
3194,no,"<p>I think what Jonathan is referring to is the loss computed by the neural network.  When you create your network, you supply a loss function (in Keras, with the compile() method), which is the function that is trying to be minimized when training your network.</p>
<p></p>
<p>You can monitor its value over the length of your run to see how it is changing over time.</p>",2019-10-26T16:04:15Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k27r8hmn4fw1ij,2019-10-26T16:04:15Z,{},project2
3195,no,"<p>I think I see what you mean, but not sure if in Keras would be like:</p>
<p></p>
<pre>history = model.fit(np.vstack(minibatch[:, 0]), y, epochs=1, verbose=False)</pre>
<p></p>
<p>Then, by the end of each episode, I save the loss history in a list to be plotted later.</p>
<pre>episodes_loss.append(history.history[&#39;loss&#39;])</pre>
<p></p>
<p></p>
<p></p>",2019-10-26T16:21:38Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k27ruu94iop29s,2019-10-26T16:21:38Z,{},project2
3196,no,"<p>Without saying too much:</p>
<p></p>
<p></p>
<pre>In [1]: 0.9**300 * 200                                                                                                                                                                                                                         
Out[1]: 3.747855407769616e-12

In [2]: 0.99**300 * 200                                                                                                                                                                                                                        
Out[2]: 9.808178814257145

In [3]: 0.995**300 * 200                                                                                                                                                                                                                       
Out[3]: 44.458439968149385

In [4]: 0.997**300 * 200                                                                                                                                                                                                                       
Out[4]: 81.20401245834547

In [5]: 0.999**300 * 200                                                                                                                                                                                                                       
Out[5]: 148.14140643121985</pre>
<p></p>",2019-10-26T23:47:04Z,25,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k287rom0drg608,2019-10-26T23:47:04Z,{},project2
3197,no,"<p>Miguel,</p>
<p></p>
<p>I think I see your point and even discussed about that a little bit in my report, but I can&#39;t see the reason of the instability of training when gamma -&gt; 1. I&#39;ve observed this with simulation with other seeds as well, so I&#39;m taking that as a fact, but I can&#39;t find a good explanation of why.</p>
<p></p>
<p>Unfortunately, due to the due date my epsilon decayed too much and I can&#39;t explore more to find my answers (sometimes I need to try a joke).  I have to wrap up, but I would like to discuss that in office hours, if time allows us for that.</p>",2019-10-27T18:57:28Z,24,Week 10/27 - 11/2,feedback,,jc6xvgjncoey,k29cv3fcwj814h,2019-10-27T18:57:28Z,{},project2
3198,no,"<p>:) </p>
<p></p>
<p>looking forward to it.</p>",2019-10-27T19:36:20Z,24,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k29e92m0ybg577,2019-10-27T19:36:20Z,{},project2
3199,no,"<p>&#34;vanishing gradients&#34;.</p>
<p></p>
<p>discounting diminishes the potential size of the update to the NN.</p>
<p></p>
<p>of which the value depends on the actual reward.</p>
<p></p>
<p>that is, you tune it to the actual target values you are seeing, to keep it from growing, and eventually the back propogation kicks in.</p>
<p></p>
<p>its similar to your learning rate, and the key to time-structuring the neuronal updates.</p>
<p></p>
<p>but it only has value, if it doesn&#39;t inject divergence (or conversely, if it injects convergence AFTER escaping a minima).</p>
<p></p>
<p>the interplay between the alpha, gamma, and ... epsilon, the replay buffer, sampling rate, and average steps on a trajectory, is tremendous.</p>
<p></p>
<p>on mine, &gt; 0.9 led to instability.  0.50 led to stable results.  and I had to vary the alpha depending on what it actually encountered. </p>",2019-10-29T00:54:33Z,24,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2b5264ae8t2ti,2019-10-29T00:54:33Z,{},project2
3200,no,"<p>Danilo, tell me if you agree with this.</p>
<p></p>
<p>Let&#39;s say we had a task with only one reward, offered at the terminal state.</p>
<p></p>
<p>If the reward is positive, then setting $$\gamma$$ lower will encourage the agent to get to the terminal state as quickly as possible, since every time step wasted dminishes the reward by a factor of $$\gamma$$.  On the other hand, setting $$\gamma$$ closer to $$1$$ will make the agent indifferent to getting there quickly or slowly - it gets close to the same reward either way.</p>
<p></p>
<p>However, if the reward is negative, then setting $$\gamma$$ lower would encourage the agent to stall.  Why should it rush to get a negative reward?  It can just wait until the discounted value of that negative reward is close to $$0$$.  Setting $$\gamma$$ higher would make it more indifferent to stalling.  It&#39;s going to get the same negative reward no matter how long it takes.</p>
<p></p>
<p>In Lunar Lander, if the agent has crashed more than it has landed, it may associate the terminal state with a negative reward, encouraging it to stall, especially if $$\gamma$$ is set lower, since this way it can discount the value of that negative reward by waiting as long as possible.</p>",2019-10-24T21:55:20Z,25,Week 10/20 - 10/26,followup,,jzfsa4a37jf4aq,k258w9wpxfh2c7,2019-10-24T21:55:20Z,{},project2
3201,no,"<p>&#34;On the other hand, setting γ closer to 1 will make the agent indifferent to getting there quickly or slowly - it gets close to the same reward either way.&#34;</p>
<p></p>
<p>yeah, Q*(s,a) = gamma^k * r in this env, where k is the shortest distance from s to the goal. So it&#39;s a man made guidence structure to enforce shortest path which doesn&#39;t exist/matter in the original problem since it had no reward for being shortest.</p>
<p></p>
<p></p>",2019-10-25T00:46:13Z,25,Week 10/20 - 10/26,feedback,,is5gzbotXmz,k25f01856wc4il,2019-10-25T00:46:13Z,{},project2
3202,no,"<p>Right!</p>
<p></p>
<p>Thanks Tianhang.</p>",2019-10-25T01:07:25Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k25frauubis6y0,2019-10-25T01:07:25Z,{},project2
3203,no,"<p>Could you clarify again? Are you talking about the 100 trials using the trained agent that has already converged? If so, should we set the epsilon to 0 when taking an action? </p>",2019-10-23T22:59:26Z,41,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k23vqv74aosdl,2019-10-23T22:59:26Z,{},project2
3204,no,<p>Well I am turning off epsilon and just reporting the rewards for last 100 episodes</p>,2019-10-23T23:34:48Z,41,Week 10/20 - 10/26,feedback,,jqrr36mqfm8M,k23x0chj77c68,2019-10-23T23:34:48Z,{},project2
3205,no,"Yes, no training and no exploration.",2019-10-23T23:43:57Z,41,Week 10/20 - 10/26,feedback,,i4op5p9vfbq5yz,k23xc46ntjo4aj,2019-10-23T23:43:57Z,{},project2
3206,no,<p>See &#64;718 as well.</p>,2019-10-24T01:16:56Z,41,Week 10/20 - 10/26,feedback,,ixty1midfufhd,k240noxtr2h3bm,2019-10-24T01:16:56Z,{},project2
3207,no,"<p>Once your algorithm is &#39;done&#39;, just save the model / weights. Now, testing time = using this weights and do a greedy policy each step. I would suggest writing a separate function for this task. You are on the right track. </p>
<p></p>
<p>edit: This is your &#39;A&#39; description</p>",2019-10-24T07:38:52Z,41,Week 10/20 - 10/26,followup,,jvfpllmsggt7p4,k24eauq671j1s1,2019-10-24T07:38:52Z,{},project2
3208,no,<p>in the DQN paper they leave the epsilon parameter to 0.05 when testing so that the algorithm will still choose some random actions to counter overfitting. I didn&#39;t do this but it sounds like a good avenue to explore.</p>,2019-10-30T04:12:55Z,40,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k2crl3yfkvy7aq,2019-10-30T04:12:55Z,{},project2
3209,no,<p>Yes I am using Tensorflow/Keras. Thanks</p>,2019-10-23T22:57:29Z,25,Week 10/20 - 10/26,followup,,j6ln9puq99s5uv,k23vocjgej94ed,2019-10-23T22:57:29Z,{},project2
3210,no,"Did you get this to work out for you?  I worked on this for most of tonight and have it much more consistent, not every run is exact -- most are though. ",2019-10-24T06:39:57Z,25,Week 10/20 - 10/26,followup,,is9so9huTMp,k24c72ydiz25we,2019-10-24T06:39:57Z,{},project2
3211,no,"<p>that&#39;s what I have too. there are several places to set random seed in tensor flow (weight, optimizer). I only set weight seed. </p>",2019-10-25T06:23:32Z,25,Week 10/20 - 10/26,feedback,,j6ln9puq99s5uv,k25r1tknlcr5bg,2019-10-25T06:23:32Z,{},project2
3212,no,"<p>With PyTorch a simple &#96;torch.manual_seed(seed)&#96; does the job.</p>
<p></p>
<p>I&#39;m not sure how for TensorFlow. Maybe <strong attention=""i4op5p9vfbq5yz"">&#64;Chris Serrano</strong> ?</p>
<p></p>",2019-10-25T13:35:43Z,25,Week 10/20 - 10/26,feedback,,hyx9thiqa6j4nn,k266hm0w6c36t6,2019-10-25T13:35:43Z,{},project2
3213,no,"<p></p><pre>tensorflow.set_random_seed(seed)</pre>
<p></p>
<p>You need this for keras too if you use tensorflow as the backend.</p>",2019-10-27T18:02:58Z,24,Week 10/27 - 11/2,feedback,,jzsv9qgvs05p3,k29ax0obv007be,2019-10-27T18:02:58Z,{},project2
3214,no,"<p>Also, remember the lesson from HW3, don&#39;t use the &#96;env.action_space.sample()&#96; function.</p>",2019-10-25T13:36:32Z,25,Week 10/20 - 10/26,followup,,hyx9thiqa6j4nn,k266iojstgd7lm,2019-10-25T13:36:32Z,{},project2
3215,no,"<p>Actually I think firing the main thruster is -0.03 points per time step, but the side thrusters might be free.</p>",2019-10-23T23:55:33Z,25,Week 10/20 - 10/26,followup,,jl3oi5v7qkSk,k23xr0sf5a146p,2019-10-23T23:55:33Z,{},project2
3216,no,"<p>No, it&#39;s -0.3 reward per time step for the main thrusters and -0.03 for the side thrusters:</p>
<p></p>
<pre>reward -= m_power*0.30  # less fuel spent is better, about -30 for heurisic landing
reward -= s_power*0.03</pre>
<p></p>
<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py"">https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py</a></p>",2019-10-24T00:11:09Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k23yb3m4yx26jr,2019-10-24T00:11:09Z,{},project2
3217,no,I&#39;m having a similar issue where the lander just hovers off the ground. This is strange since I&#39;m setting max steps to 1000 but it doesn&#39;t to be actually implemented...,2019-10-24T03:32:30Z,25,Week 10/20 - 10/26,followup,,ijablgztpPZ,k245i0qx24y2qw,2019-10-24T03:32:30Z,{},project2
3218,no,"<p>Without doing anything, max steps is 1000 - that&#39;s the default, unless you&#39;re &#34;unwrapping&#34; your instantiation of Lunar Lander.</p>
<p></p>
<p>Hovering is completely normal.  Give it some time and see if it learns to land.</p>",2019-10-24T03:45:05Z,25,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k245y78ln476oe,2019-10-24T03:45:05Z,{},project2
3219,no,<p>I am having the same issue.  I hit positive rewards around 180 episodes and after about 300-400 the lander just learns to hover in place.  This continues basically forever.  I stopped it at ~4k episodes to check and it&#39;s still just hovering.  Is it advisable to add a slight penalty to hitting some artificial max steps?  Or should I just let it keep trying (epsilon is very low by this point) and hope it eventually gets down towards the goal again.  Earlier runs have generated 200&#43; rewards but that was in the days before it decided hovering was the best it could do.</p>,2019-10-24T10:00:36Z,25,Week 10/20 - 10/26,feedback,,idfzuuu9fnI,k24jd4zpn163b9,2019-10-24T10:00:36Z,{},project2
3220,no,<p>If your agent is getting stuck in a local minima (i.e. just hovering) then maybe make sure to keep your epsilon high enough for it to have a chance of breaking out of the local minima. Giving it a small but non-negligible epsilon allows it to occasionally &#34;go wrong&#34; and land by accident. These are the episodes that are going to get it to realise that hitting the ground isn&#39;t <em>always</em> a bad thing.</p>,2019-10-24T10:25:06Z,25,Week 10/20 - 10/26,feedback,,jl3oi5v7qkSk,k24k8n6bsm45ce,2019-10-24T10:25:06Z,{},project2
3221,no,<p>I was wondering if that behavior is not related to my question in &#64;754.</p>,2019-10-24T20:38:48Z,25,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k2565uwiojk2nv,2019-10-24T20:38:48Z,{},project2
3222,no,<p>The problem is mine gets high (200&#43;) rewards around the 200 episode mark and then tanks after that and learns to just hover.</p>,2019-10-24T23:36:15Z,25,Week 10/20 - 10/26,feedback,,idfzuuu9fnI,k25ci1yvu6y3ye,2019-10-24T23:36:15Z,{},project2
3223,no,<p>Did you vectorize everything except for looping through episodes?</p>,2019-10-24T02:17:44Z,33,Week 10/20 - 10/26,followup,,jzhy4sbxyar5l4,k242tvjcv9e6ka,2019-10-24T02:17:44Z,{},project2
3224,no,"<p>Looping through episodes <em>and</em> looping through time steps <em>both </em>cannot be vectorized.  But for sure this will help, probably at least a 2x speedup.</p>",2019-10-24T02:20:36Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k242xjys87u1yv,2019-10-24T02:20:36Z,{},project2
3225,no,"<p>Yeah,  thanks for clarifying that.</p>",2019-10-24T02:21:23Z,33,Week 10/20 - 10/26,feedback,,jzhy4sbxyar5l4,k242ykoc39d753,2019-10-24T02:21:23Z,{},project2
3226,no,"<p>what is there to vectorize? </p>
<p></p>
<p>I&#39;ve been trying the simple DQN cartpole-v0 case with literally just a neural net. how do you vectorize the process when there aren&#39;t arrays to index into? </p>
<p></p>
<p>sorry, i&#39;m new to python and to how it applies vector operations. </p>",2019-10-24T04:14:14Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k246zp0t7nd4iw,2019-10-24T04:14:14Z,{},project2
3227,no,"<p>Well, since you are using NN, I think you are probably using some deep learning framework? If so, I think the problem may not be with vectorization because those packages should have taken care of it. </p>",2019-10-24T04:21:14Z,33,Week 10/20 - 10/26,feedback,,jzhy4sbxyar5l4,k2478otscyg6an,2019-10-24T04:21:14Z,{},project2
3228,no,<p>I was just using a simple Keras NN and implementing the learning agent... I wasn&#39;t aware that we were allowed to use any other packages other than those to build the network... </p>,2019-10-24T04:23:44Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k247bwv43xc1yo,2019-10-24T04:23:44Z,{},project2
3229,no,"<p>You can really only vectorize if there&#39;s a loop in your update step.  If you&#39;re not using experience replay, which it sounds like you&#39;re not, then yeah, there&#39;s probably nothing to vectorize.</p>",2019-10-24T04:27:16Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k247ggdanno6gy,2019-10-24T04:27:16Z,{},project2
3230,no,"<p>Keras is certainly one of the deep learning packages I mentioned. Vectorization basically means calculating gradients, deltas etc. by linear algebra, which is much faster than doing them by for loops. But such packages should already hide all these implementation details, so you don&#39;t need to worry about it.</p>",2019-10-24T04:28:36Z,33,Week 10/20 - 10/26,feedback,,jzhy4sbxyar5l4,k247i6hvtc1h0,2019-10-24T04:28:36Z,{},project2
3231,no,"<p>If you are running out of ram, you might consider the size of your neural networks, and the size of the experience replay memory. I don&#39;t know how much ram we are talking about here, but for my runs, it seemed to want to consume ~2GBs of ram. If you are using up all the ram, and the computer starts trying to offload that to virtual memory, it&#39;s going to kill performance quick a bit. Based on what I&#39;ve read from other posts, it seems like people have been solving lunar lander in the area of 2,000-8,000 episodes; with the earlier episodes going faster then the later ones. I&#39;m also in the 2-4k range for solving.</p>",2019-10-24T02:56:35Z,33,Week 10/20 - 10/26,followup,,ixty1midfufhd,k2447ujxnkt723,2019-10-24T02:56:35Z,{},project2
3232,no,"<p>I would also second the advice from George. Start with a small network, small episode, small minibatch, then collect your reward per episode for spot check. As long as your agent keeps learning something new, evident by your graph seems going upward, you are likely on the right track. Then you can experimentally increase those hyperparameters, to solve the problem more efficiently and to see your problem is.</p>
<p></p>
<p>I start solving the Cartpole with 10,000 episodes and 1,000,000 experience memory (just like in Mnih Nature paper used), before I realize that I was doing something totally wrong. After fixing it, I start to get very good result around 100 episodes, even with much smaller hyperparemeter.</p>",2019-10-24T03:24:00Z,33,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k24573b8a301o6,2019-10-24T03:24:00Z,{},project2
3233,no,"<p>Also check out line profiler:</p>
<p><a href=""https://github.com/rkern/line_profiler"">https://github.com/rkern/line_profiler</a></p>
<p></p>
<p>The pip installation for latest version is currently broken, you would need to do:</p>
<pre>pip install cython<br />pip install git&#43;https://github.com/rkern/line_profiler#egg=line_profiler</pre>
<p>Then place &#64;profile on the function you want to profile and call the profiler:</p>
<pre>kernprof -lv yourscript.py</pre>
<p>The bottleneck should be your call to the network and your model train, if the bottleneck is somewhere else that would likely be the problem.</p>
<p></p>",2019-10-24T03:37:22Z,33,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k245oag45kx7bn,2019-10-24T03:37:22Z,{},project2
3234,no,"<p>my neural net (which I plan on changing for the lunar landing problem, so I assume the temporary hyperparameters don&#39;t matter?) only has ~1000 parameters. no minibatch, I&#39;m following the advice of the student who said they were able to complete cartpole in a few minutes with a simple NN, no replay, and no varying parameters. </p>",2019-10-24T04:16:40Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k2472tn1no32f,2019-10-24T04:16:40Z,{},project2
3235,no,<p>How much RAM do you have on your laptop?</p>,2019-10-24T04:29:50Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k247jqzycm21wv,2019-10-24T04:29:50Z,{},project2
3236,no,"<p>8GB. I can&#39;t see a single point in my code where I&#39;m accumulating too much of any kind of memory, but there&#39;s apparently some issue with Keras .fit or .train_on_batch that can lead to excessive use of RAM? something seems out of place to me and I&#39;ve been combing for days trying to find out what it could be. </p>",2019-10-24T04:32:02Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k247ml1psh63o7,2019-10-24T04:32:02Z,{},project2
3237,no,"<p>Yeah, that seems strange.  Each run only consumes about 0.5 GB of RAM, max, for me.  So there is something very wrong going on somewhere in the pipeline for you.</p>",2019-10-24T04:33:54Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k247ozlmn6c287,2019-10-24T04:33:54Z,{},project2
3238,no,"<p>is there some kind of caching that keras does that would accumulate data? all of my variables are reset after either each timestep or each episode, except for the model itself, the environment, and two lists I use for plotting episode scores. i&#39;ve got nowhere to look, so anything is fair game for me to check... </p>
<p></p>
<p>could versioning of the packages be an issue, somehow? </p>",2019-10-24T04:37:51Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k247u2sw2jc201,2019-10-24T04:37:51Z,{},project2
3239,no,"<p>Can you try maybe incrementally adding things?  Like run your algorithm without using the neural net, monitor how much RAM you&#39;re using.  Then add a neural net with no hidden layers - just one dense layer that receives the input and has the output with a linear activation.  And maybe just make a predict() call with it, no fit().  Basically some strategy to see when memory usage jumps from ~3GB to ~8GB.</p>",2019-10-24T04:39:57Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k247ws1ok7h52p,2019-10-24T04:39:57Z,{},project2
3240,no,"<p>I can try that... give me a few minutes :) </p>
<p></p>
<p>and thanks for the help... can&#39;t express how frustrating this has been... </p>",2019-10-24T04:41:17Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k247yhm9e5e4ua,2019-10-24T04:41:17Z,{},project2
3241,no,"<p>so I can get it to happen if</p>
<pre>model.predict(state)</pre>
<p>is called, where my neural net summary is</p>
<pre>Model: &#34;sequential&#34;<br />_________________________________________________________________<br />Layer (type)                 Output Shape              Param #   <br />=================================================================<br />dense (Dense)                (None, 64)                320       <br />_________________________________________________________________<br />dense_1 (Dense)              (None, 2)                 130       <br />=================================================================<br />Total params: 450<br />Trainable params: 450<br />Non-trainable params: 0</pre>",2019-10-24T04:53:12Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k248dthw4b43sv,2019-10-24T04:53:12Z,{},project2
3242,no,<p>and no training...</p>,2019-10-24T04:53:42Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k248egg62s04p3,2019-10-24T04:53:42Z,{},project2
3243,no,"<p>Out of curiosity, why do you have 5 inputs?  CartPole only has 4 inputs.</p>",2019-10-24T04:57:21Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k248j5mdj091me,2019-10-24T04:57:21Z,{},project2
3244,no,"<p>my first layer is</p>
<pre>Dense(64, input_dim=state_size, activation=&#39;relu&#39;)</pre>
<p>so i&#39;m not actually sure how it&#39;s getting 5 inputs...?</p>",2019-10-24T05:01:25Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k248odch5v45hr,2019-10-24T05:01:25Z,{},project2
3245,no,<p>Your state_size is wrong. What are you setting it to?</p>,2019-10-24T05:02:04Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k248p7qygwn7d3,2019-10-24T05:02:04Z,{},project2
3246,no,"<p>how is that possible...? it&#39;s </p>
<pre>env.observation_space.shape[0]</pre>",2019-10-24T05:04:42Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k248sl85kmc2nw,2019-10-24T05:04:42Z,{},project2
3247,no,"<p>i also get 320 params when i put </p>
<pre>Dense(64, input_dim=4, activation=&#39;relu&#39;)</pre>",2019-10-24T05:06:43Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k248v6wmu8r4xb,2019-10-24T05:06:43Z,{},project2
3248,no,"<p>it is correct. the input dim is (4,) since param = 320 for the first layer: </p>
<p></p>
<p>i.e (320 - 64) / 64 =4 </p>",2019-10-24T05:08:09Z,33,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k248x1hl5yh2mp,2019-10-24T05:08:09Z,{},project2
3249,no,<p>why are you subtracting 64 though?</p>,2019-10-24T05:09:24Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k248yng43v31f9,2019-10-24T05:09:24Z,{},project2
3250,no,"<p>&#64;vahe. this is how the neural network work. there are weights and bias for each unit. Each unit (here we have 64) will have 4 weights as well as a bias. </p>
<p></p>
<p>(64 * 4) = weights </p>
<p>64 bias </p>
<p></p>
<p>total = 320 param</p>",2019-10-24T05:10:23Z,33,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k248zwzxxfo4mj,2019-10-24T05:10:23Z,{},project2
3251,no,"<p>i&#39;ve confirmed, any interaction with the model (.fit and .predict) cause the memory to jump up and slow down the process. and I see that for either/or and both of those function calls... </p>
<p></p>
<p>smells like a keras problem? the predict() is only called during exploitation and isn&#39;t assigned to anything, and the fit() does not get assigned to anything...</p>",2019-10-24T05:15:16Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k24966i7dg56xs,2019-10-24T05:15:16Z,{},project2
3252,no,"<p>For some reason I was thinking that the first layer was just the inputs, but it&#39;s actually the first hidden layer, so yes, I see that there should be a bias there.</p>",2019-10-24T05:15:41Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k2496qe8mr7lr,2019-10-24T05:15:41Z,{},project2
3253,no,"<p>What&#39;s the shape of the state vector that you&#39;re feeding to .predict()?</p>
<p></p>",2019-10-24T05:17:51Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k2499i118x129y,2019-10-24T05:17:51Z,{},project2
3254,no,"<p></p><pre>np.reshape(s, (1, len(s))</pre>",2019-10-24T05:19:55Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k249c6073no225,2019-10-24T05:19:55Z,{},project2
3255,no,<p>edit: see below new post</p>,2019-10-24T05:25:21Z,33,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k249j5qqhlo3la,2019-10-24T05:25:21Z,{},project2
3256,no,"<p>I don&#39;t think he&#39;s using an experience replay memory, so nothing to vectorize.</p>
<p></p>
<p>The AVX2 warning shouldn&#39;t be a problem - that just means Tensorflow isn&#39;t using your CPU&#39;s complete instruction set.  I get that warning and ignore it, with no problems.</p>",2019-10-24T05:27:58Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k249mj27w5l2e0,2019-10-24T05:27:58Z,{},project2
3257,no,"<p>well damn... train/predict on batch keeps memory low with just one sample at a time... not sure if it&#39;ll work with actual batch prediction, but maybe that&#39;ll be enough to get this thing going... </p>
<p></p>
<p>any idea why that&#39;s different than .fit() and .predict()?</p>",2019-10-24T05:32:31Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k249sdm8i172,2019-10-24T05:32:31Z,{},project2
3258,no,"<p>LOL. Well that&#39;s great.  At least you can work now.</p>
<p></p>
<p>No, no idea...</p>",2019-10-24T05:33:18Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k249tdfmh0b2y5,2019-10-24T05:33:18Z,{},project2
3259,no,<p>i&#39;ll let you know how it performs with actual batches of data when I get to replay... but SHEESH that&#39;s... that&#39;s something else...</p>,2019-10-24T05:34:19Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k249uozof0j63d,2019-10-24T05:34:19Z,{},project2
3260,no,"<p>y&#39;all are life savers... </p>
<p></p>
<p>just went from 400 epochs taking around 30 minutes (at least...) to about 2 minutes... WOW</p>",2019-10-24T05:47:03Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k24ab1z2z2y3m4,2019-10-24T05:47:03Z,{},project2
3261,no,<p>That was all Germayne.  I would have never suspected that one set of Keras calls was broken on your system.</p>,2019-10-24T05:52:23Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k24ahwq2f7x4g7,2019-10-24T05:52:23Z,{},project2
3262,no,<p>hey glad you solved it. Now hurry on to LL :)</p>,2019-10-24T07:32:01Z,33,Week 10/20 - 10/26,feedback,,jvfpllmsggt7p4,k24e21qvl5wt,2019-10-24T07:32:01Z,{},project2
3263,no,"<p>&#64;zachary i have slow training issue and i dont think i solved mine completely but maybe i can offer some tips: </p>
<p> </p>
<p>1. Vectorize your code for the TD target part. i.e your only loop should be the 2 loops for each episodes as well as each step of an episode (if you have experience replay)</p>
<p>2. use .train_on_batch instead of fit and .predict_on_batch instead of predict </p>
<p>3. use keras and tensorflow for cpu instead of GPU. </p>
<p>4. check if your tensorflow is no outputing this AVX2 warning. if so you must install tensorflow here:<a href=""https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide#Anaconda_main_win"" target=""_blank"" rel=""noopener noreferrer"">https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide#Anaconda_main_win</a> </p>
<p>(more of a windows thing)</p>
<p></p>
<p>previously i have this:</p>
<p></p>
<pre>I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 </pre>
<p>edit for 4: im not sure if it helps but from what i found, solving AVX2 issue gives performance boost. i did anyways</p>",2019-10-24T05:26:39Z,33,Week 10/20 - 10/26,followup,,jvfpllmsggt7p4,k249ktz0r8n506,2019-10-24T05:26:39Z,{},project2
3264,no,"<p>Yeah, I&#39;m sure it does give a boost.</p>",2019-10-24T05:32:26Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k249s957bsav1,2019-10-24T05:32:26Z,{},project2
3265,no,"<p>also, &#64;QuangVu, any suggestions on what got your episode count down? Was that including minibatch replay? any suggestions on the size of your NN? my agent is getting stuck in some local optima, it seems.... </p>
<p></p>
<p></p>
<p>anything that was helpful for you that&#39;s allowed to be shared would be appreciated :) </p>",2019-10-24T22:51:56Z,33,Week 10/20 - 10/26,feedback,,hbmelkhwx5a5d3,k25ax2o7hxv26t,2019-10-24T22:51:56Z,{},project2
3266,no,"<p>It was a bug in my implementation. Basically the agent get smarter the more it is trained, (let ignore overfit for now). So, more of episode does not necessary means the agent gets enough training. That was my bug.</p>
<p></p>
<p>&#64;Germayne, thanks for sharing this. Interestingly, &#96;predict_on_batch&#96; is roughly 2x faster than &#96;predict&#96; with set &#96;batch_size&#96;, even though they seem to do the same thing.</p>",2019-10-25T03:39:59Z,33,Week 10/20 - 10/26,feedback,,jl5wq8mca7o0,k25l7id6fra5s6,2019-10-25T03:39:59Z,{},project2
3267,no,"In addition to all the tips about vectorization, something that helped me a lot was setting up a Google Cloud VM. I got $300 of credit for creating the account, and it only costs $1-2/day to run the VM. I can launch my Python script and let it run overnight so I don&#39;t have to worry about the processing power of my laptop. It took me about an hour to launch the VM and configure it with the proper environment",2019-10-25T12:46:33Z,33,Week 10/20 - 10/26,followup,,jl284xdcifz44g,k264qei6s011v,2019-10-25T12:46:33Z,{},project2
3268,no,<p>chris suggested &#34;google collab&#34; (free python notebooks) in office hours.</p>,2019-10-26T07:56:28Z,33,Week 10/20 - 10/26,feedback,,jzivtxcbl6964n,k279t7d1wj3s2,2019-10-26T07:56:28Z,{},project2
3269,no,"<p>The targets start off as non-zero but since the target is the prediction from Q itself only with some values changed based on the following rule, it gradually becomes 0s.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjqob6okzSK1c%2Fk249yjtj54i%2FScreen_Shot_20191023_at_10.36.24_PM.png"" alt="""" /></p>",2019-10-24T05:38:23Z,33,Week 10/20 - 10/26,followup,,jqob6okzSK1c,k249zwkjduy70a,2019-10-24T05:38:23Z,{},project2
3270,no,And you&#39;re sure the $$y_j$$ is not 0s?,2019-10-24T05:40:27Z,33,Week 10/20 - 10/26,feedback,,i4op5p9vfbq5yz,k24a2ktgrn7ph,2019-10-24T05:40:27Z,{},project2
3271,no,<p>I think I get what you are pointing at. For me I had taken y = Q.predict(on mini_batch) and this y was only kept for that particular iteration. I mean it is not global but a local variable . Later I updated the yj and used it as the target. So most of the values were 0s. Are you suggesting you I should use a global Y.</p>,2019-10-24T05:44:28Z,33,Week 10/20 - 10/26,feedback,,jqob6okzSK1c,k24a7qjrbva436,2019-10-24T05:44:28Z,{},project2
3272,no,"<p>What Chris is referring to is the targets for each action. The target y for each action should be the same value as your model predicts for the state s, except for the action a that you have taken to go to s&#39; (this one should be by the equation that you posted above).</p>",2019-10-24T20:22:24Z,33,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k255krzipuf4ac,2019-10-24T20:22:24Z,{},project2
3273,no,<p>It turns out the issue was learning rate only. I had made it small but not small enough (should be less than 0.05). Now it is predicting non-zero values.</p>,2019-10-25T23:20:26Z,33,Week 10/20 - 10/26,feedback,,jqob6okzSK1c,k26rdkckphd1f2,2019-10-25T23:20:26Z,{},project2
3274,no,<p>check if there is a bug in your playing code. did you forget to set the next state to be your current state? </p>,2019-10-24T07:36:38Z,33,Week 10/20 - 10/26,followup,,jvfpllmsggt7p4,k24e7zm4jl55jw,2019-10-24T07:36:38Z,{},project2
3275,no,<p>I did set the next state to be current state. I will check if there are any other bugs.</p>,2019-10-24T18:23:51Z,33,Week 10/20 - 10/26,feedback,,jqob6okzSK1c,k251caxbzca5af,2019-10-24T18:23:51Z,{},project2
3276,no,"<p>And as Michael Littman pointed out in one of the lectures, weight initialization is also equivalent to reward shaping.</p>
<p></p>
<p>In reference to activation selection, this link provided by Michael Anuzis in &#64;700 : <a href=""https://playground.tensorflow.org/"" target=""_blank"" rel=""noopener noreferrer"">https://playground.tensorflow.org/</a> lets you see, visually, the differences between, for example, ReLU and tanh.  ReLU seems to have more squarish edges.</p>",2019-10-25T01:13:56Z,33,Week 10/20 - 10/26,followup,,jzfsa4a37jf4aq,k25fzokdw286ca,2019-10-25T01:13:56Z,{},project2
3277,no,"<p>interestingly, relu worked good for batch; but sequential iterations (from replay) tan-h worked better.  this is using a different loss metric (combined mae/mse) than rmse (which pretty much sucked).</p>
<p></p>",2019-10-26T06:31:58Z,33,Week 10/20 - 10/26,feedback,,jzivtxcbl6964n,k276sisdgg5326,2019-10-26T06:31:58Z,{},project2
3278,no,<p>Have you tried a smaller min_epsilon? I think min_epsilon=0.1 it may be too big to converge in only 1000 episodes. 10% of the actions being random by the end of your training cycle sounds too much.</p>,2019-10-24T18:19:19Z,33,Week 10/20 - 10/26,followup,,jc6xvgjncoey,k2516hadiqn1t1,2019-10-24T18:19:19Z,{},project2
3279,no,"<p>Also, it may happen that your agent is not seeing the future rewards, it may be the case to increase your gamma.</p>",2019-10-24T18:23:55Z,33,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k251cdxqe3v2lp,2019-10-24T18:23:55Z,{},project2
3280,no,"<p>Thank you. gamma=0.9, alpha decays from 0.001 to 0.0001, network has 3 hidden layers each 128 nodes, experience replay memory is 1e6 and batch size is 1000. I have been playing with the hyperparameters for a while, but still do not have good enough sense to know where is the problem. Would 5000 episode solve the problem? do you also set any regularization (I do not think over-fitting can cause any problem here)? In your rewards graph, do you see any decrease in average rewards?</p>",2019-10-24T19:01:47Z,33,Week 10/20 - 10/26,followup,,jzhghvgidip601,k252p3ihsrlog,2019-10-24T19:01:47Z,{},project2
3281,no,"<p>Experiment increasing gamma. There is a big influence of gamma and small changes makes difference on the training.</p>
<p>Also, I don&#39;t know how long is taking to run it for you, but I think the batch size may be delaying it. I don&#39;t have much knowledge to say how it would affect the overall performance, but my batch sizes are much smaller (less than 50).</p>",2019-10-24T20:12:41Z,33,Week 10/20 - 10/26,feedback,,jc6xvgjncoey,k25589kugz1f5,2019-10-24T20:12:41Z,{},project2
3282,stud,"<p>Thanks for responding. Is the following acceptable? </p>
<p>Note that:</p>
<ol><li>There are many scripts to run</li><li>The scripts generate the charts and data used in the paper, but do not directly generate the best hyperparameters. There are way too many combinations to do a full grid search, and so I am studying the effects of one or two HPs at a time, and the final result depends on some intuition and luck.</li></ol>
<p></p>
<p>STEP 1 - Try different hyperparameters and save the results </p>
<p>(typically run a multiple machines but can run on one machine too)</p>
<p>===============================================</p>
<p>python main.py --hyper HyperBase --test TestNN1</p>
<p>python main.py --hyper HyperBase --test TestNN2</p>
<p>python main.py --hyper HyperBase --test TestNN3</p>
<p>python main.py --hyper HyperBase --test TestNN4</p>
<p>python main.py --hyper HyperBase --test TestepsilonAlpha1</p>
<p>python main.py --hyper HyperBase --test TestepsilonAlpha2</p>
<p>python main.py --hyper HyperBase --test TestepsilonAlpha3</p>
<p>...</p>
<p></p>
<p>STEP 2: Merge the outputs and generate the charts</p>
<p>========================================</p>
<p>python merge.py </p>
<p>python gen_charts.py</p>
<p></p>
<p>STEP3: Run with the best hyperparameters and generate the charts/video</p>
<p>========================================================</p>
<p>python main.py --hyper BestHyperParameters</p>
<p></p>",2019-10-24T23:55:05Z,56,Week 10/20 - 10/26,followup,a_0,,k25d6a3fe4b5i4,2019-10-24T23:55:05Z,{},project2
3283,no,"<p>seems alright to me, perhaps include a way to setup the environment or list the steps necessary to replicate your environment you used. you could also include any necessary details needed to view the outputs (will they show up as outputs or will they be stored?). etc.</p>
<p></p>
<p></p>",2019-10-25T00:31:46Z,56,Week 10/20 - 10/26,feedback,,jl1acpoc4HA9,k25ehge9s037ci,2019-10-25T00:31:46Z,{},project2
3284,no,"I have submitted the code with README on how to run:
1. training from stretch until it got 200 scores or above
=&gt; then generate 
      a. chart of rewards during the training
      b. chart on testing for 100 episodes on the newly trained model
2. testing the successful model I built for my report for 100 episodes

&#64;instructors
Is it enough for code submission please?",2019-10-25T15:49:44Z,56,Week 10/20 - 10/26,followup,,jr7165ipLPTP,k26b9yrjof7c3,2019-10-25T15:49:44Z,{},project2
3285,no,<p>What is the difference between 1.b and 2?</p>,2019-10-25T15:57:54Z,56,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26bkh6gfpz68o,2019-10-25T15:57:54Z,{},project2
3286,no,"1b. uses the freshly trained model by 1
2. uses a saved pre-built model which was used in my report.",2019-10-25T18:20:18Z,56,Week 10/20 - 10/26,feedback,,jr7165ipLPTP,k26gnlj25771y,2019-10-25T18:20:18Z,{},project2
3287,no,"<p>Oh, I see.  You&#39;re evaluating each trained model in 1b.  Then you&#39;re re-evaluating the best one again in 2.</p>
<p></p>
<p>But you should already have that from 1b when you trained that best model, no?</p>",2019-10-25T18:23:08Z,56,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26gr8vvugd34h,2019-10-25T18:23:08Z,{},project2
3288,no,"I did not set a random seed, so the model is slightly different every time it gets trained.
I bet I should make it more reproducible, thanks.",2019-10-26T01:26:03Z,56,Week 10/20 - 10/26,feedback,,jr7165ipLPTP,k26vv4aohpr6oc,2019-10-26T01:26:03Z,{},project2
3289,no,"<p>I&#39;m under the impression that for this project, we don&#39;t have to make the graphs (exactly) reproducible.  Am I wrong?</p>",2019-10-26T01:42:17Z,56,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26wfzrxxt27lj,2019-10-26T01:42:17Z,{},project2
3290,no,<p>You should try to make the results reproducible as closely as possible.  You can set the seed for the random number generators to help out.</p>,2019-10-28T00:21:25Z,55,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k29ofplp31565b,2019-10-28T00:21:25Z,{},project2
3291,no,<p>TL;DR?</p>,2019-10-30T03:47:59Z,22,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k2cqp1zch0c437,2019-10-30T03:47:59Z,{},project1
3292,no,"<p>probably.  its a conversation ;) with a lot of points.  its a sort of grin and bear it type thing.  if you can make it through, it might stimulate some discussion.</p>
<p></p>
<p>I have a lot of conversations with people, some of them are interesting enough to repost.</p>
<p></p>
<p>this was post project 1, pre-project 2.</p>
<p></p>
<p>one of the tldr(s) is:</p>
<p></p>
<p>td suffers from nonlinear issues.  approximation via markov walk, can be transformed into a simpler mdp where the sampling probability approximates the final expected reward distribution (given an &#34;optimal&#34; policy).  there is no need to compute a time-structured markovian walk when a simpler transformed problem -gives substantially same results- (trading computational efficiency for bias).  and this predisposes the markov problem, to being particularly well-suited to RL; which makes use of MDP as a test standard, suspect.  and the non-linearity implicit in the TD algorithm, is odd, given the scaling stochastic descent, the recency weighted average AND the linear combination of features.</p>
<p></p>
<p>my understanding since then, has been significantly amplified ;) but the essential conclusions, remain unchanged.</p>
<p></p>
<p>also there is another property of the markov process that i discovered that I did not publish or discuss.  but it is super, super, interesting.</p>
<p></p>
<p>generally speaking, I prefer discussion in PM; and generally don&#39;t post my PM&#39;s.  I am trying, something new per a suggestion.</p>",2019-10-31T00:48:23Z,22,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2dzpxx4dq22t4,2019-10-31T00:48:23Z,{},project1
3293,no,<p>What&#39;s the difference between wrapped and unwrapped? I remember the suggestion to use the unwapped environment in one of the homeworks but used the plain result in the project.</p>,2019-10-30T03:46:45Z,40,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k2cqngmx6nvuu,2019-10-30T03:46:45Z,{},project2
3294,no,<p>One difference is the absence of an artificial time limit on episodes.</p>,2019-10-30T04:37:43Z,40,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2csh0huoc45rf,2019-10-30T04:37:43Z,{},project2
3295,no,<p>I actually *had* to do something similar to get my algorithm to converge at all. It kept getting to around 100-150 reward and then eventually would death spiral down. </p>,2019-10-27T04:31:38Z,55,Week 10/27 - 11/2,followup,,jzozvpx25to679,k28hxmzjw9732o,2019-10-27T04:31:38Z,{},project2
3296,no,<p>meaning narrowing the epsilon after the reward got better worked for me. </p>,2019-10-27T04:32:04Z,55,Week 10/27 - 11/2,feedback,,jzozvpx25to679,k28hy6p572k3b3,2019-10-27T04:32:04Z,{},project2
3297,no,"<p>you mean something like $$\epsilon_i = f(reward_{i-1})$$ ?</p>
<p></p>",2019-10-27T14:36:38Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k293jnk2bdu2wk,2019-10-27T14:36:38Z,{},project2
3298,no,"<p>I agree with all your points and have also been thinking about this. Your final paragraph seems to imply that we want the replay buffer to be large enough (to help with point 1) but not <em>too</em> large (to help with point 2).</p>
<p></p>
<p>This would seem to give us the best of both worlds:  samples that come from a distribution that is close to the distribution resulting from following the current policy (buffer not too large), but also making it likely that they aren&#39;t consecutive or nearly consecutive samples from the same episode, so that they aren&#39;t highly correlated (buffer large enough).</p>",2019-10-25T16:29:47Z,33,Week 10/20 - 10/26,followup,,jzfsa4a37jf4aq,k26cphat18z15v,2019-10-25T16:29:47Z,{},project2
3299,no,"<p>This paper discusses the replay memory: Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.</p>
<p></p>
<p>They are saying that the replay memory helps to converge faster (aka train the weights). For example, if your environment consists of two areas of A and B. Then your agent moves from A to B and stuck in B, neural nets tend to forgot the past experience and start adjusting the weights for just area B. The replay memory helps to remember what happened in the past aka area A. In my understanding, this is the main idea behind the replay memory. Reuse your past experience to converge faster.</p>
<p></p>
<p>Sampling of the replay area is done to make your observations independent since neural nets need independency. </p>
<p></p>
<p>Since we want to have our distributions of observations to be identically distributed, then you need to be careful wth past experiences as they might not be relevant to the present. </p>",2019-10-25T18:15:44Z,33,Week 10/20 - 10/26,followup,,jzih0fdt4sn1cq,k26ghprvdp3fe,2019-10-25T18:15:44Z,{},project2
3300,no,"<p>That&#39;s correct, so long as your replay buffer is long enough to capture those past memories. The problem is when your NN guides you down a path that squashes those past memories and you end up in a self-fulfilling pit. How would you detect that and mitigate it?</p>",2019-10-25T18:17:27Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k26gjxxlan81p8,2019-10-25T18:17:27Z,{},project2
3301,no,"<p>Not sure, if I understand what you ask, but in the paper they discuss &#34;smart&#34; sampling as well. You can alsway sample in such way, so you will be able to mitigate many things. I briefly looked at the paper yesterday, but it has a lot of good things about replay memory and different sampling methods</p>",2019-10-25T18:23:57Z,33,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k26gsaunbdx5ls,2019-10-25T18:23:57Z,{},project2
3302,no,"<p>To reference your post about needing to reset the memory buffer ... Let&#39;s say you have one that is length 50. Do you need to keep any memories in it? Your current episode will overwrite it constantly with new memories (on-policy memories perhaps), and so it won&#39;t have any past-experience bias.</p>
<p></p>
<p>Let&#39;s say you had a 10,000 memory buffer. Now you have a long term memory bank that could bring in early life memories of crashing constantly, or late life memories of landing. Hmmmm. That might have a better chance of saving me when all of my short term memories are about walking in a room, in a circle.</p>
<p></p>
<p>But I don&#39;t want to have too many of those past memories because they were all likely non-optimal policy memories that I&#39;d rather forget.</p>
<p></p>
<p>Like everything in RL, there is some strength to argue for a dynamic buffer that shortens with time so that the agent forgets useless memories but doesn&#39;t hyperfocus on current memories.</p>
<p></p>",2019-10-25T18:36:23Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k26h8a3zwwh1q5,2019-10-25T18:36:23Z,{},project2
3303,no,"<p>yes, you are right. I do remember that in the paper they discussed how to sample only good episodes. So they thought about this problem and they also agreed upon on that. I don&#39;t remember how, but you can look at the paper and find the answer on how to sample only &#34;good&#34; episodes that lead to success.</p>",2019-10-25T18:51:40Z,33,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k26hrxqifeg7b1,2019-10-25T18:51:40Z,{},project2
3304,no,"<p>So, wait, are you going to replay an entire episode, or just a trajectory of that episode? Replaying an entire episode is an interesting idea that I never followed up on. You would identify such an episode by its reward - any reward &gt; 100 would be worth keeping. You could then bias those by using the reward as your probability of selection (normalized) - which would make it easy to use np.choose(p=[...]) to get the episode you want with the best past experience. That&#39;s an interesting idea.</p>",2019-10-25T19:03:32Z,33,Week 10/20 - 10/26,feedback,,jc554vxmyuy3pt,k26i77a9d006uo,2019-10-25T19:03:32Z,{},project2
3305,no,"<p>ups, sorry. I meant to sample experiences, not episodes. The talked how to sample only good experiences aka steps.</p>",2019-10-25T19:21:18Z,33,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k26iu1s26hd57n,2019-10-25T19:21:18Z,{},project2
3306,no,"<p><strong><em>So the distribution of the experience tuples we&#39;re caching in the agent&#39;s buffer should likewise, over time, come to more closely resemble the distribution we&#39;d see when acting optimally. Is this right?</em></strong></p>
<p></p>
<p>This is correct. The replay buffer should drop off old &#34;stagnant&#34; memories in lieu of memories that are more &#34;on policy&#34;. This is more important with Q learning than say SARSA where in Q you are learning by way of your neighbor instead of yourself. Replaying a past on-policy experience will either preferentially bias you towards that on-policy experience, or divert you away from your preferential on-policy experience.</p>
<p></p>
<p>That&#39;s what you were getting at in the next statement:</p>
<p></p>
<p><strong><em>Their observed distribution would also come to more closely resemble the distribution of samples characteristic of the optimal policy over time.</em></strong></p>
<p></p>
<p>I think this is correct, but at a long period of time, right? If you are doing Q, then you are always updating off-policy and selecting on-policy. If the Q converges fully to on-policy* then any update off-policy would still be an on-policy update, otherwise we&#39;re not quite at on-policy*. That would mean that eventually the Q updates will become SARSA updates. Or maybe that&#39;s a long stretch ...</p>
<p></p>
<p>I guess that&#39;s how the contraction properties are supposed to work. At infinity, the updates are all the same because you are at the optimal policy.</p>
<p></p>",2019-10-25T18:15:57Z,33,Week 10/20 - 10/26,followup,,jc554vxmyuy3pt,k26gi0izf3m6x5,2019-10-25T18:15:57Z,{},project2
3307,no,"<p>&#34;I mean, in that case, is seems like we&#39;d always be training on data which is differently (probably <em>very</em> differently) distributed than the data in the &#39;target distribution&#39;, right? And that seems bad!&#34; Why is this bad?</p>
<p></p>
<p>Let&#39;s look closer.</p>
<p>Remember our goal to use func approx is to approx the Qtable. So we are good as long as the Q table is correct, since we can just infer a correct optimal policy from it. This is because Q learning is off policy and the entries of Q(s,a) doesn&#39;t depend on any policy because a is already fixed and the distribution of Q(s,a) is just composition of transition distribution and reward distribution right?</p>
<p></p>
<p>Of course, since we are doing a stochastic optimization problem i.e we optimize min theta E_eta f(theta, eta). Where f(., eta) is the loss function of DQN, eta is the random variable representing the data (s,a,r,s&#39;), the distribution of eta will indeed determine our E(f). However, does the distribution of eta matter a lot for our case though? If you have Qr being the Q table from random policy and Qu being the Q table from the optimal policy. They will give you the same optimal policy right?</p>
<p></p>
<p>Then think about policy gradient. It&#39;s on policy and if you give it different distribution of data it will confuse because it&#39;s trying to determine how well it does for <strong>current policy </strong>and everything is counting on it&#39;s accuracy being current so that it knows what to improve itself on. </p>",2019-10-26T00:27:45Z,33,Week 10/20 - 10/26,followup,,is5gzbotXmz,k26ts5dwmja3vq,2019-10-26T00:27:45Z,{},project2
3308,no,"<p>depends on ur sampling mechanism.  its really about data efficiency, and breaking auto-correlation in sequences so that you can get good gradient updates, with a minimal of data samples (for data sparse environments).</p>
<p></p>
<p>also depends on ur alg.  is it robust to differences in the distribution from the &#34;optimal&#34; distribution?  and also how much noise you inject (directly, indirectly).</p>
<p></p>
<p>and there is also a question of timeliness.   is it useful to replay a circumstance, long after its potential use has passed (or the policy params that generated it, have deviated)?</p>
<p></p>
<p>I like the idea of a replay buffer (well, things like it!), but DQN, its a disaster (its benefit is highly problem specific; and only on a subset of hyperparams that are in practice, difficult to control).  it added several additional hyperparameters; two of which were hidden, and in the end was not even a good fit for the actual environment (lunar lander).</p>
<p></p>
<p>on the plus side, I learned a ton.</p>
<p></p>
<p>#1 thing:</p>
<p></p>
<p>import keras.</p>
<p>NOT.</p>
<p>from keras, import tf.keras</p>
<p></p>
<p>#2 thing:</p>
<p></p>
<p>the algorithms need to be better.  too much of this is trading a smoothly varying, well-hinted &#34;state space&#34; for an extremely nonlinear, discontinous, highly-correlated, hyperparam space (with extra dimensions AND hidden parameters).</p>
<p></p>
<p>I laughed when I read Sutton&#39;s comment on the deadly triad &#34;it shouldn&#39;t work, but it does..... and here&#39;s what we think matters&#34;.  guys got decades in the field, and even so, he&#39;s at a loss to give it a good theoretical basis.   I like throwing stuff at walls at seeing what sticks.... but the algorithm, needs improvement.</p>
<p></p>
<p>caveat: I haven&#39;t tried the improvements.  but stacking crap, on top of crap, gets u more crap (thats even harder to reason about!). </p>
<p></p>
<p>... poetry, in mathematical terms, is whats needed... something akin the elegance of contractionary convergence gaurantees; where the complication is for elegance, not tuning.</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-10-29T00:44:05Z,32,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2b4op275w7eo,2019-10-29T00:44:05Z,{},project2
3309,no,"<p>For an episodal task that is short, like Lunar Lander (maximum of 1000 transitions per episode), I would think that we would want to accumulate memories over many episodes, so we <em>wouldn&#39;t</em> reset the memory every episode.  For example, if your buffer size is 10,000 and you reset it every episode, then you would never fill more than one-tenth of it, ever.</p>
<p></p>
<p>As for a task that can potentially last much longer, I don&#39;t know - maybe initializing the buffer every episode would be good.</p>",2019-10-25T18:28:00Z,33,Week 10/20 - 10/26,followup,,jzfsa4a37jf4aq,k26gxhnvepw2vy,2019-10-25T18:28:00Z,{},project2
3310,no,"<p>I&#39;m using deque with maxlen, so for long episodes I should be good. Yeah, after thinking I realizes we need big memory and should remember what happened in the previous episodes. So I&#39;m initializing the memory only once per run. Especially, for such toy environments where the distribution is always same.  </p>",2019-10-25T18:49:04Z,33,Week 10/20 - 10/26,feedback,,jzih0fdt4sn1cq,k26holnf55z1zi,2019-10-25T18:49:04Z,{},project2
3311,no,"<p>One of the difference before and after the agent is trained, is that it knows more about the environment, through the learning from its observations. Those observations are also called experiences, and are valuable to the agents, because that&#39;s all they know. Maybe in certain cases it would help if we &#34;erase&#34; that experience and make the agent learn again, but I think you would need to argue why that erasing would result in something better.</p>",2019-10-27T00:02:50Z,32,Week 10/27 - 11/2,followup,,jl5wq8mca7o0,k288bya57v672c,2019-10-27T00:02:50Z,{},project2
3312,no,"<p>Did you manage to install it? I tried and I had error messages &#39;not compatible with this and that&#39;, forced me to load an old version of gcc ... a pure nightmare, and I gave up.</p>
<p></p>
<p>But they gave me a full license with my student ID, so I&#39;d like to know how to install it, without wasting a day.</p>",2019-10-29T15:22:08Z,22,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2c01vgedif7h3,2019-10-29T15:22:08Z,{},project2
3313,no,"<p>I was able to install it on my mac and used the key from them using my gtid as well. The LL environment doesn&#39;t use MuJoCo, so it&#39;s a wash ... you need box2d for that. Would be fun to play with the MJC enviros though. I ran all of the tests on my mac without any issues. It was pretty much following the directions to get it all installed.</p>",2019-10-29T17:00:28Z,22,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2c3kcdrbzt5je,2019-10-29T17:00:28Z,{},project2
3314,no,"<p>Yeah, me too, I just wanted to have a look at what mujoco can do.  I&#39;m a EE, so I studied a lot of physics, and I was interested in the potential of that library.  Maybe another time.  </p>",2019-10-29T19:07:01Z,22,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2c832mjqsx269,2019-10-29T19:07:01Z,{},project2
3315,no,"<p>I evaluated my lander 3 times, 100 episodes each, and it does not crash or drops off the way yours does. I&#39;d say your learning rate is too high or you have too few episodes.</p>",2019-10-29T02:04:08Z,32,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k2b7jn9mcpz21j,2019-10-29T02:04:08Z,{},project2
3316,no,"<p>Yeah, thx, when I let it run for a few hours, the crashes disappear.  </p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2bu25phih47%2F100_rewards_with_NN3300_with_seed_9825.png"" alt="""" /></p>
<p></p>",2019-10-29T12:35:28Z,32,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2bu3jcl8a3gm,2019-10-29T12:35:28Z,{},project2
3317,no,"<p>How many episodes did you train your agent to get these results? I went with 2500 episodes, and still got a few results under 200 but at least 120.</p>",2019-10-30T03:42:26Z,32,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k2cqhwmddnp2qt,2019-10-30T03:42:26Z,{},project2
3318,no,"<p>Here&#39;s my latest simulation:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2cup76148ef%2Ffull_simulation_with_rewards_plot_using__3300262_5h2.png"" alt="""" /></p>
<p>Basically, after 2000 episodes, I start getting very good landings.  At the beginning, I used the heuristic function to train the NN instead of the replay memory, and it works pretty well.  The agent &#39;absorbs&#39; the skills of the heuristic function quickly, and, after I dropped the heuristic mode at episode 1000, the agent&#39;s performance dips quite a bit, as you can see in the first graph, because he&#39;s now allowed to experiment with silly actions, and crash a lot, but it recovers very quickly, in a 100 episodes as graph 3 shows, and then keeps improving.  </p>
<p>At 3300 episodes, in 100 landings, I have 99 above 218, half of which above 260, 6 above 300, with a max of 319.</p>",2019-10-30T05:49:32Z,32,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2cv1d0lhbd3i,2019-10-30T05:49:32Z,{},project2
3319,no,"<p>Thanks for the info, that was helpful.</p>
<p></p>
<p>I&#39;m doing DQN but its not too deep.</p>",2019-10-26T00:18:19Z,33,Week 10/20 - 10/26,followup,,ijalzhqktnv3z4,k26tg00d2ve2zc,2019-10-26T00:18:19Z,{},project2
3320,no,"<p>I only asked because it&#39;s possible that the memory replay mechanism is also a factor in instability if doing DQN.  I&#39;m not saying it is, just that it&#39;s another complication.</p>",2019-10-26T00:31:04Z,33,Week 10/20 - 10/26,feedback,,jzfsa4a37jf4aq,k26twefn3p97g5,2019-10-26T00:31:04Z,{},project2
3321,no,"<p>And yes, we not only stop reading at 5, but you also lose points for submitting more than the limit.</p>",2019-10-26T00:20:30Z,41,Week 10/20 - 10/26,followup,,hyx9thiqa6j4nn,k26tit46wq96ri,2019-10-26T00:20:30Z,{},project2
3322,no,<p>Acknowledged -- thanks for confirming.</p>,2019-10-26T00:34:57Z,41,Week 10/20 - 10/26,feedback,,isde332xcka1m0,k26u1en2afv3d7,2019-10-26T00:34:57Z,{},project2
3323,no,<p>Maybe reduce the font of your references? I feel less bad about it after seeing Mnih 2015 paper did the same...</p>,2019-10-26T21:37:33Z,41,Week 10/20 - 10/26,followup,,jl5wq8mca7o0,k28354awuws59a,2019-10-26T21:37:33Z,{},project2
3324,no,<p>I have the same question.</p>,2019-10-27T17:28:00Z,55,Week 10/27 - 11/2,followup,,j6lnhjhnzr3y5,k299o1kbhlk592,2019-10-27T17:28:00Z,{},hw5
3325,no,<p>I don&#39;t see HW5 at this moment. </p>,2019-10-30T05:33:55Z,55,Week 10/27 - 11/2,followup,,j6ln9puq99s5uv,k2cuhagyi063nz,2019-10-30T05:33:55Z,{},hw5
3326,no,"<p>BTW, can we make the input for the HW5 in a Python dictionary format instead of the one in the homework instructions? I guess most of people are using python and the format of the input is just not convenient. Thanks.</p>",2019-10-30T05:35:13Z,55,Week 10/27 - 11/2,feedback,,j6ln9puq99s5uv,k2cuiydgtl74ty,2019-10-30T05:35:13Z,{},hw5
3327,no,<p>I took it back. It&#39;s under some file called bar something. It takes some effort to make the format work with Python. Actually I am curious how many people use Python vs Java in this class</p>,2019-10-30T06:22:23Z,55,Week 10/27 - 11/2,feedback,,j6ln9puq99s5uv,k2cw7m767n114r,2019-10-30T06:22:23Z,{},hw5
3328,no,<p>Yeah the variable naming and declarations are definitely Java&#39;ish and I wonder about that too</p>,2019-10-31T01:06:02Z,55,Week 10/27 - 11/2,feedback,,jqmjhlg7ktv5a,k2e0cmon6r19,2019-10-31T01:06:02Z,{},hw5
3329,no,<p>All of the assignments used to be Java based using BURLAP during the first iteration of the course.  Miguel and I have been moving everything to Python but there are still some Java artifacts floating around.</p>,2019-10-31T01:17:08Z,55,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2e0qx3623l14k,2019-10-31T01:17:08Z,{},hw5
3330,no,<p>KWIK paper correct?</p>,2019-10-28T09:24:40Z,22,Week 10/27 - 11/2,followup,,jqrr36mqfm8M,k2a7uc1ldn3fx,2019-10-28T09:24:40Z,{},hw5
3331,no,That is correct. <div><br /></div>,2019-10-29T22:30:40Z,22,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2cfcze0lie1fd,2019-10-29T22:30:40Z,{},hw5
3332,no,"<p>In the office hour today, TAs mention it is possible to write a good report even with failing result. They also say when you are failing, you probably have more things to say on your report. However, what I interpret from what they didn&#39;t say, is that you should also have a good understand on the theory and the idea, in order to put up an explanation. You could just have a bug in implementation, while having a good understanding, or you are having trouble with the concept and what needs to be done. I don&#39;t know which case is more applicable, base on the information you gave. So you would need to weight it yourself.</p>
<p></p>
<p>Advising someone stays in a class or not depends heavily on the person&#39;s situation. Generally I think it is better for a person to stay in the class (unless they completely below the range of attaining a C). If you stay in the class, you <em>may </em> have a chance of passing it. If you drop it, you <em>surely</em> have re-take it or take other class. Plus, you will not have anything to do from now till next semester (January). Staying in the class, you may fail, but sure you are still learning.</p>",2019-10-26T21:32:41Z,32,Week 10/20 - 10/26,followup,,jl5wq8mca7o0,k282yuo5hf5560,2019-10-26T21:32:41Z,{},project2
3333,no,<p>Agree. Only one project should not be a reason to drop this course.</p>,2019-10-28T14:37:23Z,31,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2aj0hg5sui45g,2019-10-28T14:37:23Z,{},project2
3334,no,<p>Does the rubric for grading project 2 have a fixed penalty for not being able to solve lunar lander?</p>,2019-10-28T22:32:06Z,31,Week 10/27 - 11/2,followup,,jqob6okzSK1c,k2azyyzr7fo6c3,2019-10-28T22:32:06Z,{},project2
3335,no,<p>Yes there is a small fixed penalty for not solving.</p>,2019-11-01T01:05:50Z,31,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2ffs8e3p4a3fy,2019-11-01T01:05:50Z,{},project2
3336,no,"<p>Timothy Bail!  Tim! I hear you&#39;re into boats :)</p>
<p></p>
<p></p>",2019-10-29T00:56:52Z,31,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2b555f5b6pba,2019-10-29T00:56:52Z,{},project2
3337,no,Boats are one of many things that I work on. ,2019-10-29T00:59:35Z,31,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2b58mk4xly441,2019-10-29T00:59:35Z,{},project2
3338,no,"<p>thats all I heard.  :)  in regards to the possibility of using a neural network to learn the dynamics of control for an agent.  some of the ta&#39;s mentioned &#34;tim might be doing stuff like that&#39;. :)  I am assuming, you are working in defense?</p>",2019-10-29T06:17:26Z,31,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2bgle64z1e2jr,2019-10-29T06:17:26Z,{},project2
3339,no,Yeah some defense and commercial. I also run our internal research and development where we are applying RL and ML to dynamic control of robotic systems. ,2019-10-29T22:24:03Z,31,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2cf4h0021b52t,2019-10-29T22:24:03Z,{},project2
3340,no,<p>?</p>,2019-10-30T03:37:20Z,31,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k2cqbct2bi53kx,2019-10-30T03:37:20Z,{},project2
3341,no,"<p><strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  awesome :)  I head RL with robotics is basically its own subfield (due the complexity, the sparseness, the well-studied optimal control).  I asked a question in one of the office-hours regarding using an NN to first learn the dynamics of the control of a craft, and they mentioned this sis something you would know about.  I would love to have a short conversation in real-time about the aspects of RL that relate to robotics.  Preliminary conversation, just to get a sense of whats what, with an actual practitioner in the field.  Up to you :) we can do it hear if you like, or in slack, or by email if you are so inclined.</p>",2019-11-01T23:50:52Z,31,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2gsjo5zgxv2z,2019-11-01T23:50:52Z,{},project2
3342,no,<p>Yeah.  Doing it here on piazza in a new thread would be best.  I can be hard to catch on slack sometimes.  I will tell you as much as I am allowed (There are some details I can&#39;t discuss)</p>,2019-11-03T02:02:04Z,31,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2ico8nlzl318q,2019-11-03T02:02:04Z,{},project2
3343,no,<p>awesome! :)  no prob on the restrictions. :)  give me a little time to do some light reading on robotics &amp; RL/ML to catch up/get the jargon.</p>,2019-11-03T03:45:15Z,31,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2igcxzls221w2,2019-11-03T03:45:15Z,{},project2
3344,no,"<p>There are a whole bunch of posts about this, but it seemed to be somewhere between 2,000-10,000 based on the run.</p>",2019-10-27T01:39:01Z,32,Week 10/27 - 11/2,followup,,ixty1midfufhd,k28brmwfx851dz,2019-10-27T01:39:01Z,{},project2
3345,no,"<p>Thanks, George!</p>",2019-10-27T01:45:55Z,32,Week 10/27 - 11/2,feedback,,hzoi2qsuCAd,k28c0ibpa4g4qp,2019-10-27T01:45:55Z,{},project2
3346,no,"<p>I have some idea&#39;s as to perhaps why(mostly that my algorithm is training the NN very infrequently), but mine did not converge until &gt; 10,000 episodes(around 15k). For some reason with more frequent training the agent seems to not learn at all, and sometimes rapidly diverge to very bad values and get &#34;stuck&#34; there. </p>",2019-10-27T04:24:32Z,32,Week 10/27 - 11/2,feedback,,jzozvpx25to679,k28hoi609363oc,2019-10-27T04:24:32Z,{},project2
3347,no,"<p>With the right set of hyper parameters, I can get some agents trained in less than 1000 episodes (~700 episodes).</p>",2019-10-27T08:00:41Z,32,Week 10/27 - 11/2,feedback,,jqmfnc46kl26eg,k28pegnylrn4b3,2019-10-27T08:00:41Z,{},project2
3348,no,"same here, I get 800-1200 eps with DQN, Double Q learning, experience replay and a slow moving target network but I might be overdoing it ^.^",2019-10-27T09:52:18Z,32,Week 10/27 - 11/2,feedback,,jqmfmz2fbs55xs,k28te0a8dkm6kb,2019-10-27T09:52:18Z,{},project2
3349,no,"<p>for the DQN, are you updating the NN throughout the episodes or at the end of an episode? I tried both and different batch sizes but it seems to get close but not quite. It still fails a lot at landing it correctly. It does ok job brining it down in the center of the screen but it always oscillates too much and cause it to crash toward the end. Not sure if I am stuck in some local minima and changing the learning rate would help. </p>",2019-10-27T20:55:41Z,32,Week 10/27 - 11/2,feedback,,hzoi2qsuCAd,k29h34jpdv9aw,2019-10-27T20:55:41Z,{},project2
3350,no,"I&#39;m doing experience replay every action with quite a few memories, but I have enough GPU computer on my local machine to do a whole training run in 5 minutes. I&#39;m sure you can still get a good convergence with a smaller replay size but replay every action seemed to work best for me. Mine was oscillating before too and it turned out to be that epsilon was decrementing too fast, the agent didn&#39;t have enough time to explore.",2019-10-28T04:00:39Z,32,Week 10/27 - 11/2,feedback,,jqmfmz2fbs55xs,k29w9n3j4t76bt,2019-10-28T04:00:39Z,{},project2
3351,no,<p>I just realized that my Deep Q Learning algorithm had an implementation error.. nooooooooo!!!</p>,2019-10-28T06:22:37Z,32,Week 10/27 - 11/2,followup,,hzoi2qsuCAd,k2a1c75o75n4y2,2019-10-28T06:22:37Z,{},project2
3352,no,<p>I hope you were able to fix it!</p>,2019-10-28T14:35:28Z,32,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2aiy0nzr7722s,2019-10-28T14:35:28Z,{},project2
3353,no,<p>2500 episodes did it for me. Your other parameters though...</p>,2019-10-28T23:39:44Z,32,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k2b2dy83dm975m,2019-10-28T23:39:44Z,{},project2
3354,no,"<p>I don&#39;t think a graph with mean is a good choice because reward vs. episodes graph also shows the improvement of your agent during the training, which cannot be demonstrated by only the mean of groups.</p>",2019-10-27T13:06:12Z,40,Week 10/27 - 11/2,followup,,jl3ol6nb3a5j,k290bcwxkmg6ai,2019-10-27T13:06:12Z,{},project2
3355,no,"<p>Ah, I think I explained myself badly, I&#39;ll change the question.</p>",2019-10-27T14:04:49Z,40,Week 10/27 - 11/2,feedback,,jl3oi5v7qkSk,k292er0srmz1ex,2019-10-27T14:04:49Z,{},project2
3356,no,"<p>The higher the learning rate, the more the weights of the neural net move each step in the direction of the negative gradient.  If the learning rate is too high, they can move too much and jump over the minimum you&#39;re trying to reach.  You can end up getting oscillating, noisy behavior as the weights jump back and forth, always leapfrogging their target.  However, having a high learning rate can also help pop the agent out of a local minimum it may have otherwise gotten stuck in.</p>
<p></p>
<p>With a very low learning rate, the opposite is true.  The agent may get stuck in a local minimum, since its step-sizes aren&#39;t large enough to hop over the &#34;hills&#34; that surround the local minima.  On the other hand, it won&#39;t suffer from noisiness since it won&#39;t be jumping back and forth over the minima either.</p>
<p></p>
<p>In reference to your -9000 score.  -9000 doesn&#39;t mean it was losing 9 score units per time-step.  That&#39;s now how reward works in Lunar Lander.  It could have incurred that -9000 almost all in one shot by crashing at high speed.</p>",2019-10-27T16:34:48Z,31,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k297rmr8qkk3nz,2019-10-27T16:34:48Z,{},project2
3357,no,"<p>Did he invert the reward, by chance? The score looks like the agent is learning to crash rather than to land...</p>",2019-10-27T17:08:21Z,31,Week 10/27 - 11/2,feedback,,jl5wq8mca7o0,k298yrsbb8n3hg,2019-10-27T17:08:21Z,{},project2
3358,no,"<p>No, I don&#39;t think so.  The curves he plotted are for rewards over many episodes.  That -9000 reward episode is followed by much better episodes.  It&#39;s just a fluke in the blue curve.</p>
<p></p>
<p>It&#39;s not surprising that cranking up the learning rate too high can lead to erratic behavior.  You can see that the green, red, and purple curves are much better behaved.</p>",2019-10-27T17:57:34Z,31,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k29aq2rbntn73m,2019-10-27T17:57:34Z,{},project2
3359,no,"<p>The progression of the orange and the blue that are leading me to think the agent is learning in opposite direction. If the learning rate is too high, we should have seen at least some positive exploration, even if major of them are naturally negative. Though I admit I mistakenly thought the green and red did not do better, due to the scale of the plot. If purple is the baseline for 0-reward, and red and green are actually performing better, at ~100-reward, then I would agree with you and the extremely low scores are just erratic outliners that happen to stand out due to their magnitude.</p>",2019-10-27T18:51:23Z,31,Week 10/27 - 11/2,feedback,,jl5wq8mca7o0,k29cn9pqgl5ah,2019-10-27T18:51:23Z,{},project2
3360,no,"<p>&#34;<em>If purple is the baseline for 0-reward, and red and green are actually performing better, at ~100-reward</em>&#34;</p>
<p></p>
<p>Actually, to me, that&#39;s exactly what it looks like.  It&#39;s hard to tell from the scaling, but green and red look to me at ~&#43;100 reward.</p>
<p></p>
<p>&#34;<em>The progression of the orange and the blue...&#34;</em></p>
<p><em></em></p>
<p>I don&#39;t really see a progression for those two colors getting worse.  If you just look at the middle and right parts of the graph, orange actually seems to get <em>fewer</em> bad scores toward the end, but I think that is just variance.  If you include the very beginning of the graph, i.e. the first few hundred episodes, then yes, there seems to be a &#34;progression&#34;, but my guess is that during that part of the graph there is probably still a high exploration rate ($$\epsilon &gt;&gt; 0$$) so the actions there for <em>all</em> colors are random and not a result of the neural network weights.</p>",2019-10-27T20:03:18Z,31,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k29f7r8o7qk59s,2019-10-27T20:03:18Z,{},project2
3361,no,"<p>My thought for progression wasn&#39;t only because orange and blue are going downward, but also they seem to correlate. When orange grows negatively, blue also seems to follow that trend. And I hypotherize the correlation is due to network weight. It could also be just coincidence, which I don&#39;t have enough info to tell.</p>
<p></p>
<p>But that wasn&#39;t taking into consideration what you say about red and green perform better. It wouldn&#39;t make sense if he accidentally pick negative reward, but lower alpha makes the agent learn better. I think your original post makes the most sense, given the info we have.</p>",2019-10-27T20:27:41Z,31,Week 10/27 - 11/2,feedback,,jl5wq8mca7o0,k29g33uvih47oa,2019-10-27T20:27:41Z,{},project2
3362,no,"<p>FYI, green and red are both having rewards &gt;220. </p>",2019-10-28T00:10:00Z,31,Week 10/27 - 11/2,feedback,,jl3ol6nb3a5j,k29o10kjvh36au,2019-10-28T00:10:00Z,{},project2
3363,no,"<p>You can get away with replaying the same scenario in the lunar lander problem because the environment does not change much from one run to the next, but I don&#39;t think this is sound advice in general. As for reproducibility, calling env.seed(), random.seed() and numpy.random.seed() (if you used the latter two) is enough to get consistent runs. I worked alternatively in a Linux and a Mac computer and noticed that different versions of the packages could also produce different random sequences even with the same random seed.</p>",2019-10-28T01:21:46Z,31,Week 10/27 - 11/2,followup,,is8ald0uljj3u4,k29qlbabawc6qr,2019-10-28T01:21:46Z,{},project2
3364,no,"<p>Oh I should have thought about using good seeds.</p>
<p>I had very good scores but 5% hard crashes, I tried to isolate 100 seeds that would lead to crashes and force the agent to learn how to land in those cases, but the situation didn&#39;t improve because I think it was still crashing in other cases.  </p>
<p></p>
<p>In the end, I had to let it run for 8 hours and now I got only 1 crash in 1000 episodes.  <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2c083gnsihg%2F1000_rewards_with_NN3300_with_seed_7007.png"" alt="""" /></p>",2019-10-29T15:29:57Z,31,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2c0bxsbnwn3qd,2019-10-29T15:29:57Z,{},project2
3365,no,"<p>I had one case where I was able to get 100% positive landings with an average over 200 in 5 minutes of training my Q(s,a) discrete solution.</p>
<p></p>
<p>This would be impossible to recreate, eventhough I captured the Qs and states. The device I was on had an unusual spike in entropy that seemed to have an unpredictable result. You might even say this was a black swan event.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2c3aqyst1v%2Fqqblackswan.png"" alt="""" /></p>
<p></p>",2019-10-29T16:53:08Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2c3awdnpw6192,2019-10-29T16:53:08Z,{},project2
3366,no,"<p>I would love to have a look at your discrete solution.  This project was a blast, so much to learn, and I feel we learn a lot from each other, the strategies we used etc, even showing some code or pseudocode.  </p>",2019-10-29T19:05:35Z,31,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2c818zbyv01o8,2019-10-29T19:05:35Z,{},project2
3367,no,"<p>You already have it. HW4 is where I started. The only unique part was the discretizer and the use of a lookup table for the states. None of that is any original insight.</p>
<p></p>
<p>So, Q(s,a) maps (s) as an index into the look up table:  s = lookup(discretize(s_analog)). No magic there. The discretization used the heuristic components from the heuristic test provided in the github repo plus the velocity components provided in s_analog. The bins were predetermined using a warmup of randomized samples from the observation space.</p>
<p></p>
<p>I used my gtid as my training seed and zero as my evaluation seed. Nothing special there.</p>
<p></p>
<p>The only special was the analysis of the $$\epsilon$$ and $$\alpha$$ parameters and then calculating the rest of the parameters to target a specific experiment based upon prior experiments. It&#39;s the crafting of the hypers that makes the algorithm work. Really, without any of that, it&#39;s just HW4.</p>
<p></p>
<p>&#64;753 is where I discussed how I found my $$(\epsilon,\alpha)$$ parameters. The post we are reading now (&#64;803) is where I describe how I calculated the other parameters.</p>
<p></p>
<p>I am comfortable sharing my paper and solution if the teaching staff approve it. Mostly I&#39;ve given it away in all of the posts I&#39;ve made on piazza and on slack.</p>
<p></p>",2019-10-29T21:06:20Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2cccip6r2t5,2019-10-29T21:06:20Z,{},project2
3368,no,"<p>Thanks &#64;Jacob, very interesting.  Honestly, I was behind you and others so I couldn&#39;t understand what you guys were talking about most of the time until now that I&#39;ve completed the project.  </p>
<p>It&#39;s really fascinating.  I did it with DQN because I wanted to use NN and experiment with sthg totally new.</p>
<p></p>
<p>It&#39;s funny I also used the heuristic function provided!  I used it to pre-train the NN by forcing the agent to learn from it, ie considering it is the optimal policy for a while, instead of using the replay memory, which not only works extremely well, the algo reached scores around 200 in only 200 episodes (about 20 minutes on my computer), see graph 2 below, but also fills the replay memory with mostly good data (I start with a small epsilon, about 0.3).  After about 1000 episodes, I stop the &#39;heuristic mode&#39; and the agent is on his own, allowed to take silly actions and crash hard, and the interesting thing is that, after the scores dipping quite a lot, it recovers in about 100 episodes, because, I think, of all the good data in the replay memory.  </p>
<p></p>
<p>The figure below shows that big dip after episode 1000 (graph1) and how it recovers quickly (graph3).  </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2cn1b30xozj%2Ffull_simulation_with_rewards_plot_using__3300262_3h4.png"" alt="""" /></p>
<p></p>
<p>This heuristic training may be the reason why I didn&#39;t have to play too much with the hyperparameters... now, for the fun, I&#39;m running simulations just to see what happens without the heuristic pre-training.  </p>
<p></p>
<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  are we allowed to share our reports?  If not, is there a way we could learn from each other? How other people solved this very interesting problem? </p>
<div>
<div></div>
</div>",2019-10-30T02:09:02Z,31,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2cn5t5hgjx5s4,2019-10-30T02:09:02Z,{},project2
3369,no,<p>Very nice JP! well done getting a good DQN solution working.</p>,2019-10-30T03:27:47Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2cpz2lgds343g,2019-10-30T03:27:47Z,{},project2
3370,stud,"<p>Good point! No, I&#39;m currently using predict_on_batch() in my update function only. Another function, used by my pick_action function, for single-state predictions, is still using predict(). Will try swapping it out with predict_on_batch. Thanks, Vahe.</p>",2019-10-27T18:16:45Z,32,Week 10/27 - 11/2,followup,a_0,,k29beqrg3l57da,2019-10-27T18:16:45Z,{},project2
3371,stud,"<p>Holy shit, That did the trick! RAM use doesn&#39;t blow up as quickly anymore. Plus, training seems faster overall. Thanks agian , Vahe.</p>",2019-10-27T19:02:59Z,32,Week 10/27 - 11/2,feedback,a_0,,k29d26uqx4t47b,2019-10-27T19:02:59Z,{},project2
3372,no,<p></p>,2019-10-27T21:41:14Z,31,Week 10/27 - 11/2,followup,,idfzuuu9fnI,k29ipp8r26ssu,2019-10-27T21:41:14Z,{},project2
3373,no,"<p>I just realized that while my code generates the data sets that were used to generate the graph, I did not mention this in the readme, and I also did not include the original dataset that I used to create the graphs in my repository. Should I take the penalty and resubmit? Or is this acceptable?</p>",2019-10-28T00:36:49Z,31,Week 10/27 - 11/2,followup,,jzozvpx25to679,k29ozhzhgzm5c,2019-10-28T00:36:49Z,{},project2
3374,no,<p>There&#39;s no penalty for resubmitting before the deadline.</p>,2019-10-28T02:56:02Z,31,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k29tyjpmpi06xu,2019-10-28T02:56:02Z,{},project2
3375,no,"<p>Aha - I had thought that I switched my default settings on Instructure to display deadlines in my local time (Hong Kong), but in fact it had not. Thanks for the heads up!</p>",2019-10-28T04:13:04Z,31,Week 10/27 - 11/2,feedback,,jzozvpx25to679,k29wplskjvt3r1,2019-10-28T04:13:04Z,{},project2
3376,no,"<p>I&#39;m having a similar issue. Pasting the  &#39;atEstablishment&#39; input seems to give me an unrelated error. </p>
<p>&#34;TypeError: unhashable type: &#39;set&#39;&#34;</p>
<p></p>
<p>Also, there&#39;s no text highlighting, so it looks like the size of input being pasted all at once causes some glitch.</p>
<p></p>
<p>Maybe one option is to throw it into a txt file, and read the input from the txt file into a data structure of your choice.</p>",2019-10-28T14:55:07Z,55,Week 10/27 - 11/2,followup,,jl1b27fpaYkv,k2ajn9yv2n34fx,2019-10-28T14:55:07Z,{},hw5
3377,no,"<p>For the unhashable error replace the {&#39;s with [&#39;s, and }&#39;s with ]&#39;s</p>",2019-10-28T16:28:06Z,55,Week 10/27 - 11/2,feedback,,jl3we43d3bp15p,k2amyv2is861zc,2019-10-28T16:28:06Z,{},hw5
3378,no,<p>o</p>,2019-10-28T21:31:10Z,55,Week 10/27 - 11/2,feedback,,jl1b27fpaYkv,k2axslyjlbs5sc,2019-10-28T21:31:10Z,{},hw5
3379,no,"<p>is rldm working for you? I have tried in chrome/safari but get redirected to homepage after putting in creds. I have not had this issue in the past*</p>
<p></p>
<p>edit: <a href=""/class/jzh9tkzzxkd7ph?cid=497"">&#64;497</a> fixed this</p>",2019-10-28T17:14:22Z,55,Week 10/27 - 11/2,followup,,jcg0nzvdk8272b,k2aomcqp64h1ll,2019-10-28T17:14:22Z,{},hw5
3380,no,"<p>Sublime or vs code will make your life a lot easier</p>
<p><a href=""https://www.sublimetext.com/"">https://www.sublimetext.com/</a></p>
<p><a href=""https://code.visualstudio.com/"">https://code.visualstudio.com/</a></p>",2019-10-29T01:23:29Z,55,Week 10/27 - 11/2,followup,,jzm01jjgolv32u,k2b63degep32hh,2019-10-29T01:23:29Z,{},hw5
3381,no,"<p>Emacs FTW, although VS Code does handle the data sequences just fine. PyCharm stutters somewhat, and hangs if you try to format.</p>",2019-10-31T17:09:12Z,55,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k2eyra066x97lc,2019-10-31T17:09:12Z,{},hw5
3382,no,"<p>Been using Vi for over 22 years and still have not seen it crash.  That said, I&#39;ve been doing most of my dev in VS Code now (with the Vim add-in of course).  </p>",2019-10-30T06:04:07Z,55,Week 10/27 - 11/2,followup,,is9so9huTMp,k2cvk40a39d6bl,2019-10-30T06:04:07Z,{},hw5
3383,no,"<p>Hey Nick,</p>
<p></p>
<p>Vi is better than Emacs, right?</p>",2019-10-31T00:45:14Z,55,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2dzlw1tv244mv,2019-10-31T00:45:14Z,{},hw5
3384,no,There is a reason that Vi is everywhere. LOL. Avoiding the bait on that question. ,2019-10-31T00:55:33Z,55,Week 10/27 - 11/2,feedback,,is9so9huTMp,k2dzz5rwtjq3df,2019-10-31T00:55:33Z,{},hw5
3385,no,"<p>&#64;Vahe, let&#39;s agree that nearly any text editor is better than M$ Word (although I find myself stuck using Word more than I&#39;d wish)</p>",2019-10-31T05:03:20Z,55,Week 10/27 - 11/2,feedback,,is9so9huTMp,k2e8tsr08dy6c9,2019-10-31T05:03:20Z,{},hw5
3386,no,<p>Haha. Oh man...  I used to use Vi when I used to work on Unix systems.  I should get back to using it.  It&#39;s so lightweight and so fast.</p>,2019-10-31T05:45:30Z,55,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2eac15nd1r27e,2019-10-31T05:45:30Z,{},hw5
3387,no,<p>Yep... that&#39;s where I had the unfortunate (but now appreciated) introduction to Vi.  On Unix systems.</p>,2019-10-31T06:18:27Z,55,Week 10/27 - 11/2,feedback,,is9so9huTMp,k2ebieiafzi3y9,2019-10-31T06:18:27Z,{},hw5
3388,no,<p>notepad.</p>,2019-11-01T21:52:37Z,55,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2gobm163xi5qk,2019-11-01T21:52:37Z,{},hw5
3389,no,"<p>Having seen your answer, I guess my question really is, how does it know the bottom path is linearly dependent and the top isnt? Is there some math-ism I don&#39;t know that just makes that inherently true?</p>",2019-10-28T15:51:53Z,55,Week 10/27 - 11/2,followup,,is4nx55dinr6wk,k2aloaetjko4ea,2019-10-28T15:51:53Z,{},hw5
3390,stud,"<p>Yah. The defintions of linear dependence and linear independence. The vectors in the bottom path are lineraly dependent upon the ones in the middle path just in case one can find a set of scalars not <em>all </em>zero such that the zero vector can be expressed as a linear combination of the vectors in the middle path and the bottom path. That means that we can express the vectors in the bottom path <em>in terms ot</em> the vectors in the middle path. And yes - there are purely <a href=""https://en.wikipedia.org/wiki/Linear_independence"" target=""_blank"" rel=""noopener noreferrer"">mechanical means</a> of deciding whether sets  of vectors are linerarly dependent. </p>
<p></p>
<p></p>",2019-10-28T16:00:43Z,55,Week 10/27 - 11/2,feedback,a_0,,k2alznlqjn4540,2019-10-28T16:00:43Z,{},hw5
3391,no,"<p>Ok, I&#39;m following the linear dependence / independence, but I still don&#39;t understand how the learner know the bottom is dependent and the top isn&#39;t having never travelled those paths.</p>
<p></p>
<p>&#34;Instead of creating an approximate weight vector ˆw, it reasons about whether the costs for each edge can be obtained from the available data. The middle path, since we’ve seen all its edge costs, is definitely 13. The last edge of the bottom path has cost vector [0, 0, 0], so its cost must be zero, but the penultimate edge of this path has cost vector [0, 1, 1] &#34;</p>
<p></p>
<p>How does the learner reason out the costs for each edge having only seen the middle path?</p>",2019-10-28T16:51:32Z,55,Week 10/27 - 11/2,feedback,,is4nx55dinr6wk,k2anszmshq26zu,2019-10-28T16:51:32Z,{},hw5
3392,stud,"<p>I believe you have to assume that the agent knows how the paths are represented (i.e., it knows somthing about the overall <em>stucture</em> of the graph). At minimum, it seems like it must know what the edges are, how to represent combining them to represent paths through the graph, and what their representations as vectors are. What it <em>doesn&#39;t</em> know (I <em>think</em>) is $$\mathbf{w}$$, the weights that determine the cost for traversing an edge in the graph in conjunction with the vector representation of an edge. </p>
<p></p>
<p>It&#39;s not completely clear to me that this is what&#39;s intended, but I <em>think</em> it&#39;s in the ballpark of what they&#39;re trying to get at. It would be easier to say for certain if the issue of what the learning task is actually supposed to be were clearer. But as Vahe rightly points out beow, their description of the learning task in the paper is actually pretty muddled/hard to parse.  </p>",2019-10-28T17:09:27Z,55,Week 10/27 - 11/2,feedback,a_0,,k2aog1piz9a2jq,2019-10-28T17:09:27Z,{},hw5
3393,no,"<p>Oh, you&#39;re right. Paper definitely says the vectors are known ahead of time. Appreciate it!</p>",2019-10-28T17:14:48Z,55,Week 10/27 - 11/2,feedback,,is4nx55dinr6wk,k2aomx5rug6ot,2019-10-28T17:14:48Z,{},hw5
3394,no,"<p>I&#39;m also confused by the learning task: &#34;...to take a non-cheapest path in as few episodes as possible.&#34;</p>
<p></p>
<p>What does that mean?</p>
<p></p>
<p>Doesn&#39;t the non-KWIK agent do that by taking the middle path? (cost of middle path is 13, top path is 12, and bottom path is 14).</p>
<p></p>
<p>Why not make the learning task &#34;to take a path with the lowest cost in as few episodes as possible&#34;?</p>",2019-10-28T16:36:17Z,55,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k2an9e7o8rq4qi,2019-10-28T16:36:17Z,{},hw5
3395,stud,"<p>I took this as a rather awkward way of saying that the agent was looking to minimize the number of episodes required to find a path to the end goal <em>without</em> worrying about/factoring in the cost incurred. If that&#39;s<em> not</em> what the quoted bit from the paper is supposed to mean, then I have no idea what it means <em>either</em>.</p>
<p></p>
<p>Your rephrasing on the bottom is certainly what one <em>expects</em> the learning task to be here (i.e., it&#39;s the natural thing to do, I&#39;d wager). I just couldn&#39;t make what they actually wrote in the paper mean anything in the ballpark of this when I read it. </p>",2019-10-28T17:00:55Z,55,Week 10/27 - 11/2,feedback,a_0,,k2ao52616ad6lp,2019-10-28T17:00:55Z,{},hw5
3396,no,"<p>When I see &#34;...to take a non-cheapest path in as few episodes as possible&#34;, I remember the lesson about exploration where Littman explained that, for a deterministic MDP, we want an algorithm like RMAX that gives &#39;$$\epsilon$$-optimal&#39; rewards with a small number of mistakes.  When you explore you can&#39;t be sure you have explored all possible state-action pairs some may be very hard to reach, that&#39;s why he goes for near-optimal rewards.  </p>",2019-10-31T00:10:04Z,55,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2dyco56ymbah,2019-10-31T00:10:04Z,{},hw5
3397,no,"<p>Is the upper bound on the number of unknowns, the numOfPatrons * (numOfPatrons-1) the bar? assuming there is a maximum of 1 instigator and 1 peacemaker in patrons.</p>",2019-10-29T20:00:13Z,55,Week 10/27 - 11/2,followup,,j6ll2xkiDJf,k2c9zhljcezat,2019-10-29T20:00:13Z,{},hw5
3398,no,"<p>From HW5 requirements document:</p>
<p>&#34;The test case will be considered successful if no wrong answers are returned and number of &#34;I DON&#39;T KNOW&#34; does not exceed the max allowed&#34;</p>
<p></p>
<p>what is &#34;max allowed&#34; ? is it a number hidden to us? that the grader knows only..?</p>",2019-10-29T20:58:17Z,55,Week 10/27 - 11/2,followup,,j6ll2xkiDJf,k2cc25w75is7ns,2019-10-29T20:58:17Z,{},hw5
3399,no,"<p><em>However, the enumeration algorithm can KWIK learn this hypothesis class with a bound of n(n−1) since there is one hypothesis for each possible assignment of a patron to f and p.</em></p>
<p></p>
<p>From the paper describing Algorithm 2. The bound is n(n-1). </p>
<p></p>
<p>(correction)</p>
<p>For Algorithm 2, the &#34;I don&#39;t know&#34; bound is (H-1) (last paragraph before the Example 1).....</p>
<p></p>",2019-10-29T21:10:42Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2cci52v9yf490,2019-10-29T21:10:42Z,{},hw5
3400,no,"I think the max “I don’t know” allowed is the same as the minimum you could possibly have as well. That is, you have to use all information given at each episode to make all inferences possible and minimize the number of unknown answers. ",2019-10-29T22:27:31Z,55,Week 10/27 - 11/2,feedback,,jzhl7qlwrpagr,k2cf8x5d6zf7mz,2019-10-29T22:27:31Z,{},hw5
3401,no,I also noticed. 15 seems like a mistake. No erratum section here though like project 1 paper.<div><br /><div>I guess I would also not bother fixing that after writing such a “heavy” paper &#x1f60b;</div></div>,2019-10-30T05:18:04Z,55,Week 10/27 - 11/2,followup,,jfzaqnqvtQ1m,k2ctww021925j,2019-10-30T05:18:04Z,{},hw5
3402,no,I found GPU&#39;s to be approximately the same speed as a pretty fast CPU (8 cores on my desktop was faster than utilizing the RTX 2080TI) Presumably due to bottlenecking from transferring memory back and forth. The CPU seems to be more efficient when you use smaller models. Depends how many calculations you are doing though.,2019-10-28T19:22:23Z,31,Week 10/27 - 11/2,followup,,jzj4sh1p7pf5af,k2at703hqqe3ax,2019-10-28T19:22:23Z,{},project2
3403,no,I&#39;ve actually had a similar issue and I think it&#39;s that the wildcard completion is based on your shells behavior. I got around this with a find piped into xargs git add.,2019-10-28T16:49:58Z,30,Week 10/27 - 11/2,followup,,i4op5p9vfbq5yz,k2anqzpiv3e3jj,2019-10-28T16:49:58Z,{},other
3404,no,"<p>make a new folder</p>
<p>copy what you want to push into it</p>
<p>check size (apparently repo&#39;s have a 150 meg limit:  appparently!)</p>
<p>cd ~/project_root</p>
<p>git add -A .</p>
<p>git commit -m &#34;blah&#34;</p>
<p>git push origin</p>
<p></p>
<p></p>
<p></p>",2019-10-29T00:23:01Z,30,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2b3xlxmxln4up,2019-10-29T00:23:01Z,{},other
3405,no,"<p>Well, sure. But let&#39;s say (as in the original question) you have an existing repo and folder structure with some specific nested files that you don&#39;t want to move. OP could have used something like <strong><tt>find . -name *.md | xargs git add</tt></strong> from the parent directory to add them all in one shot.</p>",2019-10-29T01:39:24Z,30,Week 10/27 - 11/2,feedback,,i4op5p9vfbq5yz,k2b6nukhfpd7ao,2019-10-29T01:39:24Z,{},other
3406,no,"<p>absolutely, I love xargs :)  very handy utility that ;)</p>
<p></p>
<p>I mentioned explicitly copying with respect to a &#34;possible&#34; repo limit ;) yesterday I had 1.5 GB in my directory, and I did not want to find out in media res, that this broke some limit on gatech repositories (did not want to deal with the possibility of an inconsistent git state due to an aborted transfer) ;) </p>
<p></p>
<p>as luck would have it, moving the files into the &#34;upload&#34; folder, mentioned the size.  I was somewhat shocked (since it is essentially, a bunch of graph pictures).  also, the gent explicitly mentioned wanting to add .py and .md.  ;)  mine adds all files (not ignored in .gitignore).</p>
<p></p>
<p>yours works lovely (to bypass the shell expansion issue).  it is filed away for future use ;)</p>
<p></p>",2019-10-29T06:24:35Z,30,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2bgul3w3yf2w2,2019-10-29T06:24:35Z,{},other
3407,no,<p>xargs is a great idea. It takes out all the ambiguities. Thanks!! </p>,2019-10-30T00:08:17Z,30,Week 10/27 - 11/2,feedback,,jzhs489b10i78c,k2ciuip0kcn6gd,2019-10-30T00:08:17Z,{},other
3408,no,git status is your friend ,2019-10-28T17:06:19Z,30,Week 10/27 - 11/2,followup,,jh701barLeiB,k2aoc0jemmf3l3,2019-10-28T17:06:19Z,{},other
3409,no,<p>&#43;1. Make sure to check what is staged before you commit</p>,2019-10-28T17:50:32Z,30,Week 10/27 - 11/2,feedback,,is6lq4mnzu02k5,k2apwvmr7yw4zb,2019-10-28T17:50:32Z,{},other
3410,no,<p>I will make a habit of doing this.</p>,2019-10-30T03:01:02Z,30,Week 10/27 - 11/2,feedback,,jzhs489b10i78c,k2cp0ohao5w1fv,2019-10-30T03:01:02Z,{},other
3411,no,<p>Nice no new lectures the week after project 2 was due! Perfect time to catch up.</p>,2019-10-28T17:48:11Z,28,Week 10/27 - 11/2,followup,,isde332xcka1m0,k2aptv13yda2ua,2019-10-28T17:48:11Z,{},logistics
3412,no,<p>Are the game theory lectures and papers not for this week? They looked to be due Nov 2nd to me. Maybe it just depends on how we define the week.</p>,2019-10-29T02:31:21Z,28,Week 10/27 - 11/2,followup,,ixty1midfufhd,k2b8in5wo0w67w,2019-10-29T02:31:21Z,{},logistics
3413,no,"<p>For project 3, I have a few questions ...</p>
<p></p>
<p>1. For Foe-Q, idealy, does Q_A = - Q_B, i.e. the Q matrix for player A the negative of player B, because it is a zero sum game?</p>
<p>2. For Friend-Q, does R_A = R_B, because it is assuming player A and player B trying together to get the best score for player A?</p>
<p>3. For the graph, does iteration refers to the time step or the number of episodes?</p>",2019-10-31T17:21:42Z,55,Week 10/27 - 11/2,followup,,jzn63vvy8x67lj,k2ez7cirov86rr,2019-10-31T17:21:42Z,{},logistics
3414,stud,"The soccer grid size in Fig 4. is  2 X 4. However , grid size in [10] reference (Markov games as a framework for multiagent reinforcement learning) is 4 X 5. What are we supposed to consider ?<div><br /></div><div>Also, I understood that we need to create our own env for this project. Is this correct?</div>",2019-10-29T18:35:46Z,29,Week 10/27 - 11/2,followup,a_0,,k2c6yw9p3gj4no,2019-10-29T18:35:46Z,{},project3
3415,no,"<p>You need to use Figure 4 from the paper.</p>
<p></p>
<p>You will need to create the environment(unless you find one that matches) but you can share details about the environment with other students.  Just make sure that you develop the learning algorithms yourself.</p>",2019-10-31T00:21:28Z,29,Week 10/27 - 11/2,feedback,,hz7meu55mi8sd,k2dyrbsom82330,2019-10-31T00:21:28Z,{},project3
3416,no,"<p>I have a question regarding the example description &#34;The two players cannot possess the same cell. If they run into the same cell, only the first moves&#34;.</p>
<p></p>
<p>What does the first / second stand for? The two players should move simultaneously if I understand correctly. So how to decide who is first/second player if two players tend to move to the same cell? Does player A is the first player? Thanks.</p>
<p></p>
<p></p>",2019-10-31T17:22:51Z,29,Week 10/27 - 11/2,followup,,jzg6jh2hn6f43c,k2ez8tzeo4f91,2019-10-31T17:22:51Z,{},project3
3417,no,"The paper mentions &#34;The players’ actions are executed in random order&#34;. So, I think who goes first is decided randomly.",2019-10-31T17:36:40Z,29,Week 10/27 - 11/2,feedback,,jqi9pq31o4p6dt,k2ezqljvdom5lw,2019-10-31T17:36:40Z,{},project3
3418,no,<p>Thanks for the guide. </p>,2019-10-31T18:30:57Z,29,Week 10/27 - 11/2,feedback,,jzg6jh2hn6f43c,k2f1oepuf1wzd,2019-10-31T18:30:57Z,{},project3
3419,no,"<p>I submitted</p>
<p></p>
<p></p>
<p>0, 1, 0, .... </p>
<p></p>
<p>this format worked too. </p>",2019-11-03T03:49:14Z,54,Week 11/3 - 11/9,followup,,hzoi2qsuCAd,k2igi20a6su1cc,2019-11-03T03:49:14Z,{},hw5
3420,no,"<p>you get a 100% with that?</p>
<p></p>
<p>By &#34;all possible hypothesis&#34; do you mean 0 or 1 for a given patron vector?</p>
<p>Or are you keeping track of the possible instigators and peacemakers, and narrowing down the possibilities with each experience? </p>",2019-10-29T22:45:54Z,31,Week 10/27 - 11/2,followup,,jl1b27fpaYkv,k2cfwk4i61a14t,2019-10-29T22:45:54Z,{},hw5
3421,no,"<p>1. Yes</p>
<p>2.  keeping track of the possible instigators and peacemakers, and narrowing down the possibilities with each experience? </p>
<p></p>
<p>I started by generating all possible combos of instigator/peacmaker/patrons first.</p>",2019-10-29T23:43:51Z,31,Week 10/27 - 11/2,feedback,,jzj7y1ofgsro1,k2chz3q44cx3lc,2019-10-29T23:43:51Z,{},hw5
3422,no,<p>&#64;angel how did you generate all combos? Is that a method you made or some method you found somewhere?</p>,2019-10-31T20:52:51Z,31,Week 10/27 - 11/2,feedback,,jc6mqevhagl262,k2f6qw6g8qr2hh,2019-10-31T20:52:51Z,{},hw5
3423,no,"<p>You can get all the combos with a simple nested for loop.</p>
<p></p>
<p>You just want to keep track of all the possible identities that the instigator or peacemaker can take on. So iterate for the number of patrons you have, nested, keep track of every possible pair of index combos, but skip ones where the indices are the same because the same patron can&#39;t have both identities. That&#39;s how I understood it.</p>",2019-10-31T20:57:51Z,31,Week 10/27 - 11/2,feedback,,jl1b27fpaYkv,k2f6xbphqud1h6,2019-10-31T20:57:51Z,{},hw5
3424,no,"I got a 100% on my submission. What I did is to start each patron with equal probability of being a instigator, peacemaker and others. And through the episodes use memory first, if still idk produced, based on the actual result update the probabilities. For example if a fight occurs, peacemaker has to be not presented etc. And use the updated probability to try to identify each patron at each time. Not sure if this is the proper Kwik algo, but it worked.",2019-10-29T22:53:50Z,31,Week 10/27 - 11/2,followup,,jzj4205g7gd2fw,k2cg6rkq12w2wt,2019-10-29T22:53:50Z,{},hw5
3425,no,"<p>Not sure if i understood this right... So we start by assigning equal probability to each patron of being an instigator, peacemaker and others. Then, the first outcome on presenting the first entry at &#39;atEstablishment&#39;, the algorithm should then output &#39;I DONT KNOW&#39; since both the patrons have the same probability of being instigator, peacemaker and others. or, am i missing something here? But the right output seems to be 0 (NO FIGHT). </p>",2019-10-30T05:45:14Z,31,Week 10/27 - 11/2,feedback,,jc9ssyte5au1hp,k2cuvu74m2q2ly,2019-10-30T05:45:14Z,{},hw5
3426,no,"There could be also some additional rules added implied by this problem, you have to define those. Like the problem is saying if both peacemaker and insitagor appears there is no fight, so both people appears no matter what probability there should be no fight.",2019-10-30T12:45:33Z,31,Week 10/27 - 11/2,feedback,,jzj4205g7gd2fw,k2d9wda43qx6cy,2019-10-30T12:45:33Z,{},hw5
3427,no,"<p>If you go with the set of hyphoteses described in the paper, and predict only if all remaining hyphoteses agree and saying &#34;don&#39;t know&#34; otherwise, pretty much everything follows.</p>
<p>When all patrons are in the bar, every hyphotesis in the version space &#34;instigator is present and peacemaker is absent&#34; predicts false (since everyone is present). That&#39;s why the correct output is &#34;no fight&#34; in the first entry of the example.</p>",2019-10-31T17:27:01Z,31,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k2eze6zbxcg65y,2019-10-31T17:27:01Z,{},hw5
3428,no,"<p>For these: &#34;instigator is present and peacemaker is absent&#34;, I held them out of the h(x) evaluation and conveyed them to the $$\hat{H}$$ to the next observation. This worked properly. In the case of everyone being in the bar, the (X,NotPresent) condition is nonsense and doesn&#39;t map to any value..but it can map later when it is no longer nonsense.</p>",2019-10-31T20:35:32Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2f64mjfuzg4yt,2019-10-31T20:35:32Z,{},hw5
3429,no,"<p>By the way there can be patrons that are neither instigators nor peacemakers, right?</p>",2019-10-29T23:19:19Z,31,Week 10/27 - 11/2,followup,,jl1b27fpaYkv,k2ch3jqfj9y68d,2019-10-29T23:19:19Z,{},hw5
3430,no,"Yes only 1 instigator and 1 peacemaker, the rest are neither.",2019-10-29T23:24:27Z,31,Week 10/27 - 11/2,feedback,,jzj4205g7gd2fw,k2cha4uud127oz,2019-10-29T23:24:27Z,{},hw5
3431,no,"<p>This <a href=""http://proceedings.mlr.press/v19/szita11a/szita11a.pdf"" target=""_blank"" rel=""noopener noreferrer"">paper</a> is a beast, but at least it has pseudocode for KWIK.</p>",2019-10-31T00:23:24Z,31,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2dytsp29v61yj,2019-10-31T00:23:24Z,{},hw5
3432,no,"<p>I&#39;m using algorithm 2 from Littman 2008 successfully for <em>almost </em>all values, so I wanted to double check my logic. </p>
<p></p>
<p>1) If the subset &amp; outcome have been learned before: return from memory</p>
<p>2) All possible peacemakers <strong>or</strong> no one <strong>or</strong> everyone present: return no fight</p>
<p>3) All possible fighters are <strong>there</strong>, and all possible peacemakers are <strong>not</strong> there: return fight</p>
<p>4) Learn: Put subset &amp; outcome in memory</p>
<p>4b) If fight occurs =&gt; Learn: Everyone <strong>not</strong> there take out of fight set. Everyone <strong>there</strong> take out of the peace <strong>set</strong>. </p>
<p></p>
<p>See anything wrong? </p>",2019-10-31T00:51:26Z,31,Week 10/27 - 11/2,followup,,jzhdgaq99za3um,k2dztuiba7g6n6,2019-10-31T00:51:26Z,{},hw5
3433,no,<p>I think you should also include the opposite of your item 3 (if all_fighters_out or all_pacemakers_in -&gt; no_fight).</p>,2019-10-31T05:36:47Z,31,Week 10/27 - 11/2,feedback,,jc6xvgjncoey,k2ea0tio25y4yc,2019-10-31T05:36:47Z,{},hw5
3434,no,"<p>If you&#39;re following algorithm 2 you should not need a memory, just a set of hyphoteses of the form &#34;x is the instigator and y is the peacemaker&#34;. Anyone can be either except that they can&#39;t be the same person.</p>
<p>If you keep separate instigator and peacemaker sets you can&#39;t always rule out someone when there is no fight. With a single set of hyphoteses, at least one hyphotesis can be ruled out every time you receive an observation, because disagreement on the output is what led to responding &#34;don&#39;t know&#34; in the first place.</p>",2019-10-31T17:47:16Z,31,Week 10/27 - 11/2,feedback,,is8ald0uljj3u4,k2f0480yefi4jw,2019-10-31T17:47:16Z,{},hw5
3435,no,"<p>If we compute $$\hat{L}$$ as $$h(x) | h \in \hat{H}$$ then isn&#39;t $$|\hat(L)|$$ the same as $$|\hat{H}|$$ ?? Maybe || is not the cardinality operator?</p>
<p></p>
<p>If it is the cardinality operator then what&#39;s the point of computing $$\hat{L}$$?</p>
<p></p>",2019-10-31T13:31:44Z,31,Week 10/27 - 11/2,followup,,jc554vxmyuy3pt,k2eqzlrh1b61g6,2019-10-31T13:31:44Z,{},hw5
3436,no,<p>haha. nevermind. stupid question.</p>,2019-10-31T14:05:07Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2es6jt94pa6h9,2019-10-31T14:05:07Z,{},hw5
3437,no,"<p>its all </p>
<h1>☯ ♠ Ω ♤ ♣ ♧ ♥ ♡ ♦ ♢ ♔ ♕ ♚ ♛ ⚜ ★ ☆ ✮ ✯ ☄ ☾ ☽ ☼ ☀ ☁ ☂ ☃ ☻ ☺ ☹ ۞ ۩</h1>
<p> to me</p>",2019-10-31T14:13:08Z,31,Week 10/27 - 11/2,feedback,,jzj7y1ofgsro1,k2esguslhmf4vx,2019-10-31T14:13:08Z,{},hw5
3438,no,"<p>haha</p>
<p></p>
<p>Just in case anyone else&#39;s dusty neuron didn&#39;t fire</p>
<p></p>
<p>Card({[0,0,0,0]}) = 1</p>
<p>Card([0,0,0,0]) = 4</p>
<p></p>
<p>Cardinality of a string is the length of the string The cardinality of a set is the number of unique items in the set.</p>
<p>{} = set</p>
<p>[] = string</p>
<p>..... slowly resurrecting old neurons ....</p>
<p></p>",2019-10-31T14:23:42Z,31,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2esug4feij6zy,2019-10-31T14:23:42Z,{},hw5
3439,no,"<p><a href=""http://videolectures.net/icml08_li_kwik/"">http://videolectures.net/icml08_li_kwik/</a> - there is a short conference talk from one of the paper authors. You will need to use to Flash Player for the video to work though.</p>",2019-10-30T17:42:15Z,22,Week 10/27 - 11/2,followup,,jzhwdil7ssn443,k2dkhxj5f5t3h9,2019-10-30T17:42:15Z,{},hw5
3440,no,<p>Thanks! I finally found use for my Edge browser.. :)</p>,2019-10-31T11:21:39Z,22,Week 10/27 - 11/2,feedback,,jc9ssyte5au1hp,k2emcbvo1sn4il,2019-10-31T11:21:39Z,{},hw5
3441,no,"<p>edge has flash built in? :) yeesh.</p>
<p>also, good to know.</p>",2019-11-01T21:50:35Z,22,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2go8z9lye34wp,2019-11-01T21:50:35Z,{},hw5
3442,no,<p>I have the same question. fightOccurred array is already given information. So what should we predict?</p>,2019-10-30T18:18:36Z,30,Week 10/27 - 11/2,followup,,jzj6lng3im15y3,k2dlsnxkh8e34p,2019-10-30T18:18:36Z,{},hw5
3443,no,"<p>You use the fightOccurred array as training. This KWIK thing is an imperfect predictor until you get more information into it, then it slowly becomes a perfect predictor.</p>",2019-10-30T21:26:12Z,30,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2dshx9lp5c63a,2019-10-30T21:26:12Z,{},hw5
3444,no,"<p>this still doesn&#39;t make sense to me... we are given information about who was at the establishment, and whether or not a fight occurred, and then we&#39;re supposed to re-predict whether or not a fight occurred using the same data about who was at the establishment?</p>",2019-10-31T18:51:15Z,30,Week 10/27 - 11/2,followup,,jqu95q68ljj1pn,k2f2ei6o5vu78w,2019-10-31T18:51:15Z,{},hw5
3445,no,"<p>Mostly this example problem is about identifying the instigator and the peacemaker. Once you do that, it can predict any input&#39;s tendency to have a fight or not. You will see that at the end of training the algorithm is able to identify the instigator and the peacemaker.</p>",2019-10-31T20:22:54Z,30,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2f5odng69p3pe,2019-10-31T20:22:54Z,{},hw5
3446,no,"<p>I had the same question &#64;Jasmine, but what you suggest in fact, is memorising all the inputs, and only then you will be able to make 100% accurate predictions all the time (KWIK cannot make mistakes).</p>
<p>That&#39;s what algorithm 1 does, but it will take you to memorise $$2^n$$ group of patrons.  </p>
<p></p>
<p>The other clever way is to identify who are the troublemaker and the peacemaker, and then you can predict if there will be a fight or not, also with 100% accuracy, but having done a LOT less computation (which may not even be possible if n becomes too big).  </p>
<p>Also, you are not guaranteed to be given all the $$2^n$$ possible groups by the environment (ie the TA&#39;s...), so algorithm 2 seems the only way.  </p>
<p></p>",2019-10-31T23:04:59Z,30,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2fbgt757va2g2,2019-10-31T23:04:59Z,{},hw5
3447,no,"<p>sequence of predictions, consisting of {-1, 0, 1}^n where n = number of samples i.e. an array.</p>
<p></p>
<p>I ran all of the cases they took less than a second to run. </p>
<p></p>
<p>hint:  how you choose to represent ur guess (what really matters) and signify &#34;not present&#34; matters.</p>",2019-11-01T22:12:11Z,30,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2gp0reyj5636d,2019-11-01T22:12:11Z,{},hw5
3448,no,"<p>Your algorithm takes from &#96;atEstablishment&#96; one item at a time, each item defining a particular configuration on which patrons are at the bar. For each configuration, if the algorithm can determine with certainty there is going to be a fight or no fight, it outputs 1 or 0, respectively. The algorithm should not make a guess. If it cannot determine, then your algorithm outputs -1 (&#34;don&#39;t know&#34;)  and learns from the corresponding value in &#96;fightOccurred&#96; whether a fight happens for that configuration. (There is also a case when there is a contradiction of evidence and prediction is not possible, but it does not appear in the HW examples.). Your algorithm then advances to the next item in &#96;atEstablishment&#96; and repeats the above.</p>",2019-11-02T06:59:19Z,30,Week 10/27 - 11/2,followup,,jzhs489b10i78c,k2h7unw35et6xv,2019-11-02T06:59:19Z,{},hw5
3449,no,"<p>Where I&#39;m at... z becomes the prediction for y and eventually fightOccured for each atEstablishment becomes the prediction as listed in the KWIK Definition.</p>
<p></p>
<p>But this is not correct.</p>",2019-11-02T23:31:46Z,30,Week 10/27 - 11/2,feedback,,jzkke6iz3cl28t,k2i7ayo2p0n4mv,2019-11-02T23:31:46Z,{},hw5
3450,no,"<p>If you follow the explanations of how algorithm 2 (enumeration) works in 4.1 in the study, you&#39;ll understand how to deal with this problem.  </p>
<p>The language is quite &#39;cryptic&#39; because it wants to be theoretical and general, so it takes time to decode it, but it comes down to something very simple when you get it.  </p>",2019-11-02T23:47:31Z,30,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2i7v838ux13my,2019-11-02T23:47:31Z,{},hw5
3451,no,"<p>Are you talking about section 3.1 of the KWIK paper? Because this is a deterministic case, $$z=y$$ and they mean an observation from fightOccurred. $$x$$ means an item in atEstablishment. At each step, the environment provides an $$x$$ to the learner. The learner then makes a prediction $$\hat{y}$$. If $$\hat{y}=\perp$$, then an observation is made and $$y$$ is provided to the learner (meaning the value of the corresponding item from fightOccurred is revealed to the learner).  </p>
<p></p>
<p>Hint: Algorithm 2 is all you need. It is a very natural algorithm that we probably all use often even without reading this paper. The protocol described in Algorithm 2 is a specific case of that described in section 3.1.</p>",2019-11-02T23:49:17Z,30,Week 10/27 - 11/2,feedback,,jzhs489b10i78c,k2i7xhv4okk5fi,2019-11-02T23:49:17Z,{},hw5
3452,no,"<p>Didn&#39;t measure the # of updates, but a DQN-based approach solved in 450-580 episodes on average updating every 4 frames. Episode lengths varied as well with a hard max of 1500. Many episodes finished in &lt;500-800 steps, but if we assume all took the maximum 1500 steps, total # of updates is 168,750 to 217,500.  ((1500 / 4) * 450) to ((1500 / 4) * 580). If we assume the avg episode ended in closer to 750 steps, total updates is closer to 84,375 to 108,750. Each update used a minibatch size 32. If we want to count each experience within a minibatch as an &#39;update&#39;, the worst-case total update count is around 6,960,000 (217500 * 32).</p>
<p></p>
<p>My DQNs pre-populated a replay buffer with a minimum of 5000 (s,a,r,s&#39;) tuples before starting learning, which usually took around 55 episodes. DQN, Target DQN, Double Q, &amp; an experimental algorithm I made usually solved the environment within ~50-80 episodes of the first solver, which wasn&#39;t the same across multiple runs.</p>
<p></p>
<p>Interested in hearing more detailed data from peers if others tracked total # of updates more directly than me.</p>",2019-10-30T22:03:56Z,55,Week 10/27 - 11/2,followup,,isde332xcka1m0,k2dtug0sk914jf,2019-10-30T22:03:56Z,{},project2
3453,no,"<p>Update: since runtime is also an interesting performance metric, run times typically varied 33-36 minutes to solve.  (solved using CPU only on a Ryzen 7 1700)</p>
<p></p>
<p>Did a discretized approach solve faster? Seems like each update would take significantly less time than updating a DQN.</p>",2019-10-30T22:08:58Z,55,Week 10/27 - 11/2,feedback,,isde332xcka1m0,k2du0x4kpqi12p,2019-10-30T22:08:58Z,{},project2
3454,no,"<p>Well. I had one black swan that solved it in 5 minutes. Ignoring that fluke, 1-3 hours is what it took to solve with my QQ. Typical was 3 hours and 6.9M updates, similar to your updates. That&#39;s interesting!</p>",2019-10-30T22:18:53Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2dudoj85im5ey,2019-10-30T22:18:53Z,{},project2
3455,no,"<p>That is fascinating.  When you say QQ, do you mean <a href=""https://papers.nips.cc/paper/3964-double-q-learning"" target=""_blank"" rel=""noopener noreferrer"">Double Q</a>?</p>
<p></p>
<p>What motivated discretizing the state space in this case?</p>",2019-10-30T22:30:34Z,55,Week 10/27 - 11/2,feedback,,isde332xcka1m0,k2dusp6k6vw2kw,2019-10-30T22:30:34Z,{},project2
3456,no,"<p>Yup, Double Q. I chose discretizing for no particular reason other than I had already written it, so I had more time to experiment with more variance reduction techniques.</p>",2019-10-30T22:54:32Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2dvniqpw0p2wo,2019-10-30T22:54:32Z,{},project2
3457,no,"<p>It depended on the particular architecture of the agent for me.  I have a &#34;precocious&#34; architecture that can solve Lunar Lander pretty consistently in under 500 episodes.  I just ran him to get a printout of the update number.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2dyhmf81pfp%2FUpdates.png"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2dyi1dfqdoa%2FUpdates2.PNG"" alt="""" /></p>
<p></p>
<p>So the moment he had broken a trailing mean reward of 200 (episode 453) he had had 36.6 million updates.</p>",2019-10-31T00:25:37Z,55,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k2dywo18qfi4oh,2019-10-31T00:25:37Z,{},project2
3458,no,"<p>Here is plain old SARSA with nonlinear function approximation (absolutely nothing else):</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2e1rzk25e7i%2FSARSATraining.png"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2e1sj1a5ad2%2FSARSAUpdates.PNG"" alt="""" /></p>
<p></p>
<p>Much slower to train adequately, but far fewer updates to solve as there&#39;s only one update per frame.</p>",2019-10-31T01:48:44Z,55,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2e1vjyt237400,2019-10-31T01:48:44Z,{},project2
3459,no,"<p>wow! 36M updates. That&#39;s a huge number of Q(s,a) values to compute. That would have taken about 20 hours for my agent.</p>",2019-10-31T15:01:10Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2eu6mh2u2364a,2019-10-31T15:01:10Z,{},project2
3460,no,"<p>Yeah, it was DQN with 128 updates per time-step, 4x what they did in the nature paper.  But I did it in batches of 16 (as opposed to batches of 1 in the paper), so it was computationally more like 8 updates per time step.  If I did it in 16 batches of 8, I would see considerable slowdown (maybe 1.5 to 2x) but at 8 batches of 16 I didn&#39;t really see a slowdown on my computer.  What they did in the paper, 32 batches of 1, would have actually been much slower, even though they were only training 1/4 of the samples I was.</p>",2019-10-31T15:27:03Z,55,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2ev3wkxhgx2ol,2019-10-31T15:27:03Z,{},project2
3461,no,"<p>Looks like two clusters of performance in that plot (one big cloud at the top, one lower cloud that thins out after more episodes). It&#39;s interesting to see the sparse area of rewards around 100 between the two doesn&#39;t appear to merge the two clusters as training continues. Seems to suggest there&#39;s something categorically different about the scenario or the agent&#39;s behavior between the clusters. Any insights what it might be?</p>",2019-10-31T15:37:28Z,55,Week 10/27 - 11/2,feedback,,isde332xcka1m0,k2evhbc22r97jp,2019-10-31T15:37:28Z,{},project2
3462,no,"<p>Good observation on the two clusters!  I&#39;ve been digging into that.</p>
<p></p>
<p>So that graph is SARSA.  After the project, I had the luxury to try things that I should have tried earlier, but was too stressed with my &#34;official&#34; DQN solution to play with.  Specifically, I wanted to see <em>why</em> we even needed DQN. Presumably, the simpler approaches (SARSA and Q-learning) were somehow deficient and we required the innovations that DQN offers.  That may have been true for Atari but as it turns out Lunar Lander is simple enough that this isn&#39;t the case.  SARSA and Q-learning with nonlinear function approximation do just fine (although DQN does do better).  I&#39;m specifically looking at SARSA at the moment because it&#39;s on-policy, so is not a member of the group of algorithms that suffer from the <em>The Deadly Triad</em>.</p>
<p></p>
<p>From watching the lander, that sparse area is well-positioned crashes at high speed - crashes on the landing pad -  that garner ~0 reward.  So either the lander lands (&gt;~200 points), or crashes with good positioning at high speed (~0 points).  Getting ~100 points would require either landing with a bad lead-in and bad positioning, or crashing with slow speed, which it doesn&#39;t do as much.</p>
<p></p>
<p>I&#39;m not sure exactly why it&#39;s behaving this way.  The nice thing about plain old SARSA is that there are so few hyperparameters to play with, unlike DQN.  Exploration is the big thing, since it&#39;s an on-policy algorithm.  So I&#39;m going to try to play with exploration, and maybe training time, to see if I can get rid of that lower cluster.</p>
<p></p>
<p>Edit:  I should say I&#39;m trying both SARSA and something called Expected SARSA (defined in S&amp;B).  Expected SARSA is to SARSA kind of like what Double Q is to Q.  Once I see (I haven&#39;t yet) that Expected SARSA is clearly better than SARSA, I&#39;ll probably drop SARSA.</p>",2019-10-31T16:05:56Z,55,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2ewhwn6i1gpq,2019-10-31T16:05:56Z,{},project2
3463,no,"<p>I reach pretty decent scores after 250 episodes.  &#39;ev&#39; is the evaluation of the agent, ie the average reward on 10 random episodes using the trained agent after an update, ar 10 is the reward at the end of an episode (averaged over 10 samples).  </p>
<p></p>
<p>el 45:44, ep 0250, ts 057217, ar 10 172.0±111.6, 100 094.1±130.7, ev 234.1±035.2</p>
<p>el 46:05, ep 0253,ts 057685, ar 10 128.1±120.2, 100 094.9±130.5, ev 233.3±035.1</p>
<p>el 46:41, ep 0255, ts 058636, ar 10 153.2±118.2, 100 100.2±132.4, ev 233.4±035.0</p>
<p>el 47:25, ep 0257, ts 059850, ar 10 198.1±113.6, 100 106.0±133.6, ev 232.7±034.8</p>
<p>el 47:55, ep 0259, ts 060544, ar 10 219.4±102.1, 100 111.7±134.4, ev 232.7±034.8</p>
<p>el 48:18, ep 0260, ts 061129, ar 10 225.5±105.1, 100 114.6±135.4, ev 233.0±034.8</p>
<p></p>
<p>Here&#39;s my rewards graph:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2eje60yx89r%2Ffull_simulation_with_rewards_plot_using__3300262_10h29.png"" alt="""" /></p>
<p></p>
<p>and the evaluations graph:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2ejfkah7xp7%2Ffull_simulation_with_rewards_plot_using__3300262_10h31.png"" alt="""" /></p>
<p>Disregard the big dip at episode 1000, it&#39;s due to the fact I drop the heuristic learning and leave the agent on its own, allowed to do lots of stupid things, and crash a lot, but it recovers amazingly quickly.  </p>
<p></p>
<p>Around the 2500th episode, I get very good rewards, with low deviations:</p>
<p></p>
<p>el 09:32, ep 2492, ts 685122, ar 10 285.2±017.1, 100 253.7±073.0, ev 241.9±044.4<br />el 09:54, ep 2496, ts 685949, ar 10 277.3±020.4, 100 253.4±072.9, ev 241.4±044.4<br />el 10:29, ep 2499, ts 686818, ar 10 271.8±022.6, 100 253.3±073.0, ev 239.7±045.7<br />el 10:54, ep 2503, ts 687616, ar 10 275.6±023.0, 100 253.4±073.0, ev 244.0±042.1<br />el 11:23, ep 2507, ts 688688, ar 10 281.6±018.6, 100 254.9±073.2, ev 245.2±041.4<br />el 11:47, ep 2511, ts 689378, ar 10 282.0±011.4, 100 257.6±068.4, ev 243.8±041.3</p>
<p></p>
<p></p>
<p></p>",2019-10-31T10:07:08Z,55,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2ejohl2j2co8,2019-10-31T10:07:08Z,{},project2
3464,no,"<p>Do you know how many Q(s,a) values you computed to get to the solution?</p>",2019-10-31T14:59:59Z,55,Week 10/27 - 11/2,feedback,,jc554vxmyuy3pt,k2eu53ct8gh4n3,2019-10-31T14:59:59Z,{},project2
3465,no,"<p>Sorry &#64;Jacob, Chrome froze and I had to redo my post and I forgot about the timesteps (ts).</p>
<p>Look at ts in the data, it&#39;s the number of times I update the online model, so it&#39;s 690,000 for episode 2511, but if you want to be accurate, since DQN calculates two Q values, one from the online model, one from the replay memory, you should double that number.  </p>
<p></p>
<p>I&#39;m currently running simulations with different parameters, like bigger neural nets, to see if it can learn faster, but so far, from the looks of it, it seems to be slower.  </p>",2019-10-31T17:02:16Z,55,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2eyid3uwcc50g,2019-10-31T17:02:16Z,{},project2
3466,no,<p>Thanks for making this happen! Looking forward to it.</p>,2019-10-30T23:19:46Z,28,Week 10/27 - 11/2,followup,,isde332xcka1m0,k2dwjyvrwhe405,2019-10-30T23:19:46Z,{},other
3467,no,<p>&#x1f641;</p>,2019-10-30T23:20:46Z,28,Week 10/27 - 11/2,followup,,gx3c8l7z7r72zl,k2dwl90chgz5o9,2019-10-30T23:20:46Z,{},other
3468,no,"<p>Trust me, I was very sad not being able to find a replacement on time, too. But, we hope you like it!</p>
<p></p>",2019-10-30T23:48:55Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2dxlgoflpk1pw,2019-10-30T23:48:55Z,{},other
3469,no,"<p>I did, thanks Miguel!</p>",2019-11-05T03:06:57Z,27,Week 11/3 - 11/9,feedback,,gx3c8l7z7r72zl,k2l9vdrvasj2pp,2019-11-05T03:06:57Z,{},other
3470,no,"<p>Why &#39;hangouts on air&#39; instead of simply bluejeans? One can login into BJ without an account, just with the conference ID...</p>",2019-10-31T00:18:47Z,28,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2dynv1ocr791,2019-10-31T00:18:47Z,{},other
3471,no,<p>We can&#39;t record students without consent if making the video public. We make our special guest office hours public (check my YouTube for a couple more). It is also very difficult to manage lots of students when there is a guest that should get most of the camera/mic time.</p>,2019-10-31T01:28:27Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2e15gvqrvf1c0,2019-10-31T01:28:27Z,{},other
3472,no,<p>Just posted the link to it; video is uploading!</p>,2019-10-31T01:28:53Z,28,Week 10/27 - 11/2,followup,,hyx9thiqa6j4nn,k2e160zuczi1fn,2019-10-31T01:28:53Z,{},other
3473,no,"<p>as of 14 minutes later, video &#34;looks broken&#34; :) I presume you are still uploading.</p>",2019-10-31T01:44:57Z,28,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2e1qodwe6n6ys,2019-10-31T01:44:57Z,{},other
3474,no,"<p>It&#39;s at 95% processing! :)</p>
<p></p>
<p>It was a good one.</p>",2019-10-31T01:46:09Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2e1s8c6y2j3my,2019-10-31T01:46:09Z,{},other
3475,no,<p>It should be ready to watch!</p>,2019-10-31T01:52:45Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2e20ptdt8042l,2019-10-31T01:52:45Z,{},other
3476,no,"<p>excellent, thank you! :) it is working on my end :)</p>",2019-10-31T01:57:33Z,28,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2e26vxsygz3i8,2019-10-31T01:57:33Z,{},other
3477,no,<p>Super! Enjoy!</p>,2019-10-31T01:58:32Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2e2852fxt6569,2019-10-31T01:58:32Z,{},other
3478,no,"<p>What a great Q and A, really inspiring!  Thanks Miguel and Chris (and Taka and Tianhang).</p>",2019-10-31T03:34:31Z,28,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2e5nl1tela3f3,2019-10-31T03:34:31Z,{},other
3479,no,"<p>Glad you liked it, Vahe!</p>",2019-10-31T11:52:03Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2enfezw3q738b,2019-10-31T11:52:03Z,{},other
3480,no,<p>This was an interesting source of information for this course.  Thank you for coordinating this.</p>,2019-11-07T04:56:05Z,27,Week 11/3 - 11/9,feedback,,is9so9huTMp,k2o8ng9tnvg62i,2019-11-07T04:56:05Z,{},other
3481,no,"<p>Yw, Nick.</p>",2019-11-07T21:05:04Z,27,Week 11/3 - 11/9,feedback,,hyx9thiqa6j4nn,k2p79kj550w5f5,2019-11-07T21:05:04Z,{},other
3482,no,"<p>There was a paper he mentioned roughly half way through about &#34;value iteration networks&#34;. Was that a specific paper he was talking about and, if so, which one was it?</p>",2019-10-31T09:03:14Z,28,Week 10/27 - 11/2,followup,,jl3oi5v7qkSk,k2ehebeaw7ks9,2019-10-31T09:03:14Z,{},other
3483,no,"<p>It should be the Berkeley one: <a href=""https://arxiv.org/abs/1602.02867"">https://arxiv.org/abs/1602.02867</a></p>",2019-10-31T11:51:41Z,28,Week 10/27 - 11/2,feedback,,hyx9thiqa6j4nn,k2enexxinof32z,2019-10-31T11:51:41Z,{},other
3484,no,"<p>Thank you very much Miguel and all others participated in this event for your effort, very well done and questions as Chris presented them sounded much more interesting than in the dry voting post on Piazza :-) Great job!</p>",2019-10-31T21:23:13Z,28,Week 10/27 - 11/2,followup,,jqkxzdmmolGf,k2f7ty8ufq86lj,2019-10-31T21:23:13Z,{},other
3485,no,"<p>Yeah, Chris made us look good :)</p>",2019-10-31T21:43:11Z,28,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2f8jmgv5944ou,2019-10-31T21:43:11Z,{},other
3486,no,"<p>Agreed, Chris formed the questions much more concise and technical inclined, than in their raw form. Thanks Miguel and the TA team for organizing this!</p>",2019-11-02T04:03:35Z,28,Week 10/27 - 11/2,feedback,,jl5wq8mca7o0,k2h1ko54ao444,2019-11-02T04:03:35Z,{},other
3487,no,"<p>Thanks Chris, much appreciated!</p>",2019-11-01T00:07:16Z,55,Week 10/27 - 11/2,followup,,jzhl7qlwrpagr,k2fdowsqqhao9,2019-11-01T00:07:16Z,{},project3
3488,no,"<p>By &#34;simulate 1 million games of soccer&#34;, I&#39;m assuming this refers to episodes of learning, where each episode, or game, ends when a single goal is scored?</p>",2019-11-03T21:24:55Z,54,Week 11/3 - 11/9,followup,,jl2bq5rf8b67pq,k2ji7obu8vpio,2019-11-03T21:24:55Z,{},project3
3489,no,"<p>My understanding for this project is that each iteration is a step within the environment (not a complete game until a goal is scored).</p>
<p></p>
<p></p>",2019-11-13T02:29:24Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2wo1wvovf66nz,2019-11-13T02:29:24Z,{},project3
3490,no,"<p>But even after the first goal, the chart would still show Err=0 since that exact Q(s,a=south, b=stick) is updated only when another iteration reaches that state s.</p>
<p></p>
<p>Are your chart for Friend-Q looking something like that?</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk2wntvit333w%2FFriend_Q.png"" alt="""" /></p>
<p></p>
<p>Zooming in shows Err = 0 several times before it converges:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk2wnv0xc93tp%2FFriend_Q_zoom.png"" alt="""" /></p>
<p></p>",2019-11-13T02:27:16Z,53,Week 11/10 - 11/16,followup,,jc6xvgjncoey,k2wnz6n1hxw42x,2019-11-13T02:27:16Z,{},project3
3491,no,"<p>Danilo, have you figured out why your Friend-Q algorithm converges 5 times as fast as Greenwald&#39;s?</p>",2019-11-14T16:06:55Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2ywp3c9imo6bj,2019-11-14T16:06:55Z,{},project3
3492,no,"<p>Vahe,</p>
<p></p>
<p>I swear I&#39;m not trying to outperform Greenwald&#39;s algorithm :)</p>
<p></p>
<p>Actually, I can&#39;t see where my Friend-Q is converging 5 times faster as in the paper. I have a picture of Friend-Q from the 2003 paper zoomed in below. It seems it achieves an error of 0.05 in more or less 50,000 iterations, which I think is more or less what my curve above is doing, isn&#39;t it? (Maybe I didn&#39;t understand your comment).</p>
<p></p>
<p>I think I see what I&#39;m doing wrong. I plotting all the errors for each iteration, but in reality I think we should plot only the errors for the iteration where the game is at state s (as defined in the paper) and the player A goes South and B Sticks (let&#39;s call this situation &#34;P&#34;). Otherwise, we would plot a lot of zeros (like my picture above), since iterations that are not related to the event P will not update the value Q(s, South, Stick) (therefore, the error will be zero at these iterations).</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk2yyb17w1wr6%2FCapture.PNG"" alt="""" /> </p>",2019-11-14T17:09:39Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2yyxs8yixhln,2019-11-14T17:09:39Z,{},project3
3493,no,"<p>&#34;<em>I have a picture of Friend-Q from the 2003 paper zoomed in below. It seems it achieves an error of 0.05 in more or less 50,000 iterations, which I think is more or less what my curve above is doing, isn&#39;t it?&#34;</em></p>
<p><em></em></p>
<p>Your curve reaches the same point in 10,000 iterations, which is 5x faster, right?</p>
<p></p>
<p>For your second point, agreed.  But you still need to plot something for the iterations in between, no?</p>",2019-11-14T17:35:00Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2yzudcmvamsw,2019-11-14T17:35:00Z,{},project3
3494,no,"<p>Vahe,</p>
<p></p>
<p>I&#39;ll zoom in a little bit more my chart at home and check when it reaches error 0.05. It may be 10,000 as you said. If so, I don&#39;t know why it converges faster, it may be the alpha value that I&#39;m starting with and the way I&#39;m decaying it through the iterations.</p>
<p></p>
<p><em>&#34;For your second point, agreed.  But you still need to plot something for the iterations in between, no?&#34; </em>If you send only x_axis=[iterations where P occurred] and y_axis=[errors where P occurred], it would plot it:</p>
<pre>plt.plot(x_axis, y_axis, color=&#39;blue&#39;)</pre>
<p>But I don&#39;t know if the author did something like that or I&#39;m completely missing the point.</p>",2019-11-14T18:32:33Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2z1wdqkwxz6nj,2019-11-14T18:32:33Z,{},project3
3495,no,"<p>If you send only the iterations where the appropriate event occurred, then the x-axis will have far fewer than the $$10^6$$ points that Greenwald has, right?</p>",2019-11-14T19:34:08Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2z43ldpubw5z5,2019-11-14T19:34:08Z,{},project3
3496,no,"<p>Yes, both x_axis and y_axis will have far fewer points than 10^6. Although the scale of the x_axis will be the same as Greenwald&#39;s (since your saving the iteration number in the x_axis list).</p>
<p>I think Greenwald also may have considered to include only the iterations of the appropriate event, otherwise he would have lots of zeros in all the curves shown in the paper.</p>
<p></p>
<p>By the way, are you doing something like this?</p>",2019-11-14T19:46:28Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2z4jfs38297fi,2019-11-14T19:46:28Z,{},project3
3497,no,"<p>I&#39;m trying the approach you suggested now, of only including iterations from the appropriate event, but leaving the scale to the full length of the run.  My results are still much noisier than Greenwald&#39;s.  Also I need to &#34;cheat&#39; on the learning rate to slow down convergence to her convergence rate.</p>",2019-11-14T21:41:37Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2z8nj9el51q,2019-11-14T21:41:37Z,{},project3
3498,no,<p>There are other discrepancies besides convergence time though.  Take a look at the peak error in your curve and her curve.  Also take a look at the initial few values of your error (I am seeing the same discrepancies as you).</p>,2019-11-14T23:51:52Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2zdb1h5w3735a,2019-11-14T23:51:52Z,{},project3
3499,no,"<p>Vahe,</p>
<p></p>
<p>The y-axis in her curve is limited to 0.5. How did you see the peak error in her curve?</p>
<p></p>",2019-11-15T01:50:39Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2zhjsomc381le,2019-11-15T01:50:39Z,{},project3
3500,no,"<p>No, you&#39;re right.  I should have been more clear.  Her error decreases monotonically while ours goes up and down a few times.  Basically, she doesn&#39;t seem to have any noise.</p>",2019-11-15T06:18:59Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2zr4uyxcn12yv,2019-11-15T06:18:59Z,{},project3
3501,no,"<p>Vahe,</p>
<p></p>
<p>Sorry, I disappeared from this thread for a while. I couldn&#39;t figure why my picture is different from the authors&#39; chart. </p>
<p>But I think it may be more important to note if it is converging to the same results (mine is behaving like that):</p>
<p>&#34;<strong>Friend-Q, however, converges to a deterministic policy<br />for player B at state s, namely E. Learning accord-<br />ing to friend-Q, player B (fallaciously) anticipates the<br />following sequence of events: player A sticks at state<br />s, and then player A takes action E. Thus, by taking<br />action E, player B passes the ball to player A, with<br />the intent that player A score for him. Player A is in-<br />different among her actions, since she assumes player<br />B plans to score a goal for her immediately.&#34;</strong></p>
<p></p>",2019-11-20T00:49:04Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k36kjuuzsjd4rz,2019-11-20T00:49:04Z,{},project3
3502,no,<p>Maybe Bonnie? I&#39;ve used it in other classes and it works pretty well.</p>,2019-10-31T14:08:53Z,55,Week 10/27 - 11/2,followup,,jl1b27fpaYkv,k2esbdzloso5nc,2019-10-31T14:08:53Z,{},hw5
3503,no,"<p>I have this silly code snipnet I have been using for copy the input:</p>
<pre>jQuery(&#39;pre&#39;).each( (idx, e) =&gt; console.log(e.textContent) );</pre>
<p>On Chrome, this will conveniently provide a &#34;Copy&#34; button if the input is too long:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl5wq8mca7o0%2Fk2ex5yc2iv96%2Fcopy.png"" alt="""" /></p>
<p>This of course doesn&#39;t make rldm better (and I would love to use Bonnie if possible), but hopefully this can serve as a workaround meanwhile.</p>",2019-10-31T16:26:04Z,55,Week 10/27 - 11/2,followup,,jl5wq8mca7o0,k2ex7t6e58c6tv,2019-10-31T16:26:04Z,{},hw5
3504,no,<p>slick. :) </p>,2019-11-01T21:47:56Z,55,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2go5kuggtb27r,2019-11-01T21:47:56Z,{},hw5
3505,no,"<p>I did inspect element, copy innerhtml and notepad to replace all the non-python stuff.  took more time to mangle the inputs than write the alg (my editor with a macro support is on the other comp). ;)</p>",2019-11-01T21:49:39Z,55,Week 10/27 - 11/2,feedback,,jzivtxcbl6964n,k2go7sdaxlc405,2019-11-01T21:49:39Z,{},hw5
3506,no,"<p>Quang,</p>
<p>the code snippet was good.</p>
<p>got the inputs in 15 mins. had to learn how to add the snippet into chrome <a href=""https://glebbahmutov.com/blog/chrome-dev-tools-code-snippets/"">https://glebbahmutov.com/blog/chrome-dev-tools-code-snippets/</a></p>
<p></p>
<p>thanks.</p>",2019-11-02T13:59:21Z,55,Week 10/27 - 11/2,feedback,,i4t1oo4aALG,k2hmutjm6l7530,2019-11-02T13:59:21Z,{},hw5
3507,stud,"<p>&#39;Silly&#39; or not, this has been really helpful. Thanks, &#64;Quang Vu. Appreciate it. </p>",2019-11-02T14:40:18Z,55,Week 10/27 - 11/2,feedback,a_0,,k2hobhfg4h33mt,2019-11-02T14:40:18Z,{},hw5
3508,no,"I was actually able to do ctrl-a, ctrl-c and grab all the page content without issue. It took a small amount of post processing to clean up the data. While Bonnie or something would obviously be an ideal result, I felt the way the page content came copied off the website wasn&#39;t too cumbersome.",2019-10-31T17:11:11Z,55,Week 10/27 - 11/2,followup,,jh701barLeiB,k2eyttwpta25n0,2019-10-31T17:11:11Z,{},hw5
3509,no,"<p>Actually it is enough to scroll to the left-bottom side of the cell, click once, move to the top-right corner of the cell and hold SHIFT, then click another time - cell content selected to be copied.</p>",2019-11-02T16:17:08Z,55,Week 10/27 - 11/2,followup,,jzhwdil7ssn443,k2hrs0s9khf40l,2019-11-02T16:17:08Z,{},hw5
3510,no,"<p>I&#39;m saying that roughly identical is said to be the worst case in lecture, but isn&#39;t that not true? Let me draw an example.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fi4jbttw9ru63ot%2Fk2g8r87jrqyt%2F2xEpsilon.png"" alt="""" /></p>",2019-11-01T14:34:16Z,55,Week 10/27 - 11/2,followup,,i4jbttw9ru63ot,k2g8nvyv33w4hr,2019-11-01T14:34:16Z,{},other
3511,no,Also I do a ton of work in this space. Check out any of my papers from my website with the keyword interactive. ,2019-10-31T15:55:02Z,55,Week 10/27 - 11/2,followup,,gph3si6rKEb,k2ew3w80kgy4na,2019-10-31T15:55:02Z,{},other
3512,no,How is imitation learning related to what we&#39;ve learned so far?,2019-11-03T04:34:32Z,54,Week 11/3 - 11/9,followup,,is8ald0uljj3u4,k2ii4bffouj402,2019-11-03T04:34:32Z,{},other
3513,no,"<p>Yeah!  That is amazing news.  It is Deep Mind...they&#39;ve done quite a few amazing things in the last several years (AlphaZero).</p>
<p></p>
<p>I was slightly disappointed that the SC II agent isn&#39;t completely tabula-rasa.  But I guess you need to do whatever you can when the problem is hard:</p>
<p></p>
<p>&#34;Even with a strong self-play system and a diverse league of main and exploiter agents, there would be almost no chance of a system developing successful strategies in such a complex environment without some prior knowledge. Learning human strategies, and ensuring that the agents keep exploring those strategies throughout self-play, was key to unlocking AlphaStar’s performance. To do this, we used imitation learning – combined with advanced neural network architectures and techniques used for language modelling – to create an initial policy which played the game better than 84% of active players. We also used a latent variable which conditions the policy and encodes the distribution of opening moves from human games, which helped to preserve high-level strategies.&#34;</p>",2019-10-31T16:54:21Z,35,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k2ey864iax92aw,2019-10-31T16:54:21Z,{},other
3514,no,"<p>Most pros have mentors and coaches. They also study FPVODs and televised matches. While AlphaStar got the silver spoon treatment in terms of great reflexes and lots of helpful coaching, it&#39;s not that far from what promising humans get.</p>",2019-10-31T19:11:04Z,35,Week 10/27 - 11/2,feedback,,ixr4jvzzg1zba,k2f3402x5pg3au,2019-10-31T19:11:04Z,{},other
3515,no,"<p>In other words, humans are awful at SCII without the same sort of help AlphaStar got.</p>",2019-10-31T19:12:36Z,35,Week 10/27 - 11/2,feedback,,ixr4jvzzg1zba,k2f35yrkezi695,2019-10-31T19:12:36Z,{},other
3516,no,"<p>True.  My &#34;disappointment&#34; wasn&#39;t so much that AlphaStar got some kind of head start or help per se; it&#39;s more that I&#39;m always suspicious of the result when we bootstrap on human strategies.  I love the idea of the computer program not being at all biased by humans.</p>
<p></p>
<p>A good example of this is how in chess, Stockfish (the computer chess champion before AlphaZero) highly valued material, much like humans who play chess.  When AlphaZero annihilated it and they analyzed the games, they saw that there were many positions where AlphaZero gave up pieces, and Stockfish&#39;s internal evaluation engine kept inching up the value it thought it had, each time AlphaZero gave up a piece, until at some point it realized it was in bad shape and the value collapsed.  AlphaZero had no preconceived bias about material.  It just learned to win, independent of any heuristics humans had come up with over the years.</p>",2019-10-31T21:41:26Z,35,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2f8hcxrldt2p2,2019-10-31T21:41:26Z,{},other
3517,no,"<p>Aarg, I don&#39;t think enough about exploiting that great resource.  </p>
<p></p>
<p>Thanks a lot.  </p>",2019-10-31T22:50:24Z,55,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2fay2btz1i68s,2019-10-31T22:50:24Z,{},hw5
3518,no,<p>I didn&#39;t know we have subscription to Nature. Thanks for letting us know!</p>,2019-11-04T20:39:24Z,54,Week 11/3 - 11/9,followup,,jl5wq8mca7o0,k2kw0zxpaab4do,2019-11-04T20:39:24Z,{},hw5
3519,stud,<p>: )</p>,2019-11-04T20:49:19Z,54,Week 11/3 - 11/9,feedback,a_0,,k2kwdqskelf5g9,2019-11-04T20:49:19Z,{},hw5
3520,no,"<p>If you need LP resources, the GA lectures on LP are here <a href=""https://www.cc.gatech.edu/~vigoda/GA/"">https://www.cc.gatech.edu/~vigoda/GA/#</a> (scroll to the bottom) and are a good start.</p>",2019-11-01T15:36:41Z,28,Week 10/27 - 11/2,followup,,jqmfo7d3watba,k2gaw50oqp03jp,2019-11-01T15:36:41Z,{},project3
3521,no,<p>didn&#39;t know it. Thanks for sharing!</p>,2019-11-02T16:35:43Z,55,Week 10/27 - 11/2,followup,,is5gzbotXmz,k2hsfx9mf9m47c,2019-11-02T16:35:43Z,{},hw5
3522,no,"<p>I have my system of inequalities and am getting the strategy answers. I&#39;m asking more about the format when submitting the answers to &#39;herokuapp&#39;. When I submit it returns: &#39;Answer is not valid json. Expected [0.1, 0.2, 0.3] etc.&#39;. So what format should the answers be in? </p>",2019-11-01T00:22:27Z,43,Week 10/27 - 11/2,followup,,jc6kep2qY4W3,k2fe8g2t54ul,2019-11-01T00:22:27Z,{},hw6
3523,no,"<p>the expected output are the probabilities for each action for the mixed strategy. Basically: </p>
<p>[prob_action_1, prob_action_2, prob_action_3] </p>",2019-11-01T04:43:14Z,43,Week 10/27 - 11/2,followup,,jvfpllmsggt7p4,k2fnjt9j9wk57q,2019-11-01T04:43:14Z,{},hw6
3524,no,<p>That&#39;s what I put when I got the error above.</p>,2019-11-01T04:52:16Z,43,Week 10/27 - 11/2,feedback,,jc6kep2qY4W3,k2fnvfl3h3y600,2019-11-01T04:52:16Z,{},hw6
3525,no,"<p><span style=""text-decoration:line-through"">weird, it works for me as a list. Maybe it&#39;s your answers?</span></p>
<p></p>
<p></p>
<p>edit: so i tested my submission. are you including the [ and ]? if not, you will get the error. Submit format is <strong>a list</strong></p>",2019-11-01T06:07:21Z,43,Week 10/27 - 11/2,feedback,,jvfpllmsggt7p4,k2fqjzeaxn55k8,2019-11-01T06:07:21Z,{},hw6
3526,no,"<p>So you only submitted [prob_action_1, prob_action_2, prob_action_3]? In that format? Because the homework document states: </p>
<div>
<div>
<div>
<div>
<p>&#39;You will submit the objective, constraints and strategy for your Linear Program. Much like the previous assignments answers will be graded to a 0.001 precision&#39;.</p>
</div>
</div>
</div>
</div>",2019-11-01T17:09:28Z,43,Week 10/27 - 11/2,feedback,,jc6kep2qY4W3,k2ge7gz18ms4qr,2019-11-01T17:09:28Z,{},hw6
3527,no,<p>So apparently you have to put the leading zero before the decimal in order for the site to take the answer. </p>,2019-11-01T17:34:44Z,43,Week 10/27 - 11/2,feedback,,jc6kep2qY4W3,k2gf3yeucixgm,2019-11-01T17:34:44Z,{},hw6
3528,no,"<p>So the correct format is [p1, p2, p3] including square brackets, commas and leading zeros?</p>",2019-11-06T13:58:55Z,42,Week 11/3 - 11/9,feedback,,is8ald0uljj3u4,k2ncloolpzz6vl,2019-11-06T13:58:55Z,{},hw6
3529,no,"<p>Thanks, that&#39;s really helpful. I was struggled for a brief moment trying to put all objective and constraint matrix in the online HW system.</p>",2019-11-07T10:12:16Z,42,Week 11/3 - 11/9,feedback,,jzjb6ojxtuc1d3,k2ojy1z8vru5xh,2019-11-07T10:12:16Z,{},hw6
3530,no,"<p>I solved it in a very unorthodox way.  and it worked.</p>
<p></p>
<p>it is a constraints relaxation problem.</p>
<p></p>
<p>freaky (it is a first for me) :)  normally it won&#39;t work.  but in this case... :)</p>
<p></p>
<p>I also tried a stochastic milp.  that did not work (although I expected it to).</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-11-06T15:47:03Z,42,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2nggqrrhyg50f,2019-11-06T15:47:03Z,{},hw6
3531,no,I&#39;d also add that generally you don&#39;t have to have &#34;the [whole] MDP&#34; for it to be considered offline learning. ,2019-11-01T14:34:25Z,23,Week 10/27 - 11/2,followup,,hyxsfbkeit22m2,k2g8o2nawgc29v,2019-11-01T14:34:25Z,{},other
3532,no,"<p>I found this article while looking through online-vs-offline material:</p>
<p></p>
<p><a href=""https://link.springer.com/article/10.1007/s10994-017-5650-8"">https://link.springer.com/article/10.1007/s10994-017-5650-8</a></p>
<p></p>
<p>off-line learning: you run an episode and then apply the result when it&#39;s done.</p>
<p></p>
<p>on-line learning: you apply the result of each step of an episode while running an episode.</p>",2019-12-06T00:37:03Z,18,Week 12/1 - 12/7,followup,,jc554vxmyuy3pt,k3tf60rnqhv48h,2019-12-06T00:37:03Z,{},other
3533,no,"The strategy is the answer, and what you submit to rdlm. You need to provide the objectives and constraints, figuring them out and plugging into a solver is the entire problem. ",2019-11-01T17:36:47Z,55,Week 10/27 - 11/2,followup,,jzhl7qlwrpagr,k2gf6lizx69y9,2019-11-01T17:36:47Z,{},hw6
3534,no,"Aim is to frame the payoff matrix as linear program: your objective c, constraints G and a as well as the rhs h and b. The ? are the equations which you must form to solve it. <div><br /></div><div>Your output will be player R’s optimal mixed strategy. The probability of each action, which is given for you to verify </div>",2019-11-02T02:50:37Z,55,Week 10/27 - 11/2,followup,,jvfpllmsggt7p4,k2gyytmzzrj515,2019-11-02T02:50:37Z,{},hw6
3535,no,"<p>I would guess you had either</p>
<p></p>
<p>1. 6 0s where where 6 -1&#39;s should have been</p>
<p></p>
<p>or</p>
<p></p>
<p>2. Your one -1 should have been a 0, and you had 5 0s where 5 -1&#39;s should have been.</p>",2019-11-01T20:09:43Z,55,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k2gkn9qnbpm2pb,2019-11-01T20:09:43Z,{},hw5
3536,no,"<p>hello.  hm5, you need to implement dont-cares.  as in, you either have a suspicion where the fighter, peacemaker are; OR you have to explicitly presume one or both are not present.  its basically a sat-solver with don&#39;t cares, with consensus amongst the hypothesis.</p>
<p></p>
<p></p>",2019-11-01T21:43:38Z,55,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2go01xtq0u52t,2019-11-01T21:43:38Z,{},hw5
3537,no,"<p>Thx a lot! I wish I had had that.  Btw, there&#39;s a small copy/paste error in your function which defines &#39;test_cases&#39; as an arg but uses &#39;test&#39; in the code.  </p>",2019-11-02T07:09:14Z,30,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2h87f24ru04kc,2019-11-02T07:09:14Z,{},hw5
3538,stud,<p>Thanks for pointing out. I think I just fixed it. </p>,2019-11-02T07:10:17Z,30,Week 10/27 - 11/2,feedback,a_0,,k2h88rx84zm5eq,2019-11-02T07:10:17Z,{},hw5
3539,no,"<p>A regex version</p>
<p></p>
<div>
<div>def parse(establishment, fight):</div>
<div>    regex = r&#39;\{[0|1,*]&#43;\}&#39;</div>
<div>    patrons = []</div>
<div>    result = []</div>
<br />
<div>    establishmentMatch = re.finditer(regex, establishment, re.MULTILINE)</div>
<br />
<div>    for matchNum, match in enumerate(establishmentMatch, start=1):</div>
<div>        s = match.group()</div>
<div>        s = s[1:-1]</div>
<div>        episode = [*map(int, s.split(&#39;,&#39;))]</div>
<div>        patrons.append(episode)</div>
<br />
<div>    s = fight[1:-1]</div>
<div>    result = [*map(int, s.split(&#39;,&#39;))]</div>
<br />
<div>    return patrons, result</div>
</div>",2019-11-02T20:05:34Z,30,Week 10/27 - 11/2,followup,,jl33gy0yBKzn,k2hzxsg9p6g5xe,2019-11-02T20:05:34Z,{},hw5
3540,no,"<p>Thank you for posting this! Very helpful. </p>
<div></div>",2019-11-03T01:23:22Z,29,Week 11/3 - 11/9,followup,,jzjaw2l4l6m5ns,k2ibah3g2vn64h,2019-11-03T01:23:22Z,{},hw5
3541,no,"<p>Thanks Nissim.  </p>
<p></p>
<p>I was looking into DeepMind&#39;s <a href=""https://deepmind.com/research/publications/psychlab-psychology-laboratory-deep-reinforcement-learning-agents"" target=""_blank"" rel=""noopener noreferrer"">Psychlab</a> which has interesting functions for deep RL agents, and it&#39;s all in Lua.  It&#39;s not only for gaming, but more generally &#34;visual search, change detection, random dot motion discrimination&#34;.</p>
<p>Can you see a reason why they didn&#39;t choose Python? </p>",2019-11-02T09:18:47Z,32,Week 10/27 - 11/2,followup,,jzh6k6o994a6dh,k2hcu0zzp6x6n7,2019-11-02T09:18:47Z,{},other
3542,no,Smaller runtime size with more features included. Faster and more memory efficient. And those game ai programmers already know it.,2019-11-02T10:02:53Z,32,Week 10/27 - 11/2,feedback,,j6nfy5083uw5nk,k2heepz6ozx2qi,2019-11-02T10:02:53Z,{},other
3543,no,I believe the only time I&#39;ve ever run across it in an ML context is in that original DQN code. If you already know Python (or any scripting language) you effectively already know Lua modulo some syntax.,2019-11-02T15:12:54Z,32,Week 10/27 - 11/2,feedback,,i4op5p9vfbq5yz,k2hpheqcusz2pc,2019-11-02T15:12:54Z,{},other
3544,no,"<p>I looked at Lua a while ago and IIRC it integrates well with C&#43;&#43;, so if you have a C&#43;&#43; core and want to give users ability to use this core in a scripting manner Lua would be one of the easiest and most efficient choices for that.</p>",2019-11-02T19:08:02Z,32,Week 10/27 - 11/2,feedback,,jqkxzdmmolGf,k2hxvspjgak553,2019-11-02T19:08:02Z,{},other
3545,no,"<p>Interesting answers.  Imho, it&#39;s worth investigating because Deepmind chose Lua for a good reason.  They&#39;re not using it only for the Atari DQN, and with the kind of apps they develop, AlphaGo, AlphaStar, we can think they&#39;ve tried everything that can squeeze very drop of performance.  </p>
<p></p>",2019-11-02T19:48:42Z,32,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2hzc3m35ms6n7,2019-11-02T19:48:42Z,{},other
3546,no,"<p>It just goes to show how unimportant programming language is in ML and RL, I guess since the programs are so small and they&#39;re not really large scale software-engineering efforts.  What seems to be important is computational speed, and interfacing with libraries (linear algebra and deep learning) to provide that computational speed.</p>",2019-11-02T19:50:27Z,32,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2hzecgd9vx7ks,2019-11-02T19:50:27Z,{},other
3547,no,"<p>&#64;vahe it&#39;s not a question of the size of the code but how well the compiler/language is able to optimize the use of the resources (CPU, GPU, threading/multiprocessing).  </p>
<p>For instance, even if Python has optmized C libraries, still, it&#39;s possible to improve the performance of a small program using big matrices or neural nets.</p>
<p>This <a href=""https://towardsdatascience.com/optimize-your-cpu-for-deep-learning-424a199d7a87"" target=""_blank"" rel=""noopener noreferrer"">link</a> for instance gives an example of such possible optimization.  </p>
<p>There&#39;s also <a href=""https://software.intel.com/en-us/get-started-with-daal-for-windows"" target=""_blank"" rel=""noopener noreferrer"">Intel Data analytics acceleration</a> library to improve Python&#39;s performance, which I have yet to try, but it&#39;s really a hassle to have to keep up with those things, which are necessary because the language is not optimized for ML.  </p>
<p></p>
<p>That&#39;s why I was assuming/hoping that maybe Lua was, and releases us from having to worry about these things and focus on the ML part.  </p>",2019-11-02T20:39:34Z,32,Week 10/27 - 11/2,feedback,,jzh6k6o994a6dh,k2i15ii4u9hvn,2019-11-02T20:39:34Z,{},other
3548,no,"<p>&#34;...<em>but how well the compiler/language is able to optimize the use of the resources (CPU, GPU, threading/multiprocessing).&#34;</em></p>
<p></p>
<p>Isn&#39;t this exactly the point I was making when I said &#34;What seems to be important is computational speed&#34;?</p>
<p></p>
<p>&#34;<em>For instance, even if Python has optmized C libraries, still, it&#39;s possible to improve the performance of a small program using big matrices or neural nets.&#34;</em></p>
<p><em></em></p>
<p>Isn&#39;t this exactly the point I was making when I said &#34; and interfacing with libraries (linear algebra and deep learning)&#34;</p>
<p></p>
<p>&#34;<em> it&#39;s not a question of the size of the code...&#34;</em></p>
<p><em></em></p>
<p>I disagree with this.  I think that features of the language like syntax, verbosity, static typing, maintainability, backward-compatibility, become much more important as programs become less like scripts and more like large-scale software engineering projects.  For small programs, those features are largely irrelevant, and the supporting libraries, etc. are more important.</p>",2019-11-02T22:20:21Z,32,Week 10/27 - 11/2,feedback,,jzfsa4a37jf4aq,k2i4r4c8sof691,2019-11-02T22:20:21Z,{},other
3549,no,"<p><em>&#34;I think that features of the language like syntax, verbosity, static typing, ....  For small programs, those features are largely irrelevant, and the supporting libraries, etc. are more important.&#34;</em></p>
<p>Of course it&#39;s about computational speed, but that has nothing to do with the size of the code (each core can execute only x instructions at a time anyway).  </p>
<p>You can have a &#39;very long program that won&#39;t consume much resources and 5 lines of code that multiply matrices with trillions of elements.  </p>
<p></p>
<p><em>&#34;Isn&#39;t this exactly the point I was making when I said &#34;What seems to be important is computational speed&#34;?&#34;</em></p>
<p>Yes it&#39;s about computational speed, but you were saying that the language had nothing to do with it, when clearly it does.  </p>
<p>For instance, even if you load Python with all the bells and whistles, super optimized libraries by a joint effort of Intel Goole Microsot and IBM, you may still suffer from Python&#39;s difficulty to handle threads or running parallel processes... </p>
<p></p>
<p>I was just intrigued by the fact that DeepMind uses Lua over python for such computationally-intense game applications.  </p>",2019-11-03T01:01:48Z,31,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2iaiqj3en219j,2019-11-03T01:01:48Z,{},other
3550,no,"<p>It could honestly just be preference of the developers.  Lua is also used in lots of gaming engines that might make integration easier.</p>
<p></p>
<p>One other cool thing about using Lua inside other programs like Python/C&#43;&#43; is that you can change the lua script during runtime and it will be automatically updated so you can make adjustments on the fly.</p>",2019-11-03T01:40:09Z,31,Week 11/3 - 11/9,feedback,,hz7meu55mi8sd,k2ibw27yy6g13j,2019-11-03T01:40:09Z,{},other
3551,no,"<p>&#34;<em>but that has nothing to do with the size of the code (each core can execute only x instructions at a time anyway). </em></p>
<p><em>You can have a &#39;very long program that won&#39;t consume much resources and 5 lines of code that multiply matrices with trillions of elements.  </em>&#34;</p>
<p></p>
<p>This has literally <strong>nothing</strong> to do with what I said.  I was referring to how the choice of a language can have an impact on maintaining a large <em>codebase.  </em>This has absolutely nothing to do with how code is executed on a processor.</p>
<p></p>
<p>&#34;<em>but you were saying that the language had nothing to do with it, when clearly it does.</em>&#34;</p>
<p></p>
<p>I was speculating that the choice of a language is largely irrelevant when you&#39;re writing scripts to do RL/ML.  The linking to fast libraries <em>is </em>relevant.</p>",2019-11-03T03:52:03Z,31,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2iglorp9mq6zj,2019-11-03T03:52:03Z,{},other
3552,no,"<p>&#64;JP: you are probably putting too much criteria under the umbrella of &#34;programming language&#34; with Python as an example. At the core, Python is implemented on C. Would you care whether the code is pure Python, or CPython, if either of them execute with same speed (*)? But I guess you would care they have different speed. Then here come the ability of interfacing with other libraries, possibly written in another language. Python itself, as a programming language, is slow, but its ability of incorporating C-module bring us back to the (*) question above.</p>
<p></p>
<p>Secondly, there are so much more dimensions of optimization than just executing speed. Some other dimensions are: faster prototyping, easier visualization, but the most important in my opinion would be the ecosystem. Not that it is within your (or our) ability to &#34;optimize&#34; the ecosystem, but imagine asking question about the X-programming language on StackOverflow and less than 100 people can answer us, it comes out as our time cost as well. There is limited time resource, and infinitive problems need to be solve. There will never be a language that will address all the problems, even if scoped to just ML problem. A language with strong ecosystem allows us to hold off on picking which particular problem we need to solve, until the point we actually need to solve it.</p>",2019-11-04T21:45:52Z,31,Week 11/3 - 11/9,feedback,,jl5wq8mca7o0,k2kyehbpoc23bi,2019-11-04T21:45:52Z,{},other
3553,no,"<p>Hi &#64;Quang  I totally agree with you.  I just got carried away discussing other languages when the post was about Lua.  I was really surprised to see Google people using Lua when Google invested tremendously in a python library (TF)... I am not saying Google should incorporate DQN in Tensorflow, but I thought maybe they had chosen python as their preferred language for ML... never mind.</p>
<p></p>
<p>One thing though, that I am discovering about Python only now, is the difficulty sometimes to even load a library (so many people had problems loading Box2D... ).  For instance, I tried to load Mujoco for hours, endless errors during pip, I even had to reload older versions of gcc, to no avail.  [I am a EE, and honestly, if hardware was working like software does, it would be hell on Earth :-) so CS at times leaves me pondering the wisdom of some choices.]  This is also why I was intrigued about Deepmind people using Lua.  </p>
<p></p>
<p>Now, <strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  gave us the beginning of an answer, ie the capability to change the code on the fly.  THAT is extremely precious imho when you run simulations that take days to complete... I am still running simulations of the Lunar lander, for the fun, tweaking parameters to, say, improve the overall score, reduce the number of crashes, speed up the convergence, and with Pycharm pro, which is already a good IDE, I can&#39;t even pause to change, say, epsilon... Being able to modify the code on the fly is really awesome.  </p>
<p></p>",2019-11-04T23:09:44Z,31,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2l1ec6x7zf39o,2019-11-04T23:09:44Z,{},other
3554,no,"<p>thanks <strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  that &#39;recompile on the fly&#39; is truly an awesome feature... I can only imagine the comfort it gives to DeepMind people when they run their heavy code and have the capability to tweak things, change a parameter etc on the fly.  Imho, that&#39;s the kind of decisive feature that can justify using sthg not mainstream.  I better understand now why game developers use Lua.  </p>",2019-11-04T23:14:01Z,31,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2l1judi8ge1xg,2019-11-04T23:14:01Z,{},other
3555,no,<p>nice touch ;)</p>,2019-11-02T22:32:07Z,55,Week 10/27 - 11/2,followup,,jzivtxcbl6964n,k2i5695cqy65yy,2019-11-02T22:32:07Z,{},hw5
3556,no,"<p><strong attention=""jl1acpoc4HA9"">&#64;Farrukh Rahman</strong>  I have just finished the lecture for this Sunday, ie the first Game theory video, and I was surprised to see that the advised papers do not relate to the course, so now I&#39;m wondering what to do to solve hw6. </p>
<p>Since next video is for next Sunday, do I need to watch it now to solve hw6 or do you advise me to do sthg else, say look into linear programming? Thanks</p>
<p></p>",2019-11-04T00:34:35Z,27,Week 11/3 - 11/9,followup,,jzh6k6o994a6dh,k2jozlr3bd32nx,2019-11-04T00:34:35Z,{},hw6
3557,no,"<p>HW6 is meant to be an introduction into a zero sum game without perfect information, how to write the constraints, nash equilibrium,  and solving for the equilibrium.  It is similar to the simple games covered in the lecture.  You might need to search for some supplemental resources to help you.</p>",2019-11-04T01:04:10Z,27,Week 11/3 - 11/9,feedback,,hz7meu55mi8sd,k2jq1mpszc73gy,2019-11-04T01:04:10Z,{},hw6
3558,no,"<p>Hi JP, please review the recommended readings for the assignment. Littman&#39;s paper defines how to solve hw6 rather straight forwardly for the example, and its easy to extend for the rest. Linear programming is required (or else you&#39;ll be manually doing it on paper with math..); but there was a recommendation to use cvxopt to solve the linear system of equations. Also, it was recommended in &#64;839 to review the GA&#39;s lecture on linear programming, which I found to be very useful in getting an overview of how linear programming works and to get some more examples to ensure my linear programming code worked correctly.</p>",2019-11-04T01:16:09Z,27,Week 11/3 - 11/9,feedback,,ixty1midfufhd,k2jqh25p3h96ae,2019-11-04T01:16:09Z,{},hw6
3559,no,<p>Thx both for your advices.  </p>,2019-11-04T01:56:54Z,27,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2jrxg5i2ja2z8,2019-11-04T01:56:54Z,{},hw6
3560,no,"<p>One thing to note is that just because $$|\hat{H}| &gt; 1$$ does not mean you should output &#34;I don&#39;t know.&#34;</p>
<p></p>
<p>For example, say you have $$5$$ hypotheses left, but all $$5$$ hypotheses involve either patron $$0$$ or patron $$1$$ being the instigator.  If you receive an input vector with both patron $$0$$ and patron $$1$$ <em>not</em> being present, then you <em>know</em> a fight could not have occurred.</p>",2019-11-02T23:31:27Z,55,Week 10/27 - 11/2,followup,,jzfsa4a37jf4aq,k2i7ak1wkvc3yr,2019-11-02T23:31:27Z,{},hw5
3561,stud,"<p>To add to what Vahe said above, I think you might be conflating two things from the KWIK paper. Remember that it&#39;s the cardinality of the set $$L$$ that matters here, where</p>
<p>$$L = \{h(x) | h \in \hat{H}\}$$. In other words, you&#39;re essentially <em>polling</em> all of the hypotheses in your hypothesis space. If all of them agree, (i.e., $$|L| = 1$$), then you know the answer (and return it). If at least two of them <em>disagree</em> (i.e., $$|L| &gt; 1$$), only <em>then</em> do you return $$\perp$$.</p>",2019-11-03T00:50:39Z,54,Week 11/3 - 11/9,feedback,a_1,,k2ia4ea6stx3rq,2019-11-03T00:50:39Z,{},hw5
3562,stud,"<p>Thanks a lot!!! I indeed messed up ||H|| and ||L||...</p>
<p></p>
<p></p>",2019-11-03T09:37:17Z,54,Week 11/3 - 11/9,feedback,a_0,,k2isxo5hry324f,2019-11-03T09:37:17Z,{},hw5
3563,no,"<p>Ah, sorry - that fixed it. Thanks, Vahe!</p>",2019-11-03T01:33:09Z,54,Week 11/3 - 11/9,followup,,idghbt86wqe,k2ibn1vbj8ph1,2019-11-03T01:33:09Z,{},hw5
3564,no,<p>how? both patrons could have been instigators and hence causing a fight.</p>,2019-11-03T04:27:09Z,21,Week 11/3 - 11/9,followup,,jqrr36mqfm8M,k2ihuu28mmc6lo,2019-11-03T04:27:09Z,{},hw5
3565,no,There is exactly one instigator and one peacemaker.,2019-11-03T04:32:32Z,21,Week 11/3 - 11/9,followup,,is8ald0uljj3u4,k2ii1rbg3zn1nw,2019-11-03T04:32:32Z,{},hw5
3566,no,"<p>Yes, only if you assume that there is at least one patron who is instigator and one patron is peacemaker, you can solve the example.</p>
<p></p>
<p>But I think its a bug in the example to assume that. </p>
<p></p>
<p></p>",2019-11-03T04:42:04Z,21,Week 11/3 - 11/9,feedback,,jqrr36mqfm8M,k2iie0cvick1kq,2019-11-03T04:42:04Z,{},hw5
3567,no,"<p>No its not a bug, there is exactly one instigator and one peacemaker as Alejandro mentioned. </p>",2019-11-03T04:54:48Z,21,Week 11/3 - 11/9,feedback,,jqknbi6c0lHt,k2iiudk3gwphg,2019-11-03T04:54:48Z,{},hw5
3568,no,"<p>ok so if patronSize = 4, then can we assume there are 2 instigators and 2 peacemakers?</p>
<p></p>
<p>what about when patronSize = 3?</p>",2019-11-03T04:56:58Z,21,Week 11/3 - 11/9,feedback,,jqrr36mqfm8M,k2iix62bl9i2dy,2019-11-03T04:56:58Z,{},hw5
3569,no,"<p>If patronSize=4, there is one instigator, one peacemaker and 2 patrons. </p>",2019-11-03T04:58:00Z,21,Week 11/3 - 11/9,feedback,,jqknbi6c0lHt,k2iiyi6vfht3dk,2019-11-03T04:58:00Z,{},hw5
3570,no,"<p>i.e. when atEstablishment[0] = {true, true, true, true} , it means there were two instigators and two peacemakers ?</p>
<p></p>
<p>what about when atEstablishment[0]  = {true, true, true}</p>",2019-11-03T04:59:29Z,21,Week 11/3 - 11/9,feedback,,jqrr36mqfm8M,k2ij0eoz6b94xg,2019-11-03T04:59:29Z,{},hw5
3571,no,"<p>For both cases, it means all patrons are present. The challenge is its not known before who is instigator and peacemaker.</p>",2019-11-03T05:01:06Z,21,Week 11/3 - 11/9,feedback,,jqknbi6c0lHt,k2ij2hfd88k7m8,2019-11-03T05:01:06Z,{},hw5
3572,no,"<p>Also, check &#64;857.</p>",2019-11-03T05:04:15Z,21,Week 11/3 - 11/9,feedback,,jqknbi6c0lHt,k2ij6j0hdo2om,2019-11-03T05:04:15Z,{},hw5
3573,no,"<p>ok so you are saying if all patrons showed up (all true scenario), fight can never happen. </p>
<p></p>
<p>Fine that&#39;s how I solved the example, but wanted to be sure. </p>
<p></p>
<p>The hw does not specify that subset includes atleast one of instigators and one of peacemakers in atEstablishment array</p>
<p></p>
<p>The confusion is only for the case when the size of patrons = 2 :) </p>
<p></p>
<p></p>",2019-11-03T05:08:12Z,21,Week 11/3 - 11/9,feedback,,jqrr36mqfm8M,k2ijbmhwa5m71m,2019-11-03T05:08:12Z,{},hw5
3574,no,"<p>Regardless of the number of patrons, we only ever have one peacemaker and one instigator (both oh whom might or might not be present in an episode)</p>",2019-11-03T14:20:08Z,21,Week 11/3 - 11/9,feedback,,j6pmq1sglzo35i,k2j31eomkhm5rc,2019-11-03T14:20:08Z,{},hw5
3575,no,"<p>yes thanks all. I had misread the problem, and I had to redo from scratch. But it all worked at the end.</p>",2019-11-04T15:44:11Z,21,Week 11/3 - 11/9,feedback,,jqrr36mqfm8M,k2klhcounkh5ht,2019-11-04T15:44:11Z,{},hw5
3576,no,<p>Thanks. I finally figured it out. I was applying supervised learning paradigm instead of RL.</p>,2019-11-03T14:59:52Z,54,Week 11/3 - 11/9,followup,,jl2egn5k4zo4lp,k2j4gif7cc53b6,2019-11-03T14:59:52Z,{},hw5
3577,no,Thanks for confirming that particular case! Any thoughts on the last scenario?? That’s the one I’m most interested in,2019-11-03T17:24:00Z,54,Week 11/3 - 11/9,followup,,jl2bqhkbsux4r,k2j9lv24ven5cl,2019-11-03T17:24:00Z,{},hw5
3578,no,"<p>Assuming that was the first data sample you received, then your conclusion is correct.  There&#39;s a reason why this needs to be automated and not done by hand - some data samples reveal such subtle information that, only in conjunction with many other pieces of data, can one draw any useful conclusions.</p>",2019-11-03T18:13:51Z,54,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2jbdz7uftq1o2,2019-11-03T18:13:51Z,{},hw5
3579,no,"<p>Yeah I agree Vahe.  The prediction of the last case depends on how much you have &#34;learned&#34; before you see that sample.</p>
<p></p>
<p>If it is the first sample, then you wouldn&#39;t have enough information to predict.  However if you have seen this sample before, or you have already identified the instigator or peacemaker, then you could make a prediction.</p>",2019-11-04T01:08:20Z,54,Week 11/3 - 11/9,feedback,,hz7meu55mi8sd,k2jq6zv01jg4sg,2019-11-04T01:08:20Z,{},hw5
3580,no,<p>Thanks Vahe and Timothy!</p>,2019-11-04T06:20:19Z,54,Week 11/3 - 11/9,feedback,,jl2bqhkbsux4r,k2k1c7z8dnd2ta,2019-11-04T06:20:19Z,{},hw5
3581,no,<p>I&#39;ve run into the same dilemma on just one of the problems.  All others came back with full credit.  For me it was the problem with 30 patrons.</p>,2019-11-04T00:06:22Z,54,Week 11/3 - 11/9,followup,,is9so9huTMp,k2jnzauzeox2bs,2019-11-04T00:06:22Z,{},hw5
3582,no,"<p>Me too.  After looking closely at the groups given, I realized there were groups where there is only one patron.</p>
<p>I initially thought &#39;no fight if only one patron&#39; but the TA&#39;s consider one patron can fight with the bar staff.  </p>",2019-11-04T18:16:39Z,54,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2kqxf3wxz87dy,2019-11-04T18:16:39Z,{},hw5
3583,no,"<p>Lol, totally reminds me of Fight Club.</p>",2019-11-04T19:27:29Z,54,Week 11/3 - 11/9,feedback,,jl5wq8mca7o0,k2ktgisj24b71a,2019-11-04T19:27:29Z,{},hw5
3584,no,"<p>There are two things that could be wrong. You&#39;ve predicted a fight or no fight incorrectly. This can easily be determined based on the fight occurred array, as we have to match it with 100% accuracy.</p>
<p></p>
<p>The second thing that could be wrong is that you predicted a I Don&#39;t Know, or didn&#39;t predict a I Don&#39;t Know correctly.</p>
<p></p>
<p>Since it&#39;s trivial to determine the first case (by comparing arrays), then I would look at the second case. Since all your other problem sets passed, then this is likely a missing edge case with your algorithm that is only occurring in this set. There are at least two edge cases that have been discussed in the other posts around all the peacemakers or all the fighters being present. If this is covered, then it&#39;s likely going to be a manual review of all the data.</p>
<p></p>
<p>I found that manually dumping it into excel with color coding and reviewing all the states and how they relate to each other helped provide intuition in the algorithm and showed me that I was thinking about one of the cases incorrectly. It also helped when I started hand eliminating the possibilities to see what the algorithm should and shouldn&#39;t know based on what it learns whenever it declares a I don&#39;t know.</p>",2019-11-04T00:50:46Z,54,Week 11/3 - 11/9,followup,,ixty1midfufhd,k2jpkef3ctt6i9,2019-11-04T00:50:46Z,{},hw5
3585,no,"<p>I did the Excel dump and conditional color coding.  I may have an extra IDK when there was a fight, but I&#39;m not sure which one.  It&#39;s a bit odd as I think I&#39;ve covered all the edge cases pretty well.  I&#39;ll look at it some more but I&#39;ve also got the issue with 9 out of 10 submissions for that single problem.  </p>",2019-11-04T01:16:30Z,54,Week 11/3 - 11/9,feedback,,is9so9huTMp,k2jqhhqtjkk1my,2019-11-04T01:16:30Z,{},hw5
3586,no,<p>I have the same issue. Does the case with no patrons in the bar need any special treatment? My interpretation is that the instigator is not present which means that there is no fight.</p>,2019-11-04T01:16:40Z,54,Week 11/3 - 11/9,followup,,jl2egn5k4zo4lp,k2jqhq5a2ny7ed,2019-11-04T01:16:40Z,{},hw5
3587,no,"<p>I&#39;m wondering if that is a case where IDK should be applied, because I do handle that case and return a &#34;no fight&#34;.  Seems like it should be part of the common sense rules though.</p>",2019-11-04T01:58:19Z,54,Week 11/3 - 11/9,feedback,,is9so9huTMp,k2jrz9pfo7j1yr,2019-11-04T01:58:19Z,{},hw5
3588,no,"<p>yeah, the common sense rule for the all-patrons and no-patrons can be useful if you are encoding the &#34;not-there&#34; enums. Your test for &#34;is there&#34; should rule out any of the ambiguous rules. When you have all patrons and one of your enums is (Peace Maker, Not There), then that enum doesn&#39;t make sense for the environment.</p>",2019-11-04T02:01:21Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2js366v2le5ec,2019-11-04T02:01:21Z,{},hw5
3589,no,<p>I had this problem as well.. For me it ended up being a problem with my implementation. There were certain cases where I thought you could predict a fight or no fight but wasn’t thinking about it right. I found it helpful to go line by line as most of my issues were in the first few scenarios.</p><br /><p></p><br /><p>Good luck!</p><p></p>,2019-11-04T06:19:45Z,54,Week 11/3 - 11/9,followup,,jl2bqhkbsux4r,k2k1bh2y1vr1gm,2019-11-04T06:19:45Z,{},hw5
3590,no,"Generally there are two ways a linear programming problem is unfeasible: either it is unbounded or it is not possible due to conflicting constraints. Your task here is to come up with some inequalites that are feasible and solve the problem. If you are asking if an equlibrium does exisit for this particular problem, Greenwald/Hall seems to think there is one.",2019-11-04T00:57:53Z,54,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2jptjvo8fq543,2019-11-04T00:57:53Z,{},project3
3591,no,<p>Thanks. It turns out that it is a bug that leads to infeasible solution.</p>,2019-11-04T19:42:07Z,54,Week 11/3 - 11/9,feedback,,jzn63vvy8x67lj,k2ktzboc4jn6bo,2019-11-04T19:42:07Z,{},project3
3592,no,It is not always possible to convert a finite horizon MDP to an infinite horizon MDP.,2019-11-04T19:23:14Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2ktb1v9v4o200,2019-11-04T19:23:14Z,{},final_exam
3593,stud,<p>False. You can always convert a terminal state into an absorbing state with a transition to itself and reward 0</p>,2019-11-04T19:38:45Z,20,Week 11/3 - 11/9,feedback,a_0,,k2ktv0529ku15f,2019-11-04T19:38:45Z,{},final_exam
3594,stud,"<p>True -  A finite horizon MDP is bounded by finite time where as an infinite horizon MDP is not. Gamma can simulate finite time in and infinite horizon, however it does not account fully for an MDP that terminates at a specific point in time.</p>",2019-11-04T21:47:43Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kyguow2qx22b,2019-11-04T21:47:43Z,{},final_exam
3595,stud,<p>False. &#43;1 first response.</p>,2019-11-04T22:32:11Z,20,Week 11/3 - 11/9,feedback,a_3,,k2l021k8zql5ti,2019-11-04T22:32:11Z,{},final_exam
3596,no,"<p>True, If we&#39;re talking about a problem with the same mechanics, as introducing a discount factor can change the mechanics for a problem that is instead modeled with infinite sums. This is an ambiguously worded question, as if we can just convert it by introducing a discount factor, but there’s nuance here. What are you looking for?</p>",2019-11-06T13:54:32Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ncg1cjded4kq,2019-11-06T13:54:32Z,{},final_exam
3597,no,<p>False. &#43;1 to the first response. We can always have rewards = 0 after the terminal states making the sum infinite.</p>,2019-11-06T15:28:44Z,20,Week 11/3 - 11/9,feedback,,jzih0fdt4sn1cq,k2nft6thcvb1tw,2019-11-06T15:28:44Z,{},final_exam
3598,no,<p>First post nails it! False.</p>,2019-11-14T21:16:06Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z7qpy1cek7o1,2019-11-14T21:16:06Z,{},final_exam
3599,stud,"<p>Follow up question:</p>
<p></p>
<p>is it always possible to convert infinite horizon MDP to a finite horizon MDP?</p>
<p></p>
<p>-- True? Because infinite horizon MDP converges at finite time increments. </p>",2019-11-18T20:03:48Z,18,Week 11/17 - 11/23,feedback,a_5,,k34ux50dxlm1dz,2019-11-18T20:03:48Z,{},final_exam
3600,no,<p>Follow up question: when would you ever want to convert a finite horizon MDP to an infinite horizon MDP in practice?</p>,2019-11-30T18:43:08Z,17,Week 11/24 - 11/30,feedback,,isde332xcka1m0,k3lxbmkgfm75em,2019-11-30T18:43:08Z,{},final_exam
3601,no,"<p>Michael,</p>
<p></p>
<p>I understand the conversion from a finite duration MDP (either episodic or finite-horizon) to an equivalent infinite duration MDP to be purely for theoretical reasons.  Sutton and Barto talk about the primary reason for doing this being to unify notation (Section 3.4).</p>",2019-11-30T19:26:54Z,17,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3lyvx8g2ti3ey,2019-11-30T19:26:54Z,{},final_exam
3602,no,"<p>Makes sense. Thanks, Vahe.</p>",2019-11-30T19:36:33Z,17,Week 11/24 - 11/30,feedback,,isde332xcka1m0,k3lz8bcqseg6kn,2019-11-30T19:36:33Z,{},final_exam
3603,no,<p>also possible to convert any mdp into a pomdp. ;)</p>,2019-12-07T20:39:23Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w1k2vhtm010x,2019-12-07T20:39:23Z,{},final_exam
3604,stud,"<p>as Lesson 2.13 mentioned, without the the infinite horizon assumptions, we&#39;ll lose the stationarity property of Markovian. If agent only has finite life, it&#39;s not possible to covert to infinite horizon MDP.</p>",2019-12-07T23:42:46Z,16,Week 12/1 - 12/7,feedback,a_7,,k3w83x987wq3aq,2019-12-07T23:42:46Z,{},final_exam
3605,no,"<p>Yes, there is a difference between a &#34;finite horizon task&#34; and an &#34;episodic task.&#34;  In the former the environment appears non-stationary to the agent, as you said.</p>
<p>In the latter, the environment can still be stationary.</p>
<p></p>
<p>It may have been better to phrase the question as:</p>
<p></p>
<p>&#34;It is not always possible to convert the MDP for an episodic task to an MDP for a continuing task.&#34;</p>",2019-12-08T00:43:31Z,16,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3waa1x414459q,2019-12-08T00:43:31Z,{},final_exam
3606,no,"In RL, recent moves influence outcomes more than moves further in the past.",2019-11-04T19:28:44Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kti410p8h678,2019-11-04T19:28:44Z,{},final_exam
3607,stud,<p>False. Recent moves influence outcomes more than moves further in the future. </p>,2019-11-04T19:39:41Z,20,Week 11/3 - 11/9,feedback,a_0,,k2ktw76ujst2h9,2019-11-04T19:39:41Z,{},final_exam
3608,stud,"<p>^ Don&#39;t think this addresses the q, as it refers to the past. I want to say true because at *best* past and present are treated equally. Worst case, past moves are discounted by some gamma and therefore do not influence as much. I do not know of any configuration in which past moves are more important than present.</p>",2019-11-04T22:57:55Z,20,Week 11/3 - 11/9,feedback,a_3,,k2l0z4w28v9e3,2019-11-04T22:57:55Z,{},final_exam
3609,no,"<p>I argue that this is TRUE. The formula $$Q(s,a) = R(s,a) &#43; \gamma\Sigma_{s&#39;}{T(s,a,s&#39;)\max_{a&#39;}(Q(s&#39;,a&#39;))}$$ means that my decision today takes on the value of the reward plus the discounted value of all of my prior decisions. The discounting schedule is geometric, so decisions made farther into the past have less value than those made closer to the present. Yesterday&#39;s Q contributes $$\gamma^2$$ to today, and Monday&#39;s Q contributes $$\gamma^5$$, so to speak. Even in the case of a lucky Monday, the $$T(s,a,s&#39;)$$ marginalizes past experiences to further silence their influence from the deep past.</p>
<p></p>
<p></p>",2019-11-04T23:16:24Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2l1mwujdry6h0,2019-11-04T23:16:24Z,{},final_exam
3610,no,"<p>I&#39;ll go with True.</p>
<p></p>
<p>In the special case of TD methods, the &#39;backward view&#39; (which is equivalent to the forward view given by the Bellman equation), holds that reinforcement learning is weighted more by recent TD errors than by older ones.</p>",2019-11-05T03:48:51Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lbd9rvrj527n,2019-11-05T03:48:51Z,{},final_exam
3611,no,"<p>false:  there is an undiscounted case, where all are equally treated.</p>",2019-11-05T05:21:20Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2leo79kzpl6po,2019-11-05T05:21:20Z,{},final_exam
3612,no,"<p>False. We can have problems with gamma=1, which means all past rewards are treated with equal weighting. Even with TD we can have lambda=1. </p><p></p>",2019-11-06T13:57:52Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2nckc67u6s4te,2019-11-06T13:57:52Z,{},final_exam
3613,no,"<p>I&#39;ll go with False as you can lose a game (like the chess game that was mentioned by prof. Isbell in one of the earliest videos) at the beginning and no matter how perfectly you play it afterwards, you might still lose it.</p>",2019-11-06T15:51:54Z,20,Week 11/3 - 11/9,feedback,,jzih0fdt4sn1cq,k2ngmzfxla92nf,2019-11-06T15:51:54Z,{},final_exam
3614,stud,"<p>True. It is not clear to me what &#39;outcomes&#39; of RL means. If outcomes, is the outcomes of actions, I would argue that the reward and new state from taking an action is unrelated to past moves. If we are thinking of the outcomes as the final policy or values, then, in considering Lunar Lander, while earlier moves allowed the learning of better states and actions, the final policy was shaped more from later moves.</p>",2019-11-10T21:10:54Z,19,Week 11/10 - 11/16,feedback,a_4,,k2thsmee23t5ug,2019-11-10T21:10:54Z,{},final_exam
3615,no,"<p>Props to Aida for the best answer... Depending on the game/environment, you might lose on the first move. False.</p>",2019-11-14T21:19:24Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z7uyhwh434zh,2019-11-14T21:19:24Z,{},final_exam
3616,stud,"I would say False? It is true most of the time, but not all the time. For example, when gamma=1, actions in the past affect outcomes (I’m taking this to mean rewards and future actions) just as much as recent ones.",2019-11-18T23:44:28Z,18,Week 11/17 - 11/23,feedback,a_6,,k352sx1wqoz6ar,2019-11-18T23:44:28Z,{},final_exam
3617,no,"<p>&#34; you can lose a game (like the chess game that was mentioned by prof. Isbell in one of the earliest videos) at the beginning and no matter how perfectly you play it afterwards, you might still lose it.&#34; </p>
<p>Can <strong attention=""hyxsfbkeit22m2"">&#64;Alec Feuerstein</strong>  explain a bit more? What theory are we talking about? Thanks. </p>",2019-12-03T06:44:44Z,16,Week 12/1 - 12/7,feedback,,j6ln9puq99s5uv,k3phzbkfdjf54b,2019-12-03T06:44:44Z,{},final_exam
3618,no,"<p>Could one argue the Falsehood of the statement by saying: &#34; learning rate, alpha determines if past moves are more important or not, the closer the learning rate is to 0, past moves become more important&#34;</p>",2019-12-04T20:24:23Z,16,Week 12/1 - 12/7,feedback,,j6ll2xkiDJf,k3rqp8ir53762k,2019-12-04T20:24:23Z,{},final_exam
3619,no,<p>subtle distinction:  &#34;influence outcomes&#34;  not &#34;influence agent decisions&#34;.  hence Aida being the best answer.</p>,2019-12-07T20:42:34Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w1o6fx2991qr,2019-12-07T20:42:34Z,{},final_exam
3620,no,An MDP given a fixed policy is a Markov chain with rewards.,2019-11-04T19:31:10Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2ktl987um73b9,2019-11-04T19:31:10Z,{},final_exam
3621,stud,"<p>True. Since the policy is externally given and fixed, at each state it tells us what action(s) to take and this combined with $p(r,s&#39;|s,a)$ gives the transition probability to the next state as well as the reward. </p>",2019-11-04T20:06:31Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kuuphzien30g,2019-11-04T20:06:31Z,{},final_exam
3622,no,<p>Sometimes called a &#39;Markov Reward Process.&#39;</p>,2019-11-05T03:49:36Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lbe8u0apj3fh,2019-11-05T03:49:36Z,{},final_exam
3623,no,"<p>False. A fixed policy is not necessary.</p>
<p></p>
<p>Questions on the term &#34;policy&#34;:  policy here I presume, refers to the map of optimal actions over states</p>
<p>- what would you refer to the policy &#43; epsilon movement? i.e. epsilon greedy?  this is not, strictly a policy I think, since it incorporates stochasticity.</p>
<p>- would the term policy, necessarily imply argmax ?  i.e. would a policy, include value maps where argmax is implicitly implied?</p>",2019-11-05T05:27:30Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lew4zywi83fa,2019-11-05T05:27:30Z,{},final_exam
3624,no,"<p>Policies can be stochastic.  $$\epsilon$$-greedy policies are bona fide policies.</p>
<p></p>
<p>But stochastic or not, if the policy is fixed, then the agent does not have any decisions to make, in any states.  The stochasticity of the policy can be composed with the transition function of the environment.</p>",2019-11-05T06:59:45Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2li6ru5zz96b6,2019-11-05T06:59:45Z,{},final_exam
3625,no,"<p>hello vahe.  this is sort of my question; in the literature it is used somewhat casually; but policy in the mathematical sense, is somewhat more specific.  I am asking, if the term &#34;policy&#34; includes stochastic definitions, that preclude the &#34;argmax&#34; implicit to the general theory; as opposed to defining policy, by its convergent properties (epsilon greedy, being &#34;necessary&#34; to visit the states infinitely often).  its a subtle difference.</p>",2019-11-05T07:53:41Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lk44x4pvr47r,2019-11-05T07:53:41Z,{},final_exam
3626,no,<p>I see the policy as completely independent of any desire for convergence properties.  It&#39;s just a function from states to (a distribution over) actions.</p>,2019-11-05T13:54:14Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lwzswibb1pw,2019-11-05T13:54:14Z,{},final_exam
3627,no,<p>yes I think I was confusing the phrase &#34;policy&#34; with its most often used context &#34;optimal policy&#34;. ;)</p>,2019-11-06T10:13:47Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2n4k5z1r4f2yv,2019-11-06T10:13:47Z,{},final_exam
3628,no,"<p>True. It reduces to a Markov chain, and we can write one out where each state is a node, and the edges are transitions as a result of the fixed action taken, with their value being the corresponding reward. </p>",2019-11-06T14:04:34Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ncsxu2jam47n,2019-11-06T14:04:34Z,{},final_exam
3629,no,<p>you make a convincing point. :)</p>,2019-11-06T15:54:40Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2ngqjchg6b1zr,2019-11-06T15:54:40Z,{},final_exam
3630,no,<p>True since fixed policy means the agent doesn&#39;t have an option to choose an action in each state. The agent transitions from state to state according to this fixed policy (without choosing any actions) which is Markov chain.</p>,2019-11-06T16:02:38Z,20,Week 11/3 - 11/9,feedback,,jzih0fdt4sn1cq,k2nh0rw07ks2tc,2019-11-06T16:02:38Z,{},final_exam
3631,no,<p>True... As noted by your classmates.</p>,2019-11-14T21:20:52Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z7wu3qkus40p,2019-11-14T21:20:52Z,{},final_exam
3632,no,<p>Something about this questions phrasing is disturbing. <br /><br />&#34;An MDP ..is.. a fixed (universal) policy with a Markov chain with rewards.&#34;</p>,2019-12-07T20:47:14Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w1u6r1cwwsz,2019-12-07T20:47:14Z,{},final_exam
3633,no,"<p>If we know the optimal Q values, we can get the optimal V values only if we know the environment’s transition function/matrix.</p>",2019-11-04T19:34:29Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2ktpicdxlm5vd,2019-11-04T19:34:29Z,{},final_exam
3634,stud,"<p>False. $$v^*(s) = \max_a Q^*(s,a)=max_a E[R_{t&#43;1} &#43; \gamma \max_{a&#39;} q^*(S_{t&#43;1},a&#39;)|S_t=s, A_t=a]$$</p>",2019-11-04T19:46:26Z,20,Week 11/3 - 11/9,feedback,a_0,,k2ku4vy7i53io,2019-11-04T19:46:26Z,{},final_exam
3635,no,<p>Yeah. False. There is no transition model in Q learning. That only appears in E-SARSA and variants of Q where eligibility traces are applied.</p>,2019-11-04T21:45:24Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2kydvc36oo4b9,2019-11-04T21:45:24Z,{},final_exam
3636,no,"<p>&#34;If we know the optimal Q values, we can get the optimal V values <strong>with no further information</strong>.&#34;</p>",2019-11-05T03:54:40Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lbkre8jh01e7,2019-11-05T03:54:40Z,{},final_exam
3637,no,"<p>True.  Q(s,a) =&gt; V(s) sum for all a * p(s,a,s&#39;).</p>",2019-11-05T05:29:47Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lez2y75pl32d,2019-11-05T05:29:47Z,{},final_exam
3638,no,<p>False. The optimal V values are the max(a) of the Q values for each state</p>,2019-11-06T14:05:48Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ncuir45pm5fg,2019-11-06T14:05:48Z,{},final_exam
3639,no,"<p>False, we can always get optimal V from optimal Q. We don&#39;t need any more info. </p>",2019-11-06T16:05:20Z,20,Week 11/3 - 11/9,feedback,,jzih0fdt4sn1cq,k2nh4939w5t5ef,2019-11-06T16:05:20Z,{},final_exam
3640,no,"<p>This one as noted requires careful reading... False, you don&#39;t need the transition function.</p>",2019-11-14T21:26:11Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z83oosinpa9,2019-11-14T21:26:11Z,{},final_exam
3641,no,<p>Is this still False if using Correlated Equilibrium or for some other strategy that is non-deterministic?</p>,2019-11-25T14:55:09Z,17,Week 11/24 - 11/30,feedback,,jr46k9bbb1g5ju,k3ejz6cvknu4z,2019-11-25T14:55:09Z,{},final_exam
3642,no,"<p>Davis,</p>
<p></p>
<p>If we&#39;re talking about correlated equlibria we&#39;re not in the realm of MDPs any more.  But even in the generalized multi-agent Markov Game model, I don&#39;t think the transition function of the environment has anything to do the relationship between the value of a state (V) and the value of a state-(action vector) pair (Q).  Also, I suspect that the Q values of the different actions in a mixed strategy should be close (or the same), otherwise an agent would prefer the pure strategy containing the action with the higher Q-value.  Therefore V would be equal to the Q of any of the component strategies of a mixed strategy.</p>",2019-11-25T17:11:06Z,17,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3eou0l24w21hz,2019-11-25T17:11:06Z,{},final_exam
3643,no,"<p>In the gridworld MDP in “Smoov and Curly’s Bogus Journey”, if we add 10 to each state’s reward (terminal and non-terminal) the optimal policy will not change.</p>
<p></p>",2019-11-04T19:36:05Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2ktrl22poifd,2019-11-04T19:36:05Z,{},final_exam
3644,stud,<p>True? This sounds like reward shaping with a potential based shaping function. It is a provable fact about such approaches to reward shaping that they leave the optimal policy unchanged. That&#39;s why we like them. </p>,2019-11-04T20:48:01Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kwc2omwcv2o1,2019-11-04T20:48:01Z,{},final_exam
3645,stud,"<p>False, Given that the R(s) = -.04 for all states except for terminating the states which are 1,-1 respectively; An increase of 10 to all states would incentivize the learner to traverse the non-terminating states collecting .96 reward forever, instead of moving towards the positive terminating state.</p>",2019-11-04T20:50:19Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kwf16aleq116,2019-11-04T20:50:19Z,{},final_exam
3646,stud,<p>Good point. It looks like we&#39;d need to take away the 10 if the agent moved in &#34;the other&#34; direction for this to actually be potential based. And it doesn&#39;t look like we&#39;re doing that. </p>,2019-11-04T20:53:04Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kwil2pp1v6mi,2019-11-04T20:53:04Z,{},final_exam
3647,no,"<p>This is true. <a href=""https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/43887099460923"">https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/43887099460923</a> Adding any scalar to every reward is no different than adding zero. The min(a&#43;100,b&#43;100) is still the same as min(a,b) and similarly with max().</p>",2019-11-04T20:55:31Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2kwlq5f32avj,2019-11-04T20:55:31Z,{},final_exam
3648,stud,<p>Even with discounting? </p>,2019-11-04T20:56:31Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kwn0fv2p01ju,2019-11-04T20:56:31Z,{},final_exam
3649,stud,"<p>Actually, I guess I should have said, &#34;even without discounting?&#34;.</p>",2019-11-04T21:05:59Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kwz6zbxg055k,2019-11-04T21:05:59Z,{},final_exam
3650,no,"<p>Without discounting it holds true because $$\gamma=1$$ is the &#34;without discounting&#34; approach. Thanks to nifty algebra from Michael Littman, the gamma is irrelevant to the invariant scalar addition property of the Q(s,a) function:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2l05o9od3fs%2Faddingscalarlecture.png"" alt="""" /></p>
<p></p>
<p></p>
<p></p>",2019-11-04T22:35:54Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2l06tojq0p6xn,2019-11-04T22:35:54Z,{},final_exam
3651,stud,"<p>We&#39;re dividing by $$1 - \gamma$$ in the above. If $$\gamma = 1$$, that&#39;s $$0$$. So we&#39;d be dividing by $$0$$.</p>
<p></p>
<p>That proof (as written above) won&#39;t work for $$\gamma = 1$$. </p>",2019-11-04T22:46:24Z,20,Week 11/3 - 11/9,feedback,a_1,,k2l0kbpc29q1h6,2019-11-04T22:46:24Z,{},final_exam
3652,no,"<p>The $$(1-\gamma)$$ factors out and you are left with c = c, which means the discount factor is irrelevant in this case.</p>",2019-11-04T23:06:22Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2l19zxkve73c3,2019-11-04T23:06:22Z,{},final_exam
3653,stud,<p>$$\frac{0}{0}$$ factors out? Really? Is this some sort of stange <em>new math</em>? </p>,2019-11-05T01:38:35Z,20,Week 11/3 - 11/9,feedback,a_1,,k2l6pra1ami3lo,2019-11-05T01:38:35Z,{},final_exam
3654,stud,"<p>Btw, Jacob, the $$\frac{1}{1-\gamma}$$ term above comes from assuming that $$\sum_{i=0}^{\infty} \gamma^{i}$$ is a <em>converging geometric series</em>. That&#39;s only true provided that $$|\gamma| &lt; 1$$. </p>",2019-11-05T01:46:19Z,20,Week 11/3 - 11/9,feedback,a_1,,k2l6zp7ha7q4t3,2019-11-05T01:46:19Z,{},final_exam
3655,stud,"<p>The proof from lecture implicity assumes that $$\gamma &lt; 1$$. It doesn&#39;t go through as written if $$\gamma \geq 1$$. I believe the second &#34;anonymous&#34; above is correct. Without discounting, every time we hop from one non-terminal square in the grid world to another, we pick up an additional $$&#43;9.6$$ of reward. There is thus <em>no</em> incentive to reach the terminal state, since if we do, we get $$&#43;11$$ reward and <em>the game ends</em>. If, on the other hand, we keep hopping around from non-terminal state to non-terminal state (avoiding the &#34;bad&#34; state) then we pick up $$&#43;9.6$$ reward over and over again <em>ad infinitum</em>. If the goal is maximize reward, that&#39;s how you&#39;d do it in this instance. </p>",2019-11-05T01:53:32Z,20,Week 11/3 - 11/9,feedback,a_1,,k2l78z05nfl516,2019-11-05T01:53:32Z,{},final_exam
3656,stud,"<p>Wow. This has turned into an interesting discussion. After reading through the above posts, I think I am going to take some inspiration from the KWIK algorithm and say, &#34;I don&#39;t know,&#34; unless I am given more information. Specifically, I think the answer will be true if we model the terminal states as absorbing states with 0-reward transitions to themselves, but the answer is false if we model the terminal states as terminal states.</p>
<p></p>
<p>To see this, let&#39;s look at the following simple example:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhs489b10i78c%2Fk2l7hd77xz1y%2Fp1.PNG"" alt="""" /></p>
<p></p>
<p>From state A, we have two actions, walk or stay to choose from. Once we are in state B, there is only one action to choose from, which is to move.  When we are at state C or D, we terminate. $$\gamma=\frac{1}{2}$$, Let&#39;s calculate the Q values by hand:</p>
<p>$$

Q(A, \text{walk}) =  \frac{1}{2} 4 = 2 \\

Q(A, \text{stay}) =  4


$$</p>
<p>and so the optimal policy is $$\pi(A)=\text{stay}$$</p>
<p></p>
<p>Now add 8 to each reward:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhs489b10i78c%2Fk2l7mlqrn82x%2Fp2.PNG"" alt="""" /></p>
<p></p>
<p>$$

Q(A, \text{walk}) =  8&#43; \frac{1}{2} 12 = 14 \\

Q(A, \text{stay}) =  12


$$</p>
<p>and so $$\pi(A)=\text{walk}$$</p>
<p></p>
<p>If, however, we model the terminal states as absorbing states, then the math of the slide Jacob cited kicks in and the optimal policy won&#39;t change. </p>
<p></p>",2019-11-05T02:09:33Z,20,Week 11/3 - 11/9,feedback,a_0,,k2l7tkq6nwd5wn,2019-11-05T02:09:33Z,{},final_exam
3657,no,<p>You&#39;re all right. But when $$\gamma &gt;= 1$$ the problem becomes mostly uninteresting because the discounts disappear and everything goes to infinity.</p>,2019-11-05T03:44:01Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2lb71xgf9a18l,2019-11-05T03:44:01Z,{},final_exam
3658,no,"<p>So it looks like adding a constant only leaves the optimal policy unchanged <em>if there are no terminal states</em>.  Otherwise, you can&#39;t assume that the sum of future rewards forms an infinite series.</p>",2019-11-05T04:17:50Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lcejfg6ww4t6,2019-11-05T04:17:50Z,{},final_exam
3659,no,<p>true: adding constants (lifting) doesn&#39;t affect the optimal policy</p>,2019-11-05T05:30:41Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lf085vjuq40f,2019-11-05T05:30:41Z,{},final_exam
3660,no,"<p>This should be theoretically true, and is almost a trick question; but is false here, because by doing so, the reward for moving from state to state is no longer negative (it was -0.04 before), so the agent would never choose to leave the world and instead get infinite reward. </p>",2019-11-06T14:09:11Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ncyvwsflj1sc,2019-11-06T14:09:11Z,{},final_exam
3661,no,"<p>If that is truly the case, then you can&#39;t initialize your Q to anything but zero. Anything else would intrinsically bias the reward function.</p>",2019-11-07T01:07:46Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2o0htpycnl559,2019-11-07T01:07:46Z,{},final_exam
3662,stud,"<p>I am not even sure that we need $$\gamma=1$$. Consider the final grid space before the termination &#43;1 space. Call that space state $$s$$. Let&#39;s ignore randomness for simplicity. In the original problem, $$Q(s, right) = 1$$ and $$Q(s, up) = -.04 &#43; \gamma max{Q(s, a)}$$. In the new problem, $$Q(s, right) = 10 &#43; 1$$ and $$Q(s, up) = 10 &#43; (-.04) &#43; max{Q(s, a)}$$.</p>
<p></p>
<p>Let&#39;s simplify the original problem. The optimal policy was go to the terminal &#43;1 state, so $$max{Q(s,a)} = Q(s, right)$$. So, $$Q(s, up) = -.04 &#43; \gamma (1) = \gamma - .04$$. Looking at the values of $$Q(s, right)$$ and $$Q(s, up)$$, we see that $$Q(s, right) &gt; Q(s, up)$$ since $$\gamma - .04 &lt; 1$$. There is no contradiction here with the optimal policy.</p>
<p></p>
<p>Let&#39;s assume that the optimal policy would remain the same in the new problem. So, $$Q(s, up) = 10 &#43; (-.04) &#43; max{Q(s, a)} = 10 &#43; (-.04) &#43; Q(s, right) = 10 &#43; (-.04) &#43; \gamma(10 &#43; 1) = 10(\gamma &#43; 1) &#43; (\gamma - .04)$$. The problem is that it is possible that $$Q(s, up) &gt; Q(s, right)$$ since for $$\gamma &gt; \frac{1.04}{11}$$, $$10(\gamma &#43; 1) &#43; (\gamma - .04) &gt;  10 &#43; 1$$. This would create a contradiction with $$Q(s, right)$$ being the optimal policy.</p>
<p></p>
<p>It seems that adding 10 to each state, since the number of states is not constant, can create an incentive to keep playing the game without ending it. </p>",2019-11-10T22:32:00Z,19,Week 11/10 - 11/16,feedback,a_4,,k2tkox5t1xi5y8,2019-11-10T22:32:00Z,{},final_exam
3663,stud,"<p>As I showed in &#64;881, when one adds 10 to the non-terminal states, one needs to add 20 to the terminal states, to keep the optimal policy the same. </p>",2019-11-13T05:50:31Z,19,Week 11/10 - 11/16,feedback,a_0,,k2wv8jzzf01rs,2019-11-13T05:50:31Z,{},final_exam
3664,no,"<p>I think Vahe has the right idea here.... True, assuming an infinite horizon the optimal policy will be unchanged .</p>",2019-11-14T21:29:06Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z87fpovmp6k9,2019-11-14T21:29:06Z,{},final_exam
3665,stud,"<p>Struggling to understand this.</p>
<p></p>
<p>Original Question: &#34;In the gridworld MDP in “Smoov and Curly’s Bogus Journey”, if we add 10 to each state’s reward (terminal and non-terminal) the optimal policy will not change.&#34;</p>
<p></p>
<p>Vahe&#39;s Answer: &#34;So it looks like adding a constant only leaves the optimal policy unchanged <em>if there are no terminal states</em>.  Otherwise, you can&#39;t assume that the sum of future rewards forms an infinite series.&#34;</p>
<p></p>
<p>Final Answer: &#34;I think Vahe has the right idea here.... True, assuming an infinite horizon the optimal policy will be unchanged .&#34;</p>
<p></p>
<p>Vahe&#39;s answer and the final answer seem at odds with each other since there were terminal states if I remember it correctly. Am I misunderstanding this?</p>",2019-11-14T23:42:09Z,19,Week 11/10 - 11/16,feedback,a_4,,k2zcyj36735ys,2019-11-14T23:42:09Z,{},final_exam
3666,no,<p>yes hwats the answer?</p>,2019-11-24T22:50:38Z,17,Week 11/24 - 11/30,feedback,,jzj7y1ofgsro1,k3dlisva33i15x,2019-11-24T22:50:38Z,{},final_exam
3667,no,"<p>What&#39;s the final answer for this?</p>
<p></p>",2019-12-03T01:32:43Z,16,Week 12/1 - 12/7,feedback,,is4nx55dinr6wk,k3p6u2c3g65456,2019-12-03T01:32:43Z,{},final_exam
3668,no,"<p>Jacob Anderson</p>
<div>
<div><a href=""/class/jzh9tkzzxkd7ph?cid=871"">1 month ago</a>
<p>If that is truly the case, then you can&#39;t initialize your Q to anything but zero. Anything else would intrinsically bias the reward function.</p>
</div>
</div>
<p></p>
<p>--</p>
<p><br />Professor Isbell draws such a conclusion; the paper with respect to max.a Q*(s,a) as the potential shaping.</p>",2019-12-07T20:53:00Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w21lcz4i317s,2019-12-07T20:53:00Z,{},final_exam
3669,no,"<p>Final answer is true:<br /><br /><strong>In the gridworld MDP in “Smoov and Curly’s Bogus Journey”,</strong> if we add 10 to each state’s reward (terminal and non-terminal) the optimal policy will not change.</p>
<p></p>
<p>this is a discounted (has to be discounted); infinite MDP (has to be infinite).  <br /><br />... where the result of the shaping is to approximate the constant using c/(1 - gamma) ... in the limit.</p>",2019-12-07T20:54:23Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w23dzfucx2vq,2019-12-07T20:54:23Z,{},final_exam
3670,no,<p>The song &#34;Moon River&#34; was popularized by Chevy Chase.</p>,2019-11-04T19:45:57Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2ku49di92u5rx,2019-11-04T19:45:57Z,{},final_exam
3671,stud,<p>DOUBLE DOOMED!</p>,2019-11-04T20:34:13Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvuc98vwo7nz,2019-11-04T20:34:13Z,{},final_exam
3672,stud,<p>42</p>,2019-11-04T21:48:32Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kyhwlyi2f523,2019-11-04T21:48:32Z,{},final_exam
3673,no,<p>false?</p>,2019-11-24T22:50:29Z,17,Week 11/24 - 11/30,feedback,,jzj7y1ofgsro1,k3dlim9gykry9,2019-11-24T22:50:29Z,{},final_exam
3674,no,"<p>Questions only:</p>
<p></p>
<ol><li>
<p>It is not always possible to convert a finite horizon MDP to an infinite horizon MDP.</p>
</li><li>
<p>In RL, recent moves influence outcomes more than moves further in the past.</p>
</li><li>
<p>An MDP given a fixed policy is a Markov chain with rewards.</p>
</li><li>
<p>If we know the optimal Q values, we can get the optimal V values only if we know the environment’s transition function/matrix.</p>
</li><li>
<p>In the gridworld MDP in “Smoov and Curly’s Bogus Journey”, if we add 10 to each state’s reward (terminal and non-terminal) the optimal policy will not change.</p>
</li><li>The song &#34;Moon River&#34; was popularized by Chevy Chase.</li></ol>
<p></p>
<p>Extracted with <tt>$(&#39;.main_followup .actual_text&#39;).map((idx, elem) =&gt; elem.textContent).get().join(&#39;\n- &#39;)</tt>. Adapted from Quang&#39;s code &#64;832.</p>",2019-11-05T02:48:16Z,20,Week 11/3 - 11/9,followup,,is8ald0uljj3u4,k2l97d7jja520c,2019-11-05T02:48:16Z,{},final_exam
3675,no,"<p><strong attention=""hyxsfbkeit22m2"">&#64;Alec Feuerstein</strong>  <strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  Hi professor <strong attention=""gph3si6rKEb"">&#64;Charles Isbell</strong>  </p>
<p></p>
<p>I&#39;d like to know to what depth we have to go to study the readings? </p>
<p>Miguel said &#39;everything in the videos and papers is fair game&#39;, so, for instance, do we have to learn the 9 algorithms in Li-Littman-Walsh?</p>
<p>If not, how do we know what is considered material for the final? </p>
<p></p>
<p>When I see some of the questions, some necessitate a perfect understanding of the matter referred to (fair enough), but are we expected to have such level for the papers because it&#39;s common they reference other papers to justify what they say or do? </p>
<p>If not, same question as above.  </p>
<p>Thanks</p>
<div>
<div></div>
</div>",2019-11-05T19:25:50Z,20,Week 11/3 - 11/9,followup,,jzh6k6o994a6dh,k2m8u8h6ridqi,2019-11-05T19:25:50Z,{},final_exam
3676,no,"<p>We will be practicing for the final in 2 study sessions (Sat 16) to give you an idea about the questions you can expect.</p>
<p></p>
<p>Everything is fair game, how deep do you want to study is a better question, or how much time you have?</p>
<p></p>
<p>I think after the study session you should have a better idea.</p>",2019-11-05T23:25:23Z,20,Week 11/3 - 11/9,feedback,,hyx9thiqa6j4nn,k2mheb0p57c3dn,2019-11-05T23:25:23Z,{},final_exam
3677,no,<p>Hi! Was the study session mentioned by Miguel recorded? Would really help if I can get hold of it.</p>,2019-11-26T16:14:38Z,17,Week 11/24 - 11/30,feedback,,jzliwdkt88m1q3,k3g298ss7d66ev,2019-11-26T16:14:38Z,{},final_exam
3678,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  Was this the Nov 16th study session? Also wondering if the study session was recorded and added to the &#64;132 post. The Nov 16th one seemed completely around Proj 3. Or did the review posts fill in instead?</p>
<p>Thanks,</p>
<p>George</p>",2019-11-27T21:57:15Z,17,Week 11/24 - 11/30,feedback,,ixty1midfufhd,k3htxp89l4o6tk,2019-11-27T21:57:15Z,{},final_exam
3679,stud,<p>Is there a specific time when the correct answers will be posted or presented by the instructors?</p>,2019-11-10T23:05:53Z,19,Week 11/10 - 11/16,followup,a_4,,k2tlwhhkq836eu,2019-11-10T23:05:53Z,{},final_exam
3680,no,<p>Yeah... I am running a bit behind -- sorry! Catching up on this now.</p>,2019-11-14T21:22:06Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z7yfv2cea5nb,2019-11-14T21:22:06Z,{},final_exam
3681,stud,<p>Thanks!!!</p>,2019-11-14T23:42:26Z,19,Week 11/10 - 11/16,feedback,a_4,,k2zcywk8l8m6m9,2019-11-14T23:42:26Z,{},final_exam
3682,no,"<p>change, noted.</p>
<p></p>",2019-11-05T05:19:29Z,54,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2leltllfdm4dp,2019-11-05T05:19:29Z,{},logistics
3683,no,"<p>In lesson 10 (POMDP), video 7 on state estimates. How did the b(s) in the denominator come about?</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fijctp4ucNy8%2Fk2p04iv9fd99%2Fstate_estimation_screenshot.JPG"" alt="""" /></p>",2019-11-07T17:45:23Z,54,Week 11/3 - 11/9,followup,,ijctp4ucNy8,k2p04rnj1ps2zf,2019-11-07T17:45:23Z,{},logistics
3684,no,"<p>First of all, there&#39;s an error in that slide.  See the footnote below it:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2p2fjmue755%2FCapture.PNG"" alt="""" /></p>
<p></p>
<p>We can expand the denominator in that final expression as follows:</p>
<p></p>
<p>$$P(z|b,a) = \sum_{s&#39;}P(z,s&#39;|b,a) =  \sum_{s&#39;}P(z|s&#39;,b,a)\cdot P(s&#39;|b,a)$$</p>
<p></p>
<p>$$= \sum_{s&#39;}P(z|s&#39;)\cdot P(s&#39;|b,a) = \sum_{s&#39;}O(s&#39;,z)\cdot P(s&#39;|b,a)$$</p>
<p></p>
<p>$$=\sum_{s&#39;}O(s&#39;,z)\cdot \sum_s P(s&#39;,s|b,a) = \sum_{s&#39;}O(s&#39;,z)\cdot \sum_s P(s&#39;|s,b,a)P(s|b,a)$$</p>
<p></p>
<p>$$= \sum_{s&#39;}O(s&#39;,z)\cdot \sum_s T(s,a,s&#39;) b(s) = \sum_s \sum_{s&#39;} b(s)\cdot O(s&#39;,z)\cdot T(s,a,s&#39;)$$</p>",2019-11-07T19:05:14Z,54,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2p2zgrqu5c2te,2019-11-07T19:05:14Z,{},logistics
3685,no,<p>Thanks! I totally missed the footnote. </p>,2019-11-08T03:32:32Z,54,Week 11/3 - 11/9,feedback,,ijctp4ucNy8,k2pl3us0xzr7ow,2019-11-08T03:32:32Z,{},logistics
3686,no,"<p>Is it a good idea (or even possible) to solve hw5, with matrix operations? If so, can you discuss the part of calculating the output, and the updating?</p>
<p>I experiment with setting:</p>
<table border=""1"" cellspacing=""0"" cellpadding=""6px""><tbody><tr><td></td><td>P1</td><td>P2</td><td>P3</td></tr><tr><td>is_fighter</td><td>1</td><td>1</td><td>1</td></tr><tr><td>is_peace_maker</td><td>1</td><td>1</td><td>1</td></tr></tbody></table>
<p></p>
<p>And setting:</p>
<table border=""1"" cellspacing=""0"" cellpadding=""6px""><tbody><tr><td></td><td>P1</td><td>P2</td></tr><tr><td>P1</td><td>Invalid</td><td>Valid</td></tr><tr><td>P2</td><td>Valid</td><td>Invalid</td></tr></tbody></table>
<p>(where row is fighter, and column is peacemaker)</p>
<p></p>
<p>And both type of matrices do not seem feasible to calculate the output Fight/Nofight. I end up just use if/else update, but keep wondering if there is better way of forming matrix so that we can make better calculation with large data.</p>
<p></p>",2019-11-08T00:30:07Z,54,Week 11/3 - 11/9,followup,,jl5wq8mca7o0,k2pel94ich02dg,2019-11-08T00:30:07Z,{},logistics
3687,no,"<p>Hi Quang,</p>
<p></p>
<p>It seems to me that it&#39;s relatively easy to vectorize the inner loop of the algorithm.  That is, for each hypothesis matrix (each row is a hypothesis and the columns are patrons, or vice-versa), we can do matrix operations to compute our outputs (Fight/NoFight).</p>
<p></p>
<p>But every time step that we learn, we get a new hypothesis matrix.  So if I were to vectorize, I would have one for loop, over time steps, but inside of that for loop could be all matrix operations, and no if-else statements.</p>",2019-11-08T01:08:22Z,54,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2pfygicej35lh,2019-11-08T01:08:22Z,{},logistics
3688,no,"<p>Vahe, thanks for the response!</p>
<blockquote>
<p>That is, for each hypothesis matrix (each row is a hypothesis and the columns are patrons, or vice-versa), we can do matrix operations to compute our outputs (Fight/NoFight)</p>
</blockquote>
<p>Are you referring to my first setting, where the hypothesis is whether participant(x) could be a fighter, or participant(x) could be a peacer_maker? Then with the observation, I can do matrix operation to show if fighter or peace maker present in the group. But still, I would need to check the rule:</p>
<table border=""1"" cellspacing=""0"" cellpadding=""6px""><tbody><tr><td>Has p, no f</td><td>No fight</td></tr><tr><td>Has p, has f</td><td>No fight</td></tr><tr><td>No p, no f</td><td>No fight</td></tr><tr><td>No p, has f</td><td>Fight</td></tr></tbody></table>
<p>Maybe there is a math trick to encode that rule as matrix operation, but that is not immediate obvious to me. Can you elaborate, if this is what you mean?</p>
<p></p>",2019-11-08T01:47:32Z,54,Week 11/3 - 11/9,feedback,,jl5wq8mca7o0,k2phctgtljy1m7,2019-11-08T01:47:32Z,{},logistics
3689,no,"<p>Here&#39;s one example:</p>
<p></p>
<p>Let&#39;s say you have a matrix that is all 0s in each row, except for one entry in each row that is a 1 where there is a peacemaker for that hypothesis.  This is your peacemaker hypothesis matrix.</p>
<p></p>
<p>Then you have another matrix that is all 0s in each row, exept for one entry in each row that is a 1 where there is an instigator.  This is your instigator hypothesis matrix.</p>
<p></p>
<p>If you multiply the vector atEstablishment by each of those matrices, you will only get a 1 in the product vector only in the entries where the peacemaker (or instigator) is present.</p>
<p></p>
<p>If the product vector for the peacemaker is all zeros, but the product vector for the instigator isn&#39;t, then you know there&#39;s a fight, and vice-versa.  If both product vectors are non-zero, then it&#39;s an &#39;I don&#39;t know&#39;.</p>
<p></p>
<p>Then you have to multiply the vector fightOccurred with those matrices to construct your new hypothesis matrix.</p>",2019-11-08T02:00:18Z,54,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2pht85hgzd92,2019-11-08T02:00:18Z,{},logistics
3690,no,<p>https://docs.google.com/presentation/d/10YvPoUgkr3K0NSIIvE0UeT7p86359PNeulvvsjQeVjg/edit?usp=sharing</p>,2019-11-09T19:07:49Z,54,Week 11/3 - 11/9,followup,,i4i9bi8rFqk,k2rxyhdcoo23ng,2019-11-09T19:07:49Z,{},logistics
3691,no,"<p>Hey <strong attention=""i4i9bi8rFqk"">&#64;Don Jacob</strong>  in the study session/office hours we are encouraged to explain/show the policy that was learned for project 3. Can someone give an example of how to explain the policy that was learned? Would an example be if the agent always hits its head against a wall but gets all the points, note this is a random example, we just state: &#34;The agent learned a policy of banging its head against the wall which was good for x,y,z reasons.&#34;?</p>",2019-11-15T20:22:50Z,53,Week 11/10 - 11/16,followup,,gx3c8l7z7r72zl,k30la271x924yy,2019-11-15T20:22:50Z,{},logistics
3692,no,What I meant there was if player A is using a friend-Q strategy how do you verify that player A has learned the friend-Q policy ? The exact way how you show this is left upto you ( if at all you choose to show this in the report ). It looks like this is what you are alluding to as well.,2019-11-15T20:33:15Z,53,Week 11/10 - 11/16,feedback,,i4i9bi8rFqk,k30lngmjcdjy2,2019-11-15T20:33:15Z,{},logistics
3693,no,<p>Markov means RL agents are amnesiacs and forget everything up until the current state.</p>,2019-11-04T19:55:11Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kug4ky2ii6jb,2019-11-04T19:55:11Z,{},final_exam
3694,stud,<p>True. Markovian property means that we don&#39;t have to condition on anything past the current state. </p>,2019-11-04T20:18:45Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvafzkwalna,2019-11-04T20:18:45Z,{},final_exam
3695,stud,"<p>False. While Markovian does mean that only the present matters, the agent will use previous interactions with a state to update V(s) over time therefore the past is not forgotten but wrapped into V(s) over many interactions.</p>",2019-11-04T20:31:32Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kvqvltlml18z,2019-11-04T20:31:32Z,{},final_exam
3696,stud,<p>False. Justification as given above.</p>,2019-11-04T20:36:47Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kvxml5okf85,2019-11-04T20:36:47Z,{},final_exam
3697,no,<p>Markov means <b>environments</b> are amnesiacs and forget everything up until the current state.</p>,2019-11-05T04:19:54Z,20,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2lch7lzk7p4c,2019-11-05T04:19:54Z,{},final_exam
3698,no,"<p>true.  past does not effect future-1, only present.</p>
<p>false.  ... but past states can be rolled up (as can future states) into present, future-1</p>
<p></p>
<p>this question should use &#34;markov property&#34;, instead of markov.</p>
<p></p>
<p>since it directly mentions RL (which uses rollups):  false.</p>",2019-11-05T05:35:55Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lf6yw2wm06o,2019-11-05T05:35:55Z,{},final_exam
3699,no,False. Markov has nothing to do with the agent and only describes the environment. ,2019-11-05T06:22:09Z,20,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2lgufayhh44nh,2019-11-05T06:22:09Z,{},final_exam
3700,no,"<p>As long as we mean that the Markov property is the property that the current state encapsulates all relevant information of the environment for the problem, this would be true. But the agent can remember things, so ultimately</p><p>I’d say this is false.</p><p><br /></p><p>This is a strangely worded question, as “Markov” is ambiguous. Does it mean “Markov property”? Is the actual answer “Markov is actually Andrew Markov, the name of the mathematician who came up with this idea”? It feels as if you’re actively trying to trip us up. If we’re going to harp on parsing specific words and phrasing as the differentiator between getting a question wrong or right, then let that be a requirement both ways: it’s difficult for me to choose true/false for an ambiguously worded question. </p><p><br /></p><p>Unless you’re telling me you care less about True vs False and more about my explanations. Then I can give you every interpretation of “Markov” I know and say for each one, assuming you meant that, this is the True/False answer I would choose. </p><p></p><p></p><p></p>",2019-11-06T14:15:05Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2nd6guc76kap,2019-11-06T14:15:05Z,{},final_exam
3701,no,"<p>So, David brings up an interesting point about the final... We do care about True/False, but, we care about your answer too. If you parse the question wrong, get confused by double negatives, or whatever and still have the &#34;correct&#34; brief explanation you get some points. If you have no explanation or just nonsense you get no points even if your guessed True/False correctly.</p>",2019-11-14T21:35:41Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8fwim97g2yv,2019-11-14T21:35:41Z,{},final_exam
3702,no,"<p>I&#39;d go with True here: the current state is all the agent should know.</p>
<p></p>
<p>Now if you want to discuss what a &#34;current state&#34; is... Well that can get more complicated.</p>",2019-11-14T21:37:28Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8i72t8305zq,2019-11-14T21:37:28Z,{},final_exam
3703,no,Surely this does not means that the agents needs to forget everything does it? How would it learn otherwise ? Is the official answer to this question True?<div><br /></div>,2019-11-26T10:49:05Z,17,Week 11/24 - 11/30,feedback,,jqkuetouttn5,k3fqmlat7mg2fc,2019-11-26T10:49:05Z,{},final_exam
3704,no,"<p>I&#39;m not sure I buy that answer. RL agents &#34;forget everything up until the current state&#34;, is factually incorrect for the agent, as pointed out before, their value or q estimates at any current state contain information from before. If the question said, RL agents forget the sequence of events that led them to their current state, then sure but they don&#39;t forget everything, or else they would start with no estimates of the values of any states besides their starting values. </p>",2019-11-27T09:48:15Z,17,Week 11/24 - 11/30,feedback,,jzozvpx25to679,k3h3w7a9r0o29v,2019-11-27T09:48:15Z,{},final_exam
3705,no,"<p>Right, the agent itself cannot be amnesiac or it won&#39;t ever learn anything. It is the (stochastic) process that is memoryless.</p>",2019-12-06T04:12:32Z,16,Week 12/1 - 12/7,feedback,,is8ald0uljj3u4,k3tmv5a0tt75v5,2019-12-06T04:12:32Z,{},final_exam
3706,no,"<p>I see this as true, because we can only care about the current state. However, I use the justification that was given in the lectures about how historians can agree that only the present matters as long as the present state also includes information about all the past events. What this means, is of course that states never are repeated, and there are infinite of them, and its very hard to learn, but that is besides the point. the agent&#39;s utility could be thought of with a similar justification. That the utility just encompasses all the previous state information as currents state.</p>",2019-12-06T23:53:16Z,16,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3ut1kadp3m5b8,2019-12-06T23:53:16Z,{},final_exam
3707,no,"<p>George,</p>
<p></p>
<p>I think the key point of confusion here is that the states are a property of the environment, not the agent.  What makes an MDP Markov is that the $$\textit{environment}$$ needs only its current state to decide where to move to next, given an input action.</p>
<p></p>
<p>$$P(s_{t&#43;1}|s_t, s_{t-1}, ..., s_1) = P(s_{t&#43;1}|s_t)$$</p>
<p></p>
<p>Markov has nothing to do with the agent.  The agent can keep all sorts of old information from old states if it wants to, and still traverse an MDP.</p>
<p></p>",2019-12-07T01:06:09Z,16,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3uvnas7nxs6h,2019-12-07T01:06:09Z,{},final_exam
3708,no,"<p>Thanks for the response Vahe. I can see it as both ways now, if I consider a q learner that has to maintain information about all the states it&#39;s been to, (if not just all states in the environment), then I can see it as false.</p>",2019-12-07T02:01:05Z,16,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3uxlyfwp5g4ar,2019-12-07T02:01:05Z,{},final_exam
3709,no,"<p>I disagree; false is the only correct answer here:  an MDP presumes an agent traversing a markov chain with rewards.  and since the agent is an RL agent (absent some specifically exempted &#34;memoryless policies&#34; -- essentially, state independent universal plan) this means it makes those choices are made with respect to the time structuring/forward lookup of rewards.  <br /><br />This is not an amnesiac; u cannot estimate the future, without the past; nor can u simply forget the past.  Absent a pure random agent (which I do not think, would qualify as an RL agent; within the context of randomness in pursuit of learning...)...   the time structuring, is in fact, one of the key features of an RL agent.  In fact, even the choice in initialization, de-facto is a statement about the presumed optimal policy (a &#34;pseudo-history&#34;).<br /><br />This question should be rewritten to be more precise.</p>",2019-12-07T21:05:08Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w2h7pgovw1q0,2019-12-07T21:05:08Z,{},final_exam
3710,no,"<p>It&#39;s funny.  I was 100% sure this was False, all my posts in this thread were arguing False, and I just now noticed Alec&#39;s answer of True. So apologies if I confused anyone (e.g. George) with my posts, especially after Alec&#39;s answer.</p>
<p></p>
<p>I agree with Uno that I don&#39;t like the wording here. Markov is a property of state transitions. States are an attribute of the environment, not the agent. Replace the word &#39;agent&#39; with &#39;environment&#39; and I&#39;m on board.</p>",2019-12-07T22:18:06Z,16,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3w531kr9z62xq,2019-12-07T22:18:06Z,{},final_exam
3711,no,"<p>A policy that is greedy--with respect to the optimal value function--is not necessarily an optimal policy.</p>
<p></p>",2019-11-04T19:55:42Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kugsjfcih73u,2019-11-04T19:55:42Z,{},final_exam
3712,stud,"<p>False. It follows from the Bellman&#39;s equation that a policy that allows the maximum to be achieved is an optimal policy, a policy that allows the optimal value function to be achieved. </p>",2019-11-04T20:25:20Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kviwtzkz97mm,2019-11-04T20:25:20Z,{},final_exam
3713,stud,<p>False. A policy &#34;which is greedy with respect to the optimal value function&#34; is <em>by defintion</em> optimal. </p>,2019-11-04T20:38:21Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kvznmf8ag30h,2019-11-04T20:38:21Z,{},final_exam
3714,no,"<p>Isn&#39;t this a time-bounded question? While after 500 updates the policy may be locally optimal, it may not be *THE* optimal policy. At some time in the far future, when there is only The Immortal Charles Isbell to verify it, the optimal policy will have been observed, and it was greedy.</p>",2019-11-04T21:39:40Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2ky6i1li3l2fw,2019-11-04T21:39:40Z,{},final_exam
3715,stud,"<p>Not sure what you&#39;re getting at. In particular, the assumption that there is anything that answers to &#34;*THE* optimal policy&#34; is problematic.  It suggests there must be a single, unique optimal policy. But that isn&#39;t in general the case. There is often </p>
<p>more than one optimal policy (and we&#39;ve in fact seen examples of this). </p>
<p></p>
<p>If you have the optimal value function, and you have a policy which is greedy with respect to it, that policy is an optimal policy fot that MDP. Time has nothing to do with it. If you&#39;re assuming that you have only an approximation to the actual optimal value function, then it <em>might</em> be the case that a policy which is greedy with respect to it is only approximately optimal. But that isn&#39;t what the question assumes. It assumes you have the optimal value function.  </p>",2019-11-04T21:46:10Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kyevbflu56si,2019-11-04T21:46:10Z,{},final_exam
3716,no,"<p><em>with respect to the optimal value function</em></p>
<p></p>
<p>Any policy derived from this converged, unchanging, optimal value function, will be optimal by definition. Whether it be min(), max(), bobs_favorite_function(), anything - they will all be optimal by definition because they are derived from the optimal value function. </p>
<p></p>
<p>How you got to the optimal value function is the time component that I was referring to. Your &#34;with respect to&#34; seems to mean &#34;given&#34; whereas I read it as &#34;with respect to [computing].&#34; Then how do you know you have *THE* optimal value function? The convergence you found may be just a local optima and you would need to compute out to an infinite horizon to verify that you really did find *THE* optimal value function.<br /></p>
<p> </p>",2019-11-04T22:20:14Z,20,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2kzmnxh52p58n,2019-11-04T22:20:14Z,{},final_exam
3717,stud,"<p>Then maybe the objection here is that the question is ambiguously worded. I do read the question as presupposing that the optimal value function is in fact understood to be <em>given</em> to us. I have a hard time wringing the other interpretation out of what Alex wrote, but it could just be me. </p>",2019-11-04T22:43:09Z,20,Week 11/3 - 11/9,feedback,a_2,,k2l0g4uip2f33z,2019-11-04T22:43:09Z,{},final_exam
3718,stud,"<p>I get what Jacob is saying, but I think False is what Alec is fishing for. Just by simply stating that multiple policies could be optimal and one of them might not be greedy.</p>",2019-11-04T22:45:02Z,20,Week 11/3 - 11/9,feedback,a_3,,k2l0iko3i7l6m,2019-11-04T22:45:02Z,{},final_exam
3719,stud,"<p><em>&#34;I get what Jacob is saying, but I think False is what Alec is fishing for. Just by simply stating that multiple policies could be optimal and one of them might not be greedy.&#34;</em></p>
<p><em></em></p>
<p>That doesn&#39;t make sense. &#34;That multiple policies could be optimal and one of them might not be greedy [with respect to the optimal value function]&#34; could be a counterexample to the claim that &#34;every optimal policy must be greedy with respect to the optimal value function&#34; but not to the claim that &#34;a policy which is greedy with respect to the optimal value funtion must be optimal&#34;. The issue here has nothing to do with RL. This is just logic (you&#39;re confusing the claims that are being made/mixing up your quantifiers).</p>
<p></p>
<p>Moreover, I don&#39;t think the claim implicit in your response is true. Ususally, the reason for there being more than one optimal policy is that, given the optimal value function, there <em>can be ties</em>. That is, the action which produces the optimal value from a particular state need not be unique. In such cases, we can treat them as fungible. We&#39;ll thus wind up with different policies depending upon which we choose, all optimal.</p>
<p></p>
<p>But you seem to be suggesting that a policy might <em>not</em> be greedy with respect to the optimal value function and yet still be optimal overall. And I don&#39;t think that can actually happen. To see this, suppose that $$\pi_{1}$$ is greedy with respect to the optimal value function (and thus optimal), and that $$\pi_{2}$$ is optimal bit not greedy with respect to the optimal value function.</p>
<p></p>
<p>Because $$\pi_{2}$$ is not greedy with respect to the optimal value function, there is some state, $$s$$, for which $$V^{\pi_{2}} &lt; Q(s, \pi_{2}(s))$$ is</p>",2019-11-04T23:19:02Z,20,Week 11/3 - 11/9,feedback,a_2,,k2l1qa3ud0h297,2019-11-04T23:19:02Z,{},final_exam
3720,stud,"<p>Dang it. Ignore last line above. </p>
<p></p>
<p>Because $$\pi_{2}$$ is not greedy with respect to the optimal value function, there is some state $$s$$ for which $$V^{\pi_{2}}(s) &lt; V^{*}(s) = V^{\pi_{1}}(s)$$. But then, in order for $$\pi_{2}$$ to be optimal, there must be some <em>other</em> state $$s&#39;$$ for which $$V^{\pi_{2}}(s&#39;) &gt; V^{\pi_{1}}(s&#39;)=V^{*}(s&#39;)$$, by exactly the same amount that $$V^{\pi_{2}}(s) &lt; V^{*}(s)$$. But that can&#39;t happen, because the optimal value function $$V^{*}$$ is <a href=""http://www.incompleteideas.net/book/ebook/node35.html"" target=""_blank"" rel=""noopener noreferrer""><em>shared</em> by all optimal policies</a>. </p>",2019-11-04T23:28:33Z,20,Week 11/3 - 11/9,feedback,a_2,,k2l22izfmq83n2,2019-11-04T23:28:33Z,{},final_exam
3721,stud,"<p>Also, &#34;Alex&#34; above should be &#34;Alec&#34;. Apologies. </p>",2019-11-05T01:23:14Z,20,Week 11/3 - 11/9,feedback,a_2,,k2l660rlczt6yw,2019-11-05T01:23:14Z,{},final_exam
3722,no,"<p>false.  optimal action, on optimal value function, is by definition, optimal.</p>",2019-11-05T05:36:55Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lf8904gn81e6,2019-11-05T05:36:55Z,{},final_exam
3723,no,"False. Because the optimal value function encapsulates the true expected reward at any state and action, the greedy policy will always chase the highest possible reward through the simple argmax operation. ",2019-11-05T06:23:31Z,20,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2lgw67e9og61e,2019-11-05T06:23:31Z,{},final_exam
3724,no,"<p>False, but once again there’s nuance here. If we&#39;re privy to the transition model, then Greedy will always be optimal. If we&#39;re not, then a model free method like a Q learner will need to inject randomness because greedy will not be optimal and get trapped in local optima.</p><p><br /></p><p>Are you more interested in my True/False or my justification and reasoning? Given this somewhat ambiguously worded question, I could argue it both ways, as I qualified above, even though I ultimately chose false. If this is the type of ambiguous wording we’re expecting on the test, I’m going to have to qualify all of my answers with potential interpretations, to make sure I haven’t misunderstood the question.</p><p><br /></p><p>Feel free to comment and suggest alternatives. </p>",2019-11-06T14:24:02Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ndhzafkf01rv,2019-11-06T14:24:02Z,{},final_exam
3725,no,<p>False as noted succinctly by Uno: Optimal is optimal.</p>,2019-11-14T21:41:13Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8n0m2aeh69j,2019-11-14T21:41:13Z,{},final_exam
3726,no,"<p>In TD learning, the sum of the learning rates used must converge for the value function to converge.</p>",2019-11-04T19:57:06Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kuill9vnp1nz,2019-11-04T19:57:06Z,{},final_exam
3727,stud,"<p>False. According to stochastic approximation theory, the condition $$\sum_{i=1}^{\infty} \alpha_i(a)=\infty, \sum_{i=1}^{\infty} \alpha_i(a)^2&lt;\infty$$ guarantees convergence. (Question: this is not a necessary condition?)</p>",2019-11-04T20:30:31Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvpkdljx455n,2019-11-04T20:30:31Z,{},final_exam
3728,stud,"<p>I don&#39;t believe it is a necessary condition. It&#39;s merely sufficient. This is discussed elsewhere on Piazza, too. </p>",2019-11-04T20:35:31Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kvw09uf8c56l,2019-11-04T20:35:31Z,{},final_exam
3729,no,"<p>false.  the sum of the squared sums. ;)   monroe-robbins, necessary for convergence.</p>",2019-11-05T05:39:01Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lfayd5rjc4jj,2019-11-05T05:39:01Z,{},final_exam
3730,no,"<p>False. The sum has to diverge, but the sum of squares has to converge. This is for theoretical convergence, although in practice a constant is commonly used, which usually works ok in practice. </p><p><br /></p><p>Is this type of side information/knowledge useful, and will it be used to modify/soften the blow of a wrong “true/false” answer?</p><p></p>",2019-11-06T14:24:52Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ndj20rpoz2w7,2019-11-06T14:24:52Z,{},final_exam
3731,no,<p>False it is...</p>,2019-11-14T21:42:01Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8o1mpnqelz,2019-11-14T21:42:01Z,{},final_exam
3732,no,"<p>Monte Carlo is an unbiased estimator of the value function compared to TD methods. Therefore, it is the preferred algorithm when doing RL with episodic tasks.</p>",2019-11-04T19:58:38Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kukkijehw45d,2019-11-04T19:58:38Z,{},final_exam
3733,stud,<p>False. Bias vs variance. </p>,2019-11-04T20:31:00Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvq6wboai69z,2019-11-04T20:31:00Z,{},final_exam
3734,stud,"<p>&#34;Monte Carlo is an unbiased estimator of the value function compared to TD methods.&#34;</p>
<p></p>
<p>This part is true. To say that an estimator is &#34;unbiased&#34; in the sense at issue here is simply to say that it converges to expected value of the quantity it is an estimate of. But the expected value of a state (i.e., the value assigned to it by the value function) is just the sum of discounted rewards obtainable from that state. That&#39;s exactly what the Monte Carlo estimate gives you. </p>
<p></p>
<p>&#34;Therefore, it is the preferred algorithm when doing RL with episodic tasks.&#34;</p>
<p></p>
<p>I don&#39;t think this follows. There are other considerations here besides getting the value function right. We really want the optimal policy, and we should be able to get that (sometimes quicker and easier) using biased estimators too. </p>",2019-11-04T20:45:35Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kw8yavh9b1px,2019-11-04T20:45:35Z,{},final_exam
3735,no,"<p>false.  mc is an unbiased estimator, with respect to minimizing rmse, for undiscounted cases (as I recall off the top of my head).</p>",2019-11-05T05:41:29Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lfe4oo96e3b3,2019-11-05T05:41:29Z,{},final_exam
3736,no,"True,it is unbiased with respect to the episodes seen Because an update is done after each episode we approach the true distribution of the observation space as time goes on. However, because we always observe a subset and only approach the true distribution at infinity, we suffer from high variance until then. As one can see bootstrapping has less variance but this can reduce over time and may converge quite rapidly once values begin propagating. This initial bias can be a problem but the low variance leads to rapid convergence over MC. <p></p>",2019-11-05T06:28:16Z,20,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2lh2af83v2af,2019-11-05T06:28:16Z,{},final_exam
3737,no,"<p>False. </p><p><br /></p><p>Monte Carlo will converge to the the sample average over training data, which is also equivalent to TD(1). So I wouldn&#39;t say it&#39;s necessarily preferred, and if the episodes are very long, or can be potentially be stuck in infinite loops, Monte Carlo won&#39;t learn anything while TD would. </p><p></p><p></p>",2019-11-06T14:29:01Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ndoe9rk0w7ev,2019-11-06T14:29:01Z,{},final_exam
3738,no,"<p>False. As pointed out, there are other things you might want to consider as well...</p>",2019-11-14T21:44:08Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8qryfctg31t,2019-11-14T21:44:08Z,{},final_exam
3739,no,<p>Good answer David. :)</p>,2019-12-07T21:07:50Z,16,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w2ko81yvs5vn,2019-12-07T21:07:50Z,{},final_exam
3740,no,<p>The value of the returned policy is the only metric we care about when evaluating a learner.</p>,2019-11-04T20:00:05Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kumfgrukz67b,2019-11-04T20:00:05Z,{},final_exam
3741,stud,"<p></p>
<p>False. This is kind of subjective. I would say, machine time, data efficiency, the experience required from data scientists are all additional things to consider. </p>",2019-11-04T20:32:28Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvs2odylh3ey,2019-11-04T20:32:28Z,{},final_exam
3742,no,"<p>To add on this... variance, model complexity, etc. are also valid points. Model pruning and training stability are of recent interest and may be acceptable to tradeoff vs pure return policy.</p>",2019-11-04T22:35:10Z,20,Week 11/3 - 11/9,feedback,,jcg0nzvdk8272b,k2l05va19fg466,2019-11-04T22:35:10Z,{},final_exam
3743,no,"<p>false.  computational efficiency, memory efficiency, data efficiency are important metrics where significant RL research has been invested.</p>
<p></p>
<p>also... epsilon policies (within some epsilon of optimal) would not exist; and &#34;incremental updates&#34; (required for structured credit assignment) would also not exist.</p>",2019-11-05T05:43:02Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lfg40s3i75z3,2019-11-05T05:43:02Z,{},final_exam
3744,no,"<p>False. This is ambiguously worded. What does it mean when you say &#34;when evaluating&#34;? In our papers for project 2 for example, we also cared about things like time to converge. So if you mean humans &#34;evaluating a learner&#34;, then yes I would say this is false. This seems subjective. </p><p></p><p></p>",2019-11-06T14:32:54Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ndte1julr2es,2019-11-06T14:32:54Z,{},final_exam
3745,no,"<p><img src=""https://media.giphy.com/media/8VrtCswiLDNnO/giphy.gif"" alt="""" /></p>",2019-11-14T21:46:19Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8tl0ozmh6no,2019-11-14T21:46:19Z,{},final_exam
3746,no,<p>There is a bakery in the Night Watch&#39;s Castle Black.</p>,2019-11-04T20:04:52Z,20,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2kusl9y33w2a0,2019-11-04T20:04:52Z,{},final_exam
3747,stud,<p>I AM DOOMED FOR THE EXAM!!!</p>,2019-11-04T20:32:50Z,20,Week 11/3 - 11/9,feedback,a_0,,k2kvsjxi6wy4pa,2019-11-04T20:32:50Z,{},final_exam
3748,stud,"<p>I think it&#39;d actually have to be &#34;Night&#39;s Watch&#39;s&#34;, right? It&#39;s the &#34;Night&#39;s Watch&#34;, not the &#34;Night Watch&#34;. </p>
<p></p>
<p>I&#39;m going with &#34;True&#34; for this one. There&#39;s for sure a kitchen. I assume it has an oven. Oven&#39;s are for baking things, so...</p>",2019-11-04T20:33:04Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kvsv46qgb6no,2019-11-04T20:33:04Z,{},final_exam
3749,stud,<p>*Ovens*</p>,2019-11-04T20:33:31Z,20,Week 11/3 - 11/9,feedback,a_2,,k2kvtfdw35j7d3,2019-11-04T20:33:31Z,{},final_exam
3750,stud,<p>7</p>,2019-11-04T21:48:46Z,20,Week 11/3 - 11/9,feedback,a_1,,k2kyi7ds3675ho,2019-11-04T21:48:46Z,{},final_exam
3751,no,<p>lol.  1980&#39;s trivia eh? :)  I&#39;m going to like studying for this exam ;)</p>,2019-11-05T05:43:34Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2lfgsl5h5j74g,2019-11-05T05:43:34Z,{},final_exam
3752,stud,<p>1980s? What rock have you been living under? </p>,2019-11-06T14:19:13Z,20,Week 11/3 - 11/9,feedback,a_2,,k2ndbs32jve37l,2019-11-06T14:19:13Z,{},final_exam
3753,no,"Not sure, but would say False since I’ve never seen the night’s watchmen eating bread, but I definitely know there’s a Starbucks in Winterfell &#x1f61d; (funnily enough typing this up as I’m listening to the “Night King” song from the S8 soundtrack. The show arguably nosedived, but the music is top notch). <p></p>",2019-11-06T14:36:31Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2ndy1gdnyf68l,2019-11-06T14:36:31Z,{},final_exam
3754,stud,"<p>&#34;<em>The show arguably nosedived&#34;</em></p>
<p><em></em></p>
<p>It <em>really</em> did, though. As long as D&amp;D had a detailed map to work off of (in the form of Martin&#39;s books), everything was great. As soon as the show got ahead of the books, though...yikes!</p>
<p></p>
<p>Really hoping that G.R.R.M. actually finishes the final book so we can have a <em>proper</em> ending. </p>
<p></p>
<p>And I agree - the music really is great. </p>",2019-11-06T15:15:42Z,20,Week 11/3 - 11/9,feedback,a_2,,k2nfcfgq9ly439,2019-11-06T15:15:42Z,{},final_exam
3755,no,"To me it did, and I was very disappointed, but there are some who don’t feel that way, hence just left it arguably. &#x1f642;<p></p><div><br /></div><div>Agree with you on all points. </div>",2019-11-06T15:17:57Z,20,Week 11/3 - 11/9,feedback,,i4jbttw9ru63ot,k2nffbptj2f3y5,2019-11-06T15:17:57Z,{},final_exam
3756,no,"<p>at anon:</p>
<p></p>
<p>lol!  touche, I was thinking of the a collection of short stories by Steven King (&#34;night shift&#34;).  of which he often writes about Casle Rock. :)</p>
<p></p>
<p>I haven&#39;t watched game of thrones since the queen got some dragon eggs.</p>",2019-11-06T15:52:56Z,20,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2ngobb22ig3f,2019-11-06T15:52:56Z,{},final_exam
3757,no,"<p>In honesty, I just felt the need to post this:</p>
<p><img src=""https://pics.me.me/what-was-jon-snows-bakery-called-you-know-muffins-jon-51172284.png"" alt="""" /></p>",2019-11-14T21:48:59Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8wzzofvr3k7,2019-11-14T21:48:59Z,{},final_exam
3758,no,"<p>And this:</p>
<p>https://www.moviefone.com/2017/07/28/game-of-thrones-hot-pie-actor-opened-bakery-you-know-nothing-john-dough/</p>",2019-11-14T21:49:37Z,19,Week 11/10 - 11/16,feedback,,hyxsfbkeit22m2,k2z8xtmjbys1v5,2019-11-14T21:49:37Z,{},final_exam
3759,no,"<p>First a very high-level point: If the goal is to solve a specific problem, then using domain information should be a huge win in creating a better learner.  This could be feature engineering, or reward shaping, or whatever.  There are some OpenAI gym solutions (not to Lunar Lander) that solve the environment on the first episode because the agent is pre-programmed with the solution - it doesn&#39;t have to learn at all.</p>
<p></p>
<p>On the other hand, the goal might be to solve the problem with absolutely no &#34;inside&#34; information.  The same agent that solves Lunar Lander can potentially be fed to an environment which has no concept of a landing pad, or angular velocity, or anything else that LL has.  This is more along the lines of creating a &#34;general intelligence.&#34;  It all depends what your goals are.</p>
<p></p>
<p>Having said that, if the goal is creating an efficient learner and a great lander for LL, then I definitely think there are a lot of opportunities to exploit dependence of state variables in Lunar Lander.  Based on posts by Jacob, who discretized the state space, I gleaned that he only used 1 bit of information for x-position, y-position, and x-velocity.  This says a lot.  Also, you gave the example of x-position and angular velocity potentially being completely independent.  There may be other pairs of state variables that are very dependent, and perhaps introducing a state input that is a product of them (or some other nonlinear function of them) would help the NN generalize better.</p>
<p></p>
<p>I&#39;m also curious about how good of a lander is possible given the highly stochastic environment.  I&#39;ve been playing with my agents since submitting the project, trying to create an agent that can average &gt; 280 reward over, say, 10k episodes.  How much better is possible?  I don&#39;t know.</p>
<p></p>
<p>I didn&#39;t do your post justice but these are some preliminary thoughts.</p>",2019-11-05T04:51:28Z,30,Week 11/3 - 11/9,followup,,jzfsa4a37jf4aq,k2ldlt80cnq1z2,2019-11-05T04:51:28Z,{},other
3760,no,"Agree with you. I am just looking for some type of proof that perhaps correlating the state action rewards over time on average, can lead to insightful information about the form of the exact functional relationship of the Value function. Could we formally prove that using this correlation information along with maybe derivatives, second derivatives , etc can lead to an exact, analytical, solution? Maybe there is a systematic way to construct these functions using the correlation, is what im getting at. Maybe one can construct exact exponential, polynomial, sinusodial or other linear/non-linear functions given the nature of the correlation/non-correlation of these tuples I mentioned. Perhaps we can use information about derivatives and second derivatives... and so on until infinity. Perhaps we can use other type of information. But can we solve these functional relationships as easy as Cauchy and Euler analytically solved differential equations? Lets try :)<p></p>",2019-11-05T05:00:50Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2ldxu993h273t,2019-11-05T05:00:50Z,{},other
3761,no,"<p>I think it may be easier to prove this by first modifying the LL environment to have a fixed map (i.e. ground terrain), and remove stochasticity from the actions (currently you get a random &#34;tilt&#34; every time you fire a thruster).  In that modified case, it may be much easier to find a clean functional form like you want.</p>",2019-11-05T05:04:40Z,30,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2le2s0qkcl1cf,2019-11-05T05:04:40Z,{},other
3762,no,"&#64;Vahe, perhaps in the limit, the expected relationship between these tuples I described can give us information used to analytically solve the optimal value function, is what I’m getting at.",2019-11-05T05:05:33Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2le3ww77773ow,2019-11-05T05:05:33Z,{},other
3763,no,"Vahe, also, is it domain knowledge if we generalize this to not just the lunar lander or any problem with which we have prior physical intuition about, but rather over time any learner with any set of continuous states, a set of possible actions, and rewards? If we study the general correlation over time between a state variable and the actions and rewards and learn information purely based on correlation, and the influence one state action pair have on rewards, I wouldn’t say it’s domain knowledge, isn’t it just application of a general RL theory to a problem? Just as much as we have to explore and update a DQN over time one could include updating the agent based on the correlation over time. This is no more domain knowledge than learning from a replay buffer, because that’s essentially what’s being done - but instead of gradient descent it’s analytical statistical estimation no? :)",2019-11-05T06:12:31Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2lgi0ukdqj2ap,2019-11-05T06:12:31Z,{},other
3764,no,<p>No it&#39;s not domain knowledge if it&#39;s some kind of &#39;meta-learning.&#39;  An agent could observe patterns that decide the way it learns.  Or there could be distinct agents - one to do modeling and one to do learning.</p>,2019-11-05T07:15:25Z,30,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2liqxgzm226yw,2019-11-05T07:15:25Z,{},other
3765,no,"&#64;uno palabra let’s test it out and work on it! I want to develop this more rigorously and formally. If in fact it does lead to a speed up (we have so much memory, we can do large collections of data and analyze it now instead of just minibatches), maybe we can create a more powerful analytical estimator that ONLY converges to the optimal value function purely if we explore all states and actions infinitely often. Gradient descent has a lot more criteria one has to fulfill in order to converge, seems too messy :p",2019-11-05T06:17:29Z,30,Week 11/3 - 11/9,followup,,jzj4sh1p7pf5af,k2lgof9hxg01m,2019-11-05T06:17:29Z,{},other
3766,no,"<p>sure, i think its work a couple different agents to see whats what.  perhaps, after RL is done? I am def. interested in working on the theoretical aspects of RL :)</p>",2019-11-05T07:01:15Z,30,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2li8pgxi257n0,2019-11-05T07:01:15Z,{},other
3767,no,Certainly. This class won’t ever end for me :p it gives me great ideas for supervised learning too ,2019-11-05T07:12:02Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2limkd6g0k3x,2019-11-05T07:12:02Z,{},other
3768,no,"<p>indeed, this is much how I feel ;)</p>",2019-11-05T07:48:59Z,30,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2ljy3eu4xh31u,2019-11-05T07:48:59Z,{},other
3769,no,"Another point. Could this method be used to improve CNNs? Understanding correlations between weight activations at multiple levels and perhaps adding a parameter to residual connections as ResNets have. Add these connectioms between features and future layers or even the logits layer directly if you find, lets say, that particular features should be potentiated on their influence on the output. Conversely, we can remove noise by pruning with this method as well. Understand noisy features and valuable ones and appropriately “dropping out” neurons which decorrelate the input to the target output. Same analogy $$(X_i, Y_i, \hat{Y_i})$$ become the new tuples of interest. Studying them to prune and reinforce a net while it’s training would be really cool!<div><br /></div><div>For example lets say we dont have a residual layer from the first or second layer of a benign/malignant cancer classifier directly to the logits layer of 2 neurons, for malignant and benign. However, we see the network isn’t properly using this important information that a early shape or feature detected (like a huge fuzzy void which is almost always cancer) in the ultrasound) and in some cases other less important features overpower it, leading us to misclassify the image as benign. Through this analysis, we could catch that, create a new connection or perhaps layer, neuron (some architectural modfication) that would amplify the features response in detecting cancer. Maybe a direct residual connection from the GAP of this feature map directly to the logit representing cancer could be a sound decision. Then we find step by step we are not only doing gradient descent, but also prodding our network to roll down the right hills of hyperparameter space land... into the holy grail global minima.</div><div><br /></div><div>I’ll try to implement this tomorrow by tracking observations of weight activation overtime in my networks at work, and somehow applying this analysis to trim and improve the network step by step as it does gradient descent alongside. Maybe this information can also even help guide the gradient descent, or maybe it is a completely separate type of update to the architecure itself. Still trying to figure it out. I’ve been working on architecture search methods so this may fit in nicely :)</div><p></p><p></p>",2019-11-05T06:59:00Z,30,Week 11/3 - 11/9,followup,,jzj4sh1p7pf5af,k2li5t2c6n135o,2019-11-05T06:59:00Z,{},other
3770,no,"<p>I think using correlation for this might be too naive because any correlation will be linear versus the non-linearities in the CNN.</p>
<p></p>
<p>Generally skip connections are to preserve the flow of gradients to create deeper networks. I use CNN&#39;s extensively as well and your thought process behind including skip connections seems logical. eg. the first few conv layers are where the edge detection generally occurs, if I want to preserve the notion of edges in the final feature map or logits can a skip connection be created directly from the output of the first few conv units? this has given me mixed results. depends on the task lol.</p>",2019-11-05T07:44:50Z,30,Week 11/3 - 11/9,feedback,,jl1acpoc4HA9,k2ljsqs32fz4dn,2019-11-05T07:44:50Z,{},other
3771,no,"Hey Farrukh I agree the functions are nonLinear. But even so , we can observe activated features, how often, and in what classification are they present. We can understand more deeply what the nonlinearities are doing as we trace input to output activate and track those over time. Some connections we can prune, some are overpowered, and some simply have the wrong sign (-&#43;) involved. We cab use this information and shape the architutre or prune it in addition fo resssing it. <p></p>",2019-11-05T08:16:02Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2lkwvfu41w12s,2019-11-05T08:16:02Z,{},other
3772,no,About the misked tasks. I mean sending up maybe layer 3-4 where some featueres that arent so basic but are clear cancer features should create a connection directly to the logits. With this analysis some of them can be a subtraction residual ,2019-11-05T08:41:53Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2llu43vnal29m,2019-11-05T08:41:53Z,{},other
3773,no,"<p>farukkh, I mentioned to daniel that there is a precedent for this type of work, within NN literature; at least on a fixed dataset.</p>
<p></p>
<p>that is:</p>
<p>- the use of covariance matrix for preliminary feature detection (picking out promising feature spaces)</p>
<p>- the use of pretrained single-layer feature detectors (for coarse grain evaluation), as a boost for a neural net (I hear keras has this as a feature in order to speed &#34;image training&#34;)</p>
<p>- the use of spatial isolation amongst frame regions for boosting isolated feature detection.  I forget the name for the technique, but it was probably done around 3 years ago, with some success.  I have seen in passing a more sophisticated version of it, last year so I presume, the technique has merit.  also there is the adjacent set of algorithms (not neural networks) from around 6-7 years ago (from the segmentation and tracking sphere).</p>",2019-11-06T10:21:57Z,30,Week 11/3 - 11/9,feedback,,jzivtxcbl6964n,k2n4unv342k55i,2019-11-06T10:21:57Z,{},other
3774,no,"<p>sigh sorry that should not have been in students answer. my apologies, editing and reposting here.</p>
<p></p>",2019-11-05T06:59:04Z,30,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2li5w695f75rr,2019-11-05T06:59:04Z,{},other
3775,no,"<p>excellent idea daniel :)</p>
<p></p>
<p>I see no reason, since the process is automated, and based on agent-actions, that instead of prewarming a buffer, you analyze it and generate a decorrelated neural network, that this would be &#34;domain knowledge&#34;.</p>
<p></p>
<p>however, you might have issues, depending on what u are actually analyzing.   that is, presume that a random set of movements (of an exploratory nature), result in a set of observations about highly de-correlated movements; this may not extend, to the actual successful policy.   this is sort of the problem sutton mentions, with regard to TD learning; that in some cases the time structuring (credit assignment) yields better results, and in others, it actually works against it.</p>
<p></p>
<p>put more clearly:  if the massive reward of the game, is present only at the very last set of movements, and those movements, require coordinated approach (ONLY at that time); the observation vectors are highly correlated, but only at the end.   during the actual gameplay, the correlations may be quite weak (say due to reward shaping, and &#39;especially&#39; dense reward structure); or worse, of an auto-correlative type (where correlation exists solely from the time-sequencing structure of the rewards map with respect to a trajectory).</p>
<p></p>
<p>having said this, i think the idea has merit.  I can envision two cases where it might very well be exemplar:</p>
<p></p>
<p>- in a dense reward structure, where movements over time away from the goal state, always get progressively worse (suitable for a learner that punishes bad actions)</p>
<p>- as an alternate form to learn the background residue of the reward map; similar to V(s) &#43; A(s); here the notion is to observe the correlations that are weak, and clip the intermediate (middling) states, to pronounce highly positive, highly negative states.   it would be in essence, undoing, a reward shaping function, faster than random exploration and neural net reinforcement can learn to ignore it (or a q-learner can backpropogate results).  It is interesting to note, that those who coded the dueling dqn mentioned seeing no significant advantage (likely due to the dense reward structure, and consistently minimal reward changes from state to state); here I think, you would since the correlations are both initially set, and learned over time (and thus can adapt to the cases where the &#34;goal state&#34; has been achieved.</p>
<p></p>
<p>Since the structuring is provided the Q-learner, and not the neural network; this would be I think, would still be applicable; though you would have to think how you are going to update the network, if the &#34;strength&#34; of the DQN, is in fact &#34;decorrelation&#34; provided by its replay buffer (or similarly, multi-agent exploration using environments).</p>
<p></p>
<p>If you are interested in this type of thing, I think you should look at &#39;structured space exploration&#39;.   It is a similar idea (not within correlation, but analytically, with respect to systematically examining the space in order to leverage information earlier than generally applicable).</p>
<p></p>
<p>I like the idea.  ostensibly, the most improvement you will see, is where state-segments within observation vector, are in fact, independent with respect to the optimal policies predicted trajectory.  i.e. there are many cases, where in practice, the sum of the system space, is in fact, partitioned. </p>
<p></p>
<p>It would also be extremely interesting to see, how such an algorithm might work in practice, where both types of &#34;optimal policies&#34; exist; that is - where correlated actions (&#39;coordination&#39;) are required in one form of optimal action sequence; and where,  a less-coordinated approach gives you a similar (within some epsilon) optimal policy.   I would suspect highly that the training time on the latter, with ur algorithm, would in fact, be quicker; where conversely, regular algorithms, would not be (since the lack of correlation, in fact, works to disadvantage the learner, with respect to back propogation).  It is ironic to note, that the &#39;decorrelation&#39; that was popular three-to-four years ago, might in fact, be an artifact --- and that the inherent sensitivities to warmups, buffer-size, and epsilon, may in fact be a reflection that all decorrelation, is not necessarily good, for all problems.</p>
<p></p>
<p>Fascinating problem you pose; well done!</p>",2019-11-05T06:59:36Z,30,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2li6kurrvo66n,2019-11-05T06:59:36Z,{},other
3776,no,"<p>I understood it a couple of ways,</p>
<p></p>
<p>1. you are trying to reduce the search space for the optimization problem by injecting domain knowledge in the form of an inductive bias for the neural network (ie. using your correlation experiments to initialize weights).</p>
<p></p>
<p>2. &#34;correlation between (S,A,R)<span style=""vertical-align:-0.389em""></span> for all states and actions&#34;, if you add $$S&#39;$$ to this what you are describing is getting close to model based RL. its almost as if you are trying to build a model of the dynamics of the world. once you have this you can plan over it to solve the MDP.</p>
<p></p>
<p>Interesting thought, experiment and report back.</p>",2019-11-05T07:00:53Z,30,Week 11/3 - 11/9,followup,,jl1acpoc4HA9,k2li88eflqp1s4,2019-11-05T07:00:53Z,{},other
3777,no,"Will do! On my list this week, most likely tomorrow. Have been mostly writing out formulas by hand trying to understand the best way to approach it. I got a good idea now. ",2019-11-05T07:02:59Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2liaxmhaan6ej,2019-11-05T07:02:59Z,{},other
3778,no,"Also once you add S’, we begin correlating $$\textit{sequences}$$ to the action that should be taken leaving that state, to reward. We are generalizing it to Options! Understanding more abstract representations that can be represented in a neuron or sets of neurons. For example, if a network finds that neither of those actions are appropriate because an $$\textit{option neuron}$$ was called (or maybe activated, by all the states calling it) then we can use it!<div><br /></div><div>That being said I think at every state they should be limited to determining the next actions and understanding their reward, whether through this arch search arch aglorithm or some type of algorithm that helps converge. </div><div><br /></div><div>The future states as you see it can help converge the value functions we need, or in building options because ideally options are built of actions with a correlated reward. </div><p></p><p></p><p></p>",2019-11-05T07:46:10Z,30,Week 11/3 - 11/9,feedback,,jzj4sh1p7pf5af,k2ljuggitayso,2019-11-05T07:46:10Z,{},other
3779,no,"<p>I&#39;m having a hard time to understand how the y-axis are calculated for each figure 3. My understanding is that we need to plot on y-axis the following value for figures 3(a), (b) and (c):</p>
<p>$$ERR_{A}^{t}=|Q_{A}^{t}(s, a_{A}=South,a_{B}=Stick)-Q_{A}^{t-1}(s, a_{A}=South,a_{B}=Stick)|$$</p>
<p>Where $$Q_{A}^{t}(s, a_{A}=South,a_{B}=Stick)$$ is a single scalar value from the Q-table at time t.</p>
<p></p>
<p>While for figure 3(d) the y-axis is calculated by:</p>
<p>$$ERR_{A}^{t}=|Q_{A}^{t}(s, a_{A}=South)-Q_{A}^{t-1}(s, a_{A}=South)|$$</p>
<p>Where $$Q_{A}^{t}(s, a_{A}=South)$$ is a single scalar value from the Q-table at time t.</p>
<p>In figure 3(d), we don&#39;t consider the action taken by the player B, because Q-learning doesn&#39;t consider actions for two players.</p>
<p></p>
<p>Is my understanding of the y-axis calculation correct?</p>
<p></p>
<p></p>",2019-11-10T00:47:34Z,20,Week 11/10 - 11/16,followup,,jc6xvgjncoey,k2sa3e7n22p1qi,2019-11-10T00:47:34Z,{},project3
3780,no,<p>That&#39;s correct.</p>,2019-11-10T15:49:17Z,20,Week 11/10 - 11/16,feedback,,jqknbi6c0lHt,k2t6b0zhznk3t4,2019-11-10T15:49:17Z,{},project3
3781,no,<p>Thank you!</p>,2019-11-10T16:00:59Z,20,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2t6q288hyr3hl,2019-11-10T16:00:59Z,{},project3
3782,stud,"<p>Is &#39;s&#39; mentioned above the following initial state, </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6m1j6i41cz6ru%2Fk34qbsnwd6wh%2FCapture.PNG"" alt="""" width=""363"" height=""153"" /></p>
<p>Let&#39;s say this state is state# 3. Is ERR in this case difference of Q(s=3, South) between iterations/episodes (t) and (t-1) ? </p>
<p></p>
<p>Is x-axis showing the number of episodes ? </p>
<p></p>",2019-11-18T17:57:41Z,19,Week 11/17 - 11/23,feedback,a_0,,k34qeyqw1qt2ko,2019-11-18T17:57:41Z,{},project3
3783,no,"<p>&#34;Let&#39;s say this state is state# 3. Is ERR in this case difference of Q(s=3, South) between iterations/episodes (t) and (t-1) ?&#34;</p>
<p></p>
<p>Yes, for Q-learning (considering Q table for player A). For the other learning methods, you need to use Qa = (s=3, action A = South, action B = Stick).</p>
<p></p>
<p>&#34;Is x-axis showing the number of episodes ?&#34;</p>
<p></p>
<p>My understanding is no. It should be the number of iterations (in other words, the number of steps).</p>",2019-11-20T00:54:54Z,19,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k36krcrpohi5u6,2019-11-20T00:54:54Z,{},project3
3784,no,"<p>I assume Episode and iterations are the same meaning, referring to a complete game. Each episode or iteration could have max_steps or until reaching goal. Is that right</p>",2019-11-20T06:16:19Z,19,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k36w8p01mok6od,2019-11-20T06:16:19Z,{},project3
3785,no,<p>Is max_steps required? What if we let the episode run until someone scores a goal?</p>,2019-11-21T04:13:15Z,19,Week 11/17 - 11/23,feedback,,jzkpm3i0pxp2sg,k387aaijp9w7d8,2019-11-21T04:13:15Z,{},project3
3786,no,"<p>Now that I reviewed the Game Theory videos again, it makes sense.</p>
<p></p>
<p>Thank you.</p>",2019-11-06T05:13:57Z,42,Week 11/3 - 11/9,followup,,jc6xvgjncoey,k2mtukrlufa51e,2019-11-06T05:13:57Z,{},hw6
3787,no,"<p>I think that is up to the agent (so, us).  It&#39;s added complexity, but if it&#39;s useful then it may be worth it.  It may be tough to do for a continuous state space though.</p>",2019-11-05T05:01:14Z,54,Week 11/3 - 11/9,followup,,jzfsa4a37jf4aq,k2ldyd2kydu2o,2019-11-05T05:01:14Z,{},other
3788,stud,"<p>In all the examples we have been doing, it is up to the agent. But it does not have to be the case. Sometimes, the discount factor is given by the environment and the agents have no say to it. </p>",2019-11-05T05:02:29Z,54,Week 11/3 - 11/9,feedback,a_0,,k2ldzz8r4u37dy,2019-11-05T05:02:29Z,{},other
3789,stud,"<p>Sometimes, the discount factor is not only not up to the agent but also state-dependent. One thing I am interested in finding out is that if the discount factors are indeed state-dependent, the RL algorithms can still work.  (Sorry for the piecemeal update. I always forget there is no way to update responses to discussions.) </p>",2019-11-05T05:15:24Z,54,Week 11/3 - 11/9,feedback,a_0,,k2legkr335v4u5,2019-11-05T05:15:24Z,{},other
3790,stud,"<p>&#64;Farrukh Rahman, thanks for the quick but informative response. And thanks for pointing out that $$\gamma$$ is usually part of the environment and a prior to the agent being trained. I really appreciate it. I am going to check out the references. </p>",2019-11-05T07:29:28Z,54,Week 11/3 - 11/9,followup,a_0,,k2lj8zdugn31rn,2019-11-05T07:29:28Z,{},other
3791,no,"<p>at anonymous:</p>
<p></p>
<p>&gt; In other words, γ(s)<span style=""vertical-align:-0.377em""></span> can take on different values for different state s<span style=""vertical-align:-0.089em""></span>.</p>
<p></p>
<p>the idea has merit.  I would imagine it depends on the path length from the current state, to the reward state (on average, under &#34;optimal-like&#34; policy);</p>
<p></p>
<p>...and the impact of the reward state, with respect the current average reward state.</p>
<p></p>
<p>it also controls the speed of pull-up (which in turn controls a ringing like phenomenon peculiar to RL). </p>
<p></p>
<p>-</p>
<p></p>
<p>when I did sutton&#39;s paper, I originally had correct TD formula, lambda * factor put in (under the notion that it comparing apples to apples within a loss function would make sense); however, I found that this in turn, resulted in the algorithm  having substantially nonlinear &#34;hunting&#34; behavior; as in effect, comparing apples to apples, may yield a more accurate td-error; but in this case, the td-error itself was changing based on delayed feedback; said delay then causing it to move towards an erroneous past value, rather than the current one. </p>
<p></p>
<p>I experimented with a couple of different algorithms which managed to tame some of the nonlinearity, but at the cost of introducing a variance.  and this variance, incidentally, was a stabilizing one. there was a striking result in one of my experiments, where one of sutton&#39;s curves was literally shifted up in error by a small amount, but the convergence was -significantly- faster.</p>
<p></p>
<p>I also experimented, with a truncated discount; that is, a gamma discount where lambda didn&#39;t exclusively control the length of contribution; but instead (within a fixed set of steps) controlled individual contributions with respect to mixing of overall td-error (or more aptly; distribution of rewards with respect to credit assignment).   I set the arbitrary length cut off to the effective maximum constructive steps to be taken, within the context of sutton&#39;s problem.</p>
<p></p>
<p>I also experimented with abbreviated version of the markovian walk; I did not know what to call it at the time, but it is essentially, a pure bootstrapping solution combined with a partition classes; where the sample distribution for the optimal policy is approximated not by markovian walk (and backpropogation through countless value iterations) -- but instead by markovian walk, passed through a partitioning function in order to preserve the sampling distribution while substantially cutting down the space to be computed (boosted the computational efficiency quite nicely!).</p>
<p></p>
<p>taken in toto, the reach in hyperparameter space (via alpha) was extended almost two-fold.  the convergence was also quicker.   the trade off in bias vs variance was quite clearly demonstrated in the aforementioned graph.  and the computational efficiency (within the context of a simple mdp) was also boosted.</p>
<p></p>
<p>I discussed several of the results in a chat with NealKelly, which I posted. </p>
<p></p>
<p>I note, this would be prior to a substantial portion of the class, so the terminology may be off.</p>",2019-11-06T10:02:31Z,54,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2n45o6c1w957w,2019-11-06T10:02:31Z,{},other
3792,no,"33 pages is a lot of room to say more, though often what such space is really good for is making the work more self-contained. ",2019-11-05T18:06:46Z,28,Week 11/3 - 11/9,followup,,gph3si6rKEb,k2m60ko08hv5ff,2019-11-05T18:06:46Z,{},project3
3793,no,"<p>We found some differences in the description of the game&#39;s rules. Plus, the number of iterations done in the Brown paper is $$10^4$$ whereas in the 2003 ICML paper it was $$10^6$$. I think the 2003 work was quick and had some issues in their implementation. Those were fixed in the 2005 paper, but the different rules for the soccer game makes the differences irrelevant, unfortunately.</p>
<p></p>
<p>They published the paper again in 2007:</p>
<p></p>
<p><a href=""https://www.researchgate.net/publication/2925217_Correlated-Q_Learning"">https://www.researchgate.net/publication/2925217_Correlated-Q_Learning</a></p>
<p></p>
<p>At least they cited the 2003 paper in the list of refs. Too bad they didn&#39;t change the name of the paper to indicate it as an &#34;updated&#34; work.</p>",2019-11-05T18:11:48Z,28,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2m671fntwq5hp,2019-11-05T18:11:48Z,{},project3
3794,no,"<p>page 27 of the 2007 publication:</p>
<p></p>
<p><strong>8. Random Games </strong></p>
<p>Work in progress.</p>
<p></p>
<p>haha. Can I do that in my paper?</p>",2019-11-05T18:14:06Z,28,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2m69ztraxj50v,2019-11-05T18:14:06Z,{},project3
3795,no,"<p>&#34;We also thank Dinah Rosenberg and Roberto Serrano for comments on an earlier draft of this paper.&#34;</p>
<p></p>
<p>Any relation to Chris Serrano?</p>",2019-11-05T18:14:47Z,28,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2m6avlnnnj5xd,2019-11-05T18:14:47Z,{},project3
3796,no,"<p>page 28 of the 2007 version of the paper:</p>
<p></p>
<p>&#34;<em>Correlated equilibria in one-shot games can be computed in polynomial time; the computation of Nash equilibria in one-shot games is PPAD-complete</em>&#34;</p>
<p></p>
<p>Littman-Stone claims in &#34;A Polynomial-time Nash Equilibrium Algorithm for Repeated Games&#34; that &#34;<em>This result stands in contrast to the problem of computing</em><br /><em>a Nash equilibrium in a one-shot game, the complexity of which remains an important and long-standing open problem (Papadimitriou 2001)</em>.&#34;</p>
<p></p>
<p>GHZ do not offer up a reference to that PPAD-complete claim. That&#39;s disappointing. Correction, they referenced it, but did not cite it: Xi Chen and Xiaotie Deng. Settling the complexity of 2-player nash equilibrium. Technical Report 140, Electronic Colloquium on Computational Complexity, 2005.</p>
<p></p>
<p>So is this problem still open, or has it been solved?</p>
<p></p>
<p><a href=""https://ieeexplore.ieee.org/document/4031362"">https://ieeexplore.ieee.org/document/4031362</a></p>
<p></p>
<p>Seems like it has been settled as of 2006.</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-11-05T18:20:25Z,28,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2m6i4d4ggn1t3,2019-11-05T18:20:25Z,{},project3
3797,no,<p>there is a ppad solution/proof.</p>,2019-11-15T12:54:46Z,27,Week 11/10 - 11/16,feedback,,jzivtxcbl6964n,k3059u7qs5bfv,2019-11-15T12:54:46Z,{},project3
3798,no,<p>singular.</p>,2019-11-15T13:41:13Z,27,Week 11/10 - 11/16,feedback,,jzivtxcbl6964n,k306xl1kwg33he,2019-11-15T13:41:13Z,{},project3
3799,no,"<p><strong>Edit:</strong> My initial interpretation seemed inaccurate. Revising with direct excerpts about the soccer environment from Greenwald &amp; Littman.  Note that the example transition from the original post appears like it would not happen due to Littman&#39;s description below:  (ball possession shifting sounds like it could be interpreted either way, but it sounds clear player B would not move)</p>
<p></p>
<p>From Greenwald:</p>
<ul><li>&#34;The players&#39; actions are executed in random order. If this sequence of actions causes the players to collide, then only the first moves. But if the player with the ball moves second, then the ball changes possession2.&#34;</li><li>&#34;2: In other words, if the player without the ball moves into the player with the ball, attempting to steal the ball, he cannot. But if the player with the ball moves into the player without the ball, the former loses the ball to the latter. This form of the game is due to Littman [10].&#34;</li></ul>
<p></p>
<p>From Littman [10] - section &#34;6.1 SOCCER&#34; on pg 5 at <a href=""https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf"">https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf</a> :</p>
<ul><li>&#34;When a player executes an action that would take it to the square occupied by the other player, possession of the ball goes to the stationary player and the move does not take place. A good defensive maneuver, then, is to stand where the other player wants to go.&#34;</li></ul>
<p></p>",2019-11-06T15:11:01Z,54,Week 11/3 - 11/9,followup,,isde332xcka1m0,k2nf6e2opty4uu,2019-11-06T15:11:01Z,{},project3
3800,no,"<p>I&#39;m still confused about what we need to pass to the solver as input based on <a href=""https://cvxopt.org/examples/tutorial/lp.html"" target=""_blank"" rel=""noopener noreferrer"">this example</a>. As I understand it, it will be something like:</p>
<p></p>
<p>A = [[0.0, 1.0, -1.0], [-1.0, 0.0, 1.0], [1.0, -1.0, 0.0], [1.0, 1.0, 1.0]]</p>
<p></p>
<p>but I&#39;m not sure how to incorporate V into b and c matrices. Is there a way to pass information about V to the solver?</p>
<p></p>",2019-11-06T03:55:42Z,42,Week 11/3 - 11/9,followup,,jl284xdcifz44g,k2mr1xk62cv2ln,2019-11-06T03:55:42Z,{},hw6
3801,no,"<p>Dalton,</p>
<p></p>
<p>You can re-arrange the equations so that everything is on one side, less than the 0. Thus, V would be one of the x&#39;s alongside all the different pi&#39;s that you would solve for. And your goal would be to minimize -V (ie maximize V). Does that make sense?</p>",2019-11-06T04:37:03Z,42,Week 11/3 - 11/9,feedback,,jl3we43d3bp15p,k2msj3qxr0j4j8,2019-11-06T04:37:03Z,{},hw6
3802,no,"<p>I think so, so then we&#39;re essentially trying to maximize V<sub>rock</sub> &#43; V<sub>paper</sub> &#43; V<sub>scissors</sub>?</p>",2019-11-06T15:28:19Z,42,Week 11/3 - 11/9,feedback,,jl284xdcifz44g,k2nfsnl3x827fi,2019-11-06T15:28:19Z,{},hw6
3803,no,"<p>Is there a more useful CVXOPT example I should be following than the one in my link above? That example makes no mention of minimax, and Tianhang makes it seem like that&#39;s important to solving this problem. </p>",2019-11-06T03:56:53Z,42,Week 11/3 - 11/9,followup,,jl284xdcifz44g,k2mr3goxulj1fq,2019-11-06T03:56:53Z,{},hw6
3804,no,"a quick example for a min max problem using lp: http://apmonitor.com/me575/index.php/Main/MiniMax<div><br /></div><div>it&#39;s just converting a min max problem into a regular lp problem.</div><div>but i cannot find a cvxopt example just for min max. </div><div><br /></div><div>note that you should be doing max min, so please swap the operation when you look at it.</div>",2019-11-06T13:03:05Z,42,Week 11/3 - 11/9,feedback,,is5gzbotXmz,k2nalv8d4vr3bs,2019-11-06T13:03:05Z,{},hw6
3805,no,"<p>Look into <a href=""https://cvxopt.org/userguide/coneprog.html#linear-programming"" target=""_blank"" rel=""noopener noreferrer"">Cone Programming/Linear Programming</a>.  </p>",2019-11-06T20:35:12Z,42,Week 11/3 - 11/9,feedback,,jzh6k6o994a6dh,k2nqraiuztm2x0,2019-11-06T20:35:12Z,{},hw6
3806,no,"<p>James, your enthusiasm for reinforcement learning is inspiring for students like us who are thinking about entering the field.  Thank you for taking the time to answer our questions!</p>",2019-11-05T23:50:33Z,23,Week 11/3 - 11/9,followup,,jzfsa4a37jf4aq,k2miao6559t6jl,2019-11-05T23:50:33Z,{},office_hours
3807,no,"<p>Thanks for your time contributing to RL office hours, I think it&#39;s great that we got to get some perspective from a commercial practitioner of RL.  We appreciate your time greatly!</p>",2019-11-06T00:35:23Z,23,Week 11/3 - 11/9,followup,,jzhl7qlwrpagr,k2mjwbnkdl56cy,2019-11-06T00:35:23Z,{},office_hours
3808,no,"<p>Thank you James, the information you provided from your experience was very insightful!</p>",2019-11-06T03:18:57Z,23,Week 11/3 - 11/9,followup,,gx3c8l7z7r72zl,k2mpqok5wj7267,2019-11-06T03:18:57Z,{},office_hours
3809,no,"<p>Thanks James! Your conversation was really interesting to listen to, and though a few topics went over my head, its really inspiring to think that with enough patience and learning, students like us could make an impact in RL. I especially enjoyed your talk on visualization!</p>",2019-11-06T04:34:19Z,23,Week 11/3 - 11/9,followup,,jl3we43d3bp15p,k2msflmrp1jkj,2019-11-06T04:34:19Z,{},office_hours
3810,no,"<p>James, I found your talk highly informative on several aspects of RL.  Thank you for the value networks reference (and miguel for providing a link to the paper!); and for the discussion on the differences between planning and RL; and for your general overall commentary on the use of simulators with respect to RL, particularly the semi-realistic nature of presumed dense-sampling of environment far in excess of what is practicable, in real life. :)</p>",2019-11-06T10:09:08Z,23,Week 11/3 - 11/9,followup,,jzivtxcbl6964n,k2n4e6bwfqw5h9,2019-11-06T10:09:08Z,{},office_hours
3811,no,"<p>A huge thanks to you James for talking about your experiences in the field and for sharing your insights. It was very inspirational to those of us looking to get involved in the RL space, as well as giving us the opportunity to see the current state of affairs from someone who is living and breathing it. All the best with your research, current and future.</p>",2019-11-06T15:20:55Z,23,Week 11/3 - 11/9,followup,,jl3oi5v7qkSk,k2nfj4o2sru449,2019-11-06T15:20:55Z,{},office_hours
3812,no,"<p>James, much appreciated your enthusiasm and care of going through the practical aspects with RL with newcomers like ourselves. Your attitude seemed from someone that not only understands the field well but is also humble enough to recognize the &#34;art&#34; side of RL that we are experiencing throughout the course. Thank you!</p>",2019-11-06T21:23:15Z,23,Week 11/3 - 11/9,followup,,is6e83bsfvk,k2nsh3lhott5lc,2019-11-06T21:23:15Z,{},office_hours
3813,no,"<p>Hi James, most of the students in OMSCS are currently working in different fields. Work, class, and family take most of our time time, and reduce our exposure to many aspects of RL. Having you sharing your knowledge, your perspective, and your advice, are really valuable for us. Thank you for your time!</p>",2019-11-06T22:27:22Z,23,Week 11/3 - 11/9,followup,,jl5wq8mca7o0,k2nurjsewpi4c2,2019-11-06T22:27:22Z,{},office_hours
3814,no,<p>Thank you very much for sharing your valuable knowledge. </p>,2019-11-07T00:35:33Z,23,Week 11/3 - 11/9,followup,,jc6xvgjncoey,k2nzce9jxej18c,2019-11-07T00:35:33Z,{},office_hours
3815,no,"<p>Dr. MacGlashan,  Thank you for sharing your views on the RL/ML field.  I have an immense desire to explore this arena and hearing from someone on the forefront helps to better understand the current challenges and situational awareness.  I especially appreciated hearing about the future of this evolving field from your interview session.  The time you shared with us is appreciated, thank you.</p>",2019-11-07T03:55:06Z,23,Week 11/3 - 11/9,followup,,is9so9huTMp,k2o6h0e2y905fx,2019-11-07T03:55:06Z,{},office_hours
3816,no,<p>Thank you Dr. MacGlashan for your time and sharing your knowledge with our class.  Also for the great contributions in your efforts to advance reinforcement learning for everyone.  Thank you.</p>,2019-11-07T17:24:38Z,23,Week 11/3 - 11/9,followup,,jc5n7m3r8yc3br,k2oze2s4s746fk,2019-11-07T17:24:38Z,{},office_hours
3817,no,<p>Thanks! This was really insightful!</p>,2019-11-08T02:55:56Z,23,Week 11/3 - 11/9,followup,,jl561222orGT,k2pjsrxzp7r2j0,2019-11-08T02:55:56Z,{},office_hours
3818,stud,<p>This was a really fascinating office hour discussion. It helped me get a much better sense of how much more there is to learn about RL (quite a bit!) and provided some motivation to keep learning about RL after the course has ended. Thanks for contributing your knowledge and your time! We all appreciate it. </p>,2019-11-08T18:21:46Z,23,Week 11/3 - 11/9,followup,a_0,,k2qgvezxqn158k,2019-11-08T18:21:46Z,{},office_hours
3819,no,"<p>James! Thank you so much for making time for us, and answering our questions. </p>",2019-11-09T19:16:56Z,23,Week 11/3 - 11/9,followup,,jzjzgz6lrdv431,k2rya77ip0n5nr,2019-11-09T19:16:56Z,{},office_hours
3820,no,<p>It was really interesting to hear your thoughts on different areas of research and the current state of the field. Thank you for taking the time to share your experience in RL and answer our questions!</p>,2019-11-10T01:28:26Z,22,Week 11/10 - 11/16,followup,,jzifg1e23c29s,k2sbjyceth31oi,2019-11-10T01:28:26Z,{},office_hours
3821,no,"<p>Thank you very much James for the wonderful video session, it was very valuable experience to hear from the real practitioner in the field that is both complicated and developing fast, and which at the same time fascinating and frightening to study. Your talk gave us an excellent perspective to the balance between theoretical knowledge and practical ways to test new ideas in the field, and this is very much appreciated as well as your time that you have spent with us. Looking forward to hear from you again in the future!</p>",2019-11-10T02:01:39Z,22,Week 11/10 - 11/16,followup,,jqkxzdmmolGf,k2scqnyu1cl54u,2019-11-10T02:01:39Z,{},office_hours
3822,no,Thanks so much for your time and your valuable answers for all the questions!,2019-11-10T02:12:45Z,22,Week 11/10 - 11/16,followup,,jzj4205g7gd2fw,k2sd4y91yfh248,2019-11-10T02:12:45Z,{},office_hours
3823,no,"<p>Hi James, many thanks for taking the time to do this. Your interview session is informative, insightful, and stimulating. It gives me a valuable peek of what is going on at the frontier and makes me think about new questions and ideas. For someone already interested in the field, you get me completely hooked. </p>",2019-11-10T02:18:34Z,22,Week 11/10 - 11/16,followup,,jzhs489b10i78c,k2sdcfh9wpf57r,2019-11-10T02:18:34Z,{},office_hours
3824,no,"<p>Hello James! Thanks for making time in your busy day to thoughtfully answer all our questions. RL has become one of my favorite sections of ML and AI, so it was a real treat to get to hear your insights about the field.</p>",2019-11-10T16:08:05Z,22,Week 11/10 - 11/16,followup,,jl1b27fpaYkv,k2t6z73mh5k7dv,2019-11-10T16:08:05Z,{},office_hours
3825,no,<p>Thanks for taking the time to talk to us. I appreciate your time and wisdom.</p>,2019-11-10T16:53:02Z,22,Week 11/10 - 11/16,followup,,jc554vxmyuy3pt,k2t8l0e7mkt5nl,2019-11-10T16:53:02Z,{},office_hours
3826,no,"<p>James, thanks so much for the talk which certainly reminds us that there are so many possibilities in the field of reinforcement learning!</p>",2019-11-10T17:16:09Z,22,Week 11/10 - 11/16,followup,,jzj0om7qnbd4yf,k2t9eqhv1572mw,2019-11-10T17:16:09Z,{},office_hours
3827,no,That you for some very practical answers to mine and my classmate&#39;s questions. Always great to hear from someone who has experience directly applying RL!,2019-11-10T18:22:10Z,22,Week 11/10 - 11/16,followup,,j6mu8ll6cua74b,k2tbrmw6b3p254,2019-11-10T18:22:10Z,{},office_hours
3828,no,"A big thanks for taking the time to share your insights with us. It’s really a privilege to hear your answers to our questions and very helpful to get your thoughts about the field. Personally I’m super excited for the future of RL, thanks for pushing it forward!",2019-11-10T20:33:51Z,22,Week 11/10 - 11/16,followup,,jzjwcq2u8o7110,k2tggzctrba43d,2019-11-10T20:33:51Z,{},office_hours
3829,no,"<p>Very insightful and educative discussion, thank you for your time and sharing, James.</p>",2019-11-11T02:16:03Z,22,Week 11/10 - 11/16,followup,,jzjbneqoyho1hu,k2tsp1at6hu7pq,2019-11-11T02:16:03Z,{},office_hours
3830,stud,"<p>Thank you James for taking time to answer questions. I found it very interesting and helpful to hear your opinions. After hearing you talk, it made me even more interested in the field now.</p>",2019-11-11T04:45:39Z,22,Week 11/10 - 11/16,followup,a_1,,k2ty1fuikbc6e8,2019-11-11T04:45:39Z,{},office_hours
3831,no,"<p>Thank you, James!</p>",2019-11-11T18:06:40Z,22,Week 11/10 - 11/16,followup,,idghbt86wqe,k2uqnjtmt4h1wa,2019-11-11T18:06:40Z,{},office_hours
3832,no,"<p>As someone who&#39;s interested in the field, it&#39;s great to listen to thoughts from experts like you, James. A big thank you for taking time out of your schedule and motivating people like me :)</p>",2019-11-11T19:43:51Z,22,Week 11/10 - 11/16,followup,,jl8j7vzvUNs2,k2uu4iylcyl2ti,2019-11-11T19:43:51Z,{},office_hours
3833,no,"Thank you so much for your time in giving this talk. I haven’t had much time and recently got around to watching the video only today. It was interesting to hear what you have to say, Dr! Hope you come back again for another discussion some day :)",2019-11-12T00:06:43Z,22,Week 11/10 - 11/16,followup,,jzj4sh1p7pf5af,k2v3ikpns9t7eo,2019-11-12T00:06:43Z,{},office_hours
3834,no,<p>Thanks a lot James for the interesting session. It was very helpful.</p>,2019-11-12T05:11:03Z,22,Week 11/10 - 11/16,followup,,jqob6okzSK1c,k2vedyjrntrbj,2019-11-12T05:11:03Z,{},office_hours
3835,no,"<p>Thanks a lot Dr James for taking your valuable time for us OMSCS-ers.</p>
<p></p>
<p>Events like these don&#39;t happen a lot in OMSCS courses and they definitely make learning even more &#39;reinforced&#39; for students to be able to hear from RL experts like you. It was great to hear you views on the questions and otherwise too.</p>
<p></p>
<p>Thank you again and thank you to Miguel &amp; all other instructors/TAs who helped make this a success.</p>",2019-11-12T06:08:07Z,22,Week 11/10 - 11/16,followup,,jfzaqnqvtQ1m,k2vgfchhpn66nw,2019-11-12T06:08:07Z,{},office_hours
3836,no,"<p>Dr. James, thank you for taking the time to solve our questions. Your thoughts on the field, boost our interest to keep learning it.</p>",2019-11-13T03:09:39Z,22,Week 11/10 - 11/16,followup,,jzygktecb0e3i1,k2wphonm2jc7il,2019-11-13T03:09:39Z,{},office_hours
3837,no,"<p>Thank you Dr. James,</p>
<p></p>
<p>I found myself taking a lot of insight from your words, especially around how software engineering still plays a vital role even if one isn&#39;t doing the machine learning/reinforcement learning</p>
<p></p>
<p>Also super thanks to the instructors and the way they drove the conversation! </p>",2019-11-13T15:18:00Z,22,Week 11/10 - 11/16,followup,,jqnu4ff8DlD1,k2xfic3fzox5hm,2019-11-13T15:18:00Z,{},office_hours
3838,no,<p>Thank you for taking the time to answer questions from us! </p>,2019-11-14T06:11:26Z,22,Week 11/10 - 11/16,followup,,jzhy4sbxyar5l4,k2ybfb3db2n189,2019-11-14T06:11:26Z,{},office_hours
3839,no,"<p>Thank you Dr. James, It was great to hear your insights on approach to solve RL problems.Thank you so very much for sharing your valuable knowledge. </p>",2019-11-14T15:44:57Z,22,Week 11/10 - 11/16,followup,,jl2gfssgneuU,k2yvwuuoe6o5oq,2019-11-14T15:44:57Z,{},office_hours
3840,no,<p>Thank you Dr James for the interesting office hours which were laden with enthusiasm for RL!</p>,2019-11-17T05:34:54Z,21,Week 11/17 - 11/23,followup,,jqstjkc1dh1602,k32kfvhfbtj5dx,2019-11-17T05:34:54Z,{},office_hours
3841,stud,"<p><strong attention=""is5gzbotXmz"">&#64;tianhang zhu</strong>  , I got it. The discount factor can be thought of as the probability to terminate, similar to this:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhs489b10i78c%2Fk2xy0kj1v4ex%2Fdiscount.PNG"" alt="""" /></p>",2019-11-13T23:56:48Z,42,Week 11/10 - 11/16,followup,a_0,,k2xy1j15qjv1h4,2019-11-13T23:56:48Z,{},other
3842,no,exactly well done! now you have a new to think about this constant discounting business,2019-11-14T02:49:47Z,42,Week 11/10 - 11/16,feedback,,is5gzbotXmz,k2y47z76j8i35a,2019-11-14T02:49:47Z,{},other
3843,stud,<p>Very cool.</p>,2019-11-14T03:02:06Z,42,Week 11/10 - 11/16,feedback,a_0,,k2y4ntmximp4c4,2019-11-14T03:02:06Z,{},other
3844,stud,"<p>Yep, I read that already. I am wondering if there are any additional resources besides that. :-) </p>",2019-11-11T07:28:49Z,53,Week 11/10 - 11/16,followup,a_0,,k2u3v9j7gp340i,2019-11-11T07:28:49Z,{},other
3845,stud,<p>It&#39;s a 3*3 matrix and I only write the payoff to the row player. The payoff to the column player is the opposite.</p>,2019-11-06T15:18:52Z,42,Week 11/3 - 11/9,followup,a_0,,k2nfghmf2tpnx,2019-11-06T15:18:52Z,{},hw6
3846,no,"<p>Yup, that was a pretty embarrassing mistake on my behalf. I&#39;ll leave it up in case it&#39;s clarifying to anyone else, but totally agree that I was wrong there.</p>",2019-11-08T13:24:36Z,42,Week 11/3 - 11/9,feedback,,jl3oi5v7qkSk,k2q6996p63a3o4,2019-11-08T13:24:36Z,{},hw6
3847,stud,<p>So what is a definition of deterministic? </p>,2019-11-06T16:31:34Z,42,Week 11/3 - 11/9,followup,a_0,,k2ni1zpll582lk,2019-11-06T16:31:34Z,{},hw6
3848,no,"<p>In the video (see Section 4 - &#39;A Simple Game&#39;), he defines deterministic only in terms of the game tree (having no stochastic transitions), which comes before the matrix representation of the game.</p>
<p></p>
<p>If you&#39;re starting with the matrix, I guess you can try to construct a game tree with no stochastic transitions that matches the game matrix.  If you can do that, then it&#39;s a deterministic game.</p>",2019-11-06T17:10:40Z,42,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2njg9plybc47a,2019-11-06T17:10:40Z,{},hw6
3849,stud,"<p>Thanks for your reply. But then the matrix that I provided is deterministic, right? </p>",2019-11-06T17:15:34Z,42,Week 11/3 - 11/9,feedback,a_0,,k2njmkc9att2s2,2019-11-06T17:15:34Z,{},hw6
3850,no,<p>Can you share the game tree you constructed?</p>,2019-11-06T17:38:31Z,42,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2nkg37hpp95xl,2019-11-06T17:38:31Z,{},hw6
3851,stud,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzjw5y0ecos525%2Fk2nkoafjntve%2FWechatIMG6.jpeg"" alt="""" /></p>",2019-11-06T17:45:23Z,42,Week 11/3 - 11/9,feedback,a_0,,k2nkowv7sn62u2,2019-11-06T17:45:23Z,{},hw6
3852,no,"<p>The matrix for that game is much, much bigger than 3x3.</p>
<p></p>
<p>              Player B</p>
<p></p>
<p>     L  L  L  L  L L ... M  M  M  ... R  R  R ....</p>
<p>     L  L L  M M M .....</p>
<p>     L M R L  M R .....</p>
<p></p>
<p>L</p>
<p>M</p>
<p>R</p>
<p></p>
<p>^</p>
<p>|</p>
<p>Player A</p>",2019-11-06T17:50:30Z,42,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2nkvhpztwu1x6,2019-11-06T17:50:30Z,{},hw6
3853,stud,<p>I see... Understand now. Thanks!</p>,2019-11-06T17:54:57Z,42,Week 11/3 - 11/9,feedback,a_0,,k2nl17pmjr71yp,2019-11-06T17:54:57Z,{},hw6
3854,no,"<p>Hey, let me know if you can construct a deterministic game tree for your matrix.  I&#39;m guessing it&#39;s not possible.</p>",2019-11-06T18:00:39Z,42,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2nl8ji8bc633i,2019-11-06T18:00:39Z,{},hw6
3855,no,"I should be doing an overview tomorrow in office hours... But, I have not tried out the modeling thing. Sounds like I might have to update the overview.",2019-11-07T00:53:05Z,26,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2nzyxosa047mt,2019-11-07T00:53:05Z,{},hw6
3856,no,"<p>I&#39;ve been using cvxpy, solving that same problem with both libraries yields:</p>
<p></p>
<pre>from cvxopt.modeling import variable, op<br /><br />x = variable()<br />y = variable()<br />c1 = (2*x &#43; y &lt;= 3)<br />c2 = (x &#43; 2*y &lt;= 3)<br />c3 = (x &gt;= 0)<br />c4 = (y &gt;= 0)<br />lp1 = op(-4*x - 5*y, [c1,c2,c3,c4])<br />lp1.solve()<br />print(&#34;X&#34;,x.value)<br />print(&#34;Y&#34;,x.value)<br />print(&#34;Obj&#34;,lp1.objective.value())</pre>
<p></p>
<pre>import cvxpy as cp

x = cp.Variable(1)
y = cp.Variable(1)

c1 = (2*x &#43; y &lt;= 3)
c2 = (x &#43; 2*y &lt;= 3)
c3 = (x &gt;= 0)
c4 = (y &gt;= 0)


obj = cp.Minimize(-4*x - 5*y)
prob = cp.Problem(obj, [c1,c2,c3,c4])
prob.solve()
print(&#34;X&#34;,x.value)
print(&#34;Y&#34;,y.value)
print(&#34;Obj&#34;,obj.value)
</pre>
<p></p>",2019-11-07T01:21:57Z,26,Week 11/3 - 11/9,followup,,jzhl7qlwrpagr,k2o1020dsbu5zz,2019-11-07T01:21:57Z,{},hw6
3857,no,<p>The similarity is striking. The cvxopt modeling guide references cvxpy so I guess the former is based on the latter.</p>,2019-11-07T03:43:38Z,26,Week 11/3 - 11/9,feedback,,is8ald0uljj3u4,k2o629tb81s7pq,2019-11-07T03:43:38Z,{},hw6
3858,no,"<p>Wow, that is far more readable than what I did. I constructed the matrix elements element wise based on the reward matrix which makes for unreadable code.</p>",2019-11-09T06:03:02Z,26,Week 11/3 - 11/9,feedback,,jqstjkc1dh1602,k2r5x8wtf6b1tc,2019-11-09T06:03:02Z,{},hw6
3859,no,"<p>Tried cvxopt and cvxpy and I&#39;m running into issues where both return negative probabilities, despite a constraint that probabilities must be greater or equal to zero.  ( p &gt;= 0 )</p>
<p></p>
<p>Tried the default CVXOPT solver and GLPK and both run into the same issue. Anyone else experience this?</p>",2019-11-07T02:36:26Z,26,Week 11/3 - 11/9,followup,,isde332xcka1m0,k2o3nuyagsb3i1,2019-11-07T02:36:26Z,{},hw6
3860,no,"<p>Update: solved HW6 with 100% via CVXOPT, but project3 using a similar minimax approach sometimes returns probability values below 0 or above 1 despite constraints against these... Any insights what may be happening?</p>",2019-11-07T03:11:51Z,26,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2o4xemb9of2c1,2019-11-07T03:11:51Z,{},hw6
3861,stud,<p>I am having trouble with negative values as well. How did you solve it?</p>,2019-11-08T06:20:10Z,26,Week 11/3 - 11/9,feedback,a_0,,k2pr3fmqd3u4k0,2019-11-08T06:20:10Z,{},hw6
3862,no,"<p>I mostly resolved the issue by finding a bug in my implementation of the environment that allowed a player to leave the soccer field and never return. Added unit tests and state validation for each transition to ensure all states are valid, and now the solver produces stable results 99.999%&#43; of the time. Yesterday it still ran into one issue with probabilities not summing to one despite a constraint forcing it, I left the thread unresolved to see if anyone might have ideas. I think in the current case one solution might be manually normalizing probability values that in rare cases sum to something just off 1.</p>",2019-11-08T14:52:14Z,26,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2q9dy45v4q63c,2019-11-08T14:52:14Z,{},hw6
3863,stud,"<p>Thanks Michael, sorry I meant to clarify that I am getting negatives for the HW6--I tried dividing the constraints to have max coefficient of 1, but I get probabilities not summing to 1. How did you resolve for hw6?</p>",2019-11-08T15:58:28Z,26,Week 11/3 - 11/9,feedback,a_0,,k2qbr45kjmf7jw,2019-11-08T15:58:28Z,{},hw6
3864,no,<p>Never experienced any issues on hw6. Check out https://econweb.ucsd.edu/~jsobel/172aw02/notes9.pdf pages 6-11. I didn&#39;t come to the class with any prior experience in linear programming and that PDF was most helpful to me.</p>,2019-11-08T16:07:13Z,26,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2qc2dodlo62wq,2019-11-08T16:07:13Z,{},hw6
3865,no,"<p>In homework 6, I didn&#39;t see negative numbers even though I did not explicitly constrain the probabilities to be positive. I started seeing negatives yesterday when I got to implement foe-Q. I added the nonnegative constraints and still got negative values, then reviewed the G matrix again and realized I represented the constraints incorrectly. Hope this helps someone.</p>",2019-11-14T17:14:03Z,25,Week 11/10 - 11/16,feedback,,is8ald0uljj3u4,k2yz3fytj3k6pg,2019-11-14T17:14:03Z,{},hw6
3866,no,"<p>I removed stealing and added stick priority to the algorithm and the results changed a little bit. QQ changed by removing any draws in 200,000 episodes, but the convergence is very slow and the error calculation is questionable.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2p0isewjz3x%2Fexp123.png"" alt="""" /></p>",2019-11-07T17:56:42Z,54,Week 11/3 - 11/9,followup,,jc554vxmyuy3pt,k2p0jbird7s4,2019-11-07T17:56:42Z,{},project3
3867,no,<p>Thanks for sharing these insights. Can you clarify what it means to add stick priority?</p>,2019-11-07T18:48:14Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2p2dll0qnh619,2019-11-07T18:48:14Z,{},project3
3868,no,"<p>Say A&#39;s action is stick. B is E, which runs into A. B moves first, there&#39;s a collision. A should not move, but it is already not moving. Thus the priority changes and B can&#39;t move into A on a stick. (Credit for this condition goes to Sergei who brought it up on Slack)</p>
<p></p>
<p></p>",2019-11-07T18:56:28Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2p2o6n4ns43ra,2019-11-07T18:56:28Z,{},project3
3869,no,"<p>Not sure I follow. If I understand correctly, when you removed stealing, you mean the ball can only change possession when the player with the ball moves into a square occupied by the player without the ball. (as opposed to also being possible when the player without the ball moves towards the position of the player with the ball).  In your example with A sticking and B moving east, who has the ball initially and does it shift possession? What&#39;s the purpose of adjusting priority?</p>",2019-11-07T23:57:14Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2pdeywb9fo4g,2019-11-07T23:57:14Z,{},project3
3870,no,"<p>Correct. You can only change possession if you have the ball and you move into the other player. The non-possessing player can not run into you and steal the ball.</p>
<p></p>
<p>Sticking priority is important. If the priority player moves and you stick, then you should move back to your original position and the priority player keeps their move. That would result in two players in the same cell. That&#39;s an invalid state, so the sticking player must take over priority and the opposing player returns to their original position.</p>",2019-11-08T00:19:25Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2pe7iaktz15wz,2019-11-08T00:19:25Z,{},project3
3871,no,"<p>Ah, think I got it. Sounds like you&#39;re describing the player whose action is executed first as the priority player. In my implementation, any player who sticks never changes position (or has it changed by the other player), they simply stay in the original spot. If the first mover (priority player) tries moving into a sticking player, the mover stays in the same spot and ball possession transitions. If the first mover (without the ball) moves into an empty spot, and the second mover (with the ball) moves into the newly occupied spot, ball possession transitions, the first mover keeps their new spot (they got there first), and the second mover remains in their initial spot (they tried moving to an occupied position).  If I understand correctly, the environment transitions to the same resulting states as you&#39;re describing.</p>",2019-11-08T00:48:15Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2pf8kj2feq6bp,2019-11-08T00:48:15Z,{},project3
3872,no,"<p>Yep, mostly. I had done what you were describing about the ball transition en passant. I didn&#39;t think that was the intended action according to the description in Littman and Greenwald. I didn&#39;t find any real difference between these except that one player had more likelihood for winning when they had possession (in my current method). In your method, the likelihood of possession is more balanced.</p>",2019-11-08T01:17:45Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2pgai7zw0e444,2019-11-08T01:17:45Z,{},project3
3873,no,"<p>Here is the heatmap from the SARSA experiment. You can see the clear evidence of convergence.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2pd7bwqvul%2Fexp1_heatmapshare.png"" alt="""" /></p>",2019-11-07T23:51:33Z,54,Week 11/3 - 11/9,followup,,jc554vxmyuy3pt,k2pd7nlxv8kdo,2019-11-07T23:51:33Z,{},project3
3874,no,<p>Elegant way to visualize the policies learned -- just beautiful! I feel bad for students in past/future semesters who don&#39;t get to see these kinds of creative ways to analyze agent performance. :-)</p>,2019-11-07T23:58:58Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2pdh72elrl1ah,2019-11-07T23:58:58Z,{},project3
3875,no,"<p>Thanks! When this is over, I&#39;ll ask to share the heatmap code and such.</p>
<p></p>
<p>This picture really makes me happy. It refutes the claim in that 2003 paper that Q didn&#39;t work (*pshaw!), but it also gives some credibility to training two separate Q states for multiagent games. I have a theory as to why this works on Turing machines, but shouldn&#39;t work mathematically (as Littman mentions in his papers)</p>",2019-11-08T00:02:29Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2pdlq5r5qj78y,2019-11-08T00:02:29Z,{},project3
3876,no,"<p>Don&#39;t want to detract from the great insights from these policy visualizations, though both &#39;Correlated-Q Learning&#39; papers mention: &#34;Q-learners compute Q-values for each of their own possible actions, ignoring their opponents&#39; actions.&#34;  -- it appears the heatmap above is implemented with both agents aware of the other agents&#39;s actions, so the methodology is slightly different from the original. Regardless, these multi-agent SARSA policy visualizations are valuable lessons learned in any case.</p>
<p></p>
<p>On the plus side, adjusting the Q implementation slightly so each agent ignores actions of the other might help hone in on a closer replication of figure 3(d) from the paper.</p>",2019-11-08T00:13:33Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2pdzyl4leu6no,2019-11-08T00:13:33Z,{},project3
3877,no,<p>Good point. I wonder if it will have any affect.</p>,2019-11-08T00:17:03Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2pe4g3rjwr34i,2019-11-08T00:17:03Z,{},project3
3878,no,"<p>Here ya go. SARSA in blind mode.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2pg3poh50bi%2Fexp6.png"" alt="""" /></p>",2019-11-08T01:12:34Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2pg3uzqth6cz,2019-11-08T01:12:34Z,{},project3
3879,no,"<p>The symmetry is indeed compelling. Looking at their rationale against convergence from the 2003 paper: &#34;In this soccer game, Q-learning does not converge. Intuitively, the rationale for this outcome is clear: Q-learning seeks deterministic optimal strategies, but in this game no such policies exist.&#34;</p>
<p></p>
<p>Thinking about how your results make sense while Greenwald &amp; Hall may also have a point: perhaps many states converge to optimal pure strategies (e.g. when you have the ball and are 1 step from scoring with no chance of the ball being blocked or stolen, you know exactly what to do and the Q-value there will never diverge), this could explain some of the symmetry in the policy visuals. On the other hand, some game states (e.g. figure 4&#39;s state &#39;s&#39; where player B has possession and needs to pass player A for a chance at scoring) may require a mixed strategy to play optimally. e.g. if player B always does the same move in some situation, player A may learn to counter it and shift the Q-values, causing player B to shift their Q-values to counter it, causing player A to shift (etc, etc.).  Following up on discussion from slack, maybe this is part of why the evaluation metric chosen is specific to the individual Q-value at that state &#34;s&#34; where having a mixed strategy is most relevant.</p>
<p></p>
<p>Again, grateful for the insightful visualizations. :-)</p>",2019-11-08T01:32:53Z,54,Week 11/3 - 11/9,feedback,,isde332xcka1m0,k2pgtyztzdgkm,2019-11-08T01:32:53Z,{},project3
3880,no,"Hi Oscar, my current implementation is using case 3. The player with the ball goes first but since there is no space, he will not move but the ball will be given to the other player. The other player then move south. <div><br /></div><div>However I am confused in the case where if both players were to both towards each other, what could be the possible scenario? </div><div><br /></div><div>A moving right and *B* moving left</div><div><br /></div><div>In this case if B goes first, then the ball will be passed to A. Then will A move? Or simply the turn ends with both in place and A now having the ball </div><div><br /></div><div><br /></div><div>Alternatively...</div><div>Unless the logic is that both player and moving simultaneously, hence they will brush pass each other into their new position. </div><div><br /></div><div>The only collision to consider is if they move into the same cell simultaneously. But if so then case 3 is wrong and they will again brush pass each other. Ball possession will remain with B in your example.</div><div><br /></div><div>But I feel this seems to make the choice of who go first redundant. </div><div><br /></div><div>With this logic; I will go with case 3, where we process each player’s move 1 by 1</div><p></p><p></p><p></p><p></p>",2019-11-07T13:13:52Z,54,Week 11/3 - 11/9,followup,,jvfpllmsggt7p4,k2oqfl9q6ub2km,2019-11-07T13:13:52Z,{},project3
3881,stud,<p>I just want to reconfirm from the TA&#39;s that it is OK to use this above environment (<em><strong>after making sure it&#39;s correctness and citing it appropriately</strong></em>) just as we would use one of the Gym environments? </p>,2019-11-11T14:55:59Z,27,Week 11/10 - 11/16,followup,a_1,,k2ujubp2bfs2da,2019-11-11T14:55:59Z,{},project3
3882,no,"<p>This is marked as a good note by an instructor. So, you should be free to use it.</p>",2019-11-12T00:10:20Z,27,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2v3n8fdan9614,2019-11-12T00:10:20Z,{},project3
3883,no,<p>Yes.  You can use it but like the post says.  It is your responsibility to make sure it is correct.</p>,2019-11-12T01:20:36Z,27,Week 11/10 - 11/16,feedback,,hz7meu55mi8sd,k2v65l7w8wt20b,2019-11-12T01:20:36Z,{},project3
3884,no,"<p>Or, your definition of &#34;correct.&#34; The game is not fully explained in the Greenwald 2003 paper, so you have some latitude....</p>",2019-11-12T15:02:50Z,27,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2vzizs8op377y,2019-11-12T15:02:50Z,{},project3
3885,no,"<p>I was exploring the above environment and realized that the initial state seems to be random.That doesn&#39;t seem correct to me, so either it&#39;s a bug or I&#39;m missing something with the below code.</p>
<p></p>
<p>Anyone else having the same issue?</p>
<p></p>
<p>Code:</p>
<p></p>
<pre>    env = SoccerEnv()<br />    env.reset()<br />    env.render()</pre>
<p></p>
<p>Outputs:</p>
<pre>0 3 0 2 0
&#43;----&#43;
|  Ba|
|    |
&#43;----&#43;<br /><br />0 1 0 2 1
&#43;----&#43;
| Ab |
|    |
&#43;----&#43;<br /><br />1 0 1 2 0
&#43;----&#43;
|    |
|a B |
&#43;----&#43;</pre>
<p></p>
<p></p>",2019-11-12T15:02:59Z,27,Week 11/10 - 11/16,followup,,ijctp4ucNy8,k2vzj6o83ej4r4,2019-11-12T15:02:59Z,{},project3
3886,no,"<p>I am not using that env, but the Greenwald 2003 paper does not suggest random starts. I implemented both kinds of starting (random and deterministic) and didn&#39;t see much difference. Then again, I have not reproduced any of the graphs yet.</p>",2019-11-12T15:04:16Z,27,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2vzkts27ai5rh,2019-11-12T15:04:16Z,{},project3
3887,no,"<p>&#64;Jacob</p>
<p>Agreed on the part about the paper not having suggested random starts. Precisely why I&#39;m flagging this as a potential issue.</p>",2019-11-12T15:15:18Z,27,Week 11/10 - 11/16,feedback,,ijctp4ucNy8,k2vzz124o5o477,2019-11-12T15:15:18Z,{},project3
3888,no,"<p>What does have influence, though, is if A possesses the ball at first. The possessor of the ball is highly biased towards winning in this soccer game.</p>",2019-11-12T15:29:38Z,27,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2w0hfz1vi349s,2019-11-12T15:29:38Z,{},project3
3889,stud,<p>look into encoding the player&#39;s positions using the env&#39;s encode_state method. Then you can set your starting state to whatever position you would like instead of letting be random</p>,2019-11-13T02:26:23Z,27,Week 11/10 - 11/16,feedback,a_0,,k2wny1nyxra7bj,2019-11-13T02:26:23Z,{},project3
3890,stud,"<p>the position you wanted might be player A to be in 0,2 while player B in 0,1 with player A&#39;s posession to be 0</p>",2019-11-13T02:27:53Z,27,Week 11/10 - 11/16,feedback,a_0,,k2wnzz7ei8o2ng,2019-11-13T02:27:53Z,{},project3
3891,no,"<p>I am trying the  env encode_state and I keep getting random setups. </p>
<p></p>
<p>env = SoccerEnv() <br />env.encode_state(0,1,0,2,0)<br />env.render()</p>
<p></p>
<p>am I doing something wrong &#64;anon</p>",2019-11-14T18:22:24Z,27,Week 11/10 - 11/16,feedback,,jc6mqevhagl262,k2z1jc5lvp17ds,2019-11-14T18:22:24Z,{},project3
3892,stud,"<p>For most people, we have to encode the states in some manner to map it from 2 player&#39;s state to a unique state of either 112 / 128. The encode method simply returns the encode state ranging from 1 to 128 (check the source code). You will have to change alter the env&#39;s state method</p>
<pre>env.s = new_state_encode</pre>
<p></p>",2019-11-15T03:39:56Z,27,Week 11/10 - 11/16,feedback,a_0,,k2zlgc49hu169,2019-11-15T03:39:56Z,{},project3
3893,no,"<p></p>
<p>&#64;Andrew</p>
<p>I replied to you in piazza. Reposting here for the benefits of others as well. </p>
<p></p>
<p>This is a simple hack to specify a starting state instead of randomized. Basically the part that governs the initial state is below. &#34;isd&#34; is a vector (len=num states) that controls the randomness. Simply set this vector such that isd[state] = 1. for the starting state you want, and &#39;0.&#39; elsewhere. </p>
<p></p>
<pre>        isd /= isd.sum()<br />        discrete.DiscreteEnv.__init__(self, num_states, num_actions, P, isd)</pre>
<p></p>",2019-11-15T04:24:38Z,27,Week 11/10 - 11/16,feedback,,ijctp4ucNy8,k2zn1t4f9y6232,2019-11-15T04:24:38Z,{},project3
3894,no,<p>I got state 17 that matched the paper&#39;s config</p>,2019-11-16T19:11:05Z,27,Week 11/10 - 11/16,feedback,,jl33gy0yBKzn,k31y5n98rxd2av,2019-11-16T19:11:05Z,{},project3
3895,no,"<p>Hi Folks,</p>
<p></p>
<p>How do we get states as 112 here ?</p>
<p></p>
<p>TIA</p>",2019-11-18T06:32:17Z,26,Week 11/17 - 11/23,feedback,,jfzaqnqvtQ1m,k341xiuz8j15qm,2019-11-18T06:32:17Z,{},project3
3896,no,"<p>&#64;Anurag</p>
<p></p>
<p>simply modify the state after the env.reset()</p>
<p>env.s = 112  #for instance</p>",2019-11-19T01:30:05Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k356kr2ty894ki,2019-11-19T01:30:05Z,{},project3
3897,no,<p>Thanks Jean.</p>,2019-11-19T06:24:13Z,26,Week 11/17 - 11/23,feedback,,jfzaqnqvtQ1m,k35h30kbbp211x,2019-11-19T06:24:13Z,{},project3
3898,no,"<p>&#64;Jean</p>
<p></p>
<p>Can someone explain state 112? This is not the starting state of the paper is it?</p>
<p></p>
<p>When I do:</p>
<pre>env = SoccerEnv()
env.reset()
env.s = 112
env.render()</pre>
<p></p>
<p>I get:</p>
<p></p>
<pre>(Pdb) env.render()
1 2 0 0 1
&#43;----&#43;
|b   |
|  A |
&#43;----&#43;
</pre>
<p>Shouldn&#39;t env.s = 17 should be the starting state?</p>",2019-11-22T19:53:00Z,26,Week 11/17 - 11/23,feedback,,gx3c8l7z7r72zl,k3akanyuz5d2bv,2019-11-22T19:53:00Z,{},project3
3899,no,"<p>&#64;Aaron... I wrote 112 (#FOR INSTANCE), it could have written 999... it&#39;s up to you to decide.  I&#39;m not using that environment, so I didn&#39;t look under the hood too much.  </p>",2019-11-22T21:13:04Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k3an5mwnbn6m6,2019-11-22T21:13:04Z,{},project3
3900,no,"<p>Thanks Jean!</p>
<p></p>
<p>I am bit confused about how to create Q table, weather it should 128 (total states) x 25 (total actions of both players) or 128 (total states) x 5 (total actions of one player)?</p>
<p></p>
<p>Any thoughts anyone?</p>",2019-11-22T23:38:34Z,26,Week 11/17 - 11/23,feedback,,gx3c8l7z7r72zl,k3ascqo8s9g52d,2019-11-22T23:38:34Z,{},project3
3901,no,"<p>I use a np.array with 5 dimensions for posA, posB, ball, actionA, actionB... if you aggregate all the states in one variable, then you&#39;re constantly multiplying the positions b8 &#43; who has the ball to access the right cell corresponding to that state in Q.  And same for the actions.  </p>
<p>You can ignore B&#39;s actions for Q learning when the player does not need to take into account what the other player does, but I&#39;m not a reliable source on that, so I prefer to say nothing here.  </p>
<p>It&#39;s probably been discussed somewhere.</p>",2019-11-23T00:55:38Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k3av3v1u1qq294,2019-11-23T00:55:38Z,{},project3
3902,no,"<p>Find what state is encoded for (0,2,0,1,0). This state is the starting state according to the paper. </p>",2019-11-23T01:48:00Z,26,Week 11/17 - 11/23,feedback,,hzoi2qsuCAd,k3awz6z6xws6eh,2019-11-23T01:48:00Z,{},project3
3903,no,"<p>As a followup, is it fine to include the source code from this environment in the project we will be turning in, and is it alright to also modify the environment source code for use in our project?</p>
<p></p>
<p>Or are we expected to import the env through a pip dependency?</p>",2019-11-17T11:45:33Z,26,Week 11/17 - 11/23,followup,,jccyemecUB8q,k32xojfyvnd3z9,2019-11-17T11:45:33Z,{},project3
3904,no,"<p>since the TA&#39;s allowed us to use it, it means it doesn&#39;t count in the grading, so I&#39;d say we do whatever we want.</p>",2019-11-19T01:32:05Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k356nbn1uz79i,2019-11-19T01:32:05Z,{},project3
3905,no,"<p>Is anyone using this environment? without modifications?</p>
<p>Does it behave correctly?</p>",2019-11-19T01:33:31Z,26,Week 11/17 - 11/23,followup,,jzh6k6o994a6dh,k356p5r68cw2l8,2019-11-19T01:33:31Z,{},project3
3906,no,<p>Correctly? Define that. Many students appear to be using this environment. I wrote my own.</p>,2019-11-20T17:42:39Z,26,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k37krbxl7kj1b7,2019-11-20T17:42:39Z,{},project3
3907,no,"<p>Hi &#64;Jacob... I started trying an environment I downloaded, but then, to be sure it was working &#39;correctly&#39;, I went through it with a fine toothed comb and had to make changes, and it forced me to look into how it is supposed to work.</p>
<p>Honestly, I didn&#39;t see two different interpretations when you read slowly how Greenwald describes it.</p>
<p>Can you give me an example of a situation that is unclear?</p>",2019-11-20T17:48:50Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k37kz9u81u32ol,2019-11-20T17:48:50Z,{},project3
3908,no,"<p>Possession changes are the principal point of confusion. Is a collision <em>en passant</em> or do you have to land on the same cell after movement resolution?</p>
<p></p>
<p>Then the resolution of stick/move where the subordinate player sticks and a collision occurs, where does the subordinate go?</p>
<p></p>
<p>How is movement resolved? A moves into B&#39;s spot, B moves into a new spot, and B is subordinate, does that mean A collides with B before B moves on? Is this really a turn-based game, or is it simultaneous?</p>
<p></p>
<p></p>",2019-11-20T18:18:14Z,26,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k37m1337h7u53p,2019-11-20T18:18:14Z,{},project3
3909,no,"<p>I kinda knew you&#39;d raise issues like that but the text is quite clear to me: first player moves, and if there&#39;s no collision, then and only then player 2 moves, so there can&#39;t be collisions &#39;en passant&#39;.</p>
<p>They have removed all notions of simultaneity to make it easy to simulate I think.  </p>
<p>The ball passing rule is also very clear: it happens only when a player with the ball runs into the other, which makes it super simple to code since we can decide &#39;the player who&#39;s run into always gets the ball&#39;.</p>
<p>No need to test anything: if he already has it, then he keeps it.  </p>
<p></p>
<p>Anyway, does it make any difference? The results seem general so the should represent not only this game with only one set of rules, no?</p>",2019-11-20T21:54:22Z,26,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k37tr12ily11f2,2019-11-20T21:54:22Z,{},project3
3910,no,"<p>The only difference was when my A never got the ball because of ball passing. You will see results but they will be misleading. Convergence, scaling errors, but things that kinda make sense.</p>",2019-11-20T23:51:59Z,26,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k37xyaxhn814ie,2019-11-20T23:51:59Z,{},project3
3911,no,"<p>jwa - &#34; Possession changes are the principal point of confusion. Is a collision <em>en passant</em> or do you have to land on the same cell after movement resolution?&#34;</p>
<p></p>
<p>ha! :) excellent adaptation of a chess term ;) - it is ironic, that the emphasis on &#34;simultaneity&#34; of action in the paper, devolves into a turn-based game given the randomness of selection (action) with respect to first action (on ball steals).</p>",2019-11-21T22:41:41Z,26,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k39avqhshis3jd,2019-11-21T22:41:41Z,{},project3
3912,no,<p>N/A</p>,2019-11-21T23:05:20Z,26,Week 11/17 - 11/23,feedback,,hzoi2qsuCAd,k39bq5uf5ut2s7,2019-11-21T23:05:20Z,{},project3
3913,no,"<p>Hi all,</p>
<p></p>
<p>Didi anyone see any noticeable issues with the possession scheme with this environment?</p>",2019-11-23T19:41:54Z,26,Week 11/17 - 11/23,feedback,,gx3c8l7z7r72zl,k3bzc8bymcm35l,2019-11-23T19:41:54Z,{},project3
3914,no,"<p>HI all,</p>
<p></p>
<p>Are we supposed to use env.step() or env.transitions() to apply actions in this environment?</p>",2019-11-23T23:07:35Z,26,Week 11/17 - 11/23,followup,,gx3c8l7z7r72zl,k3c6or2v4qn7od,2019-11-23T23:07:35Z,{},project3
3915,no,"<p>Thanks Todd! At least I&#39;m not alone in the confusion.</p>
<p></p>
<p>In Friend-Q, are we supposed to use the same action selection as in classic Q? Rather, if we follow what Foe-Q does, then wouldn&#39;t we select an action based upon the PI(s,.) for the Friend-Q, which would be a uniform distribution? This would mean we are choosing a random action based upon the PI(s,.) PD, e.g. PD(A)?</p>",2019-11-07T23:03:01Z,54,Week 11/3 - 11/9,followup,,jc554vxmyuy3pt,k2pbh8owsne55f,2019-11-07T23:03:01Z,{},project3
3916,no,"<p>Also, in Littman&#39;s algorithm given, V(s&#39;) is the min(o&#39;, sum(a&#39;, pi * Q)), but the FFQ paper shows the NASH value (V) as max(min(sum())) .... so which is it ? MAX or MIN?</p>
<p></p>
<p></p>",2019-11-08T17:37:15Z,54,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2qfa64v3r533g,2019-11-08T17:37:15Z,{},project3
3917,no,"<p>Hey Jacob,</p>
<p></p>
<p>Did you wind up figuring this out? I&#39;m also a little hung up on the point of notation.</p>",2019-11-11T02:15:17Z,53,Week 11/10 - 11/16,followup,,jl3we43d3bp15p,k2tso28miyy6l8,2019-11-11T02:15:17Z,{},project3
3918,no,"<p>I did not figure it out. I am just using .max() on the V calculations to get the V value for [s,a]. I don&#39;t know where this argmax deal comes into play. Maybe it&#39;s a typo. The equations i am using from the papers (Littman 94, Greenwald 2003) both show maxmin as the selection criteria for V.</p>
<p></p>",2019-11-12T15:00:49Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2vzge7p7461rf,2019-11-12T15:00:49Z,{},project3
3919,no,"<p>When they have pi[s,.] does that mean the probability vector for all actions given the state? The arg max also throws me off, since then it would just be the index correct? 0-4?</p>",2019-11-11T02:52:34Z,53,Week 11/10 - 11/16,followup,,ixty1midfufhd,k2tu008vxer10h,2019-11-11T02:52:34Z,{},project3
3920,no,<p>Yup this is what I&#39;m stuck in too. </p>,2019-11-11T17:21:24Z,53,Week 11/10 - 11/16,feedback,,jl3we43d3bp15p,k2up1ceasnx3fk,2019-11-11T17:21:24Z,{},project3
3921,no,"<p>Yeah, the index doesn&#39;t make sense. gamma * index_in_V doesn&#39;t make sense in any interpretation of the equations. It has to be a typo.</p>",2019-11-12T15:01:42Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2vzhj02c6v5fq,2019-11-12T15:01:42Z,{},project3
3922,no,"<p>There&#39;s no mistake (I think there actually is a typo, but it&#39;s a minor one and I&#39;ll mention it at the end).  Take a look here:</p>
<p></p>
<p>$$\max_{\pi&#39;}\Big{(}\min_{o&#39;}\sum_{a&#39;}\pi&#39;(s,a&#39;)Q(s,a&#39;,o&#39;)\Big{)}$$</p>
<p></p>
<p>I&#39;ve removed the &#34;arg&#34; from the &#34;argmax&#34; to help explain what&#39;s going on.</p>
<p></p>
<p>Working from the inside out, we have, for a fixed mixed strategy $$\pi&#39;$$ from the agent (and remember that mixed strategy is synonymous with distribution over the agent&#39;s action space), the opponent minimizes the agent&#39;s payoff by choosing the worst action for the agent: $$o&#39;$$.  The quantity inside the large parentheses is then going to be the minimum payoff <strong>value</strong> to the agent.</p>
<p></p>
<p>The agent counters this by <em>maximizing</em> this quantity over his mixed strategies.  In other words, he finds the best mixed strategy (distribution over his action space), i.e. the one that maximizes the entire quantity in the expression.  That entire quantity is, again a <strong>value</strong>.</p>
<p></p>
<p>The particular mixed strategy that maximizes this value is the $$\text{arg}\max$$ in question, and is a mixed strategy, i.e., a vector of probabilities over the agent&#39;s action space, i.e., a distribution over his action space.</p>
<p></p>
<p>The typo I was talking about is that the inner pi[s,a] in the expression should be pi&#39;[s,a], the quantity that is being maximized over.  I.e., I think there&#39;s a prime (&#39;) missing.</p>",2019-11-12T16:51:33Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2w3et11ill1hq,2019-11-12T16:51:33Z,{},project3
3923,no,"<p>Ahh, I see now. <a href=""https://en.wikipedia.org/wiki/Arg_max"">https://en.wikipedia.org/wiki/Arg_max</a> If we forget the &#34;numpy.argmax&#34; that I am focusing on, and instead use the discrete math definition of argmax, then it makes more sense.</p>
<p></p>
<p>Thanks Vahe!</p>",2019-11-12T22:25:33Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wfcbzmcp3114,2019-11-12T22:25:33Z,{},project3
3924,no,"<p>Ah, yea, I was also thinking numpy argmax.... since we used that in previous homework... Thanks.</p>",2019-11-14T02:48:41Z,53,Week 11/10 - 11/16,feedback,,ixty1midfufhd,k2y46klwvhc4rh,2019-11-14T02:48:41Z,{},project3
3925,no,"<p>This could be a zero sum reward strategy if player A got a penalty for time and the opposite reward was granted to the opponent.</p>
<p></p>
<p>Zero sum is always Player A get R and Player B gets -R.</p>",2019-11-08T13:41:38Z,54,Week 11/3 - 11/9,followup,,jc554vxmyuy3pt,k2q6v5q8q0c461,2019-11-08T13:41:38Z,{},other
3926,no,<p>interesting question ben.</p>,2019-11-15T12:47:36Z,53,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k3050mltffs44q,2019-11-15T12:47:36Z,{},other
3927,no,Also there is a stickied post with links to all the office hours &#64;132.<div><br /></div>,2019-11-09T21:27:20Z,54,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2s2xw7fuqz3k6,2019-11-09T21:27:20Z,{},office_hours
3928,no,"<p>I think (Littman 1994) describes it in sufficient detail. The players first select their actions, which are then performed <strong>sequentially</strong> at random. If a player&#39;s action results in a move to another player&#39;s location at that moment, then the move does not happen; further, if the player had the ball, then the ball passes to the other player. The ball can thus pass more than once in the same sequence and the outcome from identical game state and action selections can vary.</p>
<p></p>
<p>Greenwald-Hald explicitly allow scoring into either goal. If you score in your own goal the reward is &#43;100, otherwise -100.</p>",2019-11-10T18:02:30Z,53,Week 11/10 - 11/16,followup,,is8ald0uljj3u4,k2tb2c46fl87ed,2019-11-10T18:02:30Z,{},project3
3929,no,"<p>Also, in Littman 1994, it mentions &#34;the board is reset to the configuration shown in the left half of the figure&#34;, implying the players are not initialized randomly. In Greenwald 2003, he did not mention how the board is initialized, but the Figure 3 that we suppose to replicate was done on the state $$s$$ of Figure 4.</p>",2019-11-12T08:32:03Z,53,Week 11/10 - 11/16,feedback,,jl5wq8mca7o0,k2vlkfr650g4kf,2019-11-12T08:32:03Z,{},project3
3930,no,"<p>Wow, nice catch.</p>
<p></p>
<p>You should do the same experiment on graphs (e) and (f), and then again on graphs (g), (h), and (i), in Figure 6 of their 2007 paper.</p>",2019-11-08T23:17:49Z,45,Week 11/3 - 11/9,followup,,jzfsa4a37jf4aq,k2qrg4kczqx4vk,2019-11-08T23:17:49Z,{},project3
3931,no,"<p>ha! I can bet they all have the same error. I want to believe that she mistakenly copied the wrong figure. Otherwise, Corr-Q is just Foe-Q, and all of that math and analysis was for nothing. Michael Littman still rules the Q.</p>
<p></p>
<p></p>",2019-11-08T23:20:51Z,45,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2qrk177en872,2019-11-08T23:20:51Z,{},project3
3932,no,<p>But how can you copy the wrong graph four times across two papers?  Seems sloppy.</p>,2019-11-08T23:21:58Z,45,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2qrlgqhzua4d0,2019-11-08T23:21:58Z,{},project3
3933,no,"<p>Copy and paste errors. I looked at the 2007 paper from Michael via slack. The quality of that paper is very poor, it only has embedded TIF encoded images, so I can&#39;t do much with scaling there. The graphs appear to be the same as the 2003 paper, so I would presume that they are the same.</p>
<p></p>
<p>Yet, the 2005 paper stands out as a vastly superior rendition of this work. So I am going to have to give Amy Greenwald the benefit of the doubt and assume that she meant to include a different graph, but Keith Hall messed it up. :)</p>",2019-11-08T23:28:40Z,45,Week 11/3 - 11/9,feedback,,jc554vxmyuy3pt,k2qru2y62g43rj,2019-11-08T23:28:40Z,{},project3
3934,no,<p>the plots being the same is not an error.</p>,2019-11-09T03:41:53Z,45,Week 11/3 - 11/9,feedback,,jl1acpoc4HA9,k2r0vq0jn1i2dv,2019-11-09T03:41:53Z,{},project3
3935,no,<p>That makes more sense...  Time to figure out what is going on.</p>,2019-11-09T03:45:14Z,45,Week 11/3 - 11/9,feedback,,jzfsa4a37jf4aq,k2r101caugz6of,2019-11-09T03:45:14Z,{},project3
3936,no,"<p>The Correlated Q-Learning (2003) paper specifically states, &#34;Moreover, CE-Q learns Q-Values (and policies) that coincide exactly with those of foe-Q.&#34; This was what justified why a and b were identical, and likely not a copy paste error, at least in my understanding.</p>",2019-11-09T04:18:10Z,45,Week 11/3 - 11/9,feedback,,ixty1midfufhd,k2r26dseir741u,2019-11-09T04:18:10Z,{},project3
3937,no,"<p>I guess this shows that they were using random number seeds then, to get such similar graphs.</p>",2019-11-09T11:03:00Z,45,Week 11/3 - 11/9,feedback,,jl3oi5v7qkSk,k2rgn0ff8ogj9,2019-11-09T11:03:00Z,{},project3
3938,no,Yeah... not a mistake.<div><br /></div>,2019-11-09T21:32:24Z,45,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2s34fevb8m1m2,2019-11-09T21:32:24Z,{},project3
3939,no,"<p>It that is true, then we can equate them</p>
<p></p>
<p>$$\sum_{\bar{a}\in{A}}\sigma(\bar{a})Q(s,\bar{a}) = max_{\sigma\in{PD(A)}} min_{\bar{a}\in{A}} \sum_{a\in{A}}Q(s,\bar{a})\sigma_a$$</p>
<p></p>
<p>roughly .... the $$max_{\pi\in{PDA(A)}}$$ is weird, it doesn&#39;t really match what Littman writes in his algorithm (algorithm says to take a random selection with probability $$\pi$$).</p>",2019-11-10T16:08:04Z,44,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2t6z65t827db,2019-11-10T16:08:04Z,{},project3
3940,no,"Sorry, am I missing a link?",2019-11-09T13:56:18Z,26,Week 11/3 - 11/9,followup,,jl3oi5v7qkSk,k2rmtvayf9o6k1,2019-11-09T13:56:18Z,{},project3
3941,no,oh my bad,2019-11-11T02:57:18Z,25,Week 11/10 - 11/16,feedback,,is5gzbotXmz,k2tu63ok137r2,2019-11-11T02:57:18Z,{},project3
3942,no,"I think your issue here is your objective function. there&#39;s no reason why we want to maximize the sum of probabilities in the problem (we already know they sum up to 1!). You will have introduce another variable $$z$$ and try to maximize that: 
max $$z$$ s.t $$3x_1 - x_2 - 4x_3 \geq z$$, etc. If you do that you will find that wikipedia&#39;s answer is correct and has objective function value $$z=-0.33$$ 

hope it helps,",2019-11-10T13:01:03Z,25,Week 11/10 - 11/16,followup,,jqkuetouttn5,k2t0aogoltx25i,2019-11-10T13:01:03Z,{},project3
3943,no,"<p>I also think his objective function is wrong, like you said above.</p>
<p>Did you solve it with cvxopt? i try to run Ben&#39;s example with my solver (using cvxopt) and it raises error:</p>
<pre>ValueError: Rank(A) &lt; p or Rank([G; A]) &lt; n</pre>
<p>I need to take a closer look on why it fail, but it could be that the matrix is not very well defined.</p>",2019-11-11T03:36:43Z,25,Week 11/10 - 11/16,feedback,,jl5wq8mca7o0,k2tvksgrlf379c,2019-11-11T03:36:43Z,{},project3
3944,no,"<p><a href=""https://www.youtube.com/watch?v=v-chgwlwqTk"">https://www.youtube.com/watch?v=v-chgwlwqTk</a> -&gt; First 30 minutes of this lecture actually explain where that other variable comes from</p>",2019-11-19T03:51:21Z,24,Week 11/17 - 11/23,feedback,,h6e6q7ftqr747f,k35bmf1rkkl1bx,2019-11-19T03:51:21Z,{},project3
3945,no,"<p><strong attention=""is5gzbotXmz"">&#64;tianhang zhu</strong>  </p>
<p></p>
<p>In the instructor answer, you mentioned about showing an example of solving min max lp, however, it appears it wasn&#39;t inserted? Can you please finish updating the post with the example?</p>",2019-11-11T00:41:15Z,25,Week 11/10 - 11/16,followup,,ixty1midfufhd,k2tpb4o5txv1cb,2019-11-11T00:41:15Z,{},project3
3946,no,this time pasted. sorry for the confusion,2019-11-11T02:58:49Z,25,Week 11/10 - 11/16,feedback,,is5gzbotXmz,k2tu81d2kji2oc,2019-11-11T02:58:49Z,{},project3
3947,no,"<p>tianhang - I think he is asking; if an n-step return (bootstrapping, reward estimation), makes an &#34;online&#34; policy, offline. ;)</p>
<p></p>
<p>at ben:</p>
<p></p>
<p>debatable.  I think the line between online and offline learners is -not- clear-cut.  I think it has more to do with the inherent algorithmic dynamics (alg specific characteristics of learning) than it does with a multi-step calculation.  independent RL has not focused super-much on this (to my knowledge, from my reading; it is side-commentary  -- bellman updates in the infinite visit sense have squashed most of it). </p>
<p></p>
<p>multi-agent apparently has it squarely in its focus (as it directly impacts the question of overfit of environment vs overfit of adaptation).  it is basically multi&#39;s version of &#34;how much exploration vs exploitation&#34;.</p>
<p></p>
<p></p>",2019-11-15T12:34:17Z,53,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k304jhz3uxn7j2,2019-11-15T12:34:17Z,{},other
3948,no,I&#39;d add each of those actions is going to be a constraint for your LP.,2019-11-09T21:30:22Z,21,Week 11/3 - 11/9,followup,,hyxsfbkeit22m2,k2s31t00lrh6rr,2019-11-09T21:30:22Z,{},project3
3949,stud,"<p>Do we have to use LP to find the max V for the correlated-Q, can&#39;t we use np.argmax() and np.max() to get the max value for the sum of Q table for player A and player B?</p>",2019-11-10T03:37:17Z,20,Week 11/10 - 11/16,followup,a_0,,k2sg5nu4r0q50d,2019-11-10T03:37:17Z,{},project3
3950,no,"<p>no, you need to compute the correlated probabilities to compute the most likely blend of Q values for a given solution. That&#39;s not going to be the max or min, but something in between....</p>",2019-11-13T01:56:51Z,20,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wmw2gxypk4cm,2019-11-13T01:56:51Z,{},project3
3951,stud,"Thanks! I think I figured that out, if it&#39;s pure strategy than we only need to do np.max() but in this case it&#39;s not so have to use LP to find the probabilities.",2019-11-14T17:33:16Z,20,Week 11/10 - 11/16,feedback,a_0,,k2yzs5rprbo3u5,2019-11-14T17:33:16Z,{},project3
3952,no,"<p>So my initial intuition seems correct. Suppose number of actions = 3, then from player R&#39;s perspective, action 1 will give us 2 inequalities. Same for action 2 and action 3. Totally to 6 inequalities. Then for player C, we will have another 6. </p><br /><p></p><br /><p>The constraint G will now have a total of 12 equalities. </p><br /><p></p><br /><p>** For each action, we can only compare with another action. I.e action 1 &gt; action 2 ; action 1 &gt; action 3 ...etc </p><p></p>",2019-11-10T09:49:16Z,20,Week 11/10 - 11/16,followup,,jvfpllmsggt7p4,k2stg1lwdsb45n,2019-11-10T09:49:16Z,{},project3
3953,no,"<p>So, applying this out for the 5 actions would mean that we have at least 20 constraints per player?</p>",2019-11-12T06:57:10Z,20,Week 11/10 - 11/16,feedback,,ixty1midfufhd,k2vi6eyzlg8jc,2019-11-12T06:57:10Z,{},project3
3954,no,"<p>yes. 5 actions will give 20 constraints (&gt;0) for player R and another 20 (&gt;0) for player C.</p>
<p></p>
<p>To give more hint, your G constraint should be (20 &#43; 20 &#43; (5*5) , (5*5)) </p>
<p></p>
<p></p>
<p>Edit: 20 for player R, 20 for player C and the remaining are identity matrices for each variable to be positive </p>
<p></p>
<p></p>",2019-11-13T08:10:25Z,20,Week 11/10 - 11/16,feedback,,jvfpllmsggt7p4,k2x08gj9inq39f,2019-11-13T08:10:25Z,{},project3
3955,no,<p>Thanks for the response Germayne. What do you mean by &#34;G&#34; though? </p>,2019-11-14T03:13:27Z,20,Week 11/10 - 11/16,feedback,,ixty1midfufhd,k2y52f1ugo51bj,2019-11-14T03:13:27Z,{},project3
3956,no,<p>&#39;G&#39; is cvxopt&#39;s name for the constraint matrix.</p>,2019-11-14T03:19:41Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2y5afk5h8j31z,2019-11-14T03:19:41Z,{},project3
3957,no,"<p>Ah... yea, that makes sense... I&#39;m used to it A_ub since I started with scipy&#39;s version until I realized how horrible slow it was for this project. Thanks Vahe</p>",2019-11-14T05:09:51Z,20,Week 11/10 - 11/16,feedback,,ixty1midfufhd,k2y9846emrc5l2,2019-11-14T05:09:51Z,{},project3
3958,no,<p>glad you got it. Just to add on to Vahe&#39;s answer: G refers to set of constraints that are bounded by &lt;= 0. That is the way cvxopt defined. </p>,2019-11-15T07:23:13Z,20,Week 11/10 - 11/16,feedback,,jvfpllmsggt7p4,k2ztfgx43e3545,2019-11-15T07:23:13Z,{},project3
3959,no,"<p>Actually, the constraints defined by the $$G$$ matrix are bounded, from above, by the $$h$$ vector, which can have non-zero elements (this can be useful, if like me, you don&#39;t use the $$A$$ matrix and $$b$$ vector).</p>",2019-11-15T10:28:15Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k3001fcpncg2m2,2019-11-15T10:28:15Z,{},project3
3960,no,"<p>&#64;Vahe, &#64;Germayne, can you share the shape of your G matrix for HW6 and project 3? I solved HW6 without specifying any G matrix, but it appears more granular constraints are necessary to get the correct behaviors for project 3. I&#39;m going back trying to solve HW6 again with the G matrix added and am a little thrown off by the &#34;(20 &#43; 20 &#43; (5*5) , (5*5))&#34; above. If I understand correctly that&#39;s specifying the number of constraints necessary, not the shape of the G matrix. None of the examples from office hours needed additional identity matrices so I&#39;m not sure how/why that comes into play <strong>**</strong>(think I may have found why &#64; Edit2).</p>
<p></p>
<p>Should the G matrix have columns equal to the number of actions (3 in HW6, 5 in P3), or the number of cells in the rewards matrix (9 in HW6, 25 in P3)? Appreciate any insights you can share.</p>
<p></p>
<p><strong>Edit:</strong> &amp; if I understand correctly, assuming both players have the same number of actions available, the total number of constraints should be (num_actions * (num_actions-1)) * 2.</p>
<p>(num_actions * (num_actions-1)) because constraints are needed for each action available, with each action needing constraits for all actions besides itself (the -1).</p>
<p>*2 because there are two players, so constraints need to processed across rows and columns.</p>
<p></p>
<p><strong>Edit 2:</strong> Possible explanation for why identity matrices are needed here but weren&#39;t in office hours examples: identity matrices apply the constraint that all values must be above zero. When I try adding this constraint I get the error &#34;Certificate of primal infeasibility found.&#34; -- without much else in terms of helpful debugging messaging...</p>",2019-11-16T00:54:12Z,20,Week 11/10 - 11/16,feedback,,isde332xcka1m0,k30uz20gnimqf,2019-11-16T00:54:12Z,{},project3
3961,no,"&#64;vahe: I guess i just do it differently. I always define matrix A as well as b. They are the ones where i define my x1&#43;x2&#43;x3 = 1. To me, the G matrix are for the constraints that are &gt; 0 . (note that later we must *-1 since we convert them to a minimization problem ) 

&#64;michael 
 let me try to explain part by part: 

1) Hw6 is basically maxmin algorithm. So for your Foe-Q&#39;s linear program, you do was what you did in hw6. If i remember correctly, my G for maxmin for say a m = 3 action problem (the rock-scissors-paper for instance) is shaped (2* m, m &#43;1). Reason is you should have 3 constraints for each of your opponent&#39;s action followed by a identity matrix. (x1 &gt; 0, x2 &gt; 0 and x3 &gt; 0)  while m&#43; 1 is to include the objective variable w (see below)

A good reference i used was:<a href=""https://www.usna.edu/Users/math/dphillip/sa305.s13/uhan-game-theory.pdf"">https://www.usna.edu/Users/math/dphillip/sa305.s13/uhan-game-theory.pdf</a> 
From the last slide: 

<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjvfpllmsggt7p4%2Fk31bzxdwn04l%2FUntitled.png"" alt="""" />

So G as well as h takes in any of the above that contains a &gt;= 0 
A takes v1,v2,v3 while b takes = 1
c is your objective function for w


2) For CE-Q, it is different from hw6. Hw6 &#43; P3&#39;s FoeQ uses what i mentioned in 1. 
As for CE, 
I define G as Rationality constraints &#43; the identiy matrix. Rationality constraints should have m*(m-1) number of constraints for each player. 2 players will be 2*m*(m-1). So m = 5 and 2 players result in 40 rationality constraints.
The identity matrix is to give the constraint for each of your sigma to be positive. The identity part is the same as what i did above. Main difference is the rationality constraints in CE. 

edit 1** the identity matrix is just a practice that i did following from maxmin problems.
edit 2** forming the rationality constraints matrix is tricky. You might have some bug. Try starting with the chicken and dare problems from greenwald 2003 as well as from lectures to see if you can solve them. 
See <a href=""https://en.wikipedia.org/wiki/Correlated_equilibrium"">https://en.wikipedia.org/wiki/Correlated_equilibrium</a>. 
edit 3 ** try to do unittest. it will help",2019-11-16T08:51:50Z,20,Week 11/10 - 11/16,feedback,,jvfpllmsggt7p4,k31c1a0pdp4q2,2019-11-16T08:51:50Z,{},project3
3962,no,"<p>&#64;Germayne  I was just pointing out that one doesn&#39;t have to constrain variables to be $$\leq 0$$; they can be $$\leq$$ any constant.</p>
<p></p>
<p>&#64;Michael  I&#39;m going to give a self-contained explanation.  Hopefully it will be useful.</p>
<p></p>
<p>In linear programming, we have only two things:</p>
<p></p>
<p>1. linear constraints</p>
<p>2. a linear objective function.</p>
<p></p>
<p>The former are all inequalities that are linear in the variables.  We can put them in a standard form such that they are all linear functions of variables <strong>less than</strong> some constant.  Standard form for the objective function is a function which we want to <b>minimize</b>.</p>
<p></p>
<p>CVXOPT at its core, only wants those two things:</p>
<p></p>
<p>1. $$G$$ is a matrix that holds the left sides of the constraint inequalities in standard form and $$h$$ is a vector that holds the right side of those inequalities.</p>
<p></p>
<p>2. $$c$$ is a vector that holds the objective function in standard form.</p>
<p></p>
<p>I made a decision to use this barebones, core version of CVXOPT when I noticed some irregularities using the modeling library.  You can also use an $$A$$ and $$b$$ matrix as Germayne does to express equalities to make things look nicer.</p>
<p></p>
<p>The way I approach LP is that we can partition the constraint inequalities into 3 classes:</p>
<p></p>
<p>(1) probability constraints</p>
<p>(2) &#34;rationality constraints&#34;</p>
<p>(3) &#34;objective constraints&#34;</p>
<p></p>
<p>Class (1) are restrictions on variables that arise from them being probabilities.  In HW6, where we were playing rock-paper-scissors, we had 3 variables that represented probabilities, each of which had to be greater than or equal to $$0$$ and the sum of which had to be $$1$$  How many inequalities does that make?  $$P_r \geq 0$$, $$P_p \geq0$$, $$P_s \geq 0$$, and finally $$P_r &#43; P_p &#43; P_s = 1$$.</p>
<p></p>
<p>That&#39;s $$3$$ inequalities and one equality.  We can&#39;t have equalities (if you&#39;re using hard-mode like I am) so we turn that equality into two inequalties:</p>
<p></p>
<p>1. $$P_r &#43; P_p &#43; P_s \geq 1$$</p>
<p>2. $$P_r &#43; P_p &#43; P_s \leq 1$$</p>
<p></p>
<p>and we&#39;re done with the probability constraints!  We have 5 of them.  Putting them all into standard form, we have:</p>
<p></p>
<p>1. $$(-1)\cdot P_r &#43; 0\cdot P_p &#43; 0\cdot P_s \leq 0$$.</p>
<p>2. $$0\cdot P_r &#43; (-1)\cdot P_p &#43;0\cdot P_s \leq 0$$.</p>
<p>3. $$0\cdot P_r &#43; 0\cdot P_p &#43; (-1)\cdot P_s \leq 0$$.</p>
<p>4. $$-P_r - P_r - P_s \leq -1$$.</p>
<p>5. $$P_r &#43; P_r &#43; P_s \leq 1$$.</p>
<p></p>
<p>Homework $$6$$ has <em>no</em> &#34;rationality constraints.&#34;</p>
<p></p>
<p>It has one &#34;objective constraint.&#34;  It&#39;s a constraint you form to express the desire to have the minimum value of Player 1&#39;s action be maximized (the maximin problem).</p>
<p></p>
<p>One can do this with a clever trick that Littman describes in his 1994 paper.  For any action your opponent takes (rock, paper, or scissors), the value of the game for you will be a function of the three variables.  If your opponent plays, say, rock, the value of the game for you will be</p>
<p></p>
<p>$$0\cdot P_r &#43; 1\cdot P_p &#43; (-1)\cdot P_s$$</p>
<p></p>
<p>Let that value be greater than or equal to some constant, $$V$$.</p>
<p></p>
<p>The trick is to realize that you can maximize the minimum value by letting all three of your game values be at least that same constant, then maximize that constant.</p>
<p></p>
<p>$$0\cdot P_r &#43; 1\cdot P_p &#43; (-1)\cdot P_s \geq V$$</p>
<p>$$(-1)\cdot P_r &#43; 0\cdot P_p &#43; 1\cdot P_s \geq V$$</p>
<p>$$1\cdot P_r &#43; (-1)\cdot P_p &#43; 0\cdot P_s \geq V$$</p>
<p></p>
<p>and we want to <strong>maximize </strong>V.  By maximizing V, we are maximizing the minimum value we have under all of our opponent&#39;s actions, exactly our objective!</p>
<p></p>
<p>So we have three &#34;objective constraints&#34;, which in standard form, are:</p>
<p></p>
<p>6. $$0\cdot P_r &#43; (-1)\cdot P_p &#43; 1\cdot P_s &#43; V\leq 0$$</p>
<p>7. $$1\cdot P_r &#43; 0\cdot P_p &#43; (-1)\cdot P_s &#43; V\leq 0$$</p>
<p>8. $$(-1)\cdot P_r &#43; 1\cdot P_p &#43; 0\cdot P_s &#43; V\leq 0$$</p>
<p></p>
<p>So, we have a total of $$8$$ constraints, all arranged in standard form.  We can now extract the coefficients of those constraints and that gives us our $$G$$ matrix and $$h$$ vector.  For homework 6, our $$G$$ matrix would be $$8$$x$$4$$ (8 constraints and 4 variables) and our $$h$$ vector would be $$8$$ elements long (the right-hand side of our constraint inequalities, which would be [0., 0., 0., -1., 1., 0., 0., 0.]).  Make sure to make all your coefficients floating points.</p>
<p></p>
<p>The relationship between $$G$$ and $$h$$ is $$G\cdot x = h$$, where $$x$$ is the implied vector of variables.  It&#39;s implied because CVXOPT deduces how many variables you have from the number of columns in the $$G$$ matrix.  Note that since we added the variable $$V$$ when creating our objective constraints, we need to add an appropriate $$0$$ term to the left hand size of all of our probability constraints to reflect the presence of this new $$V$$ variable.</p>
<p></p>
<p>Our objective function is, in standard form (i.e. that which we wish to minimize):</p>
<p></p>
<p>$$0\cdot P_r &#43; 0\cdot P_p &#43; 0\cdot P_s -V$$</p>
<p></p>
<p>We extract the coefficients and this is our $$c$$ vector [0., 0., 0., -1.].</p>
<p></p>
<p>That&#39;s it!</p>
<p></p>
<p>For Project 3, it&#39;s the same procedure, but for the correlated equilibria, the two players&#39; actions are correlated, so we now have $$5\cdot5 = 25$$ probabilities.  We also have the so-called &#34;rationality constraints&#34; which Greenwald discusses in section 2.1 of her 2003 paper.</p>
<p></p>
<p>The objective constraint depends on which of the (four) correlated equilibria you are using.</p>",2019-11-16T14:31:24Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k31o5z1z4v22l,2019-11-16T14:31:24Z,{},project3
3963,no,"cool. I think you covered it better than i did :) and to top it off, yes, G does not have to be bounded by 0. That is defined by h.",2019-11-16T15:00:26Z,20,Week 11/10 - 11/16,feedback,,jvfpllmsggt7p4,k31p7ar1n4i5l9,2019-11-16T15:00:26Z,{},project3
3964,no,"<p>&#64;Germayne, &#64;Vahe, thank you so much for the patient and clear explanations. I finally feel like I have a basic working understanding of linear programming along with how to implement it via CVXOPT&#39;s format.</p>
<p></p>
<p>Spent a few hours watching lectures on Convex Optimization yesterday because this topic seems so broadly applicable, but it wasn&#39;t enough for me to know how to proceed on the project. It&#39;s a relief to finally understand how to do this via the explanations here. Really appreciate it!</p>",2019-11-16T16:55:13Z,20,Week 11/10 - 11/16,feedback,,isde332xcka1m0,k31taxhonmd65x,2019-11-16T16:55:13Z,{},project3
3965,no,"<p>I think I am still stuck on how to get this slack variable working. I am pretty confident on G, h with all the probability constraints, rationality constraints. Rather than just use G, h which I have seen elsewhere I have been trying to stick with the stock standard form with c, G and h. My intuition was to make some kind of c with all 25 unknowns, dotted with the relevant Q values for A. Is this somewhere in the ballpark?</p>",2019-11-24T16:00:21Z,18,Week 11/24 - 11/30,feedback,,is5x0kzrzjpg7,k3d6v6b7y2h43q,2019-11-24T16:00:21Z,{},project3
3966,no,"<p>Does this seem correct , as taken from the Game of Chicken:</p>
<p></p>
<pre>             |  L  |  R  |
        -----&#43;-----&#43;-----&#43;
          T  | a/b | c/d |
        -----&#43;-----&#43;-----&#43;
          B  | e/f | g/h |
        -----&#43;-----&#43;-----&#43;

        row: a*P(L|T) &#43; c*P(R|T) &gt;= e*P(L|T) &#43; g*P(R|T)
        row: e*P(L|B) &#43; g*P(R|B) &gt;= a*P(L|B) &#43; c*P(R|B)
        col: b*P(T|L) &#43; f*P(B|L) &gt;= d*P(T|L) &#43; h*P(B|L)
        col: d*P(T|R) &#43; h*P(B|R) &gt;= b*P(T|R) &#43; f*P(B|R)
        T|L = L|T by bayes rule
</pre>
<p></p>",2019-11-16T00:19:59Z,20,Week 11/10 - 11/16,followup,,jc554vxmyuy3pt,k30tr1m5f5s16z,2019-11-16T00:19:59Z,{},project3
3967,no,"<p>The constraint equations are all correct.</p>
<p></p>
<p>Bayes rule isn&#39;t what you wrote though.  It should be</p>
<p></p>
<p>$$P(T|L) = \frac{P(T,L)}{P(L)} = \frac{P(L|T)P(T)}{P(L)}$$.</p>
<p></p>
<p>There is what I believe to be a typo in Greenwald 2003, that I mentioned during office hours, where she writes:</p>
<p></p>
<p>$$\pi(y)=\sum_x\pi(y|x) $$.</p>
<p></p>
<p>This should be $$\pi(y)=\sum_x\pi(y,x) = \sum_x\pi_{yx}$$.</p>
<p></p>
<p>This typo doesn&#39;t affect her later derivation, so it&#39;s not a catastrophic problem.</p>
<p></p>
<p>She writes, later in that paragraph, $$\pi_{TL} = \pi(L|T)\pi(T)$$ and $$\pi_{TR} = \pi(R|T)\pi(T)$$, which is completely correct,</p>
<p></p>
<p>and then she writes the first constraint equation, but skips a step, which I&#39;ll include below:</p>
<p></p>
<p>$$ -1\pi_{TL} &#43; 2\pi_{TR} \geq 0 \implies$$</p>
<p></p>
<p>$$-1\pi(L|T)\pi(T) &#43; 2\pi(R|T)\pi(T) \geq 0 \implies$$</p>
<p></p>
<p>$$\pi(T)[-1\pi(L|T) &#43; 2\pi(R|T)] \geq 0 \implies$$</p>
<p></p>
<p>$$-1\pi(L|T) &#43; 2\pi(R|T) \geq 0$$.</p>
<p></p>
<p>Strictly speaking, we need $$\pi(T) &gt; 0$$ for this to hold, but i guess it should be fine in the limit as $$\pi(T)$$ approaches $$0$$.</p>
<p></p>
<p></p>",2019-11-16T00:58:24Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30v4g6bs74pg,2019-11-16T00:58:24Z,{},project3
3968,no,"<p>Wait, then we can&#39;t swap the $$\pi_{T|L}$$ for $$\pi_{L|T}$$ ??</p>
<p></p>
<p></p>",2019-11-16T01:15:23Z,20,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30vqa017cm2vz,2019-11-16T01:15:23Z,{},project3
3969,no,"<p>$$\pi_{T|L}$$ stands for the probability of player 1 choosing the strategy &#39;T&#39; when player 2 has chosen the strategy &#39;L.&#39;</p>
<p></p>
<p>$$\pi_{L|T}$$ stands for the probability of player 2 choosing the strategy &#39;L&#39; when player 1 has chosen the strategy &#39;T.&#39;</p>
<p></p>
<p>Math aside, why do those two probabilities have to be the same?</p>",2019-11-16T01:32:47Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30wco2uspw6sj,2019-11-16T01:32:47Z,{},project3
3970,no,"<p>Somewhat related question:  For the first CE-Q equilibrium ($$\mu\text{CE}$$), where we are maximizing the <em>sum</em> of the player&#39; rewards, if it&#39;s a zero-sum game, won&#39;t the sum always be zero?</p>",2019-11-16T01:34:52Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30wfc7ud5211e,2019-11-16T01:34:52Z,{},project3
3971,no,"<p>Didn&#39;t Alec show something in the OH when explaining chicken where the $$\pi_{T|L}$$ could become $$\pi_{L|T}$$. I don&#39;t see why that would be the same in this case, but this is not my area of expertise....</p>",2019-11-16T01:48:05Z,20,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30wwc7umz6zp,2019-11-16T01:48:05Z,{},project3
3972,no,"<p>You may be referring to this, from one of Alec&#39;s slides in OH:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk30x721i9lpq%2FCapture6.PNG"" alt="""" /></p>
<p></p>
<p>$$\pi_{xy}=\pi_{yx}$$ is not the same as $$\pi(x|y) = \pi(y|x)$$.  The latter isn&#39;t true.</p>
<p></p>
<p>The first equation is really just a tautology:  it&#39;s expressing the commutativity of a joint distribution in the different variables.  The second is a false equivalence between conditional probabilities.  You <em>can </em>see the appropriate relationship between the joint and conditional probabilities in Alec&#39;s running equation.</p>",2019-11-16T01:57:35Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30x8jzebrn771,2019-11-16T01:57:35Z,{},project3
3973,no,"<p>Vahe, I couldn&#39;t follow what you meant in the office hour. But writing it down like this making it much easier to understand. Thank you for noting this down like this!</p>
<p></p>
<p>There are still something confuse me about the notation. Let say:</p>
<p></p>
<table cellspacing=""0"" cellpadding=""6px"" border=""1""><tbody><tr><td></td><td>L</td><td>R</td></tr><tr><td>T</td><td><em>X</em></td><td><em>Y</em></td></tr><tr><td>B</td><td><em>K</em></td><td><em>Z</em></td></tr></tbody></table>
<p></p>
<p>In this game matrix: <em>X </em>is a <em>join probability</em>, not conditional probability, is that correct? (<em>X </em>happens when <em>L</em> happens and <em>T</em> happens)</p>
<p></p>
<p>In Greenwald&#39;s paper, she used:</p>
<p>$$\pi(y)$$, denote as <em>marginal probability</em>. Does it has the same meaning as total probability of <em>y</em>, independently of <em>x</em>? For example: $$\pi(y) = X &#43; K, \pi(y&#39;) = Y &#43; Z$$  and  $$\pi(y) &#43; \pi(y&#39;) = 1$$?</p>
<p>Is that just another way of rephrasing what you meant by:</p>
<p>$$\pi(y) = \sum_x\pi(y, x) = \sum_x \pi_{yx}$$</p>
<p>What is the difference between $$\pi(y, x)$$  and  $$\pi_{yx}$$?</p>
<p></p>
<p>And back to Alec&#39;s OH:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl5wq8mca7o0%2Fk31tsvvfyyih%2Ff1.png"" alt="""" width=""322"" height=""130"" /></p>
<p>We basically deriving the joint probability, that Greenwald used for her CE, back to join probability, that we are accustomed to use to calculate equilibrium in Mix Strategy or Nash equilibrium.</p>
<p>So written it that way, it isn&#39;t different than what we used in Minipoker calculation, in term of type of probability variable. What is difference though, is that we are now:</p>
<p>- Consider 2 types of expected reward, one for the row player (A) and one for column player (B).</p>
<p>- Consider the player A reward among his own actions (formulate 1 &amp; 2), and among B&#39;s action (formulate 3 &amp; 4).</p>
<p></p>
<p>Is this a correct understanding of the Correlated Equilibrium section in Greenwald&#39;s paper?</p>
<p></p>
<p></p>
<p></p>",2019-11-16T17:23:07Z,20,Week 11/10 - 11/16,feedback,,jl5wq8mca7o0,k31uaswjaq050r,2019-11-16T17:23:07Z,{},project3
3974,no,"<p><em>&#34;In this game matrix: X is a join probability, not conditional probability, is that correct? (X happens when L happens and T happens)&#34;</em></p>
<p></p>
<p>Yes, it&#39;s a joint probability.</p>
<p></p>
<p>&#34;$$\pi(y)$$<em>, denote as <em>marginal probability</em>. Does it has the same meaning as total probability of <em>y</em>, independently of <em>x</em>?&#34;</em></p>
<p></p>
<p>Yes.</p>
<p></p>
<p>&#34;<em>Is that just another way of rephrasing what you meant by: </em>$$\pi(y) = \sum_x\pi(y,x) = \sum_x\pi_{yx}$$&#34;</p>
<p></p>
<p>I didn&#39;t really follow what you wrote before that, but this equation, $$\sum_x\pi(y,x) = \pi_y$$, is a fact that can be deduced from the axioms of probability if the set of $$x$$&#39;s are disjoint and span the entire sample space.  In our case, this is true.  Each player must take one of a distinct set of actions.</p>
<p></p>
<p>&#34;<em>We basically deriving the joint probability&#34;</em></p>
<p></p>
<p>Yes, we are dealing with <i>joint</i> probabilities, but to derive the rationality constraints, we need to transition to <em>conditional</em> probabilities.  That&#39;s why we make the connection here, via Bayes&#39; rule.</p>",2019-11-16T21:25:52Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k322yz7mthd3y4,2019-11-16T21:25:52Z,{},project3
3975,no,"<p>Sorry, I realize my notation is confusing. I want to see if:</p>
<p>$$\pi(L) = X &#43; K$$  and $$\pi(R) = Y &#43; Z$$  and  $$\pi(L) &#43; \pi(R) =1$$. For:</p>
<table cellspacing=""0"" cellpadding=""6px"" border=""1""><tbody><tr><td></td><td>L</td><td>R</td></tr><tr><td>T</td><td>X</td><td>Y</td></tr><tr><td>B</td><td>K</td><td>Z</td></tr></tbody></table>
<p>And I think that is what you meant by sum of disjoint probability.</p>
<p></p>
<p>One last thing though, from:</p>
<blockquote>
<p>$$\pi(y) = \sum_x \pi(y, x) = \sum_x \pi_{yx}$$</p>
</blockquote>
<p>Do $$\pi(y,x)$$  and  $$\pi_{yx}$$  both refer to joint probability, and just different notation?</p>",2019-11-16T22:06:59Z,20,Week 11/10 - 11/16,feedback,,jl5wq8mca7o0,k324fuz4ia26iw,2019-11-16T22:06:59Z,{},project3
3976,no,"<p>Yes, just two different notations for the same thing.</p>
<p></p>
<p>For the first thing you wrote, exactly - when you sum a column you&#39;re summing the two joint probabilities (which are disjoint and cover the whole conditional space) and getting the marginal.  And the sum of all four squares is $$1$$.</p>
<p></p>
<p>Edit: Careful though.  your X, Y, K, and Z here are denoting the joint probabilities of the actions in those squares, <strong>not</strong> the payoffs of those strategies, which is what people usually write inside the squares.</p>",2019-11-16T23:21:53Z,20,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k327468xuhc2ra,2019-11-16T23:21:53Z,{},project3
3977,no,<p>Thanks Vahe! This is really helpful!</p>,2019-11-17T05:18:30Z,19,Week 11/17 - 11/23,feedback,,jl5wq8mca7o0,k32jus9shud6q7,2019-11-17T05:18:30Z,{},project3
3978,no,<p>--<em>edit</em>: <em>move to correct post--</em></p>,2019-11-16T17:22:43Z,20,Week 11/10 - 11/16,followup,,jl5wq8mca7o0,k31uaa2io9g4og,2019-11-16T17:22:43Z,{},project3
3979,stud,"<p>thank you <strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  </p>",2019-11-09T22:02:09Z,39,Week 11/3 - 11/9,followup,a_0,,k2s46oej6j2a,2019-11-09T22:02:09Z,{},project2
3980,stud,<p>any eta on the Project 2 grades ??</p>,2019-11-19T06:12:15Z,37,Week 11/17 - 11/23,followup,a_0,,k35gnmhb7lr1zh,2019-11-19T06:12:15Z,{},project2
3981,no,"<p>&#43;1</p>
<p></p>",2019-11-21T03:21:21Z,37,Week 11/17 - 11/23,feedback,,jqtnwove8X4N,k385fjimg0g6oq,2019-11-21T03:21:21Z,{},project2
3982,no,<p>&#43;1</p>,2019-11-21T15:20:18Z,37,Week 11/17 - 11/23,feedback,,jziegiv4xls3zn,k38v44a9y697eu,2019-11-21T15:20:18Z,{},project2
3983,no,<p>I hope to release them tonight or tomorrow.</p>,2019-11-21T15:41:40Z,37,Week 11/17 - 11/23,followup,,hz7meu55mi8sd,k38vvlay7bs2lj,2019-11-21T15:41:40Z,{},project2
3984,no,<p>thank you for the update :)</p>,2019-11-21T22:44:42Z,37,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k39azmcg89vwh,2019-11-21T22:44:42Z,{},project2
3985,stud,LOL. It is true that it is as hard figuring out the preference for money as that for babies. ,2019-11-13T17:50:12Z,53,Week 11/10 - 11/16,followup,a_0,,k2xky2v8dll2ad,2019-11-13T17:50:12Z,{},other
3986,no,"<p>and lord, here I thought I set the epitome, for anonymity ;) :)<br /><br />...</p>
<p></p>
<p>there is a whole sub-branch of under AI &gt; game theoretic &gt; econometrics  that is devoted to figuring out human preferences. ;)</p>
<p></p>
<p>under AI; because the work was adapted to &#34;instructing&#34; algorithms (inferring reward functions).</p>
<p></p>
<p></p>",2019-11-15T12:31:58Z,53,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k304gja9f675jn,2019-11-15T12:31:58Z,{},other
3987,no,<p>Thanks for reminding.</p>,2019-11-10T01:54:30Z,28,Week 11/10 - 11/16,followup,,jzhs489b10i78c,k2schhgljwt4v4,2019-11-10T01:54:30Z,{},office_hours
3988,stud,<p>: )</p>,2019-11-14T15:51:40Z,28,Week 11/10 - 11/16,feedback,a_0,,k2yw5hlfr56315,2019-11-14T15:51:40Z,{},office_hours
3989,no,"<p>Done, thanks for reminding!</p>",2019-11-10T02:03:48Z,28,Week 11/10 - 11/16,followup,,jqkxzdmmolGf,k2sctfi2jd2zd,2019-11-10T02:03:48Z,{},office_hours
3990,stud,<p>: )</p>,2019-11-14T15:51:46Z,28,Week 11/10 - 11/16,feedback,a_0,,k2yw5m5qaax3k2,2019-11-14T15:51:46Z,{},office_hours
3991,no,"<p>Thank you for this... I haven&#39;t sent him anything yet hoping to get more people on it.</p>
<p></p>
<p>Indeed, it is one full hour, but there is also a lot more time that goes into it. A mass of emails we send back and forward for coming up with a date/time, for getting a first look at the question to gauge his interest, etc.</p>
<p></p>
<p>Definitely a privilege to have someone of his caliber/pedigree with us!</p>",2019-11-10T17:23:47Z,28,Week 11/10 - 11/16,followup,,hyx9thiqa6j4nn,k2t9ojywurt4k4,2019-11-10T17:23:47Z,{},office_hours
3992,stud,"<p>&#64;Miguel Morales, thank you for taking the time and effort organizing this. It is a lot of work! Really appreciate it!  </p>",2019-11-10T18:26:35Z,28,Week 11/10 - 11/16,feedback,a_1,,k2tbxassacm5vt,2019-11-10T18:26:35Z,{},office_hours
3993,no,"Done, thanks for the reminder, I totally missed that post &#x1f62c;",2019-11-10T20:35:40Z,28,Week 11/10 - 11/16,followup,,jzjwcq2u8o7110,k2tgjbi7a367ow,2019-11-10T20:35:40Z,{},office_hours
3994,stud,<p>: )</p>,2019-11-14T15:52:21Z,28,Week 11/10 - 11/16,feedback,a_0,,k2yw6dgq8a2425,2019-11-14T15:52:21Z,{},office_hours
3995,no,"<p>This sounds like an ambiguous detail of the paper that you can talk about in the proj 3 report; and explain\justify how you implemented it. There are a couple of ways to handle this, as there is some differences between the papers and setup as well. In Littman&#39;s 1994 paper, it was also mentioned that a change of possession goes to the &#34;stationary&#34; player. Depending on how this is interpreted, the environment could only pass the ball when one player stands and the other moves into him vs both players moving at the same time and getting a collision.</p>
<p></p>
<p>From your example, let&#39;s say that B is moving East and is going first. For every possible action of A, this would be a possession change if we checked on collision. Using the &#34;stationary&#34; player terminology, then maybe the ball only passes when A doesn&#39;t go North (since all other actions keep them in that square); or even only when they use Stick.</p>
<p></p>
<p>Several posts ask similar questions, &#64;884, &#64;818, and the basic response is it&#39;s unclear, select something, and justify it.</p>
<p></p>
<p>For now, my environment does as you suggest, but I plan to playing around with that behavior to see how it effects things. Just like Proj 1 there are plenty of unclear items to explore.</p>",2019-11-10T20:00:31Z,53,Week 11/10 - 11/16,followup,,ixty1midfufhd,k2tfa44y6ec2a9,2019-11-10T20:00:31Z,{},project3
3996,no,<p>Thanks George.</p>,2019-11-10T20:23:55Z,53,Week 11/10 - 11/16,feedback,,jc6xvgjncoey,k2tg47jq32s18o,2019-11-10T20:23:55Z,{},project3
3997,no,<p>See &#64;945 too</p>,2019-11-19T02:04:56Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k357tk0sylo6ax,2019-11-19T02:04:56Z,{},project3
3998,no,"<p>In addition to Chris&#39;s suggestion the CS 7638: Artificial Intelligence for Robotics (prev AI4R) gives you a little bit of taste of what you could expect in the real world: basically you are never 100% sure where you are and what you are doing, everything is based on probability distributions.</p>",2019-11-11T22:32:22Z,30,Week 11/10 - 11/16,followup,,jqkxzdmmolGf,k2v058ncly12tp,2019-11-11T22:32:22Z,{},other
3999,stud,"<p>I took that course and it is not much about implementing reinforcement learning, but basic robotic fundamental--really important stuff though.</p>",2019-11-12T00:49:17Z,30,Week 11/10 - 11/16,feedback,a_0,,k2v51bg6aop29u,2019-11-12T00:49:17Z,{},other
4000,no,"<p>Oh, definitely not about RL, this is not what I meant, sorry for poorly explaining myself. I just wanted to point out that if you have trained RL agent it will work much better in simulated environment than in real world (if at all), where you not completely sure what your actuators doing and your position is approximate. This can break the model completely.</p>",2019-11-12T01:59:32Z,30,Week 11/10 - 11/16,feedback,,jqkxzdmmolGf,k2v7jnbmmsvb,2019-11-12T01:59:32Z,{},other
4001,no,"<p>Yeah, from memory EM was super important for exactly that reason; it allowed you to improve with imperfect input and stochastic actions.</p>",2019-11-12T11:10:11Z,30,Week 11/10 - 11/16,feedback,,jl3oi5v7qkSk,k2vr7sw6kihxz,2019-11-12T11:10:11Z,{},other
4002,no,"<p>sergei - but oddly enough; not control theory ;)  you would be surprised at how old, some of the techniques are (i.e. they have not been improved upon despite decades of research).</p>
<p></p>
<p>at chris:  thank you kindly for the reference term ;)</p>
<p></p>
<p></p>
<p></p>",2019-11-15T12:40:35Z,30,Week 11/10 - 11/16,feedback,,jzivtxcbl6964n,k304rlz9gne5b0,2019-11-15T12:40:35Z,{},other
4003,no,"<p>Here is a capture of our chat log during the session, since many good info is in there, and I don&#39;t want them to get lost in the void.</p>
<p></p>
<blockquote>
<p>Uno 10:05 AM  hello :)<br />Dalton Bassett 10:06 AM  https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf<br />Uno 10:07 AM  Nash-Q Learning for General-Sum Stochastic Games, Junling Hu, Michael P. Wellman<br />Uno 10:09 AM  has examples for general sum games (with answers); if you are working to adapt ur zero-sum game solver, to &#34;general sum&#34; games, this gives u test cases. For the Corr-Q, they also provide a walk-through (for correlated equilibrium).<br />Shailesh 10:12 AM  yes thats true<br />Uno 10:13 AM  I saw on one of the q algorithms, it does use 2 separate q-tables; one for your values; one for the estimation of the rewards for the opponent.<br />Geoffrey Zhu 10:13 AM  are these algorithms on-policy or off-policy? from the 2007 paper, it appears that all algos are on-policy?<br />Daniel Luci 10:18 AM  i have a question about the linear optimization in soccer game minimax - q<br />Uno 10:18 AM  [q] the algorithms all reference solving for nash equilibrium reference Lemke-Howson as the nash equilbrium solver; apparently this returns equilibria in a ranked order.<br />Uno 10:19 AM  this would affect the results (as a hidden form of coordination); should we use our own lp-solver?<br />Geoffrey Zhu 10:19 AM  the paper seems to speficially say that it is on-policy Q-learning. in it it says specifically that off policy Q learning won&#39;t converge and wanted to test on policy Q learning, which I guess is SARSA<br />Geoffrey Zhu 10:19 AM  the paper = 2007 version<br />Uno 10:24 AM  you need an additional constraint Daniel; a &#43; b &#43; c &lt;= 1; and -a -b -c -1<br />Uno 10:24 AM  cvxopt doesn&#39;t have an = sign. so u have to have two of them (the positive) &#43; (the negative).<br />Uno 10:26 AM  [q] does q algorithm use the Q.t&#43;1(s,a) = (1-alpha) * Q.t(s,a,) &#43; alpha * (reward &#43; gamma(discounted)) ? This is different than the Q I have been using so far (and makes more sense!) :) the (1-alpha) is the new part; this is correct for Q?<br />Uno 10:28 AM  thank you! very clear! :)<br />Uno 10:29 AM  yes. that is the key :) which is why it made sense ;)<br />Daniel Luci 10:29 AM  One thing, isn&#39;t rationality constraint for Correlated - Q? Reading greenwald i see they implement the rationality constraint there, but not in Littman 1994 for the soccergame with just minimax-Q<br />Geoffrey Zhu 10:29 AM  uno, what were you using?<br />Daniel Luci 10:30 AM  Ok gotcha<br />Uno 10:30 AM  q &#43;= alpha * (r &#43; gamma * (td error)). convex combination makes way more sense ;)<br />Daniel Luci 10:30 AM  So its not associated with specifically Correlated q<br />Raghav 10:31 AM  [q] My program works most of the times to solve for the probabilities that help in convergence. There are times when the iterations would stop throwing an error about an infeasible solution during LP, but restarting the program makes it work. How can I make my solution more robust?<br />Daniel Luci 10:31 AM  Ok great I&#39;ll take a look after this and try it again and test on dummy states as you said<br />Daniel Luci 10:32 AM  Sounds like my problem - iget infeasible solution as well sometimes but if i restart it will go sometimes<br />Quang 10:32 AM  I have a question regard to Alec OH, can we dicuss about that?<br />Quang 10:32 AM  https://github.com/axonal/cvxopt-tutorial/blob/master/Linear%20Programming.pdf<br />Raghav 10:33 AM  Yeah, I guess I was adding some stochasticity in the starting state, as to who gets the ball in the start and where do players start.<br />Raghav 10:34 AM  Sure. Thanks<br />Quang 10:34 AM  can you guys hear me?<br />Quang 10:34 AM  sorry, I will type my question<br />Uno 10:35 AM  [q] &#34;acceptable third-party library&#34; -- is writing the constraints for nash equilibria part of the problem? can we use nashpy for the nash equilibria (not the correlated Q)? it is basically an lp-solver written specifically for matrix games that preps the constraints automatically? it also includes lemke-howson?<br />Quang 10:35 AM  regard to the 4 inequalities, what was the right hand size of the Formular 1 represent?<br />Uno 10:36 AM  thats fair ;)<br />Uno 10:36 AM  it just preps the constraints from a matrix<br />Uno 10:36 AM  i.e. you provide sets of matrices.<br />Uno 10:37 AM  ok ;) it might help people test their constraints ;)<br />David Pendleton 10:38 AM  Question: Looking at the code, it&#39;s. unclear if we are observing all actions from a state or taking a random state<br />Jacob Anderson 10:38 AM  its the comparison against the other constraints<br />Jacob Anderson 10:38 AM  row 1 versus row 2, row 3 ... etc<br />Jacob Anderson 10:38 AM  then column 1 vs column 2, column 3, etc.<br />Uno 10:38 AM  Quang: look at the very old version of the GreenWald paper, page 2. they walk you through the calculations.<br />Jacob Anderson 10:38 AM  yes - chicken<br />Uno 10:39 AM  for &#34;chicken&#34; example<br />Quang 10:39 AM  yea, I try to get understanding about the calculation<br />David Pendleton 10:39 AM  random action*<br />David Pendleton 10:40 AM  The psuedo code in the paper<br />David Pendleton 10:40 AM  page 3<br />David Pendleton 10:40 AM  yes<br />David Pendleton 10:41 AM  Its Table 1<br />David Pendleton 10:41 AM  yes<br />David Pendleton 10:41 AM  Table 1<br />Jacob Anderson 10:42 AM  that&#39;s the template<br />David Pendleton 10:42 AM  Are we talking random actions or observing all actions from a state<br />Uno 10:42 AM  quang - correlated eq is like nash equilibria, but where total reward (across both payoff matrixes are used); so you get the standard Nash Equilibrium constraints (the four listed); constrains from game theory; and you get an overall objective function (that uses total rewards) -- not a &#34;free&#34; variable.<br />David Pendleton 10:43 AM  The only difference is how the value of the state is calculated?<br />David Pendleton 10:43 AM  gotcha seems more simple than i thought<br />David Pendleton 10:43 AM  thanks!<br />Quang 10:45 AM  Thanks Uno!<br />Uno 10:45 AM  np ;)<br />Guilherme Pires 10:45 AM  hi Quang, looks like your mic has an issue<br />Uno 10:45 AM  [q] can u talk about the difference between an online and offline in the RL context? how does it relate to asynch update (updates from older q-values) given that the bellman update still gaurantees convergence?<br />Quang 10:46 AM  y ea, I am on Linux. And seems like I have 50% chance of it working right, every reboot. Sorry if this causes issue for   .<br />Li Zhong Zhang 10:47 AM  I had tried to store or not store V value, the final plociy was the same.<br />Uno 10:52 AM  thank you! :) very helpful ;)<br />Uno 10:52 AM  thats w hat i am was missing ;)  Uno 10:52 AM  &#34;cliffwalker&#34; I skimmed it ;)<br />Uno 10:54 AM  you are sneaking in a resoirvoir of &#34;old policies&#34; which might give u a chance to turn away from the cliff ;)<br />Uno 10:54 AM  it keeps you from taking the argmax of jumping off the cliff ;)<br />Uno 10:59 AM  [p3 question] Littman 1994 mentions doing random training for a 100,000 steps. this is not applicable for greenwald, &#34;Q learner&#34; ?<br />Uno 11:00 AM  Q learning</p>
</blockquote>",2019-11-16T22:17:31Z,28,Week 11/10 - 11/16,followup,,jl5wq8mca7o0,k324tes8ioj50e,2019-11-16T22:17:31Z,{},logistics
4004,no,<p>Where is the recording?</p>,2019-12-06T05:12:51Z,25,Week 12/1 - 12/7,followup,,gx3c8l7z7r72zl,k3tp0pf2z6a176,2019-12-06T05:12:51Z,{},logistics
4005,no,<p>Are office hours == study sessions ?</p>,2019-12-06T05:13:44Z,25,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3tp1uc0ump234,2019-12-06T05:13:44Z,{},logistics
4006,no,"<p>Thanks, but that does not give any information about what percentage scores are needed for each grade? Is it A &gt; 90 as in some courses? Or curved as in others?</p>",2019-11-12T18:51:44Z,21,Week 11/10 - 11/16,followup,,jzjwcq2u8o7110,k2w7pcymtzm2z2,2019-11-12T18:51:44Z,{},logistics
4007,no,"<p>See &#64;731, specifically, Professor Isbell&#39;s posts.</p>",2019-11-12T20:34:47Z,21,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2wbdvc3tnb60a,2019-11-12T20:34:47Z,{},logistics
4008,no,"<p>Thanks, I&#39;ve seen it, and assume there is some sort curve. I would just like to know what the ranges (is A usually around 90/80/70 etc?) usually end up being. But I guess they do not wish to share that information.</p>",2019-11-13T13:38:36Z,21,Week 11/10 - 11/16,feedback,,jzjwcq2u8o7110,k2xbyiq4i8775q,2019-11-13T13:38:36Z,{},logistics
4009,no,"Thanks, I remember seeing that but couldn’t find it when I posted that question. Must be the sleep deprivation. ",2019-11-12T12:03:21Z,53,Week 11/10 - 11/16,followup,,i4jbttw9ru63ot,k2vt45vlfx5v3,2019-11-12T12:03:21Z,{},other
4010,stud,You are very welcome.,2019-11-12T12:07:19Z,53,Week 11/10 - 11/16,feedback,a_0,,k2vt99rogzc5u5,2019-11-12T12:07:19Z,{},other
4011,stud,"<p>&#64;Jacob, thanks for the response. I know that. :-) But I am not just talking about operators w.r.t. the MDPs. I am asking about resources to learn operators in general.</p>
<p></p>
<p></p>",2019-11-12T22:40:48Z,28,Week 11/10 - 11/16,followup,a_0,,k2wfvxv8guyn6,2019-11-12T22:40:48Z,{},other
4012,no,"<p><em>kicks can</em> ..</p>
<p></p>
<p>I thought I was being clever. sorry - these operators remind me of Laplace transforms. I wonder if the same idea of a Laplace transform applies here...</p>",2019-11-13T00:26:44Z,28,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wjo5vmezd2le,2019-11-13T00:26:44Z,{},other
4013,stud,"<p>You can start here: <a href=""https://en.m.wikipedia.org/wiki/Operator_%28mathematics%29"" target=""_blank"" rel=""noopener noreferrer"">Operators</a></p>
<p></p>
<p>From there, good places to go are linear algebra textbooks (Check out the later chapters of &#34;Linear Algebra Done Right&#34;) and functional analysis textbooks. Mathematical physics textbooks (e.g., Boas) are also a good place to go (less rigorous, more intuitive treatment). <i>Linear</i> operators are ubiquitous in quantum mechanics, and for that reason the &#34;mathematical preliminaries&#34; chapter of many QM textbooks is also often helpful (e.g., the first chapter of Shankar&#39;s &#34;Introduction to Quantum Mechanics&#34;). Most decent introductory real analysis textbooks will discuss the formal properties of contraction mappings in particular in some level of detail (e.g., Bartles). </p>
<p></p>
<p>I usually like to learn this stuff in a more applied setting first, and then look at more formal treatments afterwards. But do whatever seems to work best for you. </p>",2019-11-13T01:50:07Z,28,Week 11/10 - 11/16,feedback,a_1,,k2wmnemc9vi3tm,2019-11-13T01:50:07Z,{},other
4014,stud,"<p>&#64;Anonymous, thank you so much for the very detailed list of resources! :-)  I did have some exposure on linear operators but don&#39;t know much beyond that. This list is hugely helpful for me to explore and learn more. </p>
<p></p>
<p>&#64;Jacob, no I was not clear about what I was looking for...  Off-topic: are you on slack? :-)</p>",2019-11-13T02:37:27Z,28,Week 11/10 - 11/16,feedback,a_0,,k2woc9ihua15rp,2019-11-13T02:37:27Z,{},other
4015,no,"<p>yes, I am on slack.</p>",2019-11-13T03:36:16Z,28,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wqfwmlwv8lf,2019-11-13T03:36:16Z,{},other
4016,stud,"<p>Happy to help!</p>
<p></p>
<p>My recommendation for functional analysis would be <em>Linear Functional Analysis (2nd ed.) </em>by Rynne and Youngson. It&#39;s a SUMS text, and you may be able to find a cheap used or electronic copy. </p>
<p></p>
<p>Unfortunately, these texts usually aren&#39;t self-contained (though this series at least usually makes an attempt to be). In particular Rynee and Youngson characterize functional analysis as &#34;infinite-dimensional linear algebra combined with analysis&#34;, and I think that&#39;s a fair characterization. So it can be tough to get through one of these works without some prior knowledge of real analysis and linear algebra. To their credit, Rynee and Youngson <em>try</em> to give you what you&#39;ll need in the first 30 pages, though. So it&#39;s worth a look even if you haven&#39;t had any intermediate/advanced linear algebra or real analysis. </p>",2019-11-13T17:41:51Z,28,Week 11/10 - 11/16,feedback,a_1,,k2xknbqlpyg1o4,2019-11-13T17:41:51Z,{},other
4017,stud,"<p>&#64;Anonymous, thanks again for your recommendation. It is really helpful. I am interested in these things. My goal is to build it up over time.</p>
<p></p>
<p></p>
<p></p>",2019-11-13T23:40:28Z,28,Week 11/10 - 11/16,feedback,a_0,,k2xxgix43po28n,2019-11-13T23:40:28Z,{},other
4018,stud,"<p><strong attention=""is5gzbotXmz"">&#64;tianhang zhu</strong>  , thanks for your suggestions and guidance. They have been very helpful. I appreciate it. </p>",2019-11-13T23:41:27Z,28,Week 11/10 - 11/16,followup,a_0,,k2xxhs6z9y63lb,2019-11-13T23:41:27Z,{},other
4019,no,"<p>I just ran across the originating mathematics, circa the 19th century.   look up fixed point theorems. back track.  alot of the foundation was layed long before 1980&#39;s.  I think there were probably three different major periods for contributions; two pure theory, one applied.   they are major results in mathematics.  you can&#39;t miss&#39;em.</p>
<p></p>
<p></p>",2019-11-15T11:35:38Z,28,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k302g305jd9g5,2019-11-15T11:35:38Z,{},other
4020,no,"<p>Thanks! So, the environment is deterministic, but the policy is probabilistic.</p>",2019-11-12T18:21:50Z,53,Week 11/10 - 11/16,followup,,jl1b27fpaYkv,k2w6mwveh0i2gy,2019-11-12T18:21:50Z,{},project3
4021,no,<p>Correct.</p>,2019-11-12T18:23:52Z,53,Week 11/10 - 11/16,feedback,,i4i9bi8rFqk,k2w6pin8gun515,2019-11-12T18:23:52Z,{},project3
4022,no,"<p>I claim the environment is stochastic, since you can lump the process of deciding which agent acts first into the environment.  But this is the only stochasticity.</p>",2019-11-12T18:29:32Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2w6wsshc96215,2019-11-12T18:29:32Z,{},project3
4023,no,<p>True. Agreed.</p>,2019-11-12T18:34:10Z,53,Week 11/10 - 11/16,feedback,,i4i9bi8rFqk,k2w72roo51g2j6,2019-11-12T18:34:10Z,{},project3
4024,no,"<p>I see. So if A moves first, and we don&#39;t let B move, that would yield a collision.</p>
<p>The usual behavior is to let the first player move. If we let A move, it&#39;s now on top of B. </p>
<p></p>
<p>That&#39;s not a legal position, right?</p>
<p>So, should the resulting position be the same as the starting one? i.e. They both can&#39;t move?</p>",2019-11-13T00:10:48Z,53,Week 11/10 - 11/16,followup,,jl1b27fpaYkv,k2wj3oauifi6hb,2019-11-13T00:10:48Z,{},project3
4025,no,<p>Right (my opinion).</p>,2019-11-13T00:12:55Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2wj6eb6b4yoe,2019-11-13T00:12:55Z,{},project3
4026,no,<p>And lets say player B had the ball. Does this mean player A steals the ball in this case? Since B was second to move?</p>,2019-11-13T00:19:26Z,53,Week 11/10 - 11/16,feedback,,jl1b27fpaYkv,k2wjes6ppjv2yp,2019-11-13T00:19:26Z,{},project3
4027,no,"<p>But they are supposed to move simultaneously, correct? Otherwise, you are doing a turn-based movement. The &#34;who moves first&#34; is only used for collision resolution. That&#39;s my interpretation anyway.</p>",2019-11-13T00:24:54Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wjlt97lok93,2019-11-13T00:24:54Z,{},project3
4028,no,"<p>The paper says &#34;The players actions are executed in a random order&#34;. So, I don&#39;t think it&#39;s simultaneous.</p>",2019-11-13T00:27:11Z,53,Week 11/10 - 11/16,feedback,,jl1b27fpaYkv,k2wjor3ffo25il,2019-11-13T00:27:11Z,{},project3
4029,no,"<p>The sequential movement <em>looks</em> simultaneous when there is no collision possible.  The sequential character only manifests itself because of collisions (and maybe other edge cases depending on your implementation).</p>
<p></p>
<p>Daniel, I&#39;m not going to answer your second question because I think we should resolve these subtleties on our own, as different people may have different interpretations and I think having people try/test different implementations is part of the fun.</p>",2019-11-13T00:40:27Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2wk5ssz3hl5sy,2019-11-13T00:40:27Z,{},project3
4030,no,"<p>In the paper, Greenwald wrote:</p>
<blockquote>
<p>If the sequence of actions causes the players to collide, then only the first moves.</p>
</blockquote>
<p>I interpret it as pass-through is possible, otherwise the players collide in mid-way and neither of them can make a move.</p>
<p></p>
<p>However, I also think it would end up does not matter whether pass-through is possible or not.</p>
<p>If pass-through, B (the defender) would end up in the back of A, and it would result in a unrecoverable lose.</p>
<p>If pass-through is not possible, both A &amp; B stay the same. But there is some possibility of A move S instead of E, then:</p>
<p>[ ][B][ ][ ]</p>
<p>[ ][A][ ][ ]</p>
<p>also results unrecoverable lose.</p>
<p></p>
<p>So, in any case (assume non-cooperative mode), the defender would never make a move toward attacker, when there is other move (ie: stay) which has equal or larger average payoff.</p>",2019-11-15T06:21:27Z,53,Week 11/10 - 11/16,feedback,,jl5wq8mca7o0,k2zr81b5cu97fl,2019-11-15T06:21:27Z,{},project3
4031,no,"<p>&#34;<em>If the sequence of actions causes the players to collide, then only the first moves.&#34;</em></p>
<p><em></em></p>
<p>I interpret this as referring to the case when both players are trying to move to the same, <strong>empty</strong> spot.  When both players are moving into each other, it is taken as obvious that neither one moves, because if only the first one were to move in that situation, they&#39;d both end up being on the same square, right?  But this is just my interpretation.</p>",2019-11-15T06:29:16Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2zri3lmt1j4jw,2019-11-15T06:29:16Z,{},project3
4032,no,"<p>I also did a comingled action version and the result is mostly similar. Alpha = 1.0 is a horrible selection of learning rate, it does dumb things.</p>",2019-11-13T02:23:49Z,53,Week 11/10 - 11/16,followup,,jc554vxmyuy3pt,k2wnuqli59l3fz,2019-11-13T02:23:49Z,{},project3
4033,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2wo8dopzaw5%2Fexp1epdecayarangedshare.png"" alt="""" /></p>",2019-11-13T02:34:33Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2wo8j4ntd66pa,2019-11-13T02:34:33Z,{},project3
4034,no,"<p>The Q value for most states can converge because the optimal strategy will be a pure one. The only Q values that cannot converge are those where a mixed strategy is required. If your Q learner is playing against a random agent it can learn to converge to a pure strategy even on those mixed strategy states because the random agent won&#39;t learn to punish pure strategies. However, consider two Q learners both learning continuously and playing against each other. If they reach a state where a mixed strategy is optimal (such as a rock paper scissors state, or state S in the paper), each agent will have to shift their policy continually to respond to the policy of the other agent. e.g. if the first agent pure strategy is rock (or moving down), the second agent may lose more for a while until it learns the pure strategy of paper (or matching the downward movement). When the other agent shifts to paper, the first wants to shift to scissors. This continues indefinitely between two Q learners since they learn pure strategies.</p>
<p></p>
<p>Another way to look at this: rather than plot shifts in Q value (which will continue to get smaller due to alpha), plot the optimal policy at state &#39;s&#39; (the mixed strategy state from 2003 fig 4). Which action is optimal for agent B at that state? In my implementation, even though the Q-value oscillation appears to settle down for that state, the policy does not converge, and continues to shift indefinitely as the other agent adapts and the first agent must adapt as well.</p>",2019-11-13T15:15:35Z,53,Week 11/10 - 11/16,followup,,isde34zracb1mz,k2xff8eeklx325,2019-11-13T15:15:35Z,{},project3
4035,no,"<p>Thanks Michael! My graphs are of two Q agents competing against each other. The alphas are held constant to refute the notion that Q converges solely because alpha is changing.</p>
<p></p>
<p>I believe your explanation of the pure strategy versus mixed strategy. I think that works for agents that use mixed strategies, but in the Q learner those are not used.</p>
<p></p>
<p>In my Correlated-Q graph, the two agents are tracking (a,b) action tuples, whereas the Classic-Q graph uses the (a) or (b) actions blindly.</p>
<p></p>
<p>The values you are seeing are the chosen state (start state in the Greenwald paper) value differences.</p>",2019-11-13T15:32:10Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2xg0khj5kt11n,2019-11-13T15:32:10Z,{},project3
4036,no,"<p>Have you tried plotting policy stability at the state in question?</p>
<p></p>
<p>If the policy does not converge, does it make sense to say associated Q values have converged?</p>",2019-11-13T15:42:52Z,53,Week 11/10 - 11/16,feedback,,isde34zracb1mz,k2xgebqendf4z3,2019-11-13T15:42:52Z,{},project3
4037,no,"<p>&#34;has converged&#34; and &#34;converging/converges&#34; - two different notions here. In the short 1e6 iterations that we are allowed in this experiment, the Q trends towards convergence (see graphs).</p>
<p></p>
<p>I&#39;ve also graphed the total Q matrix convergence and that too follows the same trend, meaning that the policy is converging (and not just the one isolated state and action pair) - see &#64;891.</p>
<p></p>
<p>Even Littman suggests in his paper that Q can produce a stable policy, but further states there is no mathematical reasoning to justify it.</p>
<p></p>
<p>I don&#39;t see any justification on the part of Greenwald to proclaim that Q does not converge. The interpretation of the alpha being the sole cause of the &#34;converging&#34; behavior is wrong given the evidence I have shown in my graphs above. At almost every constant alpha, with no decay, the policy in question is converging. Only at alpha = 0.7 is there a tail trend upwards, which could be noise, like what you can see in the alpha=1.0 graph.</p>
<p></p>
<p>I think what Greenwald should have claimed is that Q does not converge as reliably or quickly as Foe-Q or Cor-Q in the short 1e6 iterations. Those iterations could be 30,000 episodes or 115,000 episodes. This is important because the number of games played determines the value function - right? We don&#39;t get a reward until the game ends, so that would give 30,000 rewards, or 115,000 rewards, etc. To make the analysis fair all of the algorithms should have been played to the same number of rewards. What if I had only 1 game in those 1e6 iterations? That could happen if I left the step loop unbounded.</p>
<p></p>
<p>I wonder what Greenwald used as the game loop bounds? I used 500. One could graph loop bound versus Q stability as a surface plot to see if there is an interesting correlation there.</p>
<p></p>
<p></p>
<p></p>",2019-11-13T16:29:53Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2xi2s3wn3224t,2019-11-13T16:29:53Z,{},project3
4038,no,"<p>If a Q learner&#39;s policy does not (will not, cannot) converge regardless of how many training iterations it runs, is it fair to say the Q values converge? (Considering the policy is a function of the values, the answer doesn&#39;t seem ambiguous).</p>
<p></p>
<p>If you train two Q learners against each other on a paper rock scissors problem from HW6, you&#39;ll see values for each action oscillate indefinitely as each Q learner shifts its policy to punish the pure strategy of the other. This indefinite oscillation of values is what Greenwald &amp; Hall refer to as not converging. Considering true Q values in a normal MDP do not oscillate, this seems to be a valid distinction.</p>
<p></p>
<p>The underlying point is Q learners do not learn stable policies when a mixed strategy is necessary. At best, it can learn to shift between pure strategies, but one would expect it to be outperformed by an agent capable of playing perfectly optimal mixed strategies (e.g. in a hw6 paper rock scissors series of matches). If we&#39;re clear on underlying capabilities and limits, I don&#39;t see much point debating semantics.</p>",2019-11-13T16:55:31Z,53,Week 11/10 - 11/16,feedback,,isde34zracb1mz,k2xizrawa5h79o,2019-11-13T16:55:31Z,{},project3
4039,no,"<p>The problem that I have with this soccer game is that the only policy for A is Tit-for-Tat. The rules give dominance to the player with the ball, which is always B. No matter what policy B takes, no matter what algorithm, or anything at all, A&#39;s sole strategy is TfT. When A gets the ball because B messes up, then B&#39;s only policy is TfT.</p>
<p></p>
<p>This is solely because there is no stealing in the game and because you can&#39;t move diagonally. Therefore, the non-possessing player only has defense as its strategy. So if you were to Q on a strategy, where Q(TfT, Any-Q) is what I mean, then you would always choose a pure strategy based upon who possesses the ball. You would never blend your strategy because it would never benefit you to do anything but TfT when you didn&#39;t have the ball.</p>
<p></p>
<p>Am I not understanding this soccer game?</p>",2019-11-13T17:18:27Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2xjt8jxjfe48s,2019-11-13T17:18:27Z,{},project3
4040,no,"<p>Tit-for-Tat refers to choosing to cooperate or play as an adversary, mirroring whatever action the other player did previously. It can apply to games like the <a href=""https://economics-games.com/prisoner-dilemma"" target=""_blank"" rel=""noopener noreferrer"">repeated prisoner&#39;s dilemma</a> where maximizing cooperation is ideal if the game repeats indefinitely. It doesn&#39;t exactly apply to the soccer environment because both players are always adversaries with no notion of cooperation.</p>
<p></p>
<p>I&#39;d agree the soccer environment isn&#39;t a great one for demonstrating the power of mixed strategies, especially not the simplified (2-row) soccer environment used in Greenwald &amp; Hall 2003. If you imagine how to play the game optimally as a human from state &#39;s&#39; (B has the ball, both players are adjacent on the top row), B only wants to risk moving right if they believe A will be moving down, and that the randomness of the environment will have A move down before B moves right. If that transition happens, B is guaranteed able to score because A can never get in a blocking position again (and stealing isn&#39;t allowed). B is in the exact same conundrum if they&#39;re in the same starting position and A starts one row lower ==&gt; A wants to move up to block, and B only wants to move right if they believe the stochastic environment will have them move right before A gets there to block. B&#39;s best case scenario is a 50/50 regardless. A better environment for showcasing mixed strategies could provide multiple options for bypassing (akin to paper rock scissors), e.g. B bypass on left, bypass on right, bypass center (e.g. ball through legs), A must choose to move or choose a defensive strategy in its position (one that hard counters, gaining possession and a positional advantage, one neutral that shifts possession with some % chance but never changes position, and one losing that gets successfully bypassed). Ideally the environment offers different chances of success when agents approach each other from perpendicular starting points so there&#39;s some meaningful reason to shift your row and attempt faking out the other player.</p>
<p></p>
<p>Overall, the soccer environment&#39;s not great to showcase mixed strategies since perfect play results in a 50/50 at best, and as soon as the players are in the same column whoever happens to have possession at the end of the step leading to that position is guaranteed to score.</p>",2019-11-13T18:28:09Z,53,Week 11/10 - 11/16,feedback,,isde332xcka1m0,k2xmavwnxfg40f,2019-11-13T18:28:09Z,{},project3
4041,no,"<p>Turns out, all of this discourse made me go back and render the episodes. I found that my B -&gt; A possession transition was not happening properly, so I fixed that, and the classic SARSA behaves like as Greenwald suggests:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk2xpkff3kjfz%2Fexp16analysis.png"" alt="""" /></p>",2019-11-13T20:00:03Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2xpl1yhzoz18e,2019-11-13T20:00:03Z,{},project3
4042,no,<p>Nice!</p>,2019-11-13T20:10:42Z,53,Week 11/10 - 11/16,feedback,,isde332xcka1m0,k2xpyr1p79b1cl,2019-11-13T20:10:42Z,{},project3
4043,no,"<p>Can we all agree that Jacob has clearly earned the &#34;best graphs of the class award&#34;??  </p>
<p></p>
<p>Very nice Jacob, thanks for sharing these observations.</p>
<p></p>",2019-11-14T03:52:56Z,53,Week 11/10 - 11/16,feedback,,is9so9huTMp,k2y6h6v8s32wk,2019-11-14T03:52:56Z,{},project3
4044,no,"<p>&#64;Nick, couldn&#39;t agree more.  Jacob&#39;s 3D surface plots on project 1 were amazing and inspired a number of improvements (and lessons learned) in my own project. Jacob&#39;s policy heatmap visualizations are also thought provoking and help give an intuition for what&#39;s happening. I&#39;d like to learn how to do them at some point. :-)</p>",2019-11-14T18:41:03Z,53,Week 11/10 - 11/16,feedback,,isde332xcka1m0,k2z27bm98gp4ph,2019-11-14T18:41:03Z,{},project3
4045,no,"<p>Agreed.</p>
<p></p>
<p>I think this kind of data visualization can lead to drastic speed-ups in development time, as well as better understanding of the problem.  But there&#39;s an inertia to doing it because it requires an overhead of figuring out how to do it, and setting it up.</p>
<p></p>
<p>Jacob plows right through that inertia.  I think this is something we can all learn from him (I certainly can).</p>",2019-11-14T18:50:19Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2z2j87sn8h4ft,2019-11-14T18:50:19Z,{},project3
4046,no,"<p>Thanks guys. I appreciate the kind thoughts. I have much more freedom and latitude that comes with being older than most of you. The heatmap code came from the internet, and I just adapted it to work with the Q matrices.</p>
<p></p>
<p>I am happy to share with the class if the TAs allow it. There are no spoilers in the heatmap generator. It&#39;s a standalone python script that I use.</p>
<p></p>
<div>
<div>https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html</div>
<div></div>
<div></div>
</div>",2019-11-14T19:29:10Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2z3x6unt8p4qp,2019-11-14T19:29:10Z,{},project3
4047,no,<p>Being able to turn the data into information through visualization allows us humble humans to see the problem and understand it quickly.  Or at least monitor it over iterations or subsequent code runs.  This is a very valuable skill and one I do hope to become better at.  Thanks for sharing the link Jacob.  </p>,2019-11-15T03:55:30Z,53,Week 11/10 - 11/16,feedback,,is9so9huTMp,k2zm0culdhg7ou,2019-11-15T03:55:30Z,{},project3
4048,no,<p>when is it?</p>,2019-11-22T15:13:44Z,26,Week 11/17 - 11/23,followup,,jzj7y1ofgsro1,k3aabimew5t43k,2019-11-22T15:13:44Z,{},final_exam
4049,no,<p>The dummy exam will likely open a few days before the actual exam; and should be another available test in ProctorTrack.</p>,2019-11-23T02:05:17Z,26,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k3axlfgpkfoq,2019-11-23T02:05:17Z,{},final_exam
4050,no,"<p>Was hoping to test my setup today, Am getting &#34;We could not find any proctored test in this course. Please try refreshing this page and if you still cannot find your proctored test listed here, please contact your instructor.&#34;</p>",2019-12-03T15:03:08Z,24,Week 12/1 - 12/7,followup,,jzj7y1ofgsro1,k3pzs983zei2ok,2019-12-03T15:03:08Z,{},final_exam
4051,no,"<p>I created a class that has the same interface as gym (though it does not use gym since I don&#39;t see the benefit), step(), render(), reset(). Internally it keeps a list of all the possible states in a reasonable order so that finding the new state given an action is not harder adding a value to the current state index. Step, of course, has a bunch of if statements to handle collisions and edge cases.</p>",2019-11-13T19:40:47Z,53,Week 11/10 - 11/16,followup,,jzjwcq2u8o7110,k2xowain53u1gz,2019-11-13T19:40:47Z,{},project3
4052,no,"<p>I did the same, just kept the same interface as I was used to from LL, even tough internally was handled using numpy arrays.</p>",2019-11-13T22:55:47Z,53,Week 11/10 - 11/16,feedback,,is6e83bsfvk,k2xvv1si9i83y0,2019-11-13T22:55:47Z,{},project3
4053,no,"<p>I see the player&#39;s actions are in random order. Does this mean completely random, as in, player A could have 3 consistent moves or random meaning the player that goes first is at random then the next player goes and so on?</p>",2019-11-14T02:48:06Z,53,Week 11/10 - 11/16,followup,,j6m1jeidndu6wq,k2y45tf0nlk5ps,2019-11-14T02:48:06Z,{},project3
4054,no,<p>I would say the latter - the stochasticity is on who makes the move first.</p>,2019-11-14T06:13:16Z,53,Week 11/10 - 11/16,feedback,,is6e83bsfvk,k2ybhnzmkkp5sv,2019-11-14T06:13:16Z,{},project3
4055,no,"<p>There is the one &#64;895, I like that it comes with a render function but the code is cryptic and redundant. I found it easier to roll my own and think about the rules in the process. I used classes, not transition functions, which may be less efficient but models the environment clearly and allows for experimenting with the rules.</p>",2019-11-14T17:51:48Z,53,Week 11/10 - 11/16,followup,,is8ald0uljj3u4,k2z0fzfr599ng,2019-11-14T17:51:48Z,{},project3
4056,no,"<p>I agree.  It&#39;s exactly as you said:  rewards are in reality not discounted, and our discounting is merely reward shaping and a proxy to the fact that the episode can un-naturally end (after 1000 time steps).  Furthermore, the proper value of the discount factor is non-obvious and takes empirical testing to find out; in other words, if you try to find a theoretical basis for setting $$\gamma$$ you&#39;ll likely be wrong (whatever model you use will likely be insufficient).</p>",2019-11-13T18:30:33Z,29,Week 11/10 - 11/16,followup,,jzfsa4a37jf4aq,k2xmdyikjrd45s,2019-11-13T18:30:33Z,{},project2
4057,stud,"<p><strong attention=""is5gzbotXmz"">&#64;tianhang zhu</strong>  , thanks for the detailed answer. &#64;Vahe, thanks for your response. It seems I am thinking about this right... </p>",2019-11-14T23:57:14Z,29,Week 11/10 - 11/16,followup,a_0,,k2zdhxw6uzw2dq,2019-11-14T23:57:14Z,{},project2
4058,stud,"<p>Yeah, I am curious, too.</p>",2019-11-14T02:34:47Z,53,Week 11/10 - 11/16,followup,a_0,,k2y3oor790y6bg,2019-11-14T02:34:47Z,{},project3
4059,stud,"<p>On second thought, I think I will just go from first principles... </p>",2019-11-14T02:44:39Z,53,Week 11/10 - 11/16,feedback,a_0,,k2y41ddgao535,2019-11-14T02:44:39Z,{},project3
4060,no,<p>this is the unclear part which we can point out in our paper. I simply follow Greenwald 2003 paper using s&#39; to compute the V(s&#39;) value. I.e V(s&#39;) = f( Q(s&#39;) ... ) </p>,2019-11-14T02:37:15Z,53,Week 11/10 - 11/16,followup,,jvfpllmsggt7p4,k2y3rv84ust4zi,2019-11-14T02:37:15Z,{},project3
4061,no,"<p>Is it because in the Nash equilibrium, no one has an incentive to change states anymore? So you can discount it because it will be the same as your future state? Not sure, just thinking out loud. </p>",2019-11-14T04:09:30Z,53,Week 11/10 - 11/16,followup,,jl3we43d3bp15p,k2y72i6zb6a5v,2019-11-14T04:09:30Z,{},project3
4062,no,"<p>That&#39;s a good idea.  But the values of $$Q_1, ..., Q_n$$ will vary depending on the state we evaluate them at, namely either $$s$$ or $$s&#39;$$.  Depending on which one we choose, we get a different matrix game, with its own Nash equilibrium.</p>
<p></p>
<p>But in any case, isn&#39;t equation (6) a learning rule?  The Nash function is one small piece of that meant to give us a value used as part of our learning update. Since our Q-values haven&#39;t converged yet; they&#39;re just estimates and we&#39;re trying to make them more and more accurate by that assignment statement (i.e. learning).</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-11-14T04:45:49Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2y8d71lg2t3po,2019-11-14T04:45:49Z,{},project3
4063,no,"<p>No, you&#39;re absolutely right. I read Hu and Wellman (1998) and they very explicitly use s&#39; in the Nash function for their update rule. And I suppose my previous statement wasn&#39;t correct then. You can and do change states, but while updating you calculate the Nash equilibrium based on what you know about the other agents (by maintaining Q tables for all of them), and assume they all follow it starting from state s&#39;.</p>",2019-11-14T05:16:35Z,53,Week 11/10 - 11/16,feedback,,jl3we43d3bp15p,k2y9grvz5iz6p7,2019-11-14T05:16:35Z,{},project3
4064,no,"<p>I hadn&#39;t looked at Hu and Wellman.  Pasting the corresponding equation from there:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk2y9n427ndp7%2FCapture.PNG"" alt="""" /></p>
<p></p>
<p>and comparing the two, it looks like the Littman difference is probably a typo.</p>
<p></p>
<p>Thanks Shayan!</p>",2019-11-14T05:22:35Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2y9ohd5yxa7a,2019-11-14T05:22:35Z,{},project3
4065,stud,"<p><em>&#34;will this affect the policy?&#34;</em></p>
<p><em></em></p>
<p>Let&#39;s suppose it <em>doesn&#39;t. </em>That would seem to suffice to show, at most, that it might be permissible to go in for this sort of discounting if all one cared about was the impact it had on determining the optimal policy. </p>
<p></p>
<p>It still doesn&#39;t answer the question <em>why</em> one would wish to do this in the first place.</p>
<p></p>
<p>There are all sorts of violent things one can do to the update rule while leaving the optimal policy unchanged. But there must be a <em>reason</em> for doing so, right? One wouldn&#39;t, say, add a sum over a million terms which add up to zero to the update rule. It would leave the optimal policy unchanged, for sure; But <em>what would be the point</em>? </p>
<p></p>
<p>I&#39;d also like to know the reason for this alternative form of the update rule. </p>",2019-11-14T13:59:33Z,53,Week 11/10 - 11/16,followup,a_0,,k2ys5b35v1y5t7,2019-11-14T13:59:33Z,{},project3
4066,no,"i don&#39;t know but the form look like it&#39;s trying to cancel the 1/1 - gamma. so in more detail, since we are assuming finite rewards. let rmax be the max reward. then maximum episode reward would be bounded by 1/(1-gamma) * ((1-gamma) *RMAX) . multiply in you have your episode reward bounded by rmax now.",2019-11-14T18:12:04Z,53,Week 11/10 - 11/16,feedback,,is5gzbotXmz,k2z161zkc3f6bl,2019-11-14T18:12:04Z,{},project3
4067,no,"<p>Cue &#34;see what happens without it&#34;. I interpret it as a <em>tax</em> on immediate rewards to push the learner to find future rewards. As for the practical effects, foe-q makes far more drastic updates without it and converges to a different value. Hmm...</p>",2019-11-14T17:30:10Z,53,Week 11/10 - 11/16,followup,,is8ald0uljj3u4,k2yzo65rj35oh,2019-11-14T17:30:10Z,{},project3
4068,no,"<p>But all the immediate rewards are $$0$$ in this environment, so taxing $$0$$ changes nothing.  There is only one reward, and it&#39;s a future one.</p>",2019-11-14T17:37:02Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2yzwzecq7o4g8,2019-11-14T17:37:02Z,{},project3
4069,stud,<p>&#43;1</p>,2019-11-14T18:29:09Z,53,Week 11/10 - 11/16,feedback,a_0,,k2z1s0cnmhi400,2019-11-14T18:29:09Z,{},project3
4070,no,"<p>What this (1- $$\gamma$$) is supposed to do is help &#34;stabilize training&#34;. Here is my understanding but this is by no means absolute, </p>
<p></p>
<p>Similar to supervised learning normalizing the reward function helps convergence. That being said, what&#39;s done here is not quite that in my eyes, we are taking a convex combination/weighted sum between the reward (high variance term) and the expected value (lower variance term). as such this acts to smooth this out some at the cost of propagating information slower and increasing potential bias.</p>
<p></p>
<p>Great question.</p>",2019-11-14T19:00:48Z,53,Week 11/10 - 11/16,followup,,jl1acpoc4HA9,k2z2wpk3tph5t2,2019-11-14T19:00:48Z,{},project3
4071,no,"<p>Thanks Farrukh,</p>
<p></p>
<p>So in a general RL problem, it would be like introducing a second learning hyperparameter (the first one being the actual learning rate, $$\alpha$$).<strong> Edit:</strong> Although this new hyperparameter would be forever entwined with the all important $$\gamma$$ and therefore not something you could really change at will.</p>
<p></p>
<p>However, in THIS specific problem, there is no real smoothing going on since rewards are all zero.  This convex combination takes effect only once per episode, at the terminal state.  And its net effect therefore is to just reduce the magnitude of this one, terminal reward.  It&#39;s like setting the reward for scoring a goal to &#43;10 instead of &#43;100.</p>",2019-11-14T19:13:50Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k2z3dhmmyso4os,2019-11-14T19:13:50Z,{},project3
4072,stud,"<p>To me, it appears that it is defining a new reward $$R&#39;_i = (1-\gamma) R_i$$ across the board. Per our lecture on rewards shaping, it has no real effect? </p>",2019-11-14T19:49:52Z,53,Week 11/10 - 11/16,feedback,a_1,,k2z4ntozkag5el,2019-11-14T19:49:52Z,{},project3
4073,no,well potential based reward shaping is to guide the agent to the states that you find good so it has to be function of s and it&#39;s biased to some states for sure. does this have shaping effect?,2019-11-14T21:12:20Z,53,Week 11/10 - 11/16,feedback,,is5gzbotXmz,k2z7lvmxdg55ld,2019-11-14T21:12:20Z,{},project3
4074,stud,"As a precursor to building up potential based shaping, the lecture shows there is no effect to optimal policy if we multiply a positive constant to the reward function, which seems to be the case here. This is definitely not rewards shaping per se but multiplying the reward function by a constant was discussed in that lecture.",2019-11-14T21:19:06Z,53,Week 11/10 - 11/16,feedback,a_1,,k2z7ul0dgl51sy,2019-11-14T21:19:06Z,{},project3
4075,no,"<p>I implemented this just now for my Foe-Q. No real change .... As noted, the lump sum reward makes it hard to influence the policy with this kind of blending. Maybe over 1 million games (like 800M iterations) it might make a difference.</p>",2019-11-14T23:42:51Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2zczfxtf5gd5,2019-11-14T23:42:51Z,{},project3
4076,no,"<p>You can also rewrite the Greenwald update as:</p>
<p></p>
<p>$$::= Q(s,\vec{a}) = (1 - \alpha)Q(s,\vec{a}) &#43; \alpha(R_i - \gamma R_i &#43; \gamma V_i(s&#39;))$$</p>
<p></p>
<p>$$::= Q(s,\vec{a}) = (1 - \alpha)Q(s,\vec{a}) &#43; \alpha(R_i &#43; \gamma(V_i(s&#39;) - R_i))$$</p>
<p></p>
<p>This way you can see that they are adjusting the equilibrium value by the transition reward. I don&#39;t see any mathematical reason to do this...</p>",2019-11-15T00:24:41Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k2zeh8a2iim63a,2019-11-15T00:24:41Z,{},project3
4077,no,"<p>you definitely need to use LP... Foe Q aka maxmin-Q will need to solve for pie(a) / sigma(a) similar to how we did it for HW6. If you functionalize that in hw6, you can use the exact same code to solve the pie(a) / sigma(a). At least that is what i did. </p>
<p></p>
<p>For correlated-Q, you will need to frame Q table / payoff matrix to solve for the joint sigma(a) / pie(a). Greenwald 2003 provided an example in section 2. I simply extend that beyond 2 actions to 5 actions. See &#64;908</p>
<p></p>
<p>Main difference to my understanding between the 2 is Foe-Q gives you the mixed strategy probability for each of your actions. Correlated Q gives you probabilities for your action and your opponent&#39;s action. (joint action space)</p>",2019-11-15T07:16:09Z,26,Week 11/10 - 11/16,followup,,jvfpllmsggt7p4,k2zt6dmljvs7gp,2019-11-15T07:16:09Z,{},project3
4078,no,"<p>equilibrium is independent RL, applied in multi-agent setting, in concurrent sense.  there are other alternatives to using nash equilibriums that lead to better (optimal results). this is because there may be several suboptimal nash equilibrium; and independent agents may have issues cycling between them (scoring an average of the optimal equilibrium point).  a fair amount of work put in, is in many different ways of coordinating agents actions (together) s.t. they can reach optimal &#34;team&#34; performance.</p>
<p></p>
<p>at a guess.</p>
<p></p>
<p></p>
<p></p>",2019-11-15T11:29:17Z,53,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k3027xcx8g31q2,2019-11-15T11:29:17Z,{},hw6
4079,no,"<p>I&#39;m pretty sure that peer grading consent disclosure message is left over from another use case and doesn&#39;t have any affect here since we&#39;re not doing any peer grading in our course and all the assignments are auto-graded. If someone is using our assignment data for some sort of research that would be good to know so we can opt-in/out as apt, but considering it says &#34;Consent for Peer Grading research: Thank you for signing up for the peer review program.&#34; it seems like it&#39;s referring to another course.  e.g. in Human Computer Interaction last semester we had a number of peer reviewed assignments.</p>",2019-11-15T18:48:07Z,53,Week 11/10 - 11/16,followup,,isde332xcka1m0,k30hw92hly64xv,2019-11-15T18:48:07Z,{},other
4080,no,<p>Thank you both :)</p>,2019-11-16T11:55:58Z,53,Week 11/10 - 11/16,followup,,jzivtxcbl6964n,k31im2pxckp67s,2019-11-16T11:55:58Z,{},other
4081,no,"<p>Is the environment actually deterministic? I was under the impression that the randomness (as in, which player goes first) comes from the environment. </p>
<p></p>
<p>Regardless, I&#39;m still not sure what P[s&#96;|s,a] represents in the context of this problem; that&#39;s the issue I&#39;m running in to</p>",2019-11-15T17:12:08Z,53,Week 11/10 - 11/16,followup,,jl284xdcifz44g,k30egtm91336tc,2019-11-15T17:12:08Z,{},project3
4082,no,"<p>Nope, it&#39;s not deterministic for the exact reason you mentioned.</p>",2019-11-15T17:19:18Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30eq1l9ava5uk,2019-11-15T17:19:18Z,{},project3
4083,no,"<p>That&#39;s what I thought, thanks :)</p>",2019-11-15T17:30:50Z,53,Week 11/10 - 11/16,feedback,,jl284xdcifz44g,k30f4vo698k46u,2019-11-15T17:30:50Z,{},project3
4084,no,"<p>I tried using the same equation I used in the Taxi problem, and it somehow gets stuck in an infinite loop around ~600 trials. That&#39;s what made me think I was using the wrong update rule. I wrote out my equation below; does it look like there are any problems with it? I could have a bug somewhere else, I just thought I&#39;d start with the Q update rule since I don&#39;t exactly understand it as it&#39;s presented in the paper.</p>
<p></p>
<p>Q[s, a] = Q[s, a] &#43; α[(r &#43; γmaxQ[s&#39;, :]) - Q[s,a]]</p>",2019-11-15T17:30:40Z,53,Week 11/10 - 11/16,followup,,jl284xdcifz44g,k30f4nemli141o,2019-11-15T17:30:40Z,{},project3
4085,no,<p>Which loop is it getting stuck in?  How is it getting stuck in an infinite loop?  Don&#39;t you have a time-out for your simulation?</p>,2019-11-15T17:34:20Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30f9dkwkvr2wh,2019-11-15T17:34:20Z,{},project3
4086,no,"<p>Player A and B just keep moving North and South, neither attempts to score a goal. I don&#39;t have a time-out, does that just mean we should end the episode after a certain timestep?</p>",2019-11-15T17:36:30Z,53,Week 11/10 - 11/16,feedback,,jl284xdcifz44g,k30fc5zabton6,2019-11-15T17:36:30Z,{},project3
4087,no,"<p>I added a timeout and I still suspect there&#39;s a problem; my Q-Learning experiment converges around 2000 trials, and the one in the paper never converged. </p>",2019-11-15T17:39:25Z,53,Week 11/10 - 11/16,feedback,,jl284xdcifz44g,k30ffx0regiay,2019-11-15T17:39:25Z,{},project3
4088,no,"<p>The algorithm is supposed to only run for a certain number of time steps, so there should never be an actual <em>infinite</em> loop.</p>",2019-11-15T17:47:54Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30fqttkx9i3bf,2019-11-15T17:47:54Z,{},project3
4089,no,"<p>Ah, I didn&#39;t realize, is that in the Greenwald &amp; Hall paper, or maybe the Littman paper? The only reference to a limit on game time that I could find was that the Littman paper said the game could end in a tie, but it made no mention of the number of timesteps he uses to limit the game</p>",2019-11-15T18:02:07Z,53,Week 11/10 - 11/16,feedback,,jl284xdcifz44g,k30g941s45k2xb,2019-11-15T18:02:07Z,{},project3
4090,no,"<p>Your agent should also be exploring a bit, so they should eventually chose a random action that pulls out of the infinite equilibrium. Check that your &#34;who goes first&#34; is oscillating. Check that your random action selection is choosing from all 5 actions. Check that you are limiting the $$\epsilon$$ to $$max(0.01,\epsilon)$$ just like $$\alpha$$.</p>
<p></p>",2019-11-15T19:13:34Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30iszq4vop686,2019-11-15T19:13:34Z,{},project3
4091,no,"<p>&#34;<em>Ah, I didn&#39;t realize, is that in the Greenwald &amp; Hall paper,&#34;</em></p>
<p><em></em></p>
<p>From Greenwald, 2003:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk30jxkk0cah1%2FCapture.PNG"" alt="""" /></p>",2019-11-15T19:45:27Z,53,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30jxzfx7nj4d2,2019-11-15T19:45:27Z,{},project3
4092,no,"<p>T is any value you choose. I haven&#39;t seen any mention of a value in any paper. So 100, 500, 1000, whatever you want. I used 500. Someone should take a poll.</p>",2019-11-15T19:53:19Z,53,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30k83y8it75px,2019-11-15T19:53:19Z,{},project3
4093,no,"<p>In the Q_learning case I am noticing that after a few games, both the players learn how to not lose. This ends in infinitely running games with no winner. Has someone else seen this too?</p>",2019-11-15T22:28:15Z,53,Week 11/10 - 11/16,feedback,,jl2egn5k4zo4lp,k30prcjgo7b5af,2019-11-15T22:28:15Z,{},project3
4094,no,"<p>Isn&#39;t that according to expectation since Q-learning agents learn a deterministic policy?</p>
<p></p>
<p>As stated in Greenwald(2003)</p>
<pre>For example, at the state depicted in Fig. 4 (hereafter, state s), any
deterministic policy for player B is subject to indefinite blocking by player A. But if player B employs
a nondeterministic policy, then player B can hope to pass player A on his next move.</pre>
<p></p>
<p></p>",2019-11-16T07:47:39Z,53,Week 11/10 - 11/16,feedback,,ijctp4ucNy8,k319qqp2xs332t,2019-11-16T07:47:39Z,{},project3
4095,no,<p>Littman paper mentioned time steps as 1 million in section 6.2</p>,2019-11-19T08:17:54Z,52,Week 11/17 - 11/23,feedback,,jfzaqnqvtQ1m,k35l5701tfyax,2019-11-19T08:17:54Z,{},project3
4096,no,"<p>With reference to this post by Jacob Anderson:</p>
<p></p>
<p><em>Your agent should also be exploring a bit, so they should eventually chose a random action that pulls out of the infinite equilibrium. Check that your &#34;who goes first&#34; is oscillating. Check that your random action selection is choosing from all 5 actions. Check that you are limiting the ϵ to max(0.01,ϵ) just like α.</em></p>
<p><em></em></p>
<p>Are we using an epsilon-greedy approach? The only mention I can mind of epsilon-greedy in Greenwald &amp; Hall is in section 4, before they discuss the soccer game. I also can&#39;t find any mention of this &#34;<em>limiting the ϵ to max(0.01,ϵ)&#34; </em>in the two papers. Am I just not reading closely enough, or is there information in some other source that I&#39;m missing?</p>",2019-11-16T17:52:32Z,53,Week 11/10 - 11/16,followup,,jl284xdcifz44g,k31vcml0rqawt,2019-11-16T17:52:32Z,{},project3
4097,no,"<p>max(0.001, e) is where they say &#34;epsilon e -&gt; 0.001&#34;. This decay schedule is what Littman described in his paper (2003).</p>",2019-11-18T13:27:19Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k34gr9oo6wc1iw,2019-11-18T13:27:19Z,{},project3
4098,no,"<p>Hi all,</p>
<p></p>
<p>I am a bit confused what the dimensions of the Q-learning Q table should be? I know this is a zero sum game so one table will suffice but I am not sure of the total number of states/actions? I think it is</p>
<p></p>
<p>8 (boxes player 1 alone) *  8 (boxes shared) * 2 (positions states) = 128 (total states)</p>
<p></p>
<p>As for actions, that should be 5 for 1 player and 25 for both.</p>
<p></p>
<p>Any pointers would be great.</p>
<p></p>
<p></p>
<p></p>",2019-11-23T00:34:08Z,52,Week 11/17 - 11/23,followup,,gx3c8l7z7r72zl,k3auc6z9brr52a,2019-11-23T00:34:08Z,{},project3
4099,no,"<p>You&#39;re right on both the state count and the action count.</p>
<p></p>
<p>For normal Q-learning, however, the agent ignores his opponent&#39;s actions.</p>",2019-11-23T00:51:16Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3auy8nnkpc5ha,2019-11-23T00:51:16Z,{},project3
4100,no,<p>Does player one need to take into account the shared State when player one and player two are both in the same box?</p>,2019-11-23T00:55:58Z,52,Week 11/17 - 11/23,feedback,,gx3c8l7z7r72zl,k3av49r1thu7h0,2019-11-23T00:55:58Z,{},project3
4101,no,<p>In my implementation Player A and Player B are not allowed to be on top of each other.</p>,2019-11-23T01:48:10Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3awzejb5oa6ik,2019-11-23T01:48:10Z,{},project3
4102,no,"<p>Using the advice that I provided in the student answer, my Friend-Q looks like:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk30ka3chbmux%2Ffigure3cshare.png"" alt="""" /></p>",2019-11-15T19:55:18Z,25,Week 11/10 - 11/16,followup,,jc554vxmyuy3pt,k30kant3ebd4yt,2019-11-15T19:55:18Z,{},project3
4103,no,"<p>Jacob, out of curiosity, can you possibly plot the same graph, but instead of (South, Stick) at state s, plot (East, East) at state s?</p>
<p>That seems to be one of the most interesting Q values for me (very slow convergence), but it may just be my specific handling of the environment.</p>",2019-11-15T20:35:29Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30lqc5jplf7ka,2019-11-15T20:35:29Z,{},project3
4104,no,"<p>Running it now. From the heatmap you can see the (e,e) tuple has a rich value history across states. That would make it an active action tuple. The (s,stick) tuple doesn&#39;t have as much activity in it.</p>",2019-11-15T21:24:35Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30nhhju3gc4p7,2019-11-15T21:24:35Z,{},project3
4105,no,<p>So far the graph has been bland - not much to show at all. Do you have E as your zero action? I am doing random tiebreaker now with N as my zero action.</p>,2019-11-15T22:08:48Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30p2ccn5ki1ib,2019-11-15T22:08:48Z,{},project3
4106,no,"<p>No, N is my zero action.  What is happening for me is that when they both initially go E, half the time, B transfers the ball to A and A scores an own-goal.  The other half of the time, A moves east first and B then moves east, and no goal is scored.</p>
<p></p>
<p>I have a feeling that in your (and probably most others&#39;) implementations, when B transfers the ball to A, A does <em>not</em> move, i.e. its eastward move never happens.  I&#39;m trying to decide if I want to make that change.</p>",2019-11-15T22:10:28Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30p4hi7eix4q8,2019-11-15T22:10:28Z,{},project3
4107,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk30pal6qu661%2Feeforvahe.png"" alt="""" /></p>",2019-11-15T22:15:18Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30pap603wo13,2019-11-15T22:15:18Z,{},project3
4108,no,"<p>I tuned the alpha down to 0.1 -&gt; 0.001 and this popped up. If I use alpha = 1.0 -&gt; 0.001, then I got nothing interesting at all.</p>",2019-11-15T22:16:22Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30pc2mnxi61bf,2019-11-15T22:16:22Z,{},project3
4109,no,"<p>Ahhh, see, your rules are different than mine. I resolve the movements, and then if they land on eachother, resolve the conflict and handle ball transfers.</p>",2019-11-15T22:17:26Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30pdg6x9bq5yh,2019-11-15T22:17:26Z,{},project3
4110,no,"<p>Yup.</p>
<p></p>
<p>&#34;<em>Actions are executed in random order. <strong>If the sequence of actions</strong> causes the players to collide&#34;</em></p>
<p></p>
<p>That second sentence implied, to me, that the sequence, i.e., who goes first, should be taken into account in deciding whether there is a collision.  So if they both go East, but the player on the left goes first, then they will collide, since the player on the right hasn&#39;t moved yet.</p>
<p></p>
<p>If I ignored collisions for the movement phase (just put them each where they want to go, regardless of the sequence of actions), then in the East - East case the two players would never collide, or never realize that they had collided.</p>
<p></p>
<p>I&#39;m indifferent to the interpretation.  I just need to figure out what <em>Greenwald</em> did lol.</p>",2019-11-15T22:26:34Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30pp747jzg506,2019-11-15T22:26:34Z,{},project3
4111,no,"<p>So, with your interpretation, could the players swap spots without creating a collision?</p>",2019-11-15T22:42:25Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30q9kq8vdz4qs,2019-11-15T22:42:25Z,{},project3
4112,no,"<p>I assumed the players moved at the same time. If both players are moving east, then there is no way to collide, as you point out. Otherwise, we&#39;re just doing a turn based game where each player takes their turn moving. We&#39;re just resolving the decision to move at the same time.</p>
<p></p>
<p>As I re-read Littman&#39;s 1994 paper, I am more convinced that he did a subturn-based game, and not a real-time game.</p>
<p></p>
<p>Yes, players will swap spots without colliding.</p>",2019-11-15T22:43:25Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30qauwmsrx46s,2019-11-15T22:43:25Z,{},project3
4113,no,"<p>I&#39;m starting to like your approach better, at least for getting the results I&#39;m supposed to get.  Time to re-code.</p>",2019-11-15T22:44:30Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30qc98cp3y5cj,2019-11-15T22:44:30Z,{},project3
4114,no,"<p>According to Littman 94, you must stick to get the ball. You can&#39;t pass possession any other way. &#34;When a player executes an action that would take it to the square occupied by the other player, possession of the ball goes to the <strong><em>stationary</em> </strong>player and the move does not take place.&#34;</p>
<p></p>",2019-11-15T22:50:08Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30qjhqh9jc2fv,2019-11-15T22:50:08Z,{},project3
4115,no,<p>But you <em>could</em> interpret stationary there as the player who&#39;s<i> sub-turn</i> isn&#39;t active yet.  That&#39;s how I interpreted it.</p>,2019-11-15T22:51:14Z,25,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k30qkwfook17cv,2019-11-15T22:51:14Z,{},project3
4116,no,"<p>Exactly, you definitely could do that. I would not argue that my soccer is correct, but it was an easy implementation.</p>",2019-11-15T22:59:45Z,25,Week 11/10 - 11/16,feedback,,jc554vxmyuy3pt,k30qvusganu33v,2019-11-15T22:59:45Z,{},project3
4117,no,"<p>So just to be sure, the graph generated here is that after each update, we compare the differences of A going S and B Sticking for the particular state we were in. So each data point on the graph will potentially be for a different state. Just sanity checking here.</p>",2019-11-17T18:05:08Z,24,Week 11/17 - 11/23,feedback,,i4jbttw9ru63ot,k33b8oony40725,2019-11-17T18:05:08Z,{},project3
4118,no,"<p>I can&#39;t speak for Jacob, but I think most people have each time step representing potentially a different state.  If you didn&#39;t do it that way, say each time step was specifically (state S, South, Stick), then you would need many more than one million time steps simulated to get one million data points, right?</p>
<p></p>
<p>What some people <em>have</em> done, however, is not plot the time steps where they weren&#39;t at (state S, South Stick), in which case they have 1 million time steps on the graph, but far fewer than one million data points.  They do this in an effort to reduce the number of 0 values on the y-axis to make it look more like the paper&#39;s graph.</p>",2019-11-17T18:12:25Z,24,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33bi29125j2z3,2019-11-17T18:12:25Z,{},project3
4119,no,"<p></p>
<pre>If you didn&#39;t do it that way, say each time step was specifically (state S, South, Stick), then you would need many more than one million time steps simulated to get one million data points, right?</pre>
<p><br />This is if your statement means that state S is always the same. Yes then we agree.</p>
<p></p>
<pre>What some people have done, however, is not plot the time steps where they weren&#39;t at (state S, South Stick), in which case they have 1 million time steps on the graph, but far fewer than one million data points.  They do this in an effort to reduce the number of 0 values on the y-axis to make it look more like the paper&#39;s graph.</pre>
<p>But then what you are saying is that this is specifically tracking some specific State S, which is probably the initial state. That makes sense, but isn&#39;t exactly described that way in the paper, hence my confusion.</p>
<p></p>
<p>Also, the only reasonable thing to plot at that point is the previous value, so you should see a lot of &#34;flat-lines&#34; on the graph. Otherwise I can&#39;t see how you&#39;d still have 1 million time steps.</p>
<p></p>",2019-11-17T18:40:07Z,24,Week 11/17 - 11/23,feedback,,i4jbttw9ru63ot,k33choiar5a5wo,2019-11-17T18:40:07Z,{},project3
4120,no,"<p>Each data point in my graph is $$\||Q(s_{AB*},(S,Stick))_{t} - Q(s_{AB*},(S,Stick))_{t-1}||$$ where (*) denotes possession of the ball, A is the initial position of A, and B is the initial position of B. Even if the action at step t is not (S,Stick), I am still looking to compare that specific reference point. This makes the state (s,(S,Stick)) be the convergence test state for learning. Not that I agree with this methodology...</p>
<p></p>
<p>The zeros in the blowup region of my graph are indicative of the non-changing nature of that state later in the training period, and during initial training.</p>
<p></p>
<p></p>",2019-11-17T19:01:54Z,24,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33d9p5jxa41hj,2019-11-17T19:01:54Z,{},project3
4121,no,"<p>This came up in a conversation on slack. What actions does the opponent actually take? What is the opponent and what is the friend?</p>
<p></p>
<p>First, I chose A as the player of interest (the Friend) and B as the opponent.</p>
<p></p>
<p>Then Littman&#39;s description of Friend-Q in his 1994 paper uses $$max_{\vec{a}}Q(s,\vec{a})$$ as the action selector. This means you will choose a value that is the maximum in the $$\vec{a}$$ vector of (a,b) actions. For A&#39;s Q, this will be the maximum reward for A in relation to B&#39;s action.</p>
<p></p>
<p>Choosing A&#39;s action leaves you with another choice. Which action for B? Littman describes multiple experiments of Agent vs Agent vs Rando, but Greenwald does not. All of my graphs have been against Agent. When I chose an action for player B, I chose the correlated action that came with the selection for A. If the $$max_{\vec{a}}Q(s,\vec{a})$$ resulted in (E,*) for A, then my action for B would be (*) which is Stick.</p>
<p></p>
<p>You may choose another method for this. There is ambiguity in Greenwald&#39;s paper (no surprise) regarding how to select the friend. My interpretation of Littman&#39;s Friend-Q max formula is that we chose the degenerate B action from the maximal A action for player B. I also ran a test using a Q for B and taking the min of that Q - the result was marginally better for A, but the same trend.</p>
<p></p>
<p></p>",2019-11-17T21:59:35Z,24,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k33jm6ngvay53w,2019-11-17T21:59:35Z,{},project3
4122,no,<p>I think that in the limit of infinite exploration it doesn&#39;t matter what your &#34;friend&#34; does.  In the short term it will though.</p>,2019-11-17T22:30:34Z,24,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33kq12wdz664w,2019-11-17T22:30:34Z,{},project3
4123,no,"The question is, is 1,000,000 time steps “infinite” enough? ",2019-11-18T03:55:41Z,24,Week 11/17 - 11/23,feedback,,i4jbttw9ru63ot,k33wc524z917oj,2019-11-18T03:55:41Z,{},project3
4124,no,"<p>Good question. I graphed the affect of alpha on Friend-Q and found that about 1300 steps was enough to zero out any differences. There were some noisy diffs out to 25,000 at alpha=0.1, but mostly those were 1e-9 and considered &#34;zero&#34; for the most part (11 orders from the max of 100). In each case, the sum of the diffs was always 9, which is a good indication of stability. Note the inset heat map of the two Qs per player. Player A here is A and player B is B for me, you might be doing the opposite (that&#39;s ok).</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk34rsn21brlm%2Ffriendqsurveyshare.png"" alt="""" /></p>",2019-11-18T18:38:28Z,24,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k34rvelsf0846g,2019-11-18T18:38:28Z,{},project3
4125,no,"<p>OK. does the statement in Littman&#39;s paper apply to when one tests a trained agent such as a q-learner against a random/irrational player? Does this mean that during each step (test not train), there is a probability of 0.1 that agents should call the game a draw and reset their positions and start another game?</p>
<p>if so, does this only applies to playing against a random player or generally to all other agent types?</p>",2019-11-17T05:12:31Z,49,Week 11/10 - 11/16,followup,,j6ll2xkiDJf,k32jn33rump4j3,2019-11-17T05:12:31Z,{},project3
4126,no,"<p>I think Tianhang was addressing why a discount factor can be thought of as being equivalent to a probability of terminating the game.</p>
<p></p>
<p>But I think you&#39;re asking a different question:  why is Littman <em>actually</em> terminating the game at all during evaluation.  Why not just let the game play out until someone scores a goal?</p>
<p></p>
<p>It looks like he didn&#39;t discount the reward in the testing phase, and used this draw mechanism in lieu of it.  In this sense, it is, as Tianhang says, equivalent to running the game out with a discount rate.  But he states that he only tabulates wins and losses, which means he discards the draws.</p>
<p></p>
<p>Maybe it was a way to filter out games that took too long, while still emulating the game that the agents trained on.  I don&#39;t know.</p>",2019-11-17T05:56:32Z,49,Week 11/10 - 11/16,feedback,,jzfsa4a37jf4aq,k32l7pdorl11w9,2019-11-17T05:56:32Z,{},project3
4127,stud,<p>Awesome to see another real-life case of using this idea!</p>,2019-11-18T18:12:01Z,48,Week 11/17 - 11/23,followup,a_0,,k34qxdo2soz456,2019-11-18T18:12:01Z,{},project3
4128,no,"<p>Thank you Vahe, for a very detailed answer! The full derivation and step-by-step help a lot!</p>
<p></p>
<blockquote>
<p>His opponent thinks he&#39;s playing T, but instead, he&#39;s playing B.</p>
</blockquote>
<p>This is really a missing piece for me. The RHS represents the <em>misalignment</em> between a row player (let call him player A) and other player(s). In perspective of CE, it is unknown if he would actually get higher reward if he take other actions than T, but it guarantees that he will have lower reward if he takes other actions <em>while</em> everyone else thinks he will take action T.</p>
<p></p>
<p>It is similar to Nash-Equilibrium (NE), that the result is the max expected reward for all parties involved. But it is different than with NE, NE disregards the context of what everyone is thinking and planning. NE tries to find the probability to max expected reward if player A takes action $$a_i$$ and other player just happen to take action $$a_j$$. There is no linking between player A takes action $$a_i$$ and other player takes $$a_j$$. In CE, there is. The relationship is considered through the conditional probability between $$a_i$$ and $$a_j$$.</p>",2019-11-17T19:50:06Z,52,Week 11/17 - 11/23,followup,,jl5wq8mca7o0,k33ezoh3xa86nl,2019-11-17T19:50:06Z,{},project3
4129,no,"<p>&#34;<em>It is similar to Nash-Equilibrium</em> (NE)&#34;</p>
<p></p>
<p>You can think of a Nash equilibrium as a special case of a correlated equilibrium, where the players&#39; actions are independent of each other.  In that case, we would have, for example:</p>
<p></p>
<p>$$P(T,L) = P(T)P(L)$$,</p>
<p></p>
<p>for the probability of the two players taking the actions that result in the top-left corner box.</p>
<p></p>
<p>Correlated equilibria include the more general cases where the players actions <em>aren&#39;t </em>independent.  But they still include the Nash equilibria as well.</p>",2019-11-17T22:26:29Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33kkskqpqi1tj,2019-11-17T22:26:29Z,{},project3
4130,no,<p>Still confused about the missing piece? Why should it not be 7P(L| <strong>B </strong>)&#43;0P(R| <strong>B </strong>) ?</p>,2019-11-18T19:13:46Z,52,Week 11/17 - 11/23,feedback,,jzj6lng3im15y3,k34t4t5fjsc7h2,2019-11-18T19:13:46Z,{},project3
4131,no,"<p>We&#39;d like to compare the value of the game for the row player between when he plays his equilibrium strategy, to when<em> he unilaterally changes that strategy to another strategy</em>.</p>
<p></p>
<p>In the case presented above, if he <i>doesn&#39;t</i> switch strategies, the value of the game for him is $$6P(L|T)&#43;2P(R|T)$$.</p>
<p></p>
<p>What is the value of the game for him if he switches from $$T$$ to $$B$$ <em>when the equilibrium strategy says to play </em>$$T$$?</p>
<p></p>
<p>Think about what the probabilities you&#39;re writing down actually mean.</p>
<p></p>",2019-11-18T19:29:21Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k34touoc7m56cs,2019-11-18T19:29:21Z,{},project3
4132,no,<p>Thanks. Now I see. It is the belief given as T to generate actions on top of it.</p>,2019-11-18T19:54:35Z,52,Week 11/17 - 11/23,feedback,,jzj6lng3im15y3,k34ula8sg8d288,2019-11-18T19:54:35Z,{},project3
4133,no,"<p>For the 2 action case you can get away with eliminating the P(T) term, but in the 3&#43; action case, how do you deal with that? now you have the conditional probabilities and the marginal probabilities too in the inequality. Maybe we&#39;re supposed to use the P(T) as a value that is computed from historical selections. maybe there is another way to compute the marginals so that the are constants in the inequalities at time <em>t</em>?</p>
<p></p>",2019-11-17T22:07:20Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k33jw5m5ltn18e,2019-11-17T22:07:20Z,{},project3
4134,no,"<p>I&#39;m just confusing myself. Slack folks helped to clarify this. We are using the $$\pi_{A,B}$$ values as joint probabilities in the linear solver. If we were doing conditional probabilities, then we would end up with $$\pi = P(A|B)P(B)$$ which would give us a nonlinear set of inequalities to solve. Here&#39;s something that I&#39;ve been using to help visualize this:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk33mkeemi3a2%2Fqs_and_pis.png"" alt="""" /></p>",2019-11-17T23:22:19Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33mkkrde7j41j,2019-11-17T23:22:19Z,{},project3
4135,no,"<p>Using my template, one of the inequalities becomes:</p>
<p></p>
<p>$$Q_{A1} * \pi_{A1} &gt;= Q_{A2} * \pi_{A1}$$</p>
<p></p>
<p>Where * is the dot product.</p>
<p>$$Q_{A1} = ( QA[0], QA[1], QA[2], QA[3], QA[4] )$$</p>
<p>$$Q_{A2} = ( QA[5], QA[6], QA[7], QA[8], QA[9] )$$</p>
<p>$$\pi_{A1} = ( \pi_{(A1,B1)},\pi_{(A1,B2)},\pi_{(A1,B3)},\pi_{(A1,B4)},\pi_{(A1,B5)})$$</p>
<p></p>
<p>Does that seem even remotely close?</p>",2019-11-17T23:30:26Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33mv0yhe544ug,2019-11-17T23:30:26Z,{},project3
4136,no,<p>The constraint looks fine - it&#39;s a rationality constraint expressing the fact that player A should not be better off changing his strategy unilaterally from $$A1$$ to $$A2$$.  And based on how you defined $$\pi$$ and $$Q$$ it looks good.</p>,2019-11-18T00:54:15Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33put846vs167,2019-11-18T00:54:15Z,{},project3
4137,no,"<p>By the above logic, would you have QA1∗πA1&gt;=QA2∗πA1 &gt;= QA3*πA1&gt;=QA4*πA1&gt;=QA5*πA1 thus (QA1-(QA2&#43;QA3&#43;QA4&#43;QA5)*πA1 &gt;= 0?</p>
<p></p>
<p>I have confused myself considerably with this.</p>",2019-11-23T03:24:27Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k3b0f8dnnv72vz,2019-11-23T03:24:27Z,{},project3
4138,no,"<p>There is one inequality per unilateral strategy change.</p>
<p></p>
<p>So $$Q_{A1}\pi_{A1} \geq Q_{A2}\pi_{A1}$$ and $$Q_{A1}\pi_{A1} \geq Q_{A3}\pi_{A1}$$, etc.</p>
<p></p>
<p>Even if you could write them all in one chain of inequalities, your final inequality wouldn&#39;t follow since you can&#39;t manipulate relations that way.  Each relation (inequality) has to be considered separately.</p>",2019-11-23T03:47:45Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3b19727syk4tv,2019-11-23T03:47:45Z,{},project3
4139,no,<p>That&#39;s where you introduce a common &#34;slack variable&#34; that governs the maximum value that all of the inequalities satisfy..</p>,2019-11-23T04:13:16Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3b2605hyi4b3,2019-11-23T04:13:16Z,{},project3
4140,no,"<p>Ok thanks I was thinking I would end up with something like this.</p>
<p></p>
<p>(QA1-(QA2&#43;QA3&#43;QA4&#43;QA5)*πA1 &gt;= 0</p>
<p>(QA2-(QA1&#43;QA3&#43;QA4&#43;QA5)*πA2 &gt;= 0</p>
<p>(QA3-(QA1&#43;QA2&#43;QA4&#43;QA5)*πA3 &gt;= 0</p>
<p>(QA4-(QA1&#43;QA1&#43;QA3&#43;QA5)*πA4 &gt;= 0</p>
<p>(QA5-(QA1&#43;QA2&#43;QA3&#43;QA4)*πA5 &gt;= 0</p>
<p></p>
<p>Then the same but inverted for the other player given zero sum games, clearly not the case.</p>
<p></p>
<p>So that would imply 20 inequalities from one players perspective.</p>
<p></p>
<p>QA1πA1≥QA2πA1</p>
<p>QA1πA1≥QA3πA1</p>
<p>QA1πA1≥QA4πA1</p>
<p>QA1πA1≥QA5πA1</p>
<p></p>
<p>...</p>
<p></p>
<p>QA5πA5≥QA1πA5</p>
<p>QA5πA5≥QA2πA5</p>
<p>QA5πA5≥QA3πA5</p>
<p>QA5πA5≥QA5πA5</p>
<p></p>
<p>with the remaining five constraints coming from πAn for n in (1, 5)?</p>",2019-11-23T05:14:46Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k3b4d3yix8f78c,2019-11-23T05:14:46Z,{},project3
4141,no,"<p>Those 20 rationality constraints only deal with the one of the two players changing his strategy unilaterally.  There&#39;s another player who is governed by the same rules and subject to the same type of constraints.</p>
<p></p>
<p>Also, there are many more than five variables, each subject to a probability constraint (must be greater than $$0$$) and the sum of all of them must be $$1$$.</p>",2019-11-23T05:53:29Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3b5qwbs9j41o6,2019-11-23T05:53:29Z,{},project3
4142,no,"<p>Yep sorry, we have 25 joint probabilities from q table that we are trying to find, I guess I am confused over getting the total number required, I initially thought that (like the chicken game) solved for 4 unknowns (the four joint probabilities) with the required 4 equations for the constraints. From this I thought cool, 25 unknowns, and 25 equations to solve, but yes theres 20 rationality constraints per player, making 40  constraints to build the matrix for the solver which would be a (40, 25) matrix, and you still have to factor in probability constraints (which is another 25) leaving (65, 25) matrix ? For some reason in my head I thought that would be over constrained, but I think I missed the point. because the matrix algebra is fine (65, 25) (25, 1) = (65, 1).</p>",2019-11-23T06:06:46Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k3b67zd5zqh3n,2019-11-23T06:06:46Z,{},project3
4143,no,"<p>Yup, exactly.   There are potentially even more constraints... a probability can&#39;t be greater than $$1$$.  More specifically, they must all sum to $$1$$.</p>",2019-11-23T07:16:12Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3b8p9rd5cy7pi,2019-11-23T07:16:12Z,{},project3
4144,no,"<p>Yep, I think I have it now, my main issue was just misunderstanding the allowable dimensions for the matrices. :)</p>",2019-11-23T07:33:32Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k3b9bjq0xhj3aa,2019-11-23T07:33:32Z,{},project3
4145,no,<p>Many thanks!</p>,2019-11-23T07:33:40Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k3b9bqedbty3c5,2019-11-23T07:33:40Z,{},project3
4146,no,"<p>This u-CORQ thing isn&#39;t chicken as chicken is computed in the paper. u-CORQ uses the sum of the correlations across all players. Here&#39;s the G that I used to solve uCORQ-chicken:</p>
<p></p>
<pre>[&#39;LT&#39;,&#39;LB&#39;,&#39;RT&#39;,&#39;RB&#39;]<br /><br />G= [-3.00e&#43;00 -2.00e&#43;00 -7.00e&#43;00  0.00e&#43;00]<br />[-3.00e&#43;00 -2.00e&#43;00 -7.00e&#43;00  0.00e&#43;00]<br />[ 0.00e&#43;00  4.00e&#43;00 -1.00e&#43;00  9.00e&#43;00]<br />[ 0.00e&#43;00  4.00e&#43;00 -1.00e&#43;00  9.00e&#43;00]<br />[-1.00e&#43;00 -0.00e&#43;00 -0.00e&#43;00 -0.00e&#43;00]<br />[-0.00e&#43;00 -1.00e&#43;00 -0.00e&#43;00 -0.00e&#43;00]<br />[-0.00e&#43;00 -0.00e&#43;00 -1.00e&#43;00 -0.00e&#43;00]<br />[-0.00e&#43;00 -0.00e&#43;00 -0.00e&#43;00 -1.00e&#43;00]<br />[ 1.00e&#43;00  1.00e&#43;00  1.00e&#43;00  1.00e&#43;00]<br />[-1.00e&#43;00 -1.00e&#43;00 -1.00e&#43;00 -1.00e&#43;00]</pre>
<p>The chicken correlated solution used a G:</p>
<pre>G= [ 1.00e&#43;00 -2.00e&#43;00  0.00e&#43;00  0.00e&#43;00]<br />[ 1.00e&#43;00  0.00e&#43;00 -2.00e&#43;00  0.00e&#43;00]<br />[ 0.00e&#43;00  0.00e&#43;00 -1.00e&#43;00 -2.00e&#43;00]<br />[ 0.00e&#43;00 -1.00e&#43;00  0.00e&#43;00 -2.00e&#43;00]</pre>",2019-11-19T14:52:06Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k35z850gnu71x9,2019-11-19T14:52:06Z,{},project3
4147,no,"<p>My X:</p>
<pre>[ 4.82e-01]
[ 2.54e-02]
[ 4.79e-01]
[ 1.33e-02]</pre>
<p>Their X:</p>
<pre>[ 5.71e-01]
[ 2.86e-01]
[ 2.86e-01]
[-1.43e-01]</pre>
<p></p>",2019-11-19T14:52:59Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35z99zkkrq337,2019-11-19T14:52:59Z,{},project3
4148,no,"<p>It&#39;s kind of strange though, because if you look at where it&#39;s learning, in step 4, it&#39;s using the action chosen in step 5.  This seems to be forcing it to be on policy.</p>",2019-11-17T02:50:47Z,29,Week 11/17 - 11/23,followup,,jzfsa4a37jf4aq,k32ektj852k5gt,2019-11-17T02:50:47Z,{},project3
4149,stud,"<p>I think regular Q-learning does this, too. Learning is always based on the actions of the behavior policy. However, the target policy itself can be different from the behavior policy. </p>
<p></p>
<p></p>",2019-11-17T02:57:10Z,29,Week 11/17 - 11/23,feedback,a_0,,k32et1dc4j61xy,2019-11-17T02:57:10Z,{},project3
4150,no,"<p>No, regular Q-learning takes a maximum over Q values in the learning step, regardless of the action selected by the behavior policy.</p>
<p></p>
<p>If you look at step 4 above, the update rule seems to depend on the action chosen by the behavior policy.</p>
<p></p>
<p></p>",2019-11-17T03:00:27Z,29,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k32ex9c2hyd7k4,2019-11-17T03:00:27Z,{},project3
4151,stud,"<p>In off-policy learning, the update rule always depends on the action chosen by the behavior policy. For example, in regular Q-learning, the update rule is:</p>
<p>$$Q(S,a) \leftarrow Q(S,a) &#43; \alpha[R&#43;\gamma \max_{a&#39;} Q(S&#39;, a&#39;)-Q(S,a)]$$. Note in this update rule, $$a$$ is generated by the behavior policy.</p>
<p></p>
<p>Are you talking about 4.(a)? I don&#39;t think the $$a$$ in that equation refers to the $$a$$ that is the action picked by the behavior policy.</p>
<p></p>
<p>I was originally confused because of the way Greenwald wrote it -- it was in the opposite order from Sutton. :-) </p>",2019-11-17T03:10:13Z,29,Week 11/17 - 11/23,feedback,a_0,,k32f9tel87s4sd,2019-11-17T03:10:13Z,{},project3
4152,no,"<p>&#34;<em>Are you talking about 4.(a)? I don&#39;t think the a in that equation refers to the a that is the action picked by the behavior policy.&#34;</em></p>
<p><em></em></p>
<p>I was. And you&#39;re right, it looks like it&#39;s just a dummy variable.  They used the same exact variable for both the behavior policy action <em>and</em> a dummy variable used as an index for summation.  <em>That</em> is confusing.</p>
<p></p>
<p>As for Q-learning, the whole reason why it&#39;s off policy is that the target in the update step: $$\gamma \max_{a&#39;} Q(S&#39;,a&#39;)$$ does <em>not</em> depend on the action chosen by the behavior policy.  So I don&#39;t quite understand what you&#39;re saying.  An on-policy algorithm would use $$Q(S&#39;,a)$$ instead, in its target.</p>",2019-11-17T03:34:58Z,29,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k32g5ne6ktc3d1,2019-11-17T03:34:58Z,{},project3
4153,stud,"<p>I see what you mean by &#34;dependent.&#34;</p>
<p></p>
<p>Yes, the $$max_{a&#39;} Q(S&#39;,a&#39;)$$ does not depend on the action chosen by the behavior policy. However, it is still updating $$Q(S,a)$$. That is, the Q value on the $$(S,a)$$ pair chosen by the behavior policy. It is not updating Q values at random places $$(S&#39;&#39;, a&#39;&#39;)$$. </p>
<p></p>
<p>Furthermore, I do think off-policy update rules can depend on actions generated by the behavior policy. How do you think of the follow? Not saying it makes a lot of sense as a learning algo, but it shows my point. :-)</p>
<p></p>
<p>Say the behavior policy comes up with a transition $$s \rightarrow a \rightarrow s&#39; \rightarrow a&#39;$$, </p>
<p>$$

Q(s,a) \leftarrow Q(s,a) &#43; \alpha [R &#43; \gamma (\frac{1}{2} Q(s&#39;, a&#39;) &#43; \frac{1}{2} \max_{a&#39;&#39;} Q(s&#39;,a&#39;&#39;)) - Q(s,a)]


$$</p>
<p></p>
<p></p>
<p></p>",2019-11-17T03:55:23Z,29,Week 11/17 - 11/23,feedback,a_0,,k32gvwov8nhcd,2019-11-17T03:55:23Z,{},project3
4154,no,"<p>I agree that off policy update rules can depend on actions generated by the behavior policy, since by definition all on-policy algos are also off-policy algos (off policy is more general and contains on policy as a subset). I was using off policy loosely meaning &#34;not on-policy.&#34;</p>
<p></p>
<p>You&#39;re right with that example.  Cool!</p>
<p></p>
<p>&#34;Yes, the $$\max_{a′}Q(S′,a′)$$ does not depend on the action chosen by the behavior policy. However, it is still updating $$Q(S,a)$$.&#34;</p>
<p></p>
<p>That is typically the distinction between on and off policy - not the state action pair that is being updated, but rather the target state-action pair.  If you look at SARSA and Q-learning in Sutton and Barto, in both cases, the previously visited state is the one being updated, yet the former is on-policy and the latter is off policy.  The difference is the target, one uses the next state-action pair chosen by the behavior policy, the other maximizes over all state-action pairs.</p>",2019-11-17T04:11:16Z,29,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k32hgbmw5c859b,2019-11-17T04:11:16Z,{},project3
4155,stud,"<p>I agree with the last point. You are exactly right. </p>
<p></p>
<p>Thanks for liking my half SARSA half Q algo. :-) </p>
<p></p>
<p></p>",2019-11-17T04:23:14Z,29,Week 11/17 - 11/23,feedback,a_0,,k32hvpju1my3g0,2019-11-17T04:23:14Z,{},project3
4156,stud,"<p>&#64;Farrukh Rahman , thanks for the response. </p>",2019-11-17T02:57:22Z,29,Week 11/17 - 11/23,followup,a_0,,k32eta6hpfr25n,2019-11-17T02:57:22Z,{},project3
4157,no,"<p>ahh, so in CE(Q) we use max to find the mixed strategy and minimax used min. If we get a primal infeasible error while solving for the max, and the dual is usable, can we negate the z from the dual and use that as the maximum? As I understand it, the dual of the max is the minimization and the z is the coefficient matrix that minimizes the objective function??</p>
<p></p>
<p></p>",2019-11-17T19:35:30Z,21,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k33egw3q8o06of,2019-11-17T19:35:30Z,{},project3
4158,stud,<p>So the sum of rewards is always 0 and so V(s) is always 0.  How are the agents suppose to pick actions?</p>,2019-11-17T20:33:44Z,21,Week 11/17 - 11/23,followup,a_0,,k33gjsdn7m36rx,2019-11-17T20:33:44Z,{},project3
4159,no,<p>This is exactly the point of my question.  Aren&#39;t all action vectors optimal if they all lead to the same ($$0$$) reward?</p>,2019-11-17T22:33:29Z,21,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33ktskoyqe1xe,2019-11-17T22:33:29Z,{},project3
4160,no,"<p>At what point does the game become truly zero sum? After 1000 games does $$Q_A &#43; Q_B = 0$$ or does it take 2000 games? I the case of infinite games, maybe that&#39;s when the zero sum takes root, and any other limit is where the cor-Q makes sense. Therefore the limit of that inner sum of sums as the number of games approaches infinity is zero. How fast does it approach zero?</p>
<p></p>
<p>So really these correlation algorithms don&#39;t make sense for truly zero some games with infinite horizons. They are only useful for general sum games.</p>
<p></p>
<p>Greenwald mentions in the introduction that &#34;in constant sum games, the set of correlated equilibria contains the set of minimax equilibria.&#34; So that mostly confirms that cor-Q is minimax-Q/foe-Q for zero sum games. What was the point of the soccer game then? :)</p>
<p></p>",2019-11-17T23:38:27Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33n5c2j3517b,2019-11-17T23:38:27Z,{},project3
4161,no,"<p></p>
<ul><li>The sum of rewards for any individual soccer game is 0 (total gains and losses must add to zero). certain policies may lead to better expected return in the long run.</li><li>What Greenwald is showing with uCE-Q figure is that it finds the same equilibrium as minimax-Q which is in line with the theory for zero sum game problems (the only correlated equilbria is the nash). thats why the figures are identical.</li></ul>
<p></p>
<p></p>",2019-11-18T04:43:58Z,21,Week 11/17 - 11/23,feedback,,jl1acpoc4HA9,k33y284tuz66yc,2019-11-18T04:43:58Z,{},project3
4162,no,"<p> I wrote the correlated utilitarian LP solver. I am able to get the right answer for the chicken problem (greenwald paper), using this solver. However I am not sure if it works when I make that chicken problem 0 sum, as I get the solved probability to be 0 everywhere and 1 at one specific action pair. Also for the soccer environment I get the plot similar to friend Q rather than to foe Q (Not sure if its working as intended). I am still unable to understand how the CEQ is the same as the minimax Nash EQ</p>
<p></p>",2019-11-18T07:57:10Z,21,Week 11/17 - 11/23,feedback,,jzzw8pi7a7t3d4,k344yoj2j1f5ek,2019-11-18T07:57:10Z,{},project3
4163,no,"<p>Farrukh,</p>
<p></p>
<p>I&#39;ve been beating my head for a the last three days on exactly this point that you mention:</p>
<p>&#34;What Greenwald is showing with uCE-Q figure is that it finds the same equilibrium as minimax-Q which is in line with the theory for zero sum game problems (the only correlated equilbria is the nash). thats why the figures are identical.&#34;</p>
<p></p>
<p>Are the Q_tables for Foe-Q and uCE-Q identical after 1 million iterations? My two charts for both learning strategies are pretty close (but not identical), and the Q_tables are also not the same.</p>",2019-11-20T20:51:11Z,21,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k37rhs22be01ts,2019-11-20T20:51:11Z,{},project3
4164,no,<p>Yes they should be identical. Usually its possible to get something close but not identical if your constraints are not 100% correct. </p>,2019-11-21T00:07:09Z,21,Week 11/17 - 11/23,feedback,,jl1acpoc4HA9,k37yhsrp2fc7ev,2019-11-21T00:07:09Z,{},project3
4165,no,"<p>Can someone who is confident their implementation of uCE-Q is correct please share the resulting mixed strategy obtained providing this rewards matrix as input?</p>
<p></p>
<p>Values in this example aren&#39;t from the soccer env and were generated via (np.random.random([5,5]) * 10).astype(&#39;int&#39;).astype(&#39;float&#39;), just trying to get a unit test to confirm uCE-Q is implemented correctly.</p>
<p></p>
<p>EXAMPLE = np.array([</p>
<p>[4.0, 4.0, 0.0, 5.0, 7.0],</p>
<p>[1.0, 9.0, 5.0, 8.0, 1.0],<br /> [0.0, 1.0, 5.0, 5.0, 5.0],<br /> [0.0, 4.0, 4.0, 3.0, 9.0],<br /> [0.0, 2.0, 8.0, 2.0, 8.0]])</p>
<p></p>
<p><strong>Edit -</strong> sharing the values I&#39;m getting (rounded), which appear reasonable as far as I can tell.</p>
<p>[[0. 0.128 0. 0.214 0.214]<br /> [0. 0.128 0. 0.061 0. ]<br /> [0. 0. 0. 0. 0. ]<br /> [0. 0. 0. 0. 0. ]<br /> [0. 0.107 0.072 0. 0.076]]</p>
<p></p>
<p>It&#39;d be helpful if any peers can confirm whether this looks similar to what they get so we&#39;re not assuming our implementation is correct in isolation.</p>",2019-11-19T23:19:38Z,21,Week 11/17 - 11/23,followup,,isde332xcka1m0,k36hcu96daf6ht,2019-11-19T23:19:38Z,{},project3
4166,no,"<p>(deleted) this was all using the default solver which worked unpredictably for me.</p>
<p></p>
<p></p>",2019-11-19T23:55:13Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k36imllz3u02ac,2019-11-19T23:55:13Z,{},project3
4167,no,"<p>Weird? On my desktop, generating the same coefficients, I get a totally different answer (and a bad answer for chicken): A5,B1:1.0</p>
<p></p>
<p></p>",2019-11-20T17:06:49Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k37jh8kbtzd2g2,2019-11-20T17:06:49Z,{},project3
4168,stud,"<p>Thanks &#64;Jacob.  That&#39;s what I thought I had done, but all I have in gt-omscs-rldm/7642Fall2019, is a project.git file, but the files haven&#39;t been copied.  </p>
<p>Could you tell me step by step what to do, from where?</p>
<p></p>",2019-11-17T15:48:00Z,52,Week 11/17 - 11/23,followup,a_0,,k336cby3g2p4jq,2019-11-17T15:48:00Z,{},other
4169,no,"<p>Open the .git directory in your repository:</p>
<p>&gt; cd .git</p>
<p>Open the &#34;config&#34; file in your favorite plain text editor. not word, not wordpad, nothing that&#39;s going to muck with encoding. VS code, textedit, edit&#43;, vi, emacs, all are good things to use.</p>
<p></p>
<p>Add a remote:</p>
<p></p>
<pre>[remote &#34;rldm&#34;]
	url = https://github.gatech.edu/gt-omscs-rldm/7642Fall2019&lt;gt-id&gt;<br />	fetch = &#43;refs/heads/*:refs/remotes/github/*
	tagopt = --no-tags</pre>
<p>Now you can push your repository to that remote instead of to your origin.</p>
<p></p>",2019-11-17T18:50:51Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33cvh2umvz4rr,2019-11-17T18:50:51Z,{},other
4170,no,"<p>You can also use the shorthand command line to do what Jacob suggests you to do:</p>
<pre>git remote add rldm https://github.gatech.edu/gt-omscs-rldm/7642Fall2019&lt;gt-id&gt;</pre>
<p>Then push it to the newly add remote server (ie: rldm):</p>
<pre>git push rldm master</pre>
<p></p>",2019-11-17T20:08:30Z,52,Week 11/17 - 11/23,feedback,,jl5wq8mca7o0,k33fnbyx42y47a,2019-11-17T20:08:30Z,{},other
4171,no,"<p>Do what Quang Vu suggests, that&#39;s smarter. Editing the config files is always a last resort.</p>",2019-11-17T20:13:43Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33fu1ii4si4ke,2019-11-17T20:13:43Z,{},other
4172,stud,"<p>Thanks &#64;Quang Vu, but the push was rejected...</p>
<pre>git push rldm  master
To https://github.gatech.edu/gt-omscs-rldm/7642Fall2019&lt;GTID&gt;
 ! [rejected]        master -&gt; master (fetch first)
error: failed to push some refs to &#39;https://github.gatech.edu/gt-omscs-rldm/7642Fall2019&lt;GTID&gt;&#39;
hint: Updates were rejected because the remote contains work that you do
hint: not have locally. This is usually caused by another repository pushing
hint: to the same ref. You may want to first integrate the remote changes
hint: (e.g., &#39;git pull ...&#39;) before pushing again.
hint: See the &#39;Note about fast-forwards&#39; in &#39;git push --help&#39; for details.</pre>
<p>But... there is nothing trying to push anything into the freshly created &#39;rldm&#39;... </p>
<p>Could all these problems come from the fact that 7642Fall2019&lt;GTID&gt; is itself a repository, so we&#39;re trying to push a repository inside another one? </p>
<p>It&#39;s the origin of the problem: when I created my project repository, I did &#39;add repository&#39; and github opened it outside 7642Falletc, and I didn&#39;t notice it.  </p>
<p>I&#39;m such a noob with git.  I worry the TA&#39;s are going to take 50 points if I don&#39;t fix this although the files are in the depository next door.  </p>",2019-11-17T21:23:45Z,52,Week 11/17 - 11/23,feedback,a_0,,k33ic3vit7o218,2019-11-17T21:23:45Z,{},other
4173,no,"<p>1. commit local</p>
<p>2. push to your origin</p>
<p>3. pull from your new upstream - rldm</p>
<p>4. push to your new upstream - rldm</p>
<p></p>
<p>you just need to sync the heads, which the pull will do. You have a project.git in there that is likely the &#34;changes&#34; that git doesn&#39;t like.</p>",2019-11-17T21:44:17Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k33j2i8wbbt6ti,2019-11-17T21:44:17Z,{},other
4174,stud,<p>Thanks both for your help</p>,2019-11-18T06:19:25Z,52,Week 11/17 - 11/23,feedback,a_0,,k341gzdosk65ik,2019-11-18T06:19:25Z,{},other
4175,no,<p>Backward and forward TD(lambda) can be applied to the same problems.</p>,2019-11-18T03:36:11Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33vn20vtvv7mn,2019-11-18T03:36:11Z,{},review
4176,no,<p>True.  They can be shown to be equivalent.</p>,2019-11-18T03:49:23Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33w41inqi07dh,2019-11-18T03:49:23Z,{},review
4177,no,"<p>True.  Convergence gaurantees to optimal, are gaurantees... to optimal.</p>",2019-11-18T23:30:23Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352at1jdh91lz,2019-11-18T23:30:23Z,{},review
4178,stud,<p>True. The sum of all the updates over an episode is the same for the two algorithms</p>,2019-11-21T19:11:07Z,25,Week 11/17 - 11/23,feedback,a_0,,k393cy46ian2g4,2019-11-21T19:11:07Z,{},review
4179,no,"<p>Technically true, but there’s nuance here. Forward TD(lamdba) for high values of lambda (like 1) may never converge for infinite MDPs. So while I <b>could</b> use forward TD(lambda) for such problems I wouldn’t, because practically I could get no result.I <b>could</b> drive a car with my feet; that doesn’t make it a good idea.</p>
<p></p>
<p>I&#39;m being asked a seemingly simple thing, but in my head I think I know what you&#39;re <em>really</em> asking, so I start to doubt myself, but I have no idea of actually knowing. The open-endedness of the question, due to the ambiguity of the phrasing, makes this a much harder question than it should be. It seems like this mechanism is repeatedly being used to make the final artificially difficult. I have concerns.</p>",2019-11-26T14:55:05Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3fzexzxmmj5e3,2019-11-26T14:55:05Z,{},review
4180,no,"<p>Ah, I probably missed this one due to interpreting backward and forward as referring to the sequence with which states are processed. I expect the above answers (True) are correct, interpreting backward and forward as the &#34;<a href=""http://incompleteideas.net/book/first/ebook/node76.html"" target=""_blank"" rel=""noopener noreferrer"">view</a>&#34; of TD.</p>
<p></p>
<p>Related excerpt from end of section 3.2 -- I thought the question may refer to the ability for TD to be capable of an incremental implementation only when processing states in the forward direction:</p>
<ul><li>&#34;One reason lambda = 0 is not optimal for this problem is that TD(0) is relatively slow at propagating prediction levels back along a sequence. For example, suppose states D, E, and F all start with the prediction value 0.5, and the sequence xD, XE, XF, 1 is experienced. TD(0) will change only F&#39;s prediction, whereas the other procedures will also change E&#39;s and D&#39;s to decreasing extents. If the sequence is repeatedly presented, this is no handicap, as the change works back an additional step with each presentation, but for a single presentation it means slower learning. <strong>This handicap could be avoided by working backwards through the sequences</strong>. For example, for the sequence xD, xE, xF, 1, first F&#39;s prediction could be updated in light of the 1, then E&#39;s prediction could be updated toward F&#39;s new level, and so on. In this way the effect of the 1 could be propagated back to the beginning of the sequence with only a single presentation. The <strong>drawback to this technique is that it loses the implementation advantages</strong> of TD methods. <strong>Since it changes the last prediction in a sequence first, it has no incremental implementation</strong>. However, when this is not an issue, such as when learning is done offine from an existing database, working <strong>backward</strong> in this way should produce the best predictions.&#34;</li></ul>
<p></p>
<p></p>",2019-12-02T17:21:41Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3opal4fu3v7j8,2019-12-02T17:21:41Z,{},review
4181,no,<p>&#64;396</p>,2019-12-02T18:16:49Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3or9hqpzfm1rg,2019-12-02T18:16:49Z,{},review
4182,no,"<p>&#64;396 talks about the same section of the paper, but doesn&#39;t address the original question&#39;s &#34;can be applied to the same problems.&#34;  It seems worth noting if &#34;forward&#34; refers to feeding the algorithm data incrementally as it becomes available (in a forward direction, e.g. a climate prediction model with massive amounts of data too large to fit in memory), this would be a difference in the kinds of problems it can be applied to compared to one that feeds data by &#34;working backward&#34; [from the end of an episode] (excerpt from last sentence above).   Overall, I think your first answer is correct: it seems more likely the question is asking about forward vs backward &#39;<a href=""http://incompleteideas.net/book/first/ebook/node76.html"" target=""_blank"" rel=""noopener noreferrer"">views</a>&#39;, unrelated to the sequence data is presented in.</p>",2019-12-02T18:26:30Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3orlxjk4hgxi,2019-12-02T18:26:30Z,{},review
4183,no,"<p>Sorry, I provided that link only because it related to our discussion (and probably would die in obscurity if I didn&#39;t revive it), not really because it answered your question.</p>
<p></p>
<p>In terms of your question, Chapter 12 of Sutton and Barto is where I got my understanding of forward and backward views.  My understanding is that, theoretically (i.e. in terms of convergence properties given certain assumptions), they are equivalent, and so can be applied to the same problems.</p>
<p></p>
<p>But practically it&#39;s actually not the case.  See &#64;1064.</p>
<p></p>
<p>Sutton does a much better job than I did if you&#39;re willing to wade through the entire chapter.  But essentially the backward view offers short-term benefits over the forward view, as well as being the only one of the two that is even feasible for continuing (non-episodic) tasks.</p>
<p></p>
<p>In this light, if I had to re-answer this question, I would give a more nuanced answer, with assumptions stated.</p>",2019-12-02T20:11:02Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3ovcdskv596nb,2019-12-02T20:11:02Z,{},review
4184,no,"<p>I think this should be false. Backwards can work for online, forward would not.</p>",2019-12-06T17:59:35Z,23,Week 12/1 - 12/7,feedback,,jqmfnc46kl26eg,k3ugeqbec59jp,2019-12-06T17:59:35Z,{},review
4185,no,"<p>Majority rules here(and is accurate in this case)... True. As also noted, you would have to wait for a complete episode to do it forwards(i.e. forwards does not work &#34;online&#34;)...</p>",2019-12-06T21:08:57Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3un69cc2t71up,2019-12-06T21:08:57Z,{},review
4186,no,"<p>Alec; I thought this was the point of TD-lambda; that k-estimators let u handle infinite cases, in finite time.  and that any TD(1) - which amounts to a full forward lookup in the sense u mean; by definition can be approximated using some lambda i.e. there must exist some lambda s.t. TD(lambda) = TD(1).</p>",2019-12-07T21:10:38Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w2oa1o6kr48r,2019-12-07T21:10:38Z,{},review
4187,stud,<p>Would a forward TD(lambda) only applied to finite problems?</p>,2019-12-09T18:09:25Z,22,Week 12/8 - 12/14,feedback,a_1,,k3yr2y0fihut9,2019-12-09T18:09:25Z,{},review
4188,no,"<p>no it has to do with faking an &#34;in the limit&#34; convergence; u shouldn&#39;t be able to get an answer to an infinite mdp with forward lookup&#43;&#43;; but u can because the TD(1) - the full length, can be approximated using a TD(lambda) using a k-step estimator - its the whole rmax/(1-gamma) summation for an infinite mdp.  <br /><br />its the same argument I made in that other post  (the one with the infinite vs finite game, with optimal policy in the limit necessarily being contained by the optimal policy at some finite |k| step mdp).<br /><br />sort of like the argument that u don&#39;t need to run infinite trials on a k-arm bandit, just c (within some level of confidence, within some level of epsilon).<br /><br />I dunno. thats why i asked.<br /><br />&#43;&#43; remove the exp. trace/discounting.</p>",2019-12-10T11:33:47Z,22,Week 12/8 - 12/14,feedback,,jzivtxcbl6964n,k3zse06de1gff,2019-12-10T11:33:47Z,{},review
4189,no,<p>Offline algorithms are generally superior to online algorithms.</p>,2019-11-18T03:36:26Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33vndg1le74n,2019-11-18T03:36:26Z,{},review
4190,no,"<p>False.  They are generally higher variance than online algorithms.  They can also get stuck in environments where termination isn&#39;t guaranteed, since they don&#39;t learn online.  On the other hand they don&#39;t suffer from bias issues.</p>
<p></p>
<p>Edit: Based on Alec&#39;s later use of &#34;online&#34; I wonder if he means on-policy here...</p>",2019-11-18T03:53:41Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33w9kq8ofh6m7,2019-11-18T03:53:41Z,{},review
4191,no,<p>True.  Cliffwalker problem.</p>,2019-11-18T23:31:56Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352ctcsf4j31m,2019-11-18T23:31:56Z,{},review
4192,no,"<p>Both on-policy/online and off-policy algorithms can do exploration, which is the key requirement for finding optimal policies. Ignoring the illdefined superlative (superior), both algorithm styles can generate the same optimal policy, but there is no guarantee as such.</p>
<p></p>
<p>Considering the superlative, one must first define it. If by superior you mean the policy that doesn&#39;t result in falling off a cliff, then online is superior. If step-wise superiority is desired, then offline is better, but you&#39;ll fall off the cliff more often.</p>
<p></p>
<p>Let&#39;s agree not to use superlatives, they just confound the understanding of the statement.</p>
<p></p>
<p>On-Policy vs. Off-Policy Updates for Deep Reinforcement Learning</p>
<p><a href=""https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/DeepRL16-hausknecht.pdf"">https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/DeepRL16-hausknecht.pdf</a></p>",2019-11-19T04:04:43Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35c3lumfzn34j,2019-11-19T04:04:43Z,{},review
4193,stud,"<p>False. Offline may achieve better score, but online is more stable in convergence. </p>",2019-11-21T19:27:03Z,25,Week 11/17 - 11/23,feedback,a_0,,k393xg4he3b1e9,2019-11-21T19:27:03Z,{},review
4194,no,"<p>False. They’re just less risk-averse, and assume optimal conditions will arise. Basically, given GLIE and epsilon-greedy exploration, both should converge to the same values.</p>",2019-11-26T08:47:30Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3fma8ccej4489,2019-11-26T08:47:30Z,{},review
4195,no,"<p><strong>online != on-policy</strong>, and <strong>offline != off-policy  [<a href=""https://ai.stackexchange.com/a/10491"" target=""_blank"" rel=""noopener noreferrer"">ref</a>] - </strong>excerpt: &#34;Combinations that work [...] Online, on-policy prediction; online off-policy prediction, offline, on-policy prediction; offline, off-policy prediction [...] As you can see above, only one combination offline, on-policy control, causes a clash. [...]&#34;    Related refs: [<a href=""https://stats.stackexchange.com/a/209546"" target=""_blank"" rel=""noopener noreferrer"">2</a>,<a href=""https://stackoverflow.com/a/11496131"" target=""_blank"" rel=""noopener noreferrer"">3</a>,<a href=""https://en.wikipedia.org/wiki/Online_algorithm"" target=""_blank"" rel=""noopener noreferrer"">4</a>]</p>
<p></p>
<p>The original statement sounds false either way, but the justification will be different depending on whether you interpret online as it&#39;s more commonly used in the context of describing an algorithm, or likely misinterpret it as referring to on-policy vs off-policy learning. All answers (besides Vahe) seem like they may be falling into an online == on-policy trap. Good area to be careful on the exam.</p>",2019-12-02T17:58:45Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3oqm96qleusc,2019-12-02T17:58:45Z,{},review
4196,no,"online vs offline differs by when update the values , nothing to do with on-policy / off-policy. 

online = update within an episode at each step
offline = update at the end of the episode",2019-12-04T12:05:59Z,23,Week 12/1 - 12/7,feedback,,jvfpllmsggt7p4,k3r8wat6hzt254,2019-12-04T12:05:59Z,{},review
4197,no,"Need the instructors to weigh in here. The first question in Review #6 days SARSA is an “online” algorithm. SARSA is online, but it’s also on-policy. Do they have the same definitions as us? <p></p><p></p>",2019-12-04T12:08:25Z,23,Week 12/1 - 12/7,feedback,,i4jbttw9ru63ot,k3r8zflyzuw4rq,2019-12-04T12:08:25Z,{},review
4198,no,Yes it can be both. Sarsa updates on every step so it is online. At the same time it is on policy since we do epsilon greedy for action selection and update.,2019-12-04T14:16:36Z,23,Week 12/1 - 12/7,feedback,,jvfpllmsggt7p4,k3rdk9s6mmv1xu,2019-12-04T14:16:36Z,{},review
4199,no,"<p>I think this is true, based on this page:</p>
<p></p>
<p><a href=""https://en.wikipedia.org/wiki/Online_algorithm"">https://en.wikipedia.org/wiki/Online_algorithm</a></p>
<p></p>
<p>&#34;For many problems, online algorithms cannot match the performance of offline algorithms.&#34; </p>",2019-12-05T22:41:48Z,23,Week 12/1 - 12/7,feedback,,jl284xdcifz44g,k3tb1t59sph4pt,2019-12-05T22:41:48Z,{},review
4200,no,"<p>&#64;Dalton, good point. The reason I (perhaps incorrectly) lean false is because online vs offline algorithms can be uniquely well suited to handle the problems they&#39;re applied to, making it seem odd when &#34;superiority&#34; depends heavily on context. While it&#39;s possible to use online algorithms like SGD (stochastic gradient descent) for regression problems, I&#39;d almost never favor it over offline algorithms that take advantage of having all data available up front. From the other perspective, online algorithms are often the only ones capable of handling enormous problems too large to fit in memory or even on disk of a single machine. Online algorithms are &#34;superior&#34; on problems where they&#39;re the only valid option, offline algorithms that take advantage of having all data up front are often capable of better results making them superior in that context. If in someone&#39;s line of work online algorithms are their only option, their perception of offline superiority might be different. Overall, seems good to mention algorithm superiority can depend on context.</p>",2019-12-05T23:29:55Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3tcrowcqi450j,2019-12-05T23:29:55Z,{},review
4201,no,"<p>So is this true?:</p>
<p></p>
<p>online = update within an episode at each step != on-policy<br />offline = update at the end of the episode != off-policy<br /></p>",2019-12-06T04:13:26Z,23,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3tmwaxlm3j7f4,2019-12-06T04:13:26Z,{},review
4202,no,"<p>Not exactly. Sorry if the excerpt above was confusing, the <a href=""https://ai.stackexchange.com/a/10491"" target=""_blank"" rel=""noopener noreferrer"">original source</a> is 3&#43; pages and I wanted to show examples of the terms being used adjacently (e.g. <strong>on</strong>line <strong>off</strong>-policy, Q-learning would be an example, online, on-policy, SARSA would be an example). I recommend reading the source for full context. A few partial excerpts below:</p>
<p></p>
<p>Online vs Offline:</p>
<ul><li><strong>Online</strong> learning algorithms work with data as it is made available. Strictly online algorithms improve incrementally from each piece of new data as it arrives, then discard that data and do not use it again. [&lt;== partial excerpt]</li><li><strong>Offline</strong> learning algorithms work with data in bulk, from a dataset. Strictly offline learning algorithms need to be re-run from scratch in order to learn from changed data. [&lt;== partial excerpt]</li></ul>
<p></p>
<p>Related excerpt:</p>
<ul><li>&#34;Experience replay, a common RL technique, used in Deep Q Networks amongst others, is another in-between approach. Although you <em>could</em> store all the experience necessary to fully train an agent in theory, typically you store a rolling history and sample from it. It&#39;s possible to argue semantics about this, but I view the approach as being a kind of &#34;buffered online&#34;, as it requires low-level components that can work online (e.g. neural networks for DQN).&#34;</li></ul>
<p></p>
<p>On-policy vs Off-policy:</p>
<ul><li><strong>On-policy</strong> algorithms work with a single policy, often symbolised as π, and require any observations (state, action, reward, next state) to have been generated using that policy.</li><li><strong>Off-policy</strong> algorithms work with two policies (sometimes effectively more, though never more than two per step). [&lt;== partial excerpt]</li></ul>
<p></p>
<p>The discussion on stackexchange continues: &#34;Would it be possible to have an example of a real algorithm for each of your possible combinations?&#34; ==&gt; &#34;The reason you cannot think of online vs offline examples is likely because everything you are thinking of is more online than offline. Training <i>purely</i> offline [in RL context] is more niche, or might be used to pre-train an agent for real-world use if a simulated environment is not available.&#34; There are better sources out there, but I found the stackexchange discussion helpful contrasting online vs on-policy terms FWIW.</p>",2019-12-06T05:02:02Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3tomsq9rew2bo,2019-12-06T05:02:02Z,{},review
4203,no,<p>Thanks for addressing the different definitions of offline vs off-policy (and online vs on-policy).  Really appreciate you sharing the source of the info too.</p>,2019-12-06T05:26:36Z,23,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3tpie4umhp1p4,2019-12-06T05:26:36Z,{},review
4204,no,"<p>Yeah, great post Michael.  You illustrate an important point which is that these definitions aren&#39;t ironclad, and probably shouldn&#39;t be treated as such.  The relativistic aspect is important too: offline in RL is probably way more online than offline in a different context.  Example:  Sutton refers to Monte Carlo methods as offline because value function changes are not made each time step - only at the end of each episode.  But if the episodes are happening in real time, then from a different perspective this may be seen as &#39;online&#39; learning.</p>",2019-12-06T05:47:47Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3tq9msl4l24vw,2019-12-06T05:47:47Z,{},review
4205,no,"<p>This one was pulled from Gordon 1995... And the original claim of False is what I was looking for here. Generally, because it depends on the problem and a number of other things. Good and informative discussion though.</p>",2019-12-06T21:16:10Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3unfj788k44ot,2019-12-06T21:16:10Z,{},review
4206,no,"<p>Given a model (T,R) we can also sample in, we should first try TD learning.</p>",2019-11-18T03:36:41Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33vnp1lrgacx,2019-11-18T03:36:41Z,{},review
4207,no,"<p>True.  Unless the state space is unrealistically small, sampling will be more effective than planning.</p>",2019-11-18T03:55:00Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33wb9lv3gt5i,2019-11-18T03:55:00Z,{},review
4208,no,"<p>presuming T = transition, R = rewards.... False.  Sampling distribution for optimal policy, differs from transition probability and given a model, may be computable.</p>",2019-11-18T23:35:13Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352h1d2vod7lc,2019-11-18T23:35:13Z,{},review
4209,no,<p>False. Should use MC on non-Markov environments or the environment may lead to deadly triad.</p>,2019-11-21T19:45:15Z,25,Week 11/17 - 11/23,feedback,,jzg6jh2hn6f43c,k394kuietol37p,2019-11-21T19:45:15Z,{},review
4210,no,"<p>False. If we&#39;re already given a model, then why default to model-free methods?</p>",2019-11-26T15:07:32Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3fzuyjqtgn3wg,2019-11-26T15:07:32Z,{},review
4211,no,"<p>^^ A good example is backgammon - a complete model is provided, but the state space is enormous (sweeping the state space would take too long) and the most successful methods have been model-free.</p>",2019-11-26T17:24:16Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3g4qskmgri53p,2019-11-26T17:24:16Z,{},review
4212,no,"<p>Hmmm, that&#39;s a fair point, but the verbiage of this question is confusing. &#34;First try TD learning&#34;. Maybe I don&#39;t try it first, if the state space is tractable. Otherwise I do try it. What is first? What is later?</p>
<p></p>
<p>It seems like this final is trying to use ambiguous language to trip up students. IMHO it&#39;s using an analogous anti-pattern in the realm of cryptography, which is &#34;security through obscurity.&#34; I&#39;m personally not a fan.</p>",2019-11-26T18:15:01Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3g6k2a39is7nk,2019-11-26T18:15:01Z,{},review
4213,no,"<p>I agree with the &#34;first try TD learning&#34; just begging to be refuted :)  I think that&#39;s why they say to provide a brief explanation for your True/False.  If your explanation is logical, then you&#39;ll probably get at least some of the credit for the problem, since it (your explanation) will involve removing ambiguities from the question.</p>",2019-11-26T19:21:04Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3g8x0aai763jm,2019-11-26T19:21:04Z,{},review
4214,no,"<p>At this point I&#39;m assuming the entire final (or a significant portion of it) will be ambiguous: it seems to be a one-trick pony, with this strategy being employed heavily. I&#39;m not trying to be harsh here, just calling it like I see it, based on the review questions here.</p>
<p></p>
<p>Alec has stated in a previous post (Review #2) that any attempt of providing multiple answers based on interpretation of ambiguous words or phrases is an automatic deduction, with the only result being partial to no credit. Employing that or your strategy for every (or most) question(s) and your maximum score is capped at 75, maybe even 50. &#x1f44d; </p>
<p></p>
<p>As a test-taker I&#39;m incentivized to choose an interpretation and defend it, as then I may get full credit. This strategy will depend on my estimation of confidence in that interpretation in the first place (anyone care to create a payoff matrix? &#x1f61c;). This ultimately incentivizes an answer that demonstrates less understanding, IMHO, of the subject matter and the nuances surrounding it. It&#39;s what we learned in class: set the reward structure, and the behavior will follow.</p>",2019-11-26T19:25:43Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3g92zmefid47v,2019-11-26T19:25:43Z,{},review
4215,no,"<p>&#64;David, can you share the excerpt where Alec warns about deducting for answers that effectively address ambiguity? I re-read everything Alec said in Review#2 and don&#39;t see any warning resembling this. I ask because it doesn&#39;t seem reasonable punishing an answer that identifies ambiguity and clearly/concisely states correct explanations for each possible interpretation. If a warning like that exists I definitely want to take it into account. Thanks.</p>",2019-12-02T18:33:08Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3orugpcffd76g,2019-12-02T18:33:08Z,{},review
4216,no,"<p>&#64;Michael If you look in Review #2, in the first thread, Alec says:</p>
<p></p>
<blockquote>
<p>So, David brings up an interesting point about the final... We do care about True/False, but, we care about your answer too. If you parse the question wrong, get confused by double negatives, or whatever and still have the &#34;correct&#34; brief explanation you get some points. If you have no explanation or just nonsense you get no points even if your guessed True/False correctly.</p>
</blockquote>
<p>The way I understood this, was if I parsed the question wrong due to some syntactic/grammatical/logical issue, but provided a correct brief explanation, then I get <strong>some</strong> points. I believe providing multiple answers due to some observed ambiguity would fall into &#34;parsing the question wrong&#34;. Perhaps I&#39;m wrong; I&#39;d be glad if I am.</p>",2019-12-02T19:51:10Z,23,Week 12/1 - 12/7,feedback,,i4jbttw9ru63ot,k3oumtgkdah2jr,2019-12-02T19:51:10Z,{},review
4217,no,"<p>&#64;David, thanks. I interpret Alec&#39;s advice differently:</p>
<p>- If a question is ambiguous, is &#34;incorrectly&#34; parsed (the student interprets it as asking about something different than intended), the student&#39;s interpretation is reasonable in lieu of ambiguity, and the answer provided is accurate with respect to their interpretation, then they may still receive some credit.</p>
<p></p>
<p>In contrast:</p>
<p>- If a question is ambiguous and is correctly parsed as such, and multiple correct answers are provided to each potential interpretation, it seems hard to deduct anything for that.</p>
<p></p>
<p>I don&#39;t see any explicit warning against providing multiple correct answers to each possible interpretation of an ambiguous question. e.g. if Sutton took the exam and was asked an ambiguous question about TD, recognized the ambiguity directly and provided correct answers to each of two potentially intended questions, his correct answers probably shouldn&#39;t be deducted.</p>
<p></p>
<p>I remember in ML the advice was &#34;Keep answers concise, as it&#39;s not uncommon for students to answer something correctly, then elaborate unnecessarily and eventually say something incorrect which must be deducted.&#34; Seems like RL is similar, but if a question is ambiguous and you can call it out and provide correct answers to each case you should still be fine. This appeared to be how I was graded in ML.</p>",2019-12-02T20:07:02Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3ov78e5vjo39g,2019-12-02T20:07:02Z,{},review
4218,no,<p>Perhaps you&#39;re right: I think at this point it&#39;d be helpful for an instructor to give a final ruling/explanation on this issue to quell the confusion and back-and-forth.</p>,2019-12-02T20:41:29Z,23,Week 12/1 - 12/7,feedback,,i4jbttw9ru63ot,k3owfiwdp764uc,2019-12-02T20:41:29Z,{},review
4219,no,"<p>I agree with Michael&#39;s interpretation, especially in light of the fact that some of these questions may be purposefully ambiguous so that a student provides the appropriate necessary assumption that is needed to justify his side (true or false), as an indicator of greater understanding.</p>
<p></p>
<p>As for the teacher&#39;s insistence on briefness, etc., I see it as follows:</p>
<p></p>
<p>In an ideal world, students are all sincere.  They parse a True/False question, pick a side, and give a brief explanation.</p>
<p></p>
<p>Sometimes, they identify an ambiguity, state an assumption that resolves the ambiguity, and concisely answer the question given the assumption.  In some cases, they may offer answers to both sides based on two different, but valid, assumptions.</p>
<p></p>
<p>And sometimes, when they just don&#39;t know, they&#39;ll say &#34;I don&#39;t know!&#34;</p>
<p></p>
<p>In the real world, you probably have (some) students who try to bs their way to an answer that tries to cover all bases.</p>
<p></p>
<p>I can&#39;t imagine a teacher punishing a student for correctly identifying an ambiguity and giving one or more correct answers based on important assumptions that need to be made, but I can see why an instructor would ask students to pick a side, or to be brief.</p>
<p></p>
<p>If you&#39;ve ever graded a homework problem, there&#39;s nothing more annoying than a student who is intentionally trying to obfuscate.</p>",2019-12-02T21:17:55Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3oxqdzzdl758b,2019-12-02T21:17:55Z,{},review
4220,no,"<p>&#34;Given a model (T,R) we can also sample in, we should first try TD learning.&#34; -- I&#39;ve spent quite a bit of time on this one and am still not clear on an answer, despite many great discussion points above.</p>
<p></p>
<p>My initial reaction: False, if we know the model and reward we can solve for an optimal policy directly via value iteration or policy iteration.</p>
<p></p>
<p>Vahe&#39;s point on large state spaces seems great, but I don&#39;t see why TD becomes the first choice just b/c the state space is large. When I think &#34;large [or continuous] state space&#34;, TD isn&#39;t the most prominent concept that comes to mind for me, it&#39;s function approximation (e.g. DQN). Some reading online suggests algorithms like Approximate Value Iteration (<a href=""https://epubs.siam.org/doi/abs/10.1137/040614384?journalCode=sjcodc"" target=""_blank"" rel=""noopener noreferrer"">AVI</a>) are highly efficient &amp; effective for large state spaces as well (&amp; sound applicable when the model &amp; rewards are known).</p>
<p></p>
<p>It may be helpful to share our assumptions of what other options could be valid besides TD.</p>
<p>Are DP, MC, and TD the first-choice options we&#39;re considering? Are these the <em>only </em>valid options?</p>
<p>Is DQN considered a technique within TD, which makes TD inherit the ability of efficiently handling large state spaces (and thus the first choice)? If so, can someone clarify the root reason that DQN would be considered a TD approach? It&#39;s not just because it uses the Bellman equation to propagate values discounted over time, since value iteration does that as well and is considered DP, not TD. My understanding of TD is it learns by processing experience data in temporal order (e.g. the order its received or generated), but DQNs with experience replay process data in random order sampled via minibatch, so it&#39;s unclear where the essence of TD applies there.</p>
<p>Would an algorithm like AVI be considered a technique within DP? If so, can we say with confidence TD should be our first choice despite AVI being an efficient option when models &amp; rewards are known?</p>
<p></p>
<p>Overall, it&#39;s a great review question for learning. I wonder if we&#39;re all coming at it from the same assumptions.</p>
<p></p>
<p><strong>Edit:</strong> (attempting to) answer part of my own question on why DQN would be considered TD -- from Wikipedia, the essence of TD is &#34;learning by bootstrapping from the current estimate of the value function, [while] sampling from the environment.&#34;   It&#39;s interesting to realize neither of those properties necessarily imply experiences are processed in natural temporal order which was emphasized as characteristic of TD in Sutton&#39;s first paper on the topic. If bootstrapping values &amp; sampling from environment are all that&#39;s necessary to be considered &#34;TD&#34;, I guess DQN fits the definition. Still have some uncertainty about why TD would always be the preferable first choice though...</p>
<p><strong>Edit2:</strong> On further thought, I can see how it can be argued DQN sampling experiences randomly via experience replay still processes data in temporal order since the temporal order is encapsulated in each (s,a,r,s&#39;) tuple, in contrast to MC methods which ignore intermediate transitions and only focus on the outcome of each episode.</p>
<p></p>",2019-12-03T19:11:39Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3q8nui21a6bb,2019-12-03T19:11:39Z,{},review
4221,no,"<p>Bonus question (for those confident in their answer), can you write true statements completing each of these sentences?</p>
<p></p>
<ul><li>&#34;_____________, we should first try DP.&#34;</li><li>&#34;_____________, we should first try MC.&#34;</li><li>&#34;_____________, we should first try TD.&#34;</li></ul>
<p></p>
<p>(Can you isolate exactly when (and when not) to use TD first consistent with the original question?)</p>
<p>(edit: this assumes DP, MC, and TD are the only valid choices, if this seems incorrect it&#39;d be helpful to know)</p>",2019-12-03T19:37:19Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3q9kvgvv281uu,2019-12-03T19:37:19Z,{},review
4222,no,"<p>Okay, you brought up a lot of stuff.  Let&#39;s tackle the definitional issues first.</p>
<p></p>
<p>Both DP and TD methods use bootstrapping. MC methods don&#39;t.</p>
<p>Both TD and MC methods sample from the environment, DP doesn&#39;t.</p>
<p></p>
<p>DP avoids sampling by being a $$\textit{model-based}$$ algorithm: it can $$\textit{plan}$$ since it has the model.  Both TD and MC are $$\textit{model-free}$$.</p>
<p></p>
<p>TD is the only one of those three that both bootstraps $$\textit{and}$$ samples.  Both TD and DP are based on the Bellman equation.  TD approximates the Bellman equation in $$\textit{two different ways}$$.  It approximates the transition function (by sampling) and it approximates the return (by bootstrapping). DP only approximates the Bellman equation by bootstrapping.  MC approximates the expected return directly by sampling (and bypasses the Bellman equation).  I think cutting both those corners is what makes TD methods so simple, scalable, and powerful on real (i.e. complex/large) problems.</p>
<p></p>
<p>Almost every single algorithm we&#39;ve used in this course starting with homework 2 has been a TD-based algorithm.  Sarsa is just an implementation of an on-policy TD control algorithm and Q-learning is an implementation on an off-policy TD control algorithm.  DQN is Q-learning with an innovation added (experience replay).</p>
<p></p>
<p>Since everything we did was TD but not necessarily called out as TD, it&#39;s easy to not realize that this was the case.</p>
<p></p>
<p>Considering we $$\textit{never}$$ implemented a policy-gradient or a Monte Carlo algorithm in this course, when Alec asked if TD should be the &#34;first approach,&#34; I interpreted it as &#34;as opposed to a planning method,&#34; (e.g. value/policy iteration -- dynamic programming).  This is still a meaningful stand to take, because there are many environments where the model is completely specified (e.g. backgammon) but TD approaches have worked well whereas DP approaches are infeasible.</p>
<p></p>
<p>I mention policy gradient specifically above because from my limited learning outside of the scope of this course, it seems that policy-gradient methods are actually very popular in modern RL.  So it may very well turn out to be the case that Alec&#39;s statement is False or somewhat False.  But I think that, within the scope of this course, one can reasonably argue that it&#39;s True.</p>",2019-12-03T20:33:41Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3qblcrfi4vc0,2019-12-03T20:33:41Z,{},review
4223,no,"<p>Thanks for the clarification, Vahe. It&#39;s helpful to realize explicitly that everything following HW1 has been an adaptation of TD.</p>
<p></p>
<p>&#34;[DP] can plan since it has the model.&#34;<br />Also didn&#39;t realize planning is exclusive to DP. AlphaGo appears to resemble planning using MC &amp; TD via MCTS &amp; DQN. Though, like backgammon, it seems like another example where DP isn&#39;t the preferred approach due to large state space. There must be cases where it&#39;s apt to favor model-based algorithms, especially when the model is known. (e.g. AlphaZero via MCTS)</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde332xcka1m0%2Fk3qdsjkel23b%2Frlt.png"" alt="""" /></p>
<p></p>
<p>Thinking about the original: &#34;Given a model (T,R) we can also sample in, we should first try TD learning.&#34;<br />I don&#39;t see a clear signal to favor model-free vs model-based approaches based on the information given. The question reminds me of the &#39;No Free Lunch Theorem&#39;: &#34;any two optimization algorithms are equivalent when their performance is averaged across all possible problems&#34; Admit I take issue with No Free Lunch since if I were given an unknown regression problem by a client tomorrow I&#39;d expect XGBoost to outperform linear regression, because in practice non-linear problems come up all the time and some models (e.g. gradient boosting) are capable of performing reasonably well regardless. By the same rationale you can make an argument to try TD first here, primarily on the basis that its variants tend to be capable of good results in general.</p>
<p></p>
<p>Re-watching the <a href=""https://youtu.be/Y0pS_o4Fv00?t=250"" target=""_blank"" rel=""noopener noreferrer"">explanation from Dr MacGlashan</a> &#34;Many RL approaches are capable of good results (DQN, A3C, PPO, etc.), how do you choose your initial approach in practice?&#34; -- he essentially says specifics depend on context, but almost every approach outlined sounds TD-based at its roots.</p>
<p></p>
<p>Learned a lot from the discussion, thanks again.</p>
<p></p>",2019-12-03T21:38:07Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3qdw7qukh13l8,2019-12-03T21:38:07Z,{},review
4224,no,"<p>Considering everything discussed, if I had to give an answer for the exam I&#39;d probably say:</p>
<p></p>
<p>False, you should first consider the nature of the problem and what you are trying to achieve. If the state and action spaces are accessibly small and you require an optimal policy for all states (even  extremely uncommon ones), you may prefer a DP algorithm like value iteration. If the problem is too  complex for DP and/or near optimal policy can suffice for most states, TD is often a better option.</p>",2019-12-04T13:24:45Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3rbplkvdzewc,2019-12-04T13:24:45Z,{},review
4225,no,"<p>False, If one has T and R why should one try to learn them?</p>",2019-12-04T21:55:14Z,23,Week 12/1 - 12/7,feedback,,j6ll2xkiDJf,k3rty2pxkm67f9,2019-12-04T21:55:14Z,{},review
4226,no,"<p>Questions of feasibility aside: false, if you have the model one would expect it to be more efficient to use it.</p>",2019-12-06T21:21:37Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3unmjniazm1tn,2019-12-06T21:21:37Z,{},review
4227,no,"<p>kamran, david - elegant answer :)  my answer had the same flair, but from a different basis.</p>",2019-12-07T21:15:19Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w2uaqbhtt1sf,2019-12-07T21:15:19Z,{},review
4228,no,"<p>TD(1) slowly propagates information, so it does better in the repeated presentations regime rather than with single presentations.</p>",2019-11-18T03:37:01Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33vo4ptfwnz9,2019-11-18T03:37:01Z,{},review
4229,no,<p>False.  TD(0) (one-step TD) slowly propagates information.</p>,2019-11-18T04:00:49Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33wiquxx0j149,2019-11-18T04:00:49Z,{},review
4230,no,<p>False. TD-0.</p>,2019-11-18T23:32:45Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352dur9ate42h,2019-11-18T23:32:45Z,{},review
4231,no,<p>False. The optimal lamba is usually between 0 and 1 (0&lt;lambda&lt;1) in repeated presentations regime.</p>,2019-11-21T20:26:46Z,25,Week 11/17 - 11/23,feedback,,jzg6jh2hn6f43c,k39628i8glu5qt,2019-11-21T20:26:46Z,{},review
4232,no,"<p>False. TD(0) propagates information slowly. At every presentation, TD(1) is propagating backwards an outcome&#39;s information to the full extent.</p>",2019-11-26T15:09:10Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3fzx28z7sf52l,2019-11-26T15:09:10Z,{},review
4233,no,"<p>True, TD(1) is similar to supervised learning where multiple observations that are identical can reinforce the learning and give more validity to the outcome or the label (same as repeated presentations).</p>",2019-12-04T20:39:13Z,23,Week 12/1 - 12/7,feedback,,j6ll2xkiDJf,k3rr8bi72iz4la,2019-12-04T20:39:13Z,{},review
4234,no,<p>I am just going to echo Vahe: False. TD(0) propagates more slowly.</p>,2019-12-06T21:25:51Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uns06zwvb796,2019-12-06T21:25:51Z,{},review
4235,no,"<p>In TD(lambda), we should see the same general curve for best learning rate (lowest error), regardless of lambda value.</p>",2019-11-18T03:37:18Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33voi9js0618u,2019-11-18T03:37:18Z,{},review
4236,no,<p>False.  The shapes are similar (convex) but the minima are completely different (Desperately Seeking Sutton!)</p>,2019-11-18T04:06:49Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33wqg9diwv79h,2019-11-18T04:06:49Z,{},review
4237,no,"<p>True:  the graphs are different for TD-1 and TD-lambda (although TD lambda contains TD-1, so pathologically, it must be contained).</p>
<p></p>",2019-11-18T23:38:14Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352kwsxk3u3pw,2019-11-18T23:38:14Z,{},review
4238,no,"<p>This is ambiguous and depends on how far you&#39;re going with the word &#34;general&#34;.</p>
<p></p>
<p>The curves have similar shape, but there will obviously be a lambda value where TD will perform better than other values, depending on the specific problem.</p>
<p></p>
<p>I&#39;m going to say True here, but I can see it argued both ways. This is going to be a fun exam.</p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fi4jbttw9ru63ot%2Fk3g01cywa6fn%2FTDLambda.png"" alt="""" /></p>",2019-11-26T15:13:28Z,24,Week 11/24 - 11/30,feedback,,i4jbttw9ru63ot,k3g02ljl6be35x,2019-11-26T15:13:28Z,{},review
4239,no,<p>I&#39;d go with &#34;generally&#34; True here.</p>,2019-12-06T21:29:34Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3unwrv5nbf307,2019-12-06T21:29:34Z,{},review
4240,no,<p>Meat can be keep safely at 120°F indefinitely.  </p>,2019-11-18T03:40:08Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33vs53wll323j,2019-11-18T03:40:08Z,{},review
4241,no,True. The meat will be safe (but any humans that eat it will not).,2019-11-18T23:55:05Z,25,Week 11/17 - 11/23,feedback,,jl0d29plhbs6cl,k3536kwwnvr5m5,2019-11-18T23:55:05Z,{},review
4242,no,"<p>False.  If stored at 120 °F, the MRE should be used within a month. Stored at 60 °F, an MRE can last 7 years or more.</p>",2019-11-21T20:42:47Z,25,Week 11/17 - 11/23,feedback,,jzg6jh2hn6f43c,k396mtloo7u2tq,2019-11-21T20:42:47Z,{},review
4243,no,"<p><a href=""https://www.fsis.usda.gov/shared/PDF/How_Temperatures_Affect_Food.pdf"">https://www.fsis.usda.gov/shared/PDF/How_Temperatures_Affect_Food.pdf</a></p>
<p></p>
<blockquote>
<p>Bacteria grow most rapidly in the range of temperatures between 40 ° and 140 °F, doubling in number in as little as 20 minutes. This range of temperatures is often called the “Danger Zone.” That’s why the Meat and Poultry Hotline advises consumers to never leave food out of refrigeration over 2 hours. If the temperature is above 90 °F, food should not be left out more than 1 hour.</p>
</blockquote>",2019-11-29T18:40:32Z,24,Week 11/24 - 11/30,feedback,,jqkxzdmmolGf,k3khsfpdvi35so,2019-11-29T18:40:32Z,{},review
4244,no,"<p>Sergei,</p>
<p></p>
<p>What if you&#39;re trying to toughen up your immune system?</p>",2019-11-29T19:48:17Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3kk7k01buk2wr,2019-11-29T19:48:17Z,{},review
4245,no,"<p>Perhaps if you were trying to protect the meat, keeping it &#34;safe&#34; from your neighbors who like fresh meat??</p>",2019-12-01T06:26:47Z,23,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3mmgj4zjcz44j,2019-12-01T06:26:47Z,{},review
4246,no,"<p>In general, an update rule which is not a non-expansion will not converge.</p>",2019-11-18T03:55:09Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wbg63qt2e0,2019-11-18T03:55:09Z,{},review
4247,no,"<p>True, but there are exceptions that rely on different mechanisms for convergence (e.g. CoCo-Q).</p>",2019-11-18T04:11:00Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33wvu8uq2y7cb,2019-11-18T04:11:00Z,{},review
4248,no,"<p>True (even if you consider that a non-expansion, has a hard requirement of a following contraction map; verbiage yields true.)</p>",2019-11-18T23:56:26Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k3538az04sx7ma,2019-11-18T23:56:26Z,{},review
4249,no,"<p>True.</p>
<p></p>
<p>A non-expansion is {constant, contraction}, and a not non-expansion is not a constant and not a contraction, therefore it is an expansion and expansions diverge.</p>
<p></p>
<p></p>",2019-11-19T03:50:54Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35blu4ax65w6,2019-11-19T03:50:54Z,{},review
4250,no,<p>&#x1f44d;</p>,2019-12-06T21:31:43Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3unzj8uh3r68m,2019-12-06T21:31:43Z,{},review
4251,no,"<p>Late to the party, but:<br /><br />False: an update rule that is not a non-convergence is not <strong><em>guaranteed</em></strong> to converge, but there is nothing stopping them doing so. The agent must encounter a scenario which causes the update rule to plateau or diverge for it not to converge; simply having a non-non-expansion (pardon the double negative) is not enough to assume a lack of convergence in practice.</p>",2019-12-07T19:05:32Z,23,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3vy7elhx061m9,2019-12-07T19:05:32Z,{},review
4252,no,"<p>ben excellent distinction; though I wonder - if convergence is convergence in the mathematical sense (doesn&#39;t diverge); or in the optimal sense (converges to a particular fixed point).  with respect to your point I think it is more mathematical -- since the convergence gaurantee is typically predicated on a single unique fixed point within the bellman operator -- and thus is only defined when convergence is defined (within the bellman sense i.e. given munroe-robbins).<br /><br />or more simply: no fixed point, means u can&#39;t converge to that fixed point (with respect to a gaurantee).<br /><br />I would be interested in reading the originating work for the munroe-robbins criteria; as its not clear exactly where the line splits between the munroe-robbins criteria (as necessary for convergence to a fixed point); and the convex combination update inherent to bellman-like operators.  i.e. does there exist other classes of fixed points, not compassed by bellman?  and in cases of &#34;true&#34; convergence, in non-gauranteed algorithms -- what quality or character do those fixed point(s) evince?</p>",2019-12-07T21:21:59Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w32vqo9a61jy,2019-12-07T21:21:59Z,{},review
4253,no,"<p></p><div>
<div>
<p>MDPs are a type of Markov game.</p>
</div>
</div>
<div>
<div>
<div>
<div></div>
</div>
<div>
<div></div>
</div>
</div>
</div>",2019-11-18T03:55:39Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wc3sjnwbwy,2019-11-18T03:55:39Z,{},review
4254,no,<p>True.  Markov games with only one player action space and a corresponding single reward function are MDPs.</p>,2019-11-18T04:14:31Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33x0czvrai67g,2019-11-18T04:14:31Z,{},review
4255,no,<p>True.</p>,2019-11-18T23:56:59Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k35390mlwxxn7,2019-11-18T23:56:59Z,{},review
4256,no,<p>&#x1f44d;</p>,2019-12-06T21:32:01Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3unzxb7y4m6sh,2019-12-06T21:32:01Z,{},review
4257,no,"<p>Contraction mappings and non-expansions are concepts used to prove the convergence of RL algorithms, but are otherwise unrelated concepts.</p>",2019-11-18T03:55:55Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wcfsw18t17y,2019-11-18T03:55:55Z,{},review
4258,no,"<p>False.  The first part is true, but the two concepts are related: a contraction mapping is a non-expansion.</p>",2019-11-18T05:38:23Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k34007snm705a5,2019-11-18T05:38:23Z,{},review
4259,no,"<p>False.  They are totally related. A non-expansion must be followed by a contraction, in order to provide convergence guarantees.</p>",2019-11-18T23:57:59Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k353aaoaoqu1wh,2019-11-18T23:57:59Z,{},review
4260,no,Can you name two unrelated concepts from this course?,2019-11-20T02:21:05Z,25,Week 11/17 - 11/23,feedback,,jlcjoyw4Fsyi,k36nu6drks94if,2019-11-20T02:21:05Z,{},review
4261,no,<p>kyle - interesting point. </p>,2019-11-21T23:06:25Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k39brjjdm6d3uq,2019-11-21T23:06:25Z,{},review
4262,no,<p>I don&#39;t recall learning about these concepts. Can anyone recommend a lecture to review that covers this?</p>,2019-12-04T05:18:05Z,23,Week 12/1 - 12/7,feedback,,jc7qnuoloyn56p,k3qubqlelr7nc,2019-12-04T05:18:05Z,{},review
4263,no,<p>The entire convergence lecture covers this.</p>,2019-12-04T05:38:51Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3qv2g8oxkz1mw,2019-12-04T05:38:51Z,{},review
4264,no,"<p>False as noted...</p>
<p></p>
<p></p>
<p></p>
<p>This course is just Belman operators all the way down.</p>",2019-12-06T21:36:06Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uo565hidxm3,2019-12-06T21:36:06Z,{},review
4265,no,"<p>there is actually quite some subtly to the whole &#34;non-expansion&#34;; in that the non-expansion acts as a bound forcing certain updates into the contractionary process; and other updates (temporally speaking) into the non-expansion.  just some food for thought.  in a very real &#34;RL&#34; sense (not just conceptual), they are directly related.</p>",2019-12-07T21:28:59Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w3bv8t6nn54y,2019-12-07T21:28:59Z,{},review
4266,no,<p>Linear programming is the only way we are able to solve MDPs in linear time.</p>,2019-11-18T03:56:22Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wd0srmu216,2019-11-18T03:56:22Z,{},review
4267,no,"<p>False.  Where to begin?  First of all, LP takes super-linear polynomial time.  Secondly, LP is only one part of a potential RL algorithm.  Third of all, there are many methods that don&#39;t use LP that solve MDPs (though none in linear time).</p>",2019-11-18T04:23:02Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33xbb2q1le1px,2019-11-18T04:23:02Z,{},review
4268,no,<p>False.  I dont think there is a way to solve it in linear time.</p>,2019-11-19T00:01:22Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k353enaflw86pc,2019-11-19T00:01:22Z,{},review
4269,no,<p>False. Linear programming is a very general technique and does not appear to take advantage of the special structure of MDPs and this reduction is currently the only proof that MDPs are solvable in polynomial time.</p>,2019-11-27T15:59:53Z,24,Week 11/24 - 11/30,feedback,,jzg6jh2hn6f43c,k3hh64dkfod1g9,2019-11-27T15:59:53Z,{},review
4270,no,"<p>FALSE (as Michael points out). &#34;There is only one way to solve MDPs in POLYNOMIAL time, da ta da! linear programming&#34; - https://classroom.udacity.com/courses/ud600/lessons/4602578895/concepts/46029086330923</p>",2019-12-03T14:01:48Z,23,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3pxldt4gs55ja,2019-12-03T14:01:48Z,{},review
4271,no,<p>False. LPs have polynomial time complexity. The linearity refers to the objective and constraints. </p>,2019-12-04T06:10:46Z,23,Week 12/1 - 12/7,feedback,,ijctp4ucNy8,k3qw7hbl74q1df,2019-12-04T06:10:46Z,{},review
4272,no,"<p>False. Double-checking linked lecture:</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde332xcka1m0%2Fk3rnk6g4vw06%2Flp1.png"" alt="""" /></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fisde332xcka1m0%2Fk3rnkhkottx4%2Flp2.png"" alt="""" /></p>",2019-12-04T18:58:23Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3rnmnahy9m1es,2019-12-04T18:58:23Z,{},review
4273,no,"<p>I think it is false, because LP solves an MDP in polynomial time.</p>
<p></p>
<p>It is interesting that the video mentioned above says <strong>only</strong> LP solves MDP in polynomial time, but Sutton&#39;s book in chapter 4.7 also says that DP finds the optimal policy in polynomial time.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc6xvgjncoey%2Fk3syvul2ct56%2FCapture.PNG"" alt="""" /></p>
<p></p>",2019-12-05T16:52:54Z,23,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3syl4j6blj4by,2019-12-05T16:52:54Z,{},review
4274,no,"<p>Danilo,</p>
<p></p>
<p>I think the issue here may be the fine print.  DP is polynomial in $$\textbf{states and actions}$$.  LP is polynomial, period.  If you read Sutton carefully, he says:</p>
<p></p>
<p>&#34;<em><strong>If we ignore a few technical details</strong>, then the (worst case) time DP methods take to find an optimal policy is polynomial in the number of states and actions</em>.&#34;</p>
<p></p>
<p>Those &#34;technical details&#34; probably include the $$\frac{1}{1-\gamma}$$ issue that Professor Littman brings up in the video (i.e. that DP is $$\textit{not}$$ polynomial in the number of bits used to express $$\gamma$$).</p>
<p></p>
<p>Edit: Look up two sentences from your highlighted phrase.</p>
<p></p>
<p>It is also worth noting that computational complexity is an asymptotic measure.  In practice it isn&#39;t always the most useful metric to go by.  Two algorithms that are both polynomial in the number of inputs may have very different coefficients on those polynomial terms.  This is probably the case with DP and LP (for a fixed $$\gamma$$) since Sutton claims that DP scales better than LP by a factor of $$100$$.</p>",2019-12-05T17:10:45Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3sz82sqois1q1,2019-12-05T17:10:45Z,{},review
4275,no,"<p>Vahe,</p>
<p></p>
<p>Your comments make sense to me. Thank you!</p>",2019-12-05T20:32:23Z,23,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3t6fdv6kwj6x6,2019-12-05T20:32:23Z,{},review
4276,no,<p>&#x1f44d;</p>,2019-12-06T21:36:35Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uo5sfqku01uj,2019-12-06T21:36:35Z,{},review
4277,no,"<p></p><blockquote><br />DP is polynomial in states and actions. LP is polynomial, period.</blockquote>
<p>An algorithm has to be polynomial in <em>something</em>. LP is <a href=""https://en.wikipedia.org/wiki/Linear_programming#Open_problems_and_recent_work"" target=""_blank"" rel=""noopener noreferrer"">polynomial in the number of variables and constraints</a>. The linear program to <a href=""https://classroom.udacity.com/courses/ud600/lessons/4602578895/concepts/46436644190923"" target=""_blank"" rel=""noopener noreferrer"">solve an MDP</a> has one constraint for every state and action, and one variable per state. So LP in the MDP context <em>is</em> polynomial in the number of states and actions.</p>",2019-12-06T23:50:04Z,23,Week 12/1 - 12/7,feedback,,is8ald0uljj3u4,k3usxg5il9k1c4,2019-12-06T23:50:04Z,{},review
4278,no,"<p>&#34;<em>So LP in the MDP context <em>is</em> polynomial in the number of states and actions.&#34;</em></p>
<p><em></em></p>
<p>Not just states and actions; LP is polynomial in <em>everything</em>, whereas DP is only polynomial in states and actions.</p>
<p></p>
<p>The big culprit here seems to be the number of bits of precision required to express $$\gamma$$.  As Professor Littman says, the value of $$\frac{1}{1-\gamma}$$ can grow very fast relative to the number of bits required to express $$\gamma$$.  This seems to cause a problem for DP (it&#39;s exponential in the precision of $$\gamma$$), but not for LP (apparently still polynomial).</p>
<p></p>",2019-12-07T00:56:53Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3uvbdngneq1iy,2019-12-07T00:56:53Z,{},review
4279,no,<p>The objective of the dual LP presented in lecture is minimization of “policy flow”. (The minimization is because we are aiming to find an upper bound on “policy flow”.)</p>,2019-11-18T03:56:44Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wdhjq1e82e9,2019-11-18T03:56:44Z,{},review
4280,no,"<p>False.  The objective of the dual LP is to <strong>maximize</strong> &#34;policy flow&#34;, subject to conservation of flow constraints.  The minimization is for the primary LP problem, in order to find the least upper bound of value functions over states and actions.</p>",2019-11-18T05:57:03Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k340o85ikaq5ot,2019-11-18T05:57:03Z,{},review
4281,no,<p>Which lecture is this from?</p>,2019-12-03T02:42:58Z,23,Week 12/1 - 12/7,feedback,,jlcjoyw4Fsyi,k3p9cec4ybs1w7,2019-12-03T02:42:58Z,{},review
4282,no,"<p>It was from a discussion of linear programming in the context of value iteration, the &#39;AAA&#39; lecture.  It&#39;s also in Littman&#39;s dissertation.</p>",2019-12-03T03:16:01Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3paiwf8udtpi,2019-12-03T03:16:01Z,{},review
4283,no,<p>Kyle: https://classroom.udacity.com/courses/ud600/lessons/4602578895/concepts/45817826040923</p>,2019-12-03T14:06:24Z,23,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3pxrayef74sf,2019-12-03T14:06:24Z,{},review
4284,no,<p>&#x1f44d;</p>,2019-12-06T21:37:30Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uo6z60rs3d2,2019-12-06T21:37:30Z,{},review
4285,no,"False. The objective of the dual LP is maximization of reward, as regulated by the policy flow, and the constraints amount to conservation of flow in each state.",2019-12-07T00:10:15Z,23,Week 12/1 - 12/7,feedback,,is8ald0uljj3u4,k3utneng4tg39q,2019-12-07T00:10:15Z,{},review
4286,no,<p>Now remind me what the primal minimizes; the sum of all upper bounds on state-action pair constraints. ;)</p>,2019-12-07T21:30:40Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w3e16xptl76j,2019-12-07T21:30:40Z,{},review
4287,no,<p>Any optimal policy found with reward shaping is the optimal policy for the original MDP.</p>,2019-11-18T04:02:43Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wl6vbcz22fr,2019-11-18T04:02:43Z,{},review
4288,no,<p>False in general.  This is true if the reward is shaped with a potential-based shaping function.</p>,2019-11-18T04:35:43Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33xrm1wb0776d,2019-11-18T04:35:43Z,{},review
4289,no,"<p>Hmm &#34;reward shaping&#34; constrained by techniques in reward shaping that still preserve convergence?</p>
<p>OR... somebody just randomly slung reward delta&#39;s all over the place?</p>
<p>I&#39;ll go with the technical usage. True.</p>
<p></p>
<p></p>",2019-11-18T23:45:23Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352u42ka0r7io,2019-11-18T23:45:23Z,{},review
4290,no,"True? If done properly, then any optimal solution derived with reward shaping is also optimal for  the original MDP.",2019-11-18T23:58:47Z,25,Week 11/17 - 11/23,feedback,,jl0d29plhbs6cl,k353bbppg1z3jd,2019-11-18T23:58:47Z,{},review
4291,no,<p>Optimal with respect to the original reward function? Then I think should be true. </p>,2019-11-19T01:40:33Z,25,Week 11/17 - 11/23,feedback,,jl3we43d3bp15p,k356y7htygk5hs,2019-11-19T01:40:33Z,{},review
4292,no,"<p>True. optimal is optimal regardless of how you got there. Does the MDP change with your shaping function? that&#39;s the only way this would not produce the &#34;optimal policy for the original MDP.&#34;</p>
<p></p>
<p>&#34;Around 2:04, Charles justifies why Q&#39; has the same optimal policy as Q by saying &#34;you&#39;re just being shifted around by a constant, and so it doesn&#39;t change the max and therefore change the optimal policy.&#34;&#34;</p>
<p></p>
<p>https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/45342599270923</p>",2019-11-19T02:32:44Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k358tbqmfg05tn,2019-11-19T02:32:44Z,{},review
4293,no,"False, for some definitions of  &#34;shape the reward&#34; you are effectively constructing a different environment (e.g. If in P3&#39;s soccer game you add a reward for each time step you don&#39;t lose, perhaps agents will then learn to &#34;stay&#34; indefinitely and accumulate reward forever).",2019-11-23T02:01:50Z,25,Week 11/17 - 11/23,feedback,,jzhl7qlwrpagr,k3axgzss6c35u6,2019-11-23T02:01:50Z,{},review
4294,no,<p>False. Shaping your rewards doesn&#39;t necessarily mean that they are perfectly aligned with what the MDP actually wants. So an optimal policy on some particularly shaped rewards may or may not be optimal for the MDP.</p>,2019-12-01T06:26:49Z,23,Week 12/1 - 12/7,feedback,,jzliwdkt88m1q3,k3mmgkgwfcw46x,2019-12-01T06:26:49Z,{},review
4295,no,"<p>True, assuming a potential function is used, reward shaping only shifts each state by a constant which doesn&#39;t change the optimal policy. [<a href=""https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/45342599270923"" target=""_blank"" rel=""noopener noreferrer"">ref</a> - skip to 1:55]</p>",2019-12-04T19:13:07Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3ro5lmb9oh1be,2019-12-04T19:13:07Z,{},review
4296,no,<p>There&#39;s no potential function mentioning in the original question.</p>,2019-12-05T00:29:10Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3rzg111iib4qi,2019-12-05T00:29:10Z,{},review
4297,no,"<p>&#64;Sergei, true. Considering we didn&#39;t learn of any effective reward shaping techniques that don&#39;t rely on potential functions with lectures implying if you&#39;re using reward shaping you better use a potential function, and considering it&#39;s not necessarily intuitive that potential functions don&#39;t change the optimal policy, it seemed like a reasonable interpretation to think they&#39;re asking about that and to state the potential function assumption explicitly. Thinking about the question wording again, I can also see the case for stating false with the caveat about potential functions (like Vahe).</p>",2019-12-05T00:42:31Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3rzx7phpv440g,2019-12-05T00:42:31Z,{},review
4298,no,"<p>Right, this is what I thought too. Thanks Michael!</p>",2019-12-05T03:14:48Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3s5d1xrxvl7bb,2019-12-05T03:14:48Z,{},review
4299,no,<p>A good read: https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf</p>,2019-12-05T16:15:56Z,23,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3sx9l6lh037gt,2019-12-05T16:15:56Z,{},review
4300,no,"<p>I just repeat Vahe apparently... False, only potential based shaping might avoid introducing an sub-optimal policy loop.</p>",2019-12-06T21:40:51Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uobak55t712,2019-12-06T21:40:51Z,{},review
4301,no,"<p>I disagree with you Alec. &#34;Any optimal policy found with reward shaping is the optimal policy for the original MDP.&#34; - Charles Isbell clearly states that shifting (add) or multiplying the reward does not change the original optimal policy. &#34;Reward shaping&#34; is a general term that includes potential based shaping (non-linear method) and linear methods of shaping.</p>
<p></p>
<p><a href=""https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/43992086780923"">https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/43992086780923</a></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3uqezzbfu8i%2Fshapingevidence.png"" alt="""" /></p>",2019-12-06T22:39:54Z,23,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3uqf85qpqv5nu,2019-12-06T22:39:54Z,{},review
4302,no,"<p>Not sure Jacob, why is the disagreement? &#34;Any&#34; (if I read correctly) means &#34;arbitrary&#34; in this context, and Prof Littman gives examples of when the reward shaping applied incorrectly led to unwanted behavior (&#34;wiggling&#34; as he called it :-) ).</p>",2019-12-06T23:36:32Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3usg1yn5636un,2019-12-06T23:36:32Z,{},review
4303,no,"<p>I think it is false, due to the reasons specified, as well as the lecture example for soccer where if we give the agent &#43;1 for touching the ball, it will just vibrate against the ball gaining an infinite score vs putting it into the goal, which creates the sub optimal positive loops.</p>
<p></p>
<p>Thinking about Project 2, there was some discussion about reward shaping and how this would be considered domain knowledge injection. Let&#39;s consider the reward shaping where I provide more reward for the agent if it keeps itself right side up, vs at some angle. By giving the agent enough reward to make it focus on staying with only a 2 degree max swing (like we might want in cartpole) into the lunar lander, would we end up with the same optimal policy? I think not because the agent would burn more fuel trying to keep itself upright where in reality, maybe up to a 10 degree tilt is fine because it will waste less fuel, and upon touching the ground, gravity will straighten it out without fuel burn.</p>
<p></p>
<p>The issue is really down to your interpretation of &#34;reward shaping&#34;. If we consider any tweaks to the reward as reward shaping, then it&#39;s clearly false based on my examples above. If we only consider reward shaping in the context of potential functions or as adjustments that effect all reward values equally, then it&#39;s true. I feel this is similar to the clustering properties from ML for scale invariance and consistency.</p>",2019-12-07T01:37:10Z,23,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3uwr6wgphu747,2019-12-07T01:37:10Z,{},review
4304,no,<p>this is clearly false. :) I took the term &#34;reward-shaping&#34; to be the name of potential based reward shaping discussed in lecture (from memory some months ago).</p>,2019-12-07T21:32:59Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w3h11292k5dk,2019-12-07T21:32:59Z,{},review
4305,no,<p>Potential-based shaping will find an optimial policy faster than an unshaped MDP.</p>,2019-11-18T04:02:58Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wli0zj9d2qe,2019-11-18T04:02:58Z,{},review
4306,no,"<p>True if you include the identity function as a potential-based shaping function.  In that case, shaping the MDP can only speed up learning time.</p>
<p></p>
<p>Edit: Doh!</p>",2019-11-18T04:40:48Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33xy5wxzig23t,2019-11-18T04:40:48Z,{},review
4307,stud,<p>False. This works in both ways. We can totally come up with a potential based shaping that makes the algorithm to find an optimal policy slower than an unshaped MDP. </p>,2019-11-18T05:14:36Z,25,Week 11/17 - 11/23,feedback,a_0,,k33z5m76c1j3ns,2019-11-18T05:14:36Z,{},review
4308,no,<p>False.  Depends on what the optimal policy actually is.</p>,2019-11-18T23:46:21Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352vclnsk5vr,2019-11-18T23:46:21Z,{},review
4309,no,"<p>&#34;This paper examines potential-based shaping functions, which introduce artificial rewards in a particular form that is guaranteed to leave the optimal behavior unchanged yet can influence the agent’s exploration behavior to decrease the time spent trying suboptimal actions&#34;</p>
<p></p>
<p><a href=""https://www.aaai.org/Papers/AAAI/2008/AAAI08-096.pdf"">https://www.aaai.org/Papers/AAAI/2008/AAAI08-096.pdf</a></p>
<p></p>
<p>True by definition for MDPs (because MDPs use models - the T in the value iteration formula).</p>
<p></p>
<p>Littman - &#34;This gives you a huge leg-up with the learning problem&#34;</p>
<p>https://classroom.udacity.com/courses/ud600/lessons/4388428967/concepts/44376525880923</p>",2019-11-19T03:09:15Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35a49nn3p25rn,2019-11-19T03:09:15Z,{},review
4310,no,<p>doh. need to read the question closer. ;)</p>,2019-11-20T00:33:03Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k36jz8uhzha3w2,2019-11-20T00:33:03Z,{},review
4311,no,<p>False. It can increase or decrease the learning time. Using knowledge can learn faster than without knowledge. </p>,2019-11-27T18:01:59Z,24,Week 11/24 - 11/30,feedback,,jzg6jh2hn6f43c,k3hlj5opswn44,2019-11-27T18:01:59Z,{},review
4312,no,<p>True. Potentials provide extra information and also help the agent in avoiding going back and forth the same states again and again. If the potential function is correct then potential-based shaping will find the optimal policy faster or at least equal to unshaped MDP. </p>,2019-12-01T16:52:53Z,23,Week 12/1 - 12/7,feedback,,jzliwdkt88m1q3,k3n8towgdcq4e4,2019-12-01T16:52:53Z,{},review
4313,no,"<p>I know we went over reward and policy shaping, but when and in what lecture did we discus potential-based shaping?</p>
<p></p>
<p>From Jacob;&#39;s paper above, I know: <em>&#34;Potential-based shaping was designed as a way of introducing background knowledge into model-free reinforcement-learning algorithms.&#34;</em></p>
<p></p>
<p>But can someone summarize what potential-based shaping is in laymen. Is it a form of reward or policy shaping?</p>",2019-12-06T04:43:39Z,23,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3tnz5uo44l64f,2019-12-06T04:43:39Z,{},review
4314,no,<p>It&#39;s in the lecture called &#34;Messing with Rewards.&#34;  There are a set of sub-lectures in there called &#39;Potential-based Shaping.&#39;</p>,2019-12-06T05:41:37Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3tq1p008b2436,2019-12-06T05:41:37Z,{},review
4315,no,"<p>False... The if selected potential is wrong, it will slow things down.</p>",2019-12-06T21:44:06Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uofgqzvh94nu,2019-12-06T21:44:06Z,{},review
4316,no,<p>Rmax will always find the optimal policy for a properly tuned learning function.</p>,2019-11-18T04:03:11Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wlrym4ll2y1,2019-11-18T04:03:11Z,{},review
4317,no,"<p>From the authors of R-MAX: &#34;<span style=""display:!important"">In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model</span>&#34; (<a href=""http://www.jmlr.org/papers/v3/brafman02a.html"">http://www.jmlr.org/papers/v3/brafman02a.html</a>). This implies that r-max will work optimally if the environment model is accurate. R-max is only as good as the model that it derives while exploring. Could r-max get stuck in a policy rut where it follows something that cycles onto itself? This <a href=""https://web.stanford.edu/class/cs234/slides/2017/lecture10_sample_efficiency.pdf"">https://web.stanford.edu/class/cs234/slides/2017/lecture10_sample_efficiency.pdf</a> has a good algorithm sample for r-max.</p>
<p></p>
<p>Do PAC algorithms *always* find *the optimal* policy?</p>
<p></p>
<p>I don&#39;t understand what &#34;properly tuned learning function&#34; means.</p>
<p></p>",2019-11-19T02:22:00Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k358fimiyw56mi,2019-11-19T02:22:00Z,{},review
4318,no,"<p>True. Rmax is designed in a way that either it loops around the most optimal path, or it tries to explore unknown state-action pairs till it finds the most optimal path. So as long as our learning function works properly it should always find the optimal policy.</p>",2019-12-01T16:19:14Z,23,Week 12/1 - 12/7,feedback,,jzliwdkt88m1q3,k3n7mf6lzwd3kb,2019-12-01T16:19:14Z,{},review
4319,no,"<p>If <em>learning function</em> refers to the algorithm itself, then False because Rmax finds a policy that is probably approximately optimal.</p>",2019-12-02T06:50:09Z,23,Week 12/1 - 12/7,feedback,,is8ald0uljj3u4,k3o2qfg0k5zjn,2019-12-02T06:50:09Z,{},review
4320,no,"<p><strong>Edit:</strong> I was incorrect, confirmed false. (see below) Originally wrote: True, Rmax only changes the exploration strategy increasing priority of unexplored states that may contain more optimal solutions. </p>",2019-12-04T19:22:51Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3roi3sxopp71e,2019-12-04T19:22:51Z,{},review
4321,no,"<p>I think this is technically False (although true in spirit).  The problem is the word &#39;always.&#39;  As Jacob alludes to, in these PAC-style bounds, there is always a small probability that the desired goal isn&#39;t reached, and if reached it&#39;s not quite optimal.</p>",2019-12-04T19:53:04Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3rpkz95mqn39s,2019-12-04T19:53:04Z,{},review
4322,no,"<p>Good point, this is certainly False. Reference: <a href=""https://web.stanford.edu/class/cs234/slides/2017/lecture10_sample_efficiency.pdf"" target=""_blank"" rel=""noopener noreferrer"">CS234: RL Lecture 10</a>, see slide 60.  The only guarantee is a bound on the number of epsilon-suboptimal episodes, and because epsilon needs to be determined in advance there&#39;s no guarantee of exploring enough to guarantee finding optimal policies.</p>",2019-12-04T20:43:24Z,23,Week 12/1 - 12/7,feedback,,isde332xcka1m0,k3rrdp626w62es,2019-12-04T20:43:24Z,{},review
4323,no,"<p>I think it is false. Rmax will find the optimal policy only for the states visited by the agent. The states not visited, it is assumed to be an infinite loop with the maximum reward to itself, but no policy can be calculated to those non-visited states.</p>",2019-12-04T21:27:52Z,23,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3rsyvwg1vt15w,2019-12-04T21:27:52Z,{},review
4324,no,<p>False as noted...</p>,2019-12-06T21:47:14Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uoji3f9qi5vh,2019-12-06T21:47:14Z,{},review
4325,no,<p>False:  the exploration is still stochastic; and may (depending on different discount factor) have radically different policies.</p>,2019-12-07T21:35:11Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w3jujtmg61g9,2019-12-07T21:35:11Z,{},review
4326,no,<p>Q-learning converges only under certain exploration decay conditions.</p>,2019-11-18T04:03:29Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wm6ehd1v3al,2019-11-18T04:03:29Z,{},review
4327,no,"<p>False. Q-learning is off policy so exploration is important, but greediness of the behavior policy isn&#39;t.</p>",2019-11-18T05:37:13Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33zypllmch3uw,2019-11-18T05:37:13Z,{},review
4328,no,"<p>Hmmm.  Infinitely often, is a condition; and it does not exist, if you don&#39;t decay your exploration towards states that have not yet been explored sufficiently...</p>
<p></p>
<p>True.</p>",2019-11-18T23:48:27Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352y1yc67djf,2019-11-18T23:48:27Z,{},review
4329,no,"<p>False. The temporal decay is part of the Q update. This results in smaller adjustments from the past and an eventual convergence to the optimal policy despite any tricks about exploration or learning rate. So long as the discount rate satisfies the contraction constraints, this holds true. Obviously in the case of $$\gamma &gt; 1$$ the learning will diverge.</p>",2019-11-19T02:30:04Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k358pvxvojz302,2019-11-19T02:30:04Z,{},review
4330,no,"<p>False. I think convergence is guaranteed by the contraction property of the Bellman operator, which does not include any assumptions on the exploration rate. </p>",2019-12-05T11:21:55Z,23,Week 12/1 - 12/7,feedback,,ijctp4ucNy8,k3smrhec1q07gj,2019-12-05T11:21:55Z,{},review
4331,no,"<p>I&#39;ll check this again when I hit Q-learning in the lectures; but for the moment I still think, u have to visit states infinitely often (with respect to other states) to get the actual convergence gaurantees.  Put another way; if you don&#39;t explore enough, u might not hit states that actually are states visited by an optimal policy.  The Bellman Operator gaurantees convergence IF u sample the space thoroughly enough.</p>",2019-12-06T10:41:25Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3u0r8oenz360z,2019-12-06T10:41:25Z,{},review
4332,no,"<p>&#64;uno that is right, but notice that the question asks about decay.</p>",2019-12-06T14:02:40Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3u7y1v7c0dki,2019-12-06T14:02:40Z,{},review
4333,stud,"<p>True? Exploration decay does effect the exploration rate. If the decay goes too fast, there does not have enough exploration. </p>",2019-12-06T20:30:40Z,23,Week 12/1 - 12/7,feedback,a_1,,k3ult0tzw3w5aq,2019-12-06T20:30:40Z,{},review
4334,no,<p>False as we are only concerned with exploration decay.</p>,2019-12-06T21:48:43Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uolelqgbrgw,2019-12-06T21:48:43Z,{},review
4335,no,"<p><a href=""https://classroom.udacity.com/courses/ud600/lessons/4436560172/concepts/44332503010923"">https://classroom.udacity.com/courses/ud600/lessons/4436560172/concepts/44332503010923</a></p>
<p></p>
<p>he literally states it in his convergence proof as a &#34;hidden assumption&#34; ;)   almost in the direct language I used.<br /><br />with respect to the &#34;infinitely often&#34;; yes this requires a decay with respect to exploration of states, since q-learning uses argmax.  there are of course, various ways to get this infinitely often.  some of which more explicit than others.</p>
<p></p>
<p>addendum LOLO.   I only watched a part of this lecture before I posted ;) the next part explains that the infinitely often is part and parcel of the convergence; and the learning rates need to decay; and since learning rates, bound the td-error, and the td-error is a function of exploration (initial explorations yield higher td-error as the Q-estimate is further away from Q* :: contraction mapping on estimates; and exploration is infinitely often (required property).  it seems the technically correct answer is:  the decay is necessary.    Just thought it was funny.  <br /><br />I agree with the class consensus, limited to decaying an epsilon constant, this is not strictly necessary.<br /><br />2nd addendum: i am very happy that we will be able to explain our reasoning.  ;)</p>",2019-12-06T22:22:17Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3upskl6jnq7o8,2019-12-06T22:22:17Z,{},review
4336,no,"<p>belated post (as I had to go do some things).</p>
<p></p>
<p>I note Alex&#39;s response and post to be complete.</p>",2019-12-06T22:22:43Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3upt4tqkziiu,2019-12-06T22:22:43Z,{},review
4337,no,"<p>Surely this is true then? With my exploration decay condition being: &#34;it never decays to zero&#34;, which is another way of saying that it must continue to explore all states forever, which is absolutely a requirement.</p>",2019-12-07T19:13:19Z,23,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3vyheanipd2du,2019-12-07T19:13:19Z,{},review
4338,no,"<p>yes well this is the thrust of my distinction ;) that u both have to explore infinitely often (in the limit) AND do so in a manner that favors states that are proportional to their optimality (with respect to action selection, and action rank).  and to do this infinitely often; and not continiously favor the then-current optimal state, some explicit or implicit steps must be taken, in order to ensure that low-rank action states are explored sufficiently, with respect to the total state-action pair visitation.   You see this &#39;soft&#34; requirement show up in several forms of RL&#43;&#43; (from different disciplines); as it is essentially an inescapable requirement.<br />--<br />&#43;&#43; depending on if and how they estimate their transition probabilities for an optimal policy.<br /><br />However in the context of this question, there is a much simpler reading of &#34;exploration decay&#34; i.e. epsilon decay.   But since epsilon itself is bounded by learning rate and gamma decay, it need not strictly decay, in order to effectively decay... and so therefore the class consensus on &#34;false&#34; is a better fit.</p>",2019-12-07T21:41:28Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w3rx37gfk9s,2019-12-07T21:41:28Z,{},review
4339,no,<p>The tradeoff between exploration and exploitation is not applicable to finite bandit domains since we are able to sample all options.</p>,2019-11-18T04:03:47Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wmk5o6rg3n8,2019-11-18T04:03:47Z,{},review
4340,no,<p>False.  You may still need to explore &#34;non-optimal&#34; arms of the bandit since there may be noise in the estimate of its value.</p>,2019-11-18T04:30:27Z,25,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33xkubr2jz6ye,2019-11-18T04:30:27Z,{},review
4341,no,<p>False.  Adversarial bandits.</p>,2019-11-18T23:47:14Z,25,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352wh5qb371sl,2019-11-18T23:47:14Z,{},review
4342,no,"<p>False. Exploitation doesn&#39;t presume exhaustive exploration. If every decision returned the same value for any action, then you&#39;re doing uniform selection of actions (implied exploration), or rutting on the $$n^{th}$$ action each time, such as going N every time in soccer. The only way to sample all options in a mode-free environment is to explore.</p>
<p></p>
<p>&#34;<span style=""display:!important"">exemplifies the exploration-exploitation tradeoff dilemma. &#34; - <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"">https://en.wikipedia.org/wiki/Multi-armed_bandit</a> (ewww wikipedia)</span></p>
<p><span style=""display:!important""></span></p>
<p><span style=""display:!important""><a href=""https://arxiv.org/pdf/1904.10040.pdf"">https://arxiv.org/pdf/1904.10040.pdf</a></span></p>
<p></p>",2019-11-19T03:25:17Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35aovxze9e34b,2019-11-19T03:25:17Z,{},review
4343,no,"<p>False. One can even repeatedly sample all options in an MDP to model it correctly, but we would like to maximise our reward as we go about modelling our environment. Same is the case for bandits.</p>",2019-12-01T16:14:19Z,23,Week 12/1 - 12/7,feedback,,jzliwdkt88m1q3,k3n7g39q2ii7cx,2019-12-01T16:14:19Z,{},review
4344,no,<p>What is &#34;finite bandit domain&#34; and where I can find it&#39;s definition?</p>,2019-12-06T16:51:35Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3udzag3bd86lv,2019-12-06T16:51:35Z,{},review
4345,no,"<p>I think Alec was just emphasizing the fact that bandits have only one state with the adjective &#34;finite.&#34;</p>
<p></p>
<p>Although, it is possible to expand the state space of a bandit to be infinite by incorporating knowledge of the current estimate of the reward distribution for each arm (see David Silver&#39;s lecture on exploration).</p>",2019-12-06T17:23:40Z,23,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3uf4jnq8bm22q,2019-12-06T17:23:40Z,{},review
4346,no,<p>Thank you Vahe! This looks plausible. </p>,2019-12-06T21:22:45Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3uno0mwkbn3b2,2019-12-06T21:22:45Z,{},review
4347,no,<p>&#x1f44d;</p>,2019-12-06T21:49:55Z,23,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uomy65gaq22z,2019-12-06T21:49:55Z,{},review
4348,no,<p>False: its has confidence bounds which implies exploration.</p>,2019-12-07T21:57:35Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w4cn8lk9q3na,2019-12-07T21:57:35Z,{},review
4349,no,<p>vahe - very interesting! :) I presume this is similar to the &#34;bank&#34; extension for our first homework?</p>,2019-12-07T21:58:07Z,23,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w4dc2jqs64fu,2019-12-07T21:58:07Z,{},review
4350,no,<p>Luck dragons don&#39;t fly so much as fail to hit the ground after jumping off a high perch due to their intrinsic good luck.</p>,2019-11-18T04:07:40Z,25,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33wrjnuewp730,2019-11-18T04:07:40Z,{},review
4351,no,"<p>Plus, you can fly, just look away. It&#39;s a widely known fact in the universe that dying is only a consequence of paying attention. If you look away while falling, you&#39;ll float. Plus, keeping a bone in your beard is a good way to keep a snack handy.</p>
<p></p>
<p><strong>*I DISBELIEVE!*</strong></p>",2019-11-20T01:26:26Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k36lvwvlyy5hr,2019-11-20T01:26:26Z,{},review
4352,no,"<p><span style=""text-align:left;display:!important"">“There is an art to </span><b style=""text-align:left"">flying</b><span style=""text-align:left;display:!important"">, or rather a knack. The knack lies in learning how to throw yourself at the ground and miss.”</span> - Hitchhikers Guide to the Galaxy (Douglas Adams).</p>",2019-11-22T19:37:05Z,25,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3ajq6udqu53ah,2019-11-22T19:37:05Z,{},review
4353,no,"<p></p><blockquote>
<p>Max : Well?<br />John Hatcher : One thought he was invincible... the other thought he could fly.<br />Max : So?<br />John Hatcher : They were both wrong.</p>
</blockquote>",2019-12-01T16:55:28Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3n8x13zqbj4ws,2019-12-01T16:55:28Z,{},review
4354,no,"<p><a href=""https://en.wikipedia.org/wiki/Talk%3ALuckdragon"">https://en.wikipedia.org/wiki/Talk%3ALuckdragon</a></p>
<blockquote>
<p><i>Luckdragons are creatures of air, warmth, and pure joy. Despite their great size, they are as light as a summer cloud, and consequently need no wings for flying. They swim in the air of heaven as fish swim in the water. Seen from the earth, they look like slow lightning flashe</i></p>
</blockquote>",2019-12-03T23:13:47Z,23,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3qhb8v16iw2i2,2019-12-03T23:13:47Z,{},review
4355,no,"<p>Since SARSA is an “online” algorithm, it will never learn the optimal policy for an environment.</p>",2019-11-18T04:28:19Z,21,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33xi3l6pio5g9,2019-11-18T04:28:19Z,{},review
4356,no,"<p>I&#39;ll take &#34;online&#34; to mean &#34;on-policy.&#34;  False.  If the SARSA implementation is GLIE, then it can learn an optimal policy.</p>",2019-11-18T05:29:55Z,21,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33zpbf9dg923q,2019-11-18T05:29:55Z,{},review
4357,no,<p>False.</p>,2019-11-18T23:38:36Z,21,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352ldqju3l4n2,2019-11-18T23:38:36Z,{},review
4358,no,"<p>False. Being online doesn&#39;t affect whether or not it learns the optimal policy. The amount of exploration done by the algorithm determines its ability to learn *the* optimal path. Case in point is the cliff walker example where the SARSA agent does not learn *the* optimal (step-wise optimal) path, but it learns the better path that is safer for the walker (a different definition of optimal). Cliff walker is in the Sutton-Barto book.</p>
<p></p>
<p><a href=""https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html"">https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html</a></p>",2019-11-19T03:31:46Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35ax8a1bpk63w,2019-11-19T03:31:46Z,{},review
4359,no,<p>False. SARSA converges to an optimal policy as long as all state-action are visited an infinite number of times and the policy converges to a greedy policy.</p>,2019-11-26T00:28:03Z,20,Week 11/24 - 11/30,feedback,,jc6xvgjncoey,k3f4fy1jrkx5gd,2019-11-26T00:28:03Z,{},review
4360,no,"<p>Jacob,</p>
<p></p>
<p>I think it is worth to point that SARSA would learn the optimal policy in the Cliff walking problem, if epsilon is gradually reduced (in other words, if the policy converges to a greedy policy).</p>",2019-12-05T20:41:59Z,19,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3t6rq8n6e9v,2019-12-05T20:41:59Z,{},review
4361,no,"<p>WAIT, so does online = on-policy here and for question 2 in review 3?</p>
<p></p>
<p>Also, what is GLIE? Greedy in the Limit with Infinite Exploration?</p>",2019-12-06T04:51:34Z,19,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3to9bt7rn698,2019-12-06T04:51:34Z,{},review
4362,no,"<p>Aaron,</p>
<p></p>
<p>My understanding is that SARSA is an on-police algorithm and can be used online (you don&#39;t need to wait a complete episode to update the Q-table).</p>
<p></p>
<p>GLIE is exactly what you said.</p>",2019-12-06T15:19:14Z,19,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3uaoigcnnp4iq,2019-12-06T15:19:14Z,{},review
4363,no,"<p>If I understand correctly, SARSA is online and on-policy.</p>
<p></p>
<p><a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf"">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf</a></p>
<p></p>
<p></p>",2019-12-06T16:03:20Z,19,Week 12/1 - 12/7,feedback,,jqmfnc46kl26eg,k3uc98fw7h124b,2019-12-06T16:03:20Z,{},review
4364,no,"<p>I think Michael has a comprehensive post here:</p>
<p></p>
<p><a href=""/class/jzh9tkzzxkd7ph?cid=953"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=953</a><a href=""/class/jzh9tkzzxkd7ph?cid=953""></a></p>
<p></p>
<p>Also... False as noted.</p>",2019-12-06T21:53:48Z,19,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uorxr6j033i8,2019-12-06T21:53:48Z,{},review
4365,no,<p>RL with linear function approximation only works on toy problems.</p>,2019-11-18T04:28:35Z,21,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33xig1urao5nw,2019-11-18T04:28:35Z,{},review
4366,no,"<p>False.  If you engineer nonlinear features, then linear function approximation can still lead to quite expressive functions.</p>",2019-11-18T05:31:21Z,21,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33zr6a2z27ne,2019-11-18T05:31:21Z,{},review
4367,no,<p>False.  Nonlinear approximation can be approximated by linear piece-wise construction.</p>,2019-11-18T23:39:05Z,21,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352lzohyqc63d,2019-11-18T23:39:05Z,{},review
4368,no,"<p>False by demonstration where RL with LF has been used in controlling non-correlated functions. Sutton and Barto (pages 233 - 237) describe techniques using linear function approximation.</p>
<p></p>
<p>Depends on your definition of a &#39;toy&#39; problem though. A toy to one is a complicated machine to another.</p>",2019-11-19T03:37:44Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35b4ww38w24st,2019-11-19T03:37:44Z,{},review
4369,no,<p>&#x1f44d;</p>,2019-12-06T21:54:43Z,19,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uot3vmp1p526,2019-12-06T21:54:43Z,{},review
4370,no,<p>KWIK algorithms are an improvement of MB (mistake bound) algorithms and as such will have same or better time complexity for all classes of problems.</p>,2019-11-18T04:28:51Z,21,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33xis7ygsc60e,2019-11-18T04:28:51Z,{},review
4371,no,<p>True (I have no idea about MB; but the point of KWIK is to improve on sampling efficiency).</p>,2019-11-18T23:39:37Z,21,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352moznbgo6sb,2019-11-18T23:39:37Z,{},review
4372,no,"<p>FALSE, this was described in the KWIK paper provided for the homework that we all loved doing.</p>
<p></p>
<p>&#34;However, some hypothesis classes are exponentially harder to learn in the KWIK setting than in the MB setting.&#34; - <a href=""https://www.eecs.wsu.edu/~holder/courses/CptS570/fall09/present/LiICML08.pdf"">https://www.eecs.wsu.edu/~holder/courses/CptS570/fall09/present/LiICML08.pdf</a></p>
<p></p>
<p>The wording of &#34;all classes of problems,&#34; makes this false.</p>",2019-11-19T03:39:08Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35b6pleum2750,2019-11-19T03:39:08Z,{},review
4373,stud,<p>False. Some hypothesis classes are exponentially harder to learn in the KWIK setting than in the MB setting.</p>,2019-12-02T15:33:33Z,19,Week 12/1 - 12/7,feedback,a_0,,k3olfiy2cpf6v8,2019-12-02T15:33:33Z,{},review
4374,no,<p>Can we get a TA to endorse one of these answers please :)</p>,2019-12-06T04:57:07Z,19,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3toghajcmp2fi,2019-12-06T04:57:07Z,{},review
4375,no,"<p>This one is False for sure.  It&#39;s spelled out pretty clearly in Section 3.2 of Li, Littman, Walsh.</p>
<p></p>
<p>A counterexample is a conjunction of an unknown subset of $$n$$ Boolean variables.  MB learns in at most $$n&#43;1$$ mistakes, whereas KWIK can take on the order of $$2^n$$ &#39;I don&#39;t know&#39;s to learn.</p>",2019-12-06T05:38:22Z,19,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3tpxj25v6crg,2019-12-06T05:38:22Z,{},review
4376,no,<p>False. All classes of problems is too general.</p>,2019-12-06T21:56:28Z,19,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uovd0vevs25b,2019-12-06T21:56:28Z,{},review
4377,no,<p>Vahe solid answer. :)</p>,2019-12-07T22:08:01Z,19,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w4q2jmudi2y4,2019-12-07T22:08:01Z,{},review
4378,no,"<p>With a classic update using linear function approximation, we will always converge to some values, but they may not be optimal.</p>",2019-11-18T04:29:07Z,21,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33xj4z55wj6vt,2019-11-18T04:29:07Z,{},review
4379,no,<p>False.  Combining even LFA with off policy TD methods can lead to divergence.</p>,2019-11-18T05:33:57Z,21,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33zui2c48z2xt,2019-11-18T05:33:57Z,{},review
4380,no,"<p>Interesting question.  I know convergence guarantees (to optimal) don&#39;t exist for function approximation; and I know nonlinear can blow up. </p>
<p></p>
<p>...but linear combinations? Should converge eventually given a sufficiently small alpha.  Premature convergence is a def. risk.</p>
<p></p>
<p>True.</p>",2019-11-18T23:42:17Z,21,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352q3zlrec2m1,2019-11-18T23:42:17Z,{},review
4381,no,"<p>False. Sutton and Barto, some uses of linear and non-linear function approximation can diverge to infinity. Also, &#34;However, generalization can cause divergence in the case of repeated boostrapped temporaldifference updates.&#34; - <a href=""https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/DeepRL16-hausknecht.pdf"">https://www.cs.utexas.edu/~pstone/Papers/bib2html-links/DeepRL16-hausknecht.pdf</a></p>",2019-11-19T03:44:09Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35bd5y0mkw5xg,2019-11-19T03:44:09Z,{},review
4382,stud,<p>False. Off-policy is less stable and may diverge even linear function approximation is used. </p>,2019-12-02T15:50:01Z,19,Week 12/1 - 12/7,feedback,a_0,,k3om0p2w3ei4vf,2019-12-02T15:50:01Z,{},review
4383,no,<p>False. Baird&#39;s counter-example shows how classic update using linear function approximation can lead to divergence.</p>,2019-12-03T04:21:48Z,19,Week 12/1 - 12/7,feedback,,jzqia0ganjm1ou,k3pcvi47lqp2eu,2019-12-03T04:21:48Z,{},review
4384,no,"<p>&#x1f44d;</p>
<p>Baird&#39;s counter-example.</p>",2019-12-06T21:57:27Z,19,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uowmqhtff3ew,2019-12-06T21:57:27Z,{},review
4385,no,"<p>&#34;classic update using linear function approximation&#34; def. false.</p>
<p></p>
<p>technical note: there are linear function approximations that do gaurantee convergence; but they alter the loss function and project loss into the linear function approximation space.  G.. and G...2.</p>",2019-12-07T22:11:54Z,19,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w4v2p8qj67bw,2019-12-07T22:11:54Z,{},review
4386,no,<p>Applying generalization with an &#34;averager&#34; on an MDP results in another MDP.</p>,2019-11-18T04:29:23Z,21,Week 11/17 - 11/23,followup,,hyxsfbkeit22m2,k33xjh32576787,2019-11-18T04:29:23Z,{},review
4387,no,<p>True.  The composition of the averaging function with the probability transition function creates new function that has the form of a probability transition function.</p>,2019-11-18T05:33:14Z,21,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k33ztlc5sxe2bz,2019-11-18T05:33:14Z,{},review
4388,no,<p>No idea.  Is this covered somewhere in class?</p>,2019-11-18T23:44:02Z,21,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k352sdkgdok5ko,2019-11-18T23:44:02Z,{},review
4389,no,"<p>True, &#34;The Age of Gordon,&#34; in the lectures covers this nicely. This is just a transformation function from one MDP to another. The question I have - are the equivalent MDPs? I don&#39;t think Littman proves this in the lectures, and we are spoiled with his relentless proofs in the lectures....</p>",2019-11-19T03:45:40Z,21,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k35bf46qkdo66,2019-11-19T03:45:40Z,{},review
4390,no,"<p>False, Generalization has to do with Agent learning the Q function. MDP has to do with the environment or modeling. (After reviewing Generalization lecture - this seems to be True if one is referring to basis states.)</p>",2019-11-26T20:49:07Z,20,Week 11/24 - 11/30,feedback,,j6ll2xkiDJf,k3gc28dg8zz4av,2019-11-26T20:49:07Z,{},review
4391,no,"<p>True. When averager is applied to V(s&#39;), we can rearrange the summation terms and write a new MDP with its value function in terms of transitions between state S and all other basis/anchor states.</p>",2019-12-04T13:36:30Z,19,Week 12/1 - 12/7,feedback,,jzliwdkt88m1q3,k3rc4p2bozj2hj,2019-12-04T13:36:30Z,{},review
4392,no,"<p>True. As noted in Gordon (1995), you will have less states...</p>",2019-12-06T21:59:26Z,19,Week 12/1 - 12/7,feedback,,hyxsfbkeit22m2,k3uoz6nalrf7kv,2019-12-06T21:59:26Z,{},review
4393,no,<p>ah the bag of vectors/basis states trick.  true.</p>,2019-12-07T22:12:31Z,19,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w4vv7uo1kua,2019-12-07T22:12:31Z,{},review
4394,no,"<p>Adventures into the unknown:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk35bhyfqrgl5%2Frubsocks.jpg"" alt="""" /></p>",2019-11-19T03:48:02Z,21,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k35bi564uzd2v1,2019-11-19T03:48:02Z,{},review
4395,no,"<p>they specifically state a player moving first (with respect to a collision), takes the spot.  all other players noop (stay in place).</p>
<p></p>
<p>they also state, that if the player with the ball collides with another player, that the ball is transferred so long as the player with the ball moves second.</p>
<p>-- this seems to me, to indicate the opposite; that a player with first mover advantage, colliding with another player, should steal the ball.</p>
<p></p>
<p>correct me if I am wrong; but does this mean that there is a third case? that if a player without the ball, attempts to collide into a player with the ball; that the ball is not transferred so long as the ball player hasn&#39;t moved (nooped explicitly (stand-still) or NESW into an invalid corner)?</p>
<p></p>
<p></p>",2019-11-18T22:22:29Z,41,Week 11/17 - 11/23,followup,,jzivtxcbl6964n,k34zvhtirbc16l,2019-11-18T22:22:29Z,{},project3
4396,no,"<p>Everyone seems to have a different opinion on how the environment should work, so I don&#39;t want to push mine onto you, but much like you, initially I tried to parse Greenwald&#39;s text very literally and I think I was wrong.  I ended up adopting a much simpler approach that I believe is the one Greenwald intended.  See &#64;940.  Jacob&#39;s approach is the one I use now.</p>
<p></p>
<p>Also, see the technical report, where she adds a phrase that wasn&#39;t in the original, reproduced here for convenience:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3523vxmbfjt%2FCapture.PNG"" alt="""" /></p>",2019-11-18T23:25:59Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k35255dhnpa26w,2019-11-18T23:25:59Z,{},project3
4397,no,"<p>lol.  thanks for the repro.  I think the problem is inherent, i.e. simultaneity; with respect to collision, and occupation of space, and a &#34;ranked initiative&#34;. </p>
<p></p>
<p>I think I have to rewrite because I did not consider, the case where a player does not move; and therefore there is no other square to bump them to (i.e. preempt their movement) in deference to the ranked initiative.</p>",2019-11-19T00:37:34Z,41,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k354p7r9omt1l8,2019-11-19T00:37:34Z,{},project3
4398,no,"<p>I did find the recordings of the Thursday sessions in the bluejeans section on canvas, but not saturday&#39;s... </p>",2019-11-20T06:27:10Z,52,Week 11/17 - 11/23,followup,,jzh6k6o994a6dh,k36wmnkl18o5k3,2019-11-20T06:27:10Z,{},logistics
4399,no,<p>Thoughts on if we will get our project 2 feedback a few days before Proj 3 is due so that we might be able to incorporate some of it.</p>,2019-11-21T01:35:25Z,52,Week 11/17 - 11/23,followup,,ixty1midfufhd,k381nb3dye81bg,2019-11-21T01:35:25Z,{},logistics
4400,no,Just released &#64;990,2019-11-22T01:47:47Z,52,Week 11/17 - 11/23,feedback,,i4op5p9vfbq5yz,k39hj2peknw3kl,2019-11-22T01:47:47Z,{},logistics
4401,no,"<p>I&#39;m still confused on the update for Q for the lp-solve; I switch to argmax (rather than following the policy directly).  the update I am using is Q(last_s, last_o, last_a) = alpha * ( last_r &#43; gamma * V(s)) where V(s) is the expected utility from the game (from lpsolve) * the new_policy(last_a) returned from the lpsolve.   There is -no- incremental update (it is not &#43;=!).   </p>
<p></p>
<p>The other possibility is that I should instead, be using ONLY the new policy returned; and using the &#34;older&#34; version of Q(s,a,o) weighted by the probability of transition; that is dotproduct(policy * Q[s,o,:])...</p>
<p></p>
<p>Which is correct the correct interpretation ? </p>",2019-11-23T18:02:23Z,45,Week 11/17 - 11/23,followup,,jzivtxcbl6964n,k3bvs9apcd67ki,2019-11-23T18:02:23Z,{},logistics
4402,no,"<p>the question is: should I replace the Q-value with the expected value * policy (for last mvoe); or should I just explicitly apply a weighted average over old q-values?</p>
<p></p>
<p>which should dominate in the Q-update?  the Q-table::NewPolicy; or a straight replace (LPSOLVE.utility::NewPolicy).</p>
<p></p>
<p>the former gives me an iterative update; the latter uses game theory to determine the utility.</p>",2019-11-23T18:32:23Z,45,Week 11/17 - 11/23,followup,,jzivtxcbl6964n,k3bwuu6ir8558g,2019-11-23T18:32:23Z,{},logistics
4403,no,"<p>Chat log for our session today:</p>
<p></p>
<blockquote>
<p>10:05 AM Chris so pi[s] could be [.2,.2,.2,.2,.2] meaning you have an equal chance of each action<br />10:05 AM Aida if pi[s] = [1,0,0,0] then you return the action1<br />10:05 AM Andrew Wiley do I take the argmax of that?<br />10:06 AM Andrew Wiley so [.1,.1,.1,.1,.6] return action 4?<br />10:06 AM Chris no, its percent chance of being chosen<br />10:06 AM Dan Tadmor Is time supposed to represent a game or a turn in a game?<br />10:06 AM Chris so in your example you would chose 4 60% of the time<br />10:06 AM Aida some peope do, some people use random function and return based on probs<br />10:07 AM Aida it seems it doesn&#39;t matter according to piazza<br />10:07 AM Chris that might be a good question for them<br />10:07 AM Chris them = TAs<br />10:07 AM Andrew Wiley is there a helpful function choosing an action based on the PI?<br />10:07 AM Dan Tadmor Which question?<br />10:08 AM Chris whether or not pi is supposed to be a probability choice or an argmax<br />10:08 AM Aida does it matter how we choose an action for ce-Q?<br />10:08 AM Chris and to andrew - i do probability and use np.random.choice<br />10:09 AM Aida here they choose based on distribution: https://github.com/openai/gym/blob/master/gym/envs/toy_text/discrete.py<br />10:09 AM Aida you can use their function<br />10:09 AM Aida to pick an action based on probs<br />10:10 AM Aida the function is ategorical_sample(prob_n, np_random)<br />10:10 AM Aida categorical_sample(prob_n, np_random)<br />10:11 AM Chris is categorical_sample built into numpy?<br />10:11 AM Aida nope<br />10:11 AM uno https://piazza.com/class/jzh9tkzzxkd7ph?cid=962<br />10:11 AM Aida it&#39;s in gym openai<br />10:12 AM uno ^^ my question<br />10:13 AM Aida my question: how can we test our code?<br />10:14 AM Aida is there any state, actions that we expect to converge or do something?<br />10:29 AM uno yes. it also returns expected utility OF that probablity distribution<br />10:30 AM uno the question is: should I replace the Q-value with the expected value * policy (for last mvoe); or should I just explicitly apply a weighted average over old q-values?<br />10:30 AM Dan Tadmor one of the values is the value of the state<br />10:31 AM uno which should dominate in the Q-update? the Q-table::NewPolicy; or a straight replace (LPSOLVE.utility::NewPolicy).<br />10:31 AM uno the former gives me an iterative update; the latter uses game theory to determine the utility.<br />10:34 AM Zachary Pearson can you talk through the rationality constraints for ceq?<br />10:41 AM uno for the next state<br />10:41 AM uno and by value u mean probablity distribution<br />10:42 AM uno yes ok.<br />10:42 AM uno thank you :)<br />10:42 AM Zachary Pearson σ ∈ arg max σ∈CE X i∈I X ~a∈A σ(~a)Qi(s, ~a)<br />10:42 AM Zachary Pearson i understand the standard constraints introduced by virtue of σ being a probability distribution<br />10:43 AM Zachary Pearson but not sure how to turn the argmax into a set of constraints<br />10:44 AM Zachary Pearson sorry you said it was 917 (assuming that means https://piazza.com/class/jzh9tkzzxkd7ph?cid=917), but that isn&#39;t found - is it still private<br />10:44 AM Zachary Pearson ?<br />10:47 AM Binod Purushothaman sorry, what was that piazza ref again?<br />10:47 AM Zachary Pearson 908<br />10:47 AM Binod Purushothaman got it, thanks<br />10:47 AM Zachary Pearson ah i see<br />10:47 AM Zachary Pearson it&#39;s just that it&#39;s &gt;= then<br />10:48 AM Zachary Pearson that makes sense though, thanks<br />10:48 AM Zachary Pearson no the &#34;instructor answer&#34;<br />10:51 AM Zachary Pearson no<br />10:51 AM Zachary Pearson a&#39; is action by player i (other than i)<br />10:51 AM Aida i think it&#39;s aother action than a_i<br />10:52 AM Aida action by player i (other than ai)<br />10:53 AM Binod Purushothaman which one Don?<br />10:53 AM Binod Purushothaman Thanks<br />10:54 AM Zachary Pearson thanks that helped!<br />10:57 AM uno can u confirm that foe-q can be approached using zero-sum absed on a single players utility table (assuming mirror for opponent) EVEN when foe-q is run against foe-q?<br />10:58 AM uno Or do we have to do a general sum; where the player either maintains a second q-table &#34;mirroring&#34; the oppoenents moves,rewards;<br />10:58 AM uno OR literallya ccessing the opponents Qtable.<br />10:58 AM uno yes ok :) that simplifies things ;)<br />10:58 AM uno thank you :)<br />10:59 AM uno yes. u need general sum definately.<br />10:59 AM uno :)<br />10:59 AM uno thank you :)<br />11:00 AM Ahmad Haggag page 34 of which paper ?<br />11:01 AM Don Jacob http://www.cs.toronto.edu/~lyan/CSE-2010-07.pdf</p>
</blockquote>",2019-11-23T19:07:43Z,45,Week 11/17 - 11/23,followup,,jl5wq8mca7o0,k3by4abkq02o7,2019-11-23T19:07:43Z,{},logistics
4404,no,<p>Can we get the video link either here or in &#64;132 please?</p>,2019-11-24T03:23:43Z,44,Week 11/24 - 11/30,followup,,ixty1midfufhd,k3cfu557m6w6ry,2019-11-24T03:23:43Z,{},logistics
4405,no,<p>Its there now. Thanks.</p>,2019-11-24T05:19:16Z,44,Week 11/24 - 11/30,feedback,,i4i9bi8rFqk,k3cjyqswl2323b,2019-11-24T05:19:16Z,{},logistics
4406,no,"<p>Kamran, to be honest, I don&#39;t think your linear program is correct.  Try that problem in &#64;906 and see if you get the right solution.  I don&#39;t remember anymore, but I think all of the matrices in HW6 may have been skew-symmetric, and your program actually may be okay for that special case, which is probably why you passed it.  You can read a tutorial I wrote in &#64;908 for how to systematically write the rock-paper-scissors linear program.</p>
<p></p>
<p>Here are a few issues:</p>
<p></p>
<p>1. Your objective function is actually serving the role of three of your constraints, that you didn&#39;t provide as constraints.  Specifically, by maximizing the sum of the three probabilities, you end up, coincidentally, also making them greater than equal to $$0$$.  $$P(R) \geq 0$$, $$P(P) \geq 0$$, and $$P(S) \geq 0$$ are constraints on the variables, since those variables are probabilities, that you probably should be providing as constraints.</p>
<p></p>
<p>2. You&#39;re constraining the row player&#39;s value for each row to be less than or equal to $$0$$.  Why?  What if you&#39;re presented a game where a row&#39;s value is greater than $$0$$  You won&#39;t be able to solve that problem.</p>
<p></p>
<p>3. You&#39;re constraining the sum of the probabilities to be less than or equal to one.  You should also constrain them to be greater than equal to one (since we know they <em>must</em> sum to one).</p>
<p></p>
<p>One suggestion.  You&#39;re naming CVXOPT&#39;s G matrix and h vector A and b, which is a bit confusing.  A and b are actually completely different entities that provide equality constraints within CVXOPT.  So you should probably rename A and b.</p>",2019-11-19T03:17:19Z,24,Week 11/17 - 11/23,followup,,jzfsa4a37jf4aq,k35aen1ey4d1g2,2019-11-19T03:17:19Z,{},project3
4407,no,"<p>Vahe; Thanks for the response. I will read it and perhaps could figure it out. Hoping...</p>
<p>I used A,b,c from the example tutorial: <a href=""http://cvxopt.org/examples/tutorial/lp.html"">http://cvxopt.org/examples/tutorial/lp.html</a><a href=""http://cvxopt.org/examples/tutorial/lp.html""></a></p>
<p></p>
<p>2) I picked 0, since I thought the game is a zero sum game so the equilibrium is when is at most when Net/Net both players sum of rewards over infinite games would be zero.</p>
<p></p>
<p>Magically it seemed to work for all of my HW6 inputs. I also solved them by hand to be sure....</p>
<p></p>",2019-11-19T03:57:47Z,24,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k35buox5srk126,2019-11-19T03:57:47Z,{},project3
4408,no,"<p>Aaron,</p>
<p></p>
<p>I had the same question 3 answered &#64;877.</p>",2019-11-20T00:51:25Z,41,Week 11/17 - 11/23,followup,,jc6xvgjncoey,k36kmvb2408kh,2019-11-20T00:51:25Z,{},project3
4409,no,<p>Thanks Danilo.</p>,2019-11-21T04:21:00Z,41,Week 11/17 - 11/23,feedback,,gx3c8l7z7r72zl,k387k918wkv17j,2019-11-21T04:21:00Z,{},project3
4410,no,<p>Thanks! :)</p>,2019-11-20T16:47:45Z,52,Week 11/17 - 11/23,followup,,jzjwcq2u8o7110,k37isq82wjs6v9,2019-11-20T16:47:45Z,{},final_exam
4411,no,"<p>I am seeing the same thing in my G matrix, which results in RANK errors:  Rank(A) &lt; p or Rank([G; A]) &lt; n</p>
<p></p>
<pre>G= [ 4.66e-10  2.27e&#43;07  1.59e&#43;07  2.32e&#43;07  2.43e&#43;07 -2.27e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00  1.29e&#43;07  2.17e&#43;06  3.92e&#43;02  7.35e&#43;05 -1.29e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00 -9.15e&#43;06  0.00e&#43;00  6.15e&#43;03  1.16e&#43;04  9.15e&#43;06  0.00e&#43;00 ... ]
[ 1.28e-09  2.26e&#43;07  3.19e&#43;03  2.73e&#43;05  0.00e&#43;00 -2.26e&#43;07  0.00e&#43;00 ... ]
[-1.86e-09 -3.47e&#43;07 -2.78e&#43;07 -5.79e&#43;06 -3.76e&#43;07  3.47e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00 -1.29e&#43;07 -2.17e&#43;06  0.00e&#43;00 -1.62e&#43;03  1.29e&#43;07  0.00e&#43;00 ... ]
[ 2.89e-10 -2.03e&#43;07 -4.00e&#43;02 -6.16e&#43;03 -2.72e&#43;05  2.03e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00 -2.13e&#43;07 -7.35e&#43;05 -1.16e&#43;04  1.57e&#43;03  2.13e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00 -2.83e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  2.83e&#43;07 -3.73e-09 ... ]
[ 0.00e&#43;00  2.94e&#43;06  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06  0.00e&#43;00 ... ]
[ 0.00e&#43;00  2.94e&#43;06  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06 -9.31e-10 ... ]
[ 0.00e&#43;00  2.94e&#43;06  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06  0.00e&#43;00 ... ]
[ 0.00e&#43;00  1.63e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -1.63e&#43;07 -4.66e-10 ... ]
[ 0.00e&#43;00 -1.49e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.49e&#43;07 -1.86e-09 ... ]
[ 0.00e&#43;00 -1.49e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.49e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00 -1.49e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.49e&#43;07  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00 -3.13e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00 -1.57e&#43;03  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  3.13e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  1.49e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  8.41e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -3.13e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -1.57e&#43;03  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  3.13e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.49e&#43;07  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -8.41e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -8.41e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -3.13e&#43;07  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 -2.94e&#43;06  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.57e&#43;03  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.57e&#43;03  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  3.13e&#43;07  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  1.49e&#43;07  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
[ 0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  0.00e&#43;00  8.41e&#43;00  0.00e&#43;00  0.00e&#43;00 ... ]
</pre>
<p></p>",2019-11-19T16:43:31Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k3637fvhf3d5o8,2019-11-19T16:43:31Z,{},project3
4412,no,"<p>I get that error as well, I just did some exception handling to skip that.</p>
<p></p>",2019-11-19T16:44:34Z,52,Week 11/17 - 11/23,feedback,,jzzw8pi7a7t3d4,k3638rzkvhj1fh,2019-11-19T16:44:34Z,{},project3
4413,no,<p>What are you using as your &#34;$$V_1$$&#34; and &#34;$$V_2$$&#34;?</p>,2019-11-19T18:09:00Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3669cvgiiw2z,2019-11-19T18:09:00Z,{},project3
4414,no,"<p>$$V=Q_i * \sigma_i$$ where <em>i</em> is the player.</p>
<p></p>
<p>I run a correlation of Player A vs B, and then a correlation of B vs A. That produces two $$\pi$$ vectors ($$\sigma$$ in Greenwald). The max of the $$\pi$$ vector is the value used to scale $$Q_i$$. This is not correct since it produces an unbounded expansion. </p>
<p></p>
<p></p>",2019-11-20T00:23:45Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k36jnao6bpn6jp,2019-11-20T00:23:45Z,{},project3
4415,no,"<p>My V values are enormous:</p>
<pre>0.00E&#43;00
1.10E&#43;15
3.56E&#43;08
3.55E&#43;08
9.55E&#43;03
6.41E&#43;14
3.60E&#43;08
7.21E&#43;10
2.23E&#43;24
0.00E&#43;00
7.78E&#43;30
1.13E&#43;11
-3.51E&#43;06
8.30E&#43;37
3.59E&#43;08
1.12E&#43;11
</pre>
<p></p>",2019-11-20T00:25:20Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k36jpbij20b3o1,2019-11-20T00:25:20Z,{},project3
4416,no,"<p>&#34;$$V = Q_i \cdot \sigma_i$$&#34;</p>
<p></p>
<p>How large are each of those two vectors?  Are you doing component-wise multiplication or a dot-product?</p>
<p></p>
<p>Edit: It looks like you are doing component-wise multiplication on two 25-element vectors, although what you pasted has 16 entries. (I&#39;m confused)</p>
<p></p>
<p>Shouldn&#39;t V be a scalar, the value of a state?</p>",2019-11-20T00:32:05Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k36jy0b3cxw2we,2019-11-20T00:32:05Z,{},project3
4417,no,"<p>(1,25) and I am doing dot product. So the $$\pi$$ vector is always 25 probabilities for one of the correlated actions.</p>",2019-11-20T00:50:41Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k36klxvyevy44d,2019-11-20T00:50:41Z,{},project3
4418,no,"<p>Oh, you just posted a bunch of your V values.</p>",2019-11-20T00:53:08Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k36kp2k2mwl78d,2019-11-20T00:53:08Z,{},project3
4419,no,<p>I normalized the max(pi) values that I resolved after solving the inequalities and that seems to have settled the expansion of V.</p>,2019-11-20T17:41:21Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k37kpnzkr562hh,2019-11-20T17:41:21Z,{},project3
4420,no,"<p>Hey Jacob,</p>
<p></p>
<p>Sorry for the lack of responses.  I realized I had bugs and have been debugging (with long turn-around times in runs) for the last day and a half.  See my post &#64;947 for my updated &#39;Student&#39;s Answer.&#39;</p>",2019-11-21T02:21:42Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k383au3sska3bb,2019-11-21T02:21:42Z,{},project3
4421,no,"<p>Thanks Vahe! i&#39;ve started using the primal slack value as a test for correctness in my Q*pi calculation (thanks to another student who realized this). Once you get that slack value returning reasonable values (and not all zeros), things start to drop into place for foe-Q.</p>
<p></p>
<p>There is no slack on u-CE, as you noted in your answer. That&#39;s because the correlation uses the counter correlates as the slack value, so to speak. The counter correlates for row i, for instance, are rows -i.</p>",2019-11-21T05:48:58Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k38apdbip537o9,2019-11-21T05:48:58Z,{},project3
4422,no,"<p>Greenwald 2003 has a separate Q table for each player. See table 1 on page 3.</p>
<p></p>
<p>Each agent decides its own action from a pi (mixed strategy) reflecting their perspective of their Q table. Agents never choose actions for other agents. <strong>Edit2:</strong> Daniel brings up a great point on centralized vs decentralized coordination below.</p>
<p></p>
<p>Caveat: 2003 paper never explicitly mentions agents always face a mirror match vs the same kind of agent, only how each independent agent learns based on its type. Office hours mentioned agents could play against random agents for the purposes of learning. I&#39;m implementing everything as mirror matches, but either way agents do not sample actions for other agents.</p>
<p></p>
<p>Edit1: since the game is zero sum and Q values are mirror images, you probably could have agents share the same Q matrix with one viewing the inverse of its rewards, but since one metric of success for project 3 is how well you can reproduce results of the original work, you probably don&#39;t want multiple agents accelerating learning by both updating a single Q table (twice for each step).</p>",2019-11-19T16:57:07Z,52,Week 11/17 - 11/23,followup,,isde332xcka1m0,k363oxhri556gh,2019-11-19T16:57:07Z,{},project3
4423,no,"Michael, It says in the paper &#34;In our Implementation of CE-q  learning, a centralized mechanism computes a correlated equilibrium and the learners do not collide while playing the game&#34; (Greenwald, 2003). This implies they do not decide their own actions, because then there is a chance for collision if they are not coordinated by one action chooser.",2019-11-19T17:09:29Z,52,Week 11/17 - 11/23,feedback,,jzj4sh1p7pf5af,k3644tkc3i81xh,2019-11-19T17:09:29Z,{},project3
4424,no,"I have them each have separate Q tables that is true, and updates based on their own perspective, but there is a &#34;correlator&#34; that determines the mixed policy.",2019-11-19T17:10:36Z,52,Week 11/17 - 11/23,feedback,,jzj4sh1p7pf5af,k36469clfbl6cc,2019-11-19T17:10:36Z,{},project3
4425,no,"<p>I thought each agent just keeps track of the other agent&#39;s Q table by observing their actions and rewards, in addition to its own action and rewards. That way they can independently come up with a correlated equilibrium. But yeah central would make things a lot faster. </p>",2019-11-19T17:34:37Z,52,Week 11/17 - 11/23,feedback,,jl3we43d3bp15p,k36514wqzto242,2019-11-19T17:34:37Z,{},project3
4426,no,"<p>&#64;Daniel, good catch on that detail. The centralized vs decentralized coordination was elaborated in 2005 (ftp://ftp.cs.brown.edu/pub/techreports/05/cs05-08.pdf  &lt;= copy/paste should work, piazza breaks the ftp by forcing http hyperlink) with psuedocode for each approach on page 12 (labeled as page 10, pg 12 of PDF). Sections 3.1-3.3 explain the role of centralized vs decentralized coordinators determining the joint probability distribution for Q-value updates. However, it&#39;s my understanding each agent still chooses actions from their mixed strategy (which might collide, a necessary possibility in the environment for ball to shift possession).</p>",2019-11-19T17:43:04Z,52,Week 11/17 - 11/23,feedback,,isde332xcka1m0,k365c0h8gsv10e,2019-11-19T17:43:04Z,{},project3
4427,no,"<p>Even if you have separate Q tables, if both tables are initialized to the same state, and the reward given to one agent is the additive inverse of the reward given to the other agent (zero-sum game), then what will be the relationship between these two Q tables when both agents use the same learning algorithm?</p>",2019-11-19T17:44:15Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k365djfefdn2a9,2019-11-19T17:44:15Z,{},project3
4428,no,"&#64;Michael thanks for clarifying. Aren&#39;t we just replicating 2003 where they say the agents won&#39;t collide? Also, I can&#39;t access that ftp Link.",2019-11-19T18:00:08Z,52,Week 11/17 - 11/23,feedback,,jzj4sh1p7pf5af,k365xyxllz976x,2019-11-19T18:00:08Z,{},project3
4429,no,"<p>&#64;Daniel, sorry about the broken link, copy/pasting the FTP link above should work, piazza was breaking the hyperlink by forcing http:// in front of it.</p>
<p></p>
<p>Good question about agents not being supposed to collide... I initially typed some thoughts on that, but the original experiment design is mind boggling in a few ways that seem contradictory:</p>
<p>- foe-Q and uCE-Q supposedly converge to identical policies and values (according to 2003 paper)</p>
<p>- a foe-Q agent without the ball seeking to maximize their own reward and minimize their adversary&#39;s reward would certainly want the environment to allow for collisions, which are necessary for the ball possession to transition.</p>
<p>- if we assume players start on opposite sides of the field from the goal where they&#39;re trying to score, a defending player always has a chance to take an action that could result in a block (and transition of possession). If a coordinator forbid this from happening, the player with the ball could simply walk past and score. a CE-Q coordinator might forbid this for some reason, but a Foe-Q player should certainly seek to score if they can.</p>
<p></p>
<p>The whole idea that a coordinator forces action selection that prevent the possibility of collisions... in a stochastic environment where chances of collisions are a necessary to minmax your play... where an uncoordinated foe-Q and coordinated uCE-Q apparently end up learning identical policies &amp; values... seems like a silly experiment*.</p>
<p></p>
<p>*- the silliness of the experiment seems exacerbated in the 2003 paper, pg 7, excerpt &#34;At state s, CE-Q and foe-Q converge to nondeterministic policies for both players, where each one randomizes between sticking and heading south.&#34;   &lt;== think about what this means. Both player&#39;s &#34;optimal mixed strategy&#34; involves never intentionally approaching the opposing goal from state &#39;s&#39; (figure 4 in 2003), and state &#39;s&#39; is the logical equivalent of a similar state where either agent is one row lower, since from both cases a defending player has a 50/50 chance of blocking an offensive player attempting to move towards their goal. &lt;== the implication? the only time either agent makes a move towards either goal is via epsilon-random action, with player B (having the ball in state &#39;s&#39;) being a single random action away from scoring on itself.</p>
<p></p>
<p>When asked about the strangeness of the experiment in office hours, one TA made the astute observation &#34;You mean there&#39;s a chance two graduate students didn&#39;t have a clear idea what they were doing?&#34;  &lt;== This can explain some of the silliness of the experiment design and the environment, but makes me wonder why this (of all available RL research) was chosen for us to reproduce for the final project.  Granted, it&#39;s still interesting research in theory, the execution here just leaves much to be desired.</p>",2019-11-19T18:26:49Z,52,Week 11/17 - 11/23,feedback,,isde332xcka1m0,k366w9r8ilz7kt,2019-11-19T18:26:49Z,{},project3
4430,no,"^ Yeh, so do we replicate the 2003 experiment or use the future 2005&#39;s elaboration on it?",2019-11-19T18:28:19Z,52,Week 11/17 - 11/23,feedback,,jzj4sh1p7pf5af,k366y7627qg1vl,2019-11-19T18:28:19Z,{},project3
4431,no,"<p>TA in office hours said we&#39;re primarily evaluated on how well we reproduce 2003, but we can refer to information from 2005 to justify our analysis of what appears to be happening in 2003 figures.</p>",2019-11-19T18:29:33Z,52,Week 11/17 - 11/23,feedback,,isde332xcka1m0,k366zs9q3j72ut,2019-11-19T18:29:33Z,{},project3
4432,stud,"<p>&#64;Michael,</p>
<p>Just want to be sure i am not misinterpreting  your response</p>
<p>You meant that each a player should decide his own action using his own mixed strategy based on his own separate Q table and perspective of the opponent&#39;s.</p>
<p>as pi1(s,a1,a2) != pi2(s,a1,a2) (even for zero-sum game with both players using the same learning algorithm on their own Q table and perspective of the opponent&#39;s Q table)?</p>
<p>Because i have been getting pi1(s,a1,a2) = pi2(s,a1,a2) when i tried with separate Q tables and solved for mixed strategy separately ,for both Corr E and Foe E.</p>
<p></p>
<p>&#64;Daniel and Michael,</p>
<p>Thanks for pointing out &#34;In our Implementation of CE-q learning, a centralized mechanism computes a correlated equilibrium and the learners do not collide while playing the game&#34;</p>
<p>It seems like centralized and decentralized mechanism will lead to the same outcome, quoting from the paper Michael referenced &#34;sufficient information is available to perform this updating exactly as the central coordinator would in the centralized version of multiagent Q-learning&#34;</p>
<p>But now im confused by what they mean by &#34;learner not colliding while playing the game.&#34; by using centralized mechanism vs decentralized mechanism</p>
<p></p>
<p>&#64;Vahe,</p>
<p>Regarding &#34;what will be the relationship between these two Q tables when both agents use the same learning algorithm&#34;, it seems like Q1 will just be -Q2<br /></p>",2019-11-20T02:38:40Z,52,Week 11/17 - 11/23,feedback,a_0,,k36ogscktqq1eg,2019-11-20T02:38:40Z,{},project3
4433,no,"<p>&#64;Anonymous, confirming that&#39;s my understanding, but please don&#39;t interpret my understanding as definitive. I have to admit the experiment design seems contradictory in a few ways (elaborated above) and it&#39;s hard to say what their methodology was for sure. Last week&#39;s office hours recommended the 2005 paper (linked above) to shed light on common pitfalls experienced reading 2003 alone. We&#39;re evaluated primarily on 2003, but if 2005 helps your understanding of 2003 results and you can justify how/why, it sounds like that was one option for approaching the project. Can&#39;t say I feel comfortable sharing much more about my interpretation of the methodology beyond the areas discussed above.</p>",2019-11-20T02:49:03Z,52,Week 11/17 - 11/23,feedback,,isde332xcka1m0,k36ou59msuv74k,2019-11-20T02:49:03Z,{},project3
4434,no,"<p>I just read P3 and the paper, and interpreted Q  as a rank (nstate) tensor, with other dimensions being actions * actions of each player. Wouldn&#39;t you just pull out the slice you require for sprime, run CVXopt for the constraints and maximise based on the various V choices (nash, CEu) etc?</p>
<p></p>
<p>However the more comments I read on Piazza the more I think Ive completely cooked it.</p>
<p></p>
<p></p>",2019-11-20T16:03:14Z,52,Week 11/17 - 11/23,feedback,,is5x0kzrzjpg7,k37h7h8qzvl2nc,2019-11-20T16:03:14Z,{},project3
4435,no,"<p>&#64;Daniel, re-reading 2003 today I realize the quote about a central coordinator preventing collisions is only mentioned for the Grid Games experiment sections (grid games 1,2,3), where colliding is always counter-productive. The 2003 paper doesn&#39;t explicitly mention anything about preventing collisions in the soccer environment where collisions are a necessary part of learning optimal behaviors in the environment. I&#39;m still not at a point where I&#39;d bet $1M Greenwald didn&#39;t prevent collisions in the soccer environment as well, but seeing it only mentioned as part of the methodology for the non-Soccer grid games makes more sense as far as I can tell.</p>",2019-11-20T20:05:31Z,52,Week 11/17 - 11/23,feedback,,isde34zracb1mz,k37pv2djo9a746,2019-11-20T20:05:31Z,{},project3
4436,no,"<p>Yup I agree with &#64;Michael that that line applies to the Grid Games experiments only. If it did apply to the Soccer Env, then there must be a way that the FoeQ experiment also did not have collisions to get the same graph, which is not true.</p>",2019-11-22T08:45:55Z,52,Week 11/17 - 11/23,feedback,,jqmjhlg7ktv5a,k39wgs7tvub2yw,2019-11-22T08:45:55Z,{},project3
4437,stud,"<p>&#64;Vahe, thanks for the clear explanation!</p>",2019-11-20T05:46:30Z,52,Week 11/17 - 11/23,followup,a_0,,k36v6cu7mbw5x4,2019-11-20T05:46:30Z,{},project3
4438,no,<p>They refer to iterations (steps). </p>,2019-11-20T00:43:56Z,52,Week 11/17 - 11/23,followup,,jc6xvgjncoey,k36kd8yblyk50o,2019-11-20T00:43:56Z,{},project3
4439,no,<p>Actually... I&#39;m assuming that you are talking about figure 3 of Greenwald-Hall 2003.</p>,2019-11-20T00:45:12Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k36kevl6x216pw,2019-11-20T00:45:12Z,{},project3
4440,no,"<p>Yes, that&#39;s right. Ok, I think that makes sense.</p>",2019-11-20T03:33:11Z,52,Week 11/17 - 11/23,feedback,,jl4eendwXRTz,k36qewcm9kx5pc,2019-11-20T03:33:11Z,{},project3
4441,no,"<p>I am still confused about the x-axis here. </p>
<p></p>
<p>Do we really have to run 1 million episodes of the game? And each episode will have some x number of time-steps?</p>
<p></p>
<p>Or should we only run 50,000 episodes and each episode can run until 20 time-steps (or until a goal is scored). Then we plot the difference between the q-values for each time-step?</p>",2019-11-22T03:00:02Z,52,Week 11/17 - 11/23,followup,,jzkpm3i0pxp2sg,k39k3za99dh579,2019-11-22T03:00:02Z,{},project3
4442,no,<p>I believe the time steps refer to updates of the Q-table.  So if there are 1 milliion time steps (some people are calling them iterations) then there will be far fewer soccer games than that.</p>,2019-11-22T03:09:38Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39kgc1yp1j3yi,2019-11-22T03:09:38Z,{},project3
4443,no,"<p><em>Sigh</em>. It was going so well until the last one, where I&#39;m getting:</p>
<p></p>
<pre>[[0.143, 0.143, 0.143]
 [0.143,   0.0, 0.143]
 [0.143, 0.143,   0.0]]</pre>
<p>Which is clearly wrong. I even have the same <em>G</em> as you.</p>",2019-11-21T21:55:42Z,52,Week 11/17 - 11/23,followup,,jl3oi5v7qkSk,k3998lxkj4q7e8,2019-11-21T21:55:42Z,{},project3
4444,no,"<p>Except for if I use the default solver, in which case I get:</p>
<p></p>
<pre>[[0.056, 0.139, 0.139]
 [0.139, 0.056, 0.139]
 [0.139, 0.139, 0.056]</pre>
<p></p>
<p>Go home linear programming, you&#39;re drunk.</p>
<p></p>",2019-11-21T22:02:01Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k399gpw6gna37i,2019-11-21T22:02:01Z,{},project3
4445,no,"<p>I have seen the same, and then some days I get &#34;status=unknown&#34; and then the next day &#34;optimal&#34; with no changes.</p>
<p></p>
<p>The solvers use random numbers. I recommend setting all of the seeds that you can to ensure a consistent solution.</p>",2019-11-21T22:28:11Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39aedsmcur6n9,2019-11-21T22:28:11Z,{},project3
4446,no,"<p>I forgot to mention that I used the glpk solver. With the default solver, I get the same first and third result but the second is (.5, 0, 0, .5). I need to double check.</p>
<p>The results don&#39;t vary across runs for me, even though I&#39;m not setting a random seed.</p>",2019-11-21T22:58:31Z,52,Week 11/17 - 11/23,feedback,,is8ald0uljj3u4,k39bhe2trym4ie,2019-11-21T22:58:31Z,{},project3
4447,no,<p>&#64;Ben Lomax I have the same results as you. Still trying to figure out where the issue is. (0.143 that is)</p>,2019-11-22T03:05:01Z,52,Week 11/17 - 11/23,feedback,,i4jbttw9ru63ot,k39kadmmdfclo,2019-11-22T03:05:01Z,{},project3
4448,no,"<p>Alejandro I have the same result as you. I think that answer makes sense if you are optimizing on the combined score. That is, using utilitarian CE.</p>",2019-11-22T13:07:16Z,52,Week 11/17 - 11/23,feedback,,jl88dnp8AZzD,k3a5svyy3k95a5,2019-11-22T13:07:16Z,{},project3
4449,no,"<p>&#64;David - interestingly, I was getting the 0.143 matrix when using an <em>egalitarian</em> CE, and I&#39;m now getting the correct answer when using a <em>utilitarian</em> CE.<br /><br />&#64;Jacob - good shout on setting the seed for cvxopt. I had completely forgotten to do that one.</p>",2019-11-22T14:08:58Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k3a808gxuzh61r,2019-11-22T14:08:58Z,{},project3
4450,no,<p>This is a very good observation Ben Lomax. :) I have been doing egalitarian all this time because i am not using a slack variable. Now I understand why we need that slack variable for uCEQ.</p>,2019-11-22T15:02:27Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3a9x0cesu54e2,2019-11-22T15:02:27Z,{},project3
4451,no,"<p>If you look at the second example, I think there is more than one right answer for $$\mu$$CE.  Utilitarian CE seeks to maximize the <em>sum</em> of the players&#39; rewards.  [1,0,0,0] results in a game value of 3, but so does [0,0,0,1].  And so does Alejandro&#39;s answer of [0.5, 0, 0, 0.5].</p>
<p></p>
<p>And in fact, any convex combination of those two entries works because of the diagonal zeros in the payoff matrices.  Any of these solutions is a Nash equilibrium because any player unilaterally switching downgrades to a zero.</p>",2019-11-22T17:08:02Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3aeeihyjiq6ix,2019-11-22T17:08:02Z,{},project3
4452,no,"<p>Alejandro,</p>
<p></p>
<p>Thank you very much! This was really helpful. So, it seems my uCE Linear programming is working. So, I may have a problem in my Foe-Q, because my Q tables for Foe and CE don&#39;t match... </p>
<p></p>
<p>At least I can isolate the problem and focus on my Foe to make it work.</p>",2019-11-23T02:22:20Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k3ay7cpksp011f,2019-11-23T02:22:20Z,{},project3
4453,no,"<p>I matched the policy for the second test case using a slack variable. The final value in the list is the slack value.</p>
<p></p>
<pre>[ 5.00e-01]
[ 0.00e&#43;00]
[ 0.00e&#43;00]
[ 5.00e-01]
[ 5.00e-01]</pre>
<p>When I play chicken though, I get something different:</p>
<p></p>
<pre>[ 0.00e&#43;00]
[ 5.00e-01]
[ 5.00e-01]
[ 0.00e&#43;00]
[ 5.00e-01]</pre>
<p></p>",2019-11-23T20:43:45Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3c1jrqc7ij4gg,2019-11-23T20:43:45Z,{},project3
4454,no,"<p></p>
<blockquote>
<p>If you look at the second example, I think there is more than one right answer for μ<span style=""vertical-align:-0.347em""></span>CE. Any convex combination of those two entries works because of the diagonal zeros in the payoff matrices</p>
</blockquote>
<p></p>
<p>That makes sense.</p>
<p></p>
<p>Danilo, glad it helped.</p>",2019-11-24T04:09:16Z,51,Week 11/24 - 11/30,feedback,,is8ald0uljj3u4,k3chgpkm30s4ol,2019-11-24T04:09:16Z,{},project3
4455,no,"<p>Thanks for the test cases. I was able to match them, but my graphs still look wrong... so I must have another bug...</p>",2019-11-22T03:00:32Z,52,Week 11/17 - 11/23,followup,,ixty1midfufhd,k39k4mfkzd91ou,2019-11-22T03:00:32Z,{},project3
4456,no,"<p>Very interesting, thanks for sharing. Added to my reading list.</p>",2019-11-21T00:10:12Z,52,Week 11/17 - 11/23,followup,,jl1acpoc4HA9,k37ylpz85wc5py,2019-11-21T00:10:12Z,{},other
4457,no,"<p>super interesting, thanks for the post. ;)</p>",2019-11-21T22:42:47Z,52,Week 11/17 - 11/23,followup,,jzivtxcbl6964n,k39ax5hwbet4n8,2019-11-21T22:42:47Z,{},other
4458,no,"<p>Vahe can you expand that explanation a little more? I&#39;m also confused how to calculate V(s) and I&#39;m not fully understanding what is necessary in these linear equations. From my understanding when observing player A in a specific state, I&#39;m grabbing all the Q values from player A with in the various Q values for the 5 potential actions to take. In the minimal case, the action that maximizes the player A reward and minimizes player B reward is the value to grab (or in other words the max value from the 5 actions returned in the linear equation). Am I missing something here?</p>",2019-11-21T04:31:22Z,52,Week 11/17 - 11/23,followup,,j6m1jeidndu6wq,k387xkombr76sf,2019-11-21T04:31:22Z,{},project3
4459,no,"<p>You&#39;re trying to explain in words what I think is really best explained by the math.</p>
<p></p>
<p>What this algorithm is doing is absolutely no different than what Littman does a few pages earlier in the Rock-Paper-Scissors example.  In that example, he was given a game matrix with the payoffs.  In this algorithm, a Q table (like you said) takes the role of the game matrix.  But in principle, if you can implement rock-paper-scissors, this is exactly the same.</p>
<p></p>
<p>V(s) is the value of the game at the equilibrium solution, and Q(s) is the source of the game matrix you&#39;ll be using to do the maximin optimization.</p>",2019-11-21T05:04:36Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3894b6zrudt7,2019-11-21T05:04:36Z,{},project3
4460,no,"<p>Very well said. I was thinking about the rock-paper-scissors examples and how that correlated to this problem. My next question or understanding is the values used in the Q table. Unlike Rock Paper Scissors where no state was changed when solving the matrix, there are multiple states in this example. When solving for the value of a state, the entire Q table is not used correct? Only the values for the agent in state, S, and the values for the other player in their state. Just trying to make sure I understand this. Those values in those two player&#39;s states is what is used to solve V(s) for a player which is similar to rock-paper-scissors</p>",2019-11-21T05:26:19Z,52,Week 11/17 - 11/23,feedback,,j6m1jeidndu6wq,k389w8zf5qj11u,2019-11-21T05:26:19Z,{},project3
4461,no,"<p>That is correct David, you are solving for a specific state of Q as dictated by (s, a, o) in Littman&#39;s notation, where &#39;s&#39; is constant..</p>",2019-11-21T22:30:11Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39agxzxz722vq,2019-11-21T22:30:11Z,{},project3
4462,no,"<p>I&#39;m not sure I follow; what exactly does &#34;the dummy variable for minimization/summation&#34; mean? </p>
<p></p>
<p>To get a little more specific, what does this summation look like without the use of dummy variables? What terms are we summing?</p>
<p></p>
<p></p>",2019-11-21T16:44:20Z,52,Week 11/17 - 11/23,followup,,jl284xdcifz44g,k38y46xrwys140,2019-11-21T16:44:20Z,{},project3
4463,no,"<p>The dummy variable for minimization is the variable being minimized over.  The dummy variable for summation is the index of summation.  You can&#39;t not use these variables.  Here is the assignment in math:</p>
<p></p>
<p>$$V[s] \leftarrow \min\limits_{o&#39;} \sum\limits_{a&#39;}\Big{(}\pi[s, a&#39;]\cdot Q[s,a&#39;,o&#39;]\Big{)}$$</p>",2019-11-21T17:04:53Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k38yumek6uz32q,2019-11-21T17:04:53Z,{},project3
4464,no,"<p>I understand Q[s,a′,o′] where the Q-values for the state are grabbed. But I&#39;m not understanding a′and why a summation is done?</p>
<p></p>",2019-11-21T18:38:44Z,52,Week 11/17 - 11/23,feedback,,j6m1jeidndu6wq,k3927aq4yj71km,2019-11-21T18:38:44Z,{},project3
4465,no,"<p>a&#39; = set of newly selected actions for player a</p>
<p>o&#39; = set of newly selected actions for player o</p>
<p></p>
<p>But you haven&#39;t selected anything yet! (throws papers)</p>
<p></p>
<p>The minimization is what you are using to select the a&#39;. For each a&#39; in {A}, given o&#39; in {O} where |A| and |O| are 1:1, you minimize the values of pi for the dot product of your pi vector at a&#39; (the probability of a&#39;) and the Q vector at (s,a&#39;) which gives you Qs{a1&#39;o&#39;, a2&#39;o&#39;, a3&#39;o&#39;, a4&#39;o&#39;, a5&#39;o&#39;}. Remember that you are iterating over o too, so you will have |O| number of equations to minimize.</p>",2019-11-21T22:34:30Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39amhq7w9s2s,2019-11-21T22:34:30Z,{},project3
4466,stud,"<p>On my fairly old computer (i7-2600), it takes about 30 minutes for foe Q and 40 minutes for CE Q</p>",2019-11-21T00:06:49Z,52,Week 11/17 - 11/23,followup,a_0,,k37yhd3wlxj2ez,2019-11-21T00:06:49Z,{},project3
4467,no,<p>I feel much better now :)</p>,2019-11-21T02:29:49Z,52,Week 11/17 - 11/23,feedback,,jl2egn5k4zo4lp,k383l9i3tgb3li,2019-11-21T02:29:49Z,{},project3
4468,stud,<p>Just found a faster computer (single Google vCPU) and doubled the speed.</p>,2019-11-21T02:48:50Z,52,Week 11/17 - 11/23,feedback,a_0,,k3849ptumms7i0,2019-11-21T02:48:50Z,{},project3
4469,stud,<p>Actually it 5x the speed. Now it takes 20 minutes to run compared to 70 minutes! Almost time to upgrade my computer!</p>,2019-11-21T02:51:55Z,52,Week 11/17 - 11/23,feedback,a_0,,k384dojsww3vz,2019-11-21T02:51:55Z,{},project3
4470,no,"<p>Out of curiosity, did you choose one of the &#39;highcpu&#39; options, and select &#39;Intel Skylake or better&#39; under CPU platform?  Or did you go with defaults?</p>",2019-11-21T02:54:59Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k384hmq8fep12b,2019-11-21T02:54:59Z,{},project3
4471,stud,<p>No. Just the standard ones. </p>,2019-11-21T02:55:33Z,52,Week 11/17 - 11/23,feedback,a_0,,k384id20mh01ec,2019-11-21T02:55:33Z,{},project3
4472,no,"<p>During Office Hours all TAs were suggesting to use glpk solver. With default solver my lp for CE-Q runs 1.5 hour, with glpk it runs 9 minutes (3.5 for Foe-Q).</p>",2019-11-21T03:17:10Z,52,Week 11/17 - 11/23,feedback,,jqkxzdmmolGf,k385a640r4846u,2019-11-21T03:17:10Z,{},project3
4473,stud,<p>I have to try that!  I find it makes a big difference whether the build of the optimizer is linked with MKL. </p>,2019-11-21T03:18:15Z,52,Week 11/17 - 11/23,feedback,a_0,,k385bk0xr7x137,2019-11-21T03:18:15Z,{},project3
4474,no,"<p>Wow Sergei, much faster.... Thanks!</p>",2019-11-21T03:29:12Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k385pn00irv2oz,2019-11-21T03:29:12Z,{},project3
4475,stud,"<p>&#43;1 GLPK!!</p>
<p></p>
<p>Another 7x speed up. </p>
<p></p>",2019-11-21T03:30:09Z,52,Week 11/17 - 11/23,feedback,a_0,,k385qv8v3yq3rh,2019-11-21T03:30:09Z,{},project3
4476,stud,<p>It appears both switching to glpk and switching machines can cause subtle changes to my solutions. GLPK seems to make my foeQ better. Switching to a Linux machine seems to have changed solutions for ceQ. I just adjusted the parameters for the optimizer and things seem much better now on both machines. </p>,2019-11-21T06:45:07Z,52,Week 11/17 - 11/23,feedback,a_0,,k38cpkt3mgt7kn,2019-11-21T06:45:07Z,{},project3
4477,no,"<p>What parameters did you adjust for GLPK?  I&#39;m having numerical instability issues, probably due to round-off errors.  I tried playing with the feasibility tolerance and lower and upper objective limits, but it didn&#39;t help.</p>",2019-11-21T08:15:30Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k38fxtvjiz910v,2019-11-21T08:15:30Z,{},project3
4478,stud,"<p>I am using the default optimizer for ce-Q. There are things like solvers.options[&#39;feastol&#39;] and #solvers.options[&#39;abstol&#39;]</p>
<p></p>",2019-11-21T15:32:50Z,52,Week 11/17 - 11/23,feedback,a_0,,k38vk8hybkh69b,2019-11-21T15:32:50Z,{},project3
4479,no,"<p>Vahe, if you have your constraints mostly right don&#39;t fight several instability errors encountered by glpk here and there (I may have a few for a 1 million run for particular HP configurations), in order to not be stuck on glpk trying different approaches for solving use tm_lim or it_lim parameters (I prefer it_lim because of determinism), and you can find this parameters description here: https://rdrr.io/cran/glpkAPI/man/glpkConstants.html</p>",2019-11-21T16:08:21Z,52,Week 11/17 - 11/23,feedback,,jqkxzdmmolGf,k38wtwcugsz1jh,2019-11-21T16:08:21Z,{},project3
4480,no,<p>Thanks Sergei!  Those limits definitely work to prevent what seem like infinite loops.  I&#39;ll play around with it.</p>,2019-11-21T16:57:31Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k38yl50r7qp6db,2019-11-21T16:57:31Z,{},project3
4481,no,<p>Is someone running in Windows 10? My Foe-Q takes hours and my CE-Q is even worse (I leave it running overnight).</p>,2019-11-23T01:05:48Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k3avgxbz3nkqc,2019-11-23T01:05:48Z,{},project3
4482,no,<p>Windows 10 here. My Foe-Q usually runs in about 10 minutes. The CE-Q will take about 1.5 hours. I had to add short circuit breaks for glpk to get out of infinite loops.</p>,2019-11-23T20:26:02Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3c0wzh0fn4ga,2019-11-23T20:26:02Z,{},project3
4483,no,"<p>Jacob,</p>
<p></p>
<p>Thank you for responding that. What are &#34;short circuit breaks&#34; that you are talking about?  How did you do that?</p>",2019-11-23T21:11:16Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k3c2j6c54at5ou,2019-11-23T21:11:16Z,{},project3
4484,no,<p>Has anyone been getting stuck around 328744/1000000? Program seems to chug along and then hang as soon as I get around this range and stop making progress for CEQ. -.- </p>,2019-11-22T22:45:55Z,52,Week 11/17 - 11/23,followup,,jcg0nzvdk8272b,k3aqh1gnqxp5pv,2019-11-22T22:45:55Z,{},project3
4485,no,"<p>Yes, mine is stuck - it would seem. I gave it some limit arguments, but that didn&#39;t seem to do much:</p>
<p></p>
<div>
<div>        solvers.options[&#39;glpk&#39;] = {&#39;LPX_K_MSGLEV&#39;: 0, &#39;msg_lev&#39;: &#34;GLP_MSG_OFF&#34;,&#39;TM_LIM&#39;:60000,&#39;IT_LIM&#39;:2000000}</div>
<div> </div>
</div>",2019-11-22T23:36:04Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3as9iw8j4u3yp,2019-11-22T23:36:04Z,{},project3
4486,no,<p>I hacked my way through this by assigning the previous equilibrium solution to the current one when no solution was find-able.</p>,2019-11-22T23:48:58Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3asq40plo452s,2019-11-22T23:48:58Z,{},project3
4487,no,"<p>Seems like bad seed values, ironically of the 10&#43; seeds ive used the one I started with is the only one that hangs. But now my error is nonexistant :| </p>
<p></p>
<p>&#64;Vahe hmm, not following how that would still be CEQ? </p>",2019-11-22T23:53:14Z,52,Week 11/17 - 11/23,feedback,,jcg0nzvdk8272b,k3asvlgg7ci3km,2019-11-22T23:53:14Z,{},project3
4488,no,"<p>I will be computing an errant Q update for one cycle, every once in a long while, over the course of hundreds of thousands of updates.  If it doesn&#39;t affect convergence then I&#39;m okay with it.  It&#39;s a trade-off between that and using the default solver and waiting five times as long.</p>",2019-11-23T00:09:33Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3atglduhyr35s,2019-11-23T00:09:33Z,{},project3
4489,no,"<p>is this the &#34;hanging&#34; issue that you guys were talking about:</p>
<p></p>
<pre>Perturbing LP to avoid stalling...</pre>
<p>I ran into the same issue after converting my code to use GLPK. It seems to be related to GLPK&#39;s Simplex solver. There is a solution that I found on:</p>
<p><a href=""http://vlsicad.eecs.umich.edu/BK/Slots/cache/www-unix.mcs.anl.gov/otc/Guide/faq/linear-programming-faq.html"">http://vlsicad.eecs.umich.edu/BK/Slots/cache/www-unix.mcs.anl.gov/otc/Guide/faq/linear-programming-faq.html</a></p>
<p>(Q6.15: &#34;How can I combat cycling in the Simplex algorithm?&#34;)</p>
<p></p>
<p>So basically, I use &#34;magic numbers&#34; (ie: 0.000568498, 0.000524888, 0.000282861...) for my LP conditions, as oppose to (0.0, 0.0, 0.0....) as any normal Muggle would do.</p>",2019-11-24T22:57:20Z,51,Week 11/24 - 11/30,feedback,,jl5wq8mca7o0,k3dlrf64bve4ml,2019-11-24T22:57:20Z,{},project3
4490,no,"<p>I am not sure what I am doing but it is taking about 3 seconds to do 100 iterations.. that means 8.66 hrs for 1 million iterations.</p>
<p></p>
<p>I guess I got 1 shot at getting it perfect xD</p>",2019-11-24T21:27:29Z,51,Week 11/24 - 11/30,followup,,hzoi2qsuCAd,k3dijv24q1e76q,2019-11-24T21:27:29Z,{},project3
4491,no,"<p>i-7 7800X, 32GB Ram... Definitely something I am doing is SUPER inefficient... all 16 hyperthreading maxed at 100%</p>",2019-11-24T21:27:58Z,51,Week 11/24 - 11/30,feedback,,hzoi2qsuCAd,k3diki1adbd7p8,2019-11-24T21:27:58Z,{},project3
4492,no,"<p>If you see 100% CPU usage, I am guessing you are using default solver, which are magnitude slower than GLPK solver. Also, I have observed couple time the default solver yield different results than the GLPK solver. For example solving Q vs solving Q.T, GLPK has the diff of 1e-7, while default solver has 1.xxx and -2.xxx.</p>",2019-11-24T23:02:44Z,51,Week 11/24 - 11/30,feedback,,jl5wq8mca7o0,k3dlycwh9z71pn,2019-11-24T23:02:44Z,{},project3
4493,no,"<p>I asked a question similar to 1 in last week&#39;s study session (the one on Saturdays, not the office hours, if you want to look for the recording) and I was told to NOT decay alpha in Q-Learning, and to set it to some arbitrarily high value (at least to test convergence or lack thereof) like 0.8 or 1.0. As a disclaimer, I tried this, and my Q-Learning still converges quite quickly, but maybe it will help you.</p>",2019-11-21T16:51:48Z,52,Week 11/17 - 11/23,followup,,jl284xdcifz44g,k38yds60121357,2019-11-21T16:51:48Z,{},project3
4494,no,"<p>Well, in the paper, it clearly says that Q learning error is decreasing is due to decaying alpha. Unless this is not correct. Waiting for instructors to confirm. </p>",2019-11-21T19:35:41Z,52,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k3948jh5io71yq,2019-11-21T19:35:41Z,{},project3
4495,no,"<p>Hence my confusion, I was told by the TAs to not decay alpha, which seems to contradict the paper, and also didn&#39;t solve my problem that my Q-learner converges too quickly</p>",2019-11-21T21:55:24Z,52,Week 11/17 - 11/23,feedback,,jl284xdcifz44g,k39987jh82suf,2019-11-21T21:55:24Z,{},project3
4496,no,"<p>Q learner with an alpha decay will follow the alpha decay. That&#39;s where it becomes a misnomer. Note &#64;927 where I discuss this with another student and realize where I was misunderstanding it too. If you graph alpha = { 0.1, 0.2, ..., 1.0 } using decay = 1.0 (no decay), you will see that Q does not converge at any alpha. Only when you decay the alpha does it seem like there is convergence.</p>",2019-11-21T22:26:18Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39abyj9wlu4wj,2019-11-21T22:26:18Z,{},project3
4497,no,<p>Thank you. Jacob. That makes sense. Decaying alpha will cause the model to converge. </p>,2019-11-21T22:53:55Z,52,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39bbgtuabu4ng,2019-11-21T22:53:55Z,{},project3
4498,no,"<p>I tried it without decaying and it does not converge at all. The error stays about the same throughout the 100,000 iterations for Q learning. Once I implemented some sort of alpha decay, it started to look similar to the one in the paper. </p>",2019-11-22T04:25:56Z,52,Week 11/17 - 11/23,feedback,,hzoi2qsuCAd,k39n6fr470t3qh,2019-11-22T04:25:56Z,{},project3
4499,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fhzoi2qsuCAd%2Fk39nc8zp7gr2%2FQlearner.png"" alt="""" /></p>
<p>This is what I got after decaying the learning rate over 100k iteration. the trick is to find a number that would continuously decay throughout 100k iterations without becoming excessively too small.</p>
<p></p>
<p>Without decaying, it is just a solid block that is not interesting to look at (imagine a rectangle from 0 to 100k) :(</p>",2019-11-22T04:32:25Z,52,Week 11/17 - 11/23,feedback,,hzoi2qsuCAd,k39nes07cs73fz,2019-11-22T04:32:25Z,{},project3
4500,no,"<p>I see. Looks like for all the 4 algorithms, we need alpha decay. </p>",2019-11-22T05:32:53Z,52,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39pkjuwudp4yi,2019-11-22T05:32:53Z,{},project3
4501,no,"<p>For Friend-Q, V is with respect with to the agent&#39;s Q table. The value of each will be different. Conceptually, A expects B to step back and score for it in state s while B expects A to stay still then score for it the following turn. Both are expecting 100 reward within 2 turns. So for your 2nd question, action_b and the action a expects can be different. You have the action a expects.</p>",2019-11-21T16:18:58Z,41,Week 11/17 - 11/23,followup,,i4lzw47xh1u57w,k38x7km2q1o6p0,2019-11-21T16:18:58Z,{},project3
4502,no,<p>But don&#39;t we only have one Q table for both a and b? That is what I am referring to by a 3d numpy array. If I only have one Q table then I only have one max value I thought...</p>,2019-11-21T17:27:11Z,41,Week 11/17 - 11/23,feedback,,jc6mqevhagl262,k38zna1mzt82l2,2019-11-21T17:27:11Z,{},project3
4503,no,"<p>I have 2 Q tables. In the paper they have 2 Q tables Q = [Q_a, Q_b]. It&#39;s easier this way. I think you can implement with one Q table as the game is symmetric, but in one step you are updating twice your Q table through A and B. Also it might be harder to implement.</p>",2019-11-21T18:39:32Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3928bmj27h6cq,2019-11-21T18:39:32Z,{},project3
4504,no,"<p>Aida, just wondering are your states difference for an agent when it has the ball and doesn&#39;t have the ball. For example if player A were in state 4, would that state be different in the Q-table when A has the ball vs when A doesn&#39;t have the ball?</p>",2019-11-21T18:50:33Z,41,Week 11/17 - 11/23,feedback,,j6m1jeidndu6wq,k392mhwgvjboh,2019-11-21T18:50:33Z,{},project3
4505,no,"<p>David, I&#39;ve implemented my environment as player1_possession can be 0 or 1. Then if it equals to 0, then it means the player1 has the ball. If it equals to 1, then the player2 has the ball. So in this case, my environment knows who has the ball. </p>
<p></p>
<p>As for you example, my Q-table will have different states for having the ball vs not having the ball as there are 8*8*2 states</p>",2019-11-21T18:57:36Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k392vk8mzh238t,2019-11-21T18:57:36Z,{},project3
4506,stud,I only had one Q table similar to Matt as it contains the actions for player A and B ,2019-11-21T19:14:03Z,41,Week 11/17 - 11/23,feedback,a_0,,k393gpkupof7j7,2019-11-21T19:14:03Z,{},project3
4507,no,"<p>If you have 1 Q-table, then how do you update it as it&#39;s updated based on reward and rewards are different for each player? For example, if player A has the ball and score to the opponent&#39;s goal, then the player A gets 100 and B gets -100. Then 2 different Q-tables will be updated differently, no? Same state and set of actions meaning each Q-table needs to update the same cell, but differently? Maybe I&#39;m confused. </p>",2019-11-21T19:26:55Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k393x9eelvf642,2019-11-21T19:26:55Z,{},project3
4508,no,<p>Updating Q[0] then Q[1] is the same as QA then QB</p>,2019-11-21T19:29:49Z,41,Week 11/17 - 11/23,feedback,,i4lzw47xh1u57w,k3941009wgy3mv,2019-11-21T19:29:49Z,{},project3
4509,no,"<p>Yes, I have a dictionary Q which has two Q-tables, meaning we have 2 Q-tables: Q[0] and Q[1]. I just don&#39;t know how you can use 1 Q-table</p>",2019-11-21T19:31:42Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3943eqihsi5mb,2019-11-21T19:31:42Z,{},project3
4510,no,"<p>You effective have 1 Q-table from the perspective of that dictionary being a Q table with indices agent, state, action vector.</p>
<p></p>
<p>Another way to have 1 q-table is to only track Q_A as they are the same but flipped (zero-sum game). min of Q_A is the same as the max of Q_B.</p>",2019-11-21T19:35:49Z,41,Week 11/17 - 11/23,feedback,,i4lzw47xh1u57w,k3948pmvet03qk,2019-11-21T19:35:49Z,{},project3
4511,no,<p>From the persepective of having 2 Q tables. One for A and One for B. Do step 1 from the original question to get V for a and V for B. The action for A or B would be the index found in their respective Q table using step 2 from the original question ie A takes the action of the row which contains the highest Q value in Q_A. B takes the action of the col which contains the highest Q value in Q_B. For row=actionA and col=actionB</p>,2019-11-21T19:41:00Z,41,Week 11/17 - 11/23,feedback,,i4lzw47xh1u57w,k394fddb6w22te,2019-11-21T19:41:00Z,{},project3
4512,no,"<p>Right, you can do this by using min instead of max for player B. Although the second input will work for Q-learning, but for friend-Q, again you need to use min instead of max. I think it&#39;s easier and cleaner to maintain 2 Q-tables or 1 Q-table with agent, state, actions as you said.</p>",2019-11-21T19:52:05Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k394tmnd4l31n2,2019-11-21T19:52:05Z,{},project3
4513,no,"<p> I have a Q table with size (num_states, 25) to avoid the 3d table. For friend Q, assuming the table is for player 1. When I take an action, I take the argmax(Q[s,:]), which is a combination of both players action. So it makes sense to me to make one table. If player 2 has its own table, how to get player 2 action separately? since they are coordinating, can we assume that player 2 will allow player 1 to achieve max score at each state? </p>
<p></p>
<p>Another way to ask this question is, if we use action=argmax(Q1[s,:]), is this correct? How do I assure the actions from the above is optimal for player 2?</p>",2019-11-21T21:58:39Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k399ce6fvb222z,2019-11-21T21:58:39Z,{},project3
4514,no,"<p>Friend-Q is not supposed to know about the opponent&#39;s actions, so you don&#39;t correlate them. Your Q for Friend-Q should be |s| x 5.</p>",2019-11-21T22:24:06Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39a94cdu2e1ot,2019-11-21T22:24:06Z,{},project3
4515,no,"<p>Well, how does it differ from the normal Q learning then? Based on currently approach, player A always assume player B will take the action West to reach its own goal so player A is rewarded</p>",2019-11-21T22:49:40Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39b5zunp4l4t2,2019-11-21T22:49:40Z,{},project3
4516,no,"<p>Yeah, Zhenning, I got it what you are saying. I have 2 Q-tables, so to update both it seems I need to check action_a and action_b for the first Q-table, then check again action_a and action_b for the second Q-table</p>",2019-11-21T22:50:28Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39b71inhmmbc,2019-11-21T22:50:28Z,{},project3
4517,no,"<p>&#96;Friend-Q is not supposed to know about the opponent&#39;s actions, so you don&#39;t correlate them. Your Q for Friend-Q should be |s| x 5.&#96; This is exactly what I implanted with Q learner btw. &#64;Jacob </p>",2019-11-21T22:50:58Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39b7o21i557it,2019-11-21T22:50:58Z,{},project3
4518,no,"<p>&#64;Aida how do you pick the final action then for both players? Updating rule seems clear because we always want to pick the max in the Q[s,:] table. When we take an action, what if there&#39;s conflict between player A and player B&#39;s best action from their own table?</p>
<p></p>
<p>This problem sounds like related to your opponents&#39;s learning strategy. If player B is always using FriendQ (which looks like we should follow here), then we need to pick the best action from their respective Q tables.</p>
<p></p>",2019-11-21T23:02:32Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39bmk4lsju3kg,2019-11-21T23:02:32Z,{},project3
4519,no,"<p>So I&#39;m implementing 2 Q-tables and where both player are playing friend-Q strategy, I&#39;m choosing an action for the player1 from 0 to 24, same for the player 2. Both choose these actions expecting that other is a friend. Then when I&#39;m doing env.step() I&#39;m decoding both actions from 0-24 to tuple (0-4,0-4) and picking actual actions from these 2 tuples for the player1 and player2</p>",2019-11-21T23:06:08Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39br6rv4cdko,2019-11-21T23:06:08Z,{},project3
4520,no,"<p>&#34;playing friend-Q strategy, I&#39;m choosing an action for the player1 from 0 to 24&#34;</p>
<p></p>
<p>You are choosing an action { 1, 2, 3, 4, 5 }</p>
<p></p>
<p></p>",2019-11-21T23:06:54Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39bs5oanrz62o,2019-11-21T23:06:54Z,{},project3
4521,no,"<p>So let&#39;s say the player 1 picks action 17 which is in my case (3,2), while player 2 picks 21 which is (4,1). The the final action would be (3,1)</p>",2019-11-21T23:10:31Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39bwtaldt72yk,2019-11-21T23:10:31Z,{},project3
4522,no,<p>Correct me if I&#39;m wrong please</p>,2019-11-21T23:10:54Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39bxb1bxbi3eb,2019-11-21T23:10:54Z,{},project3
4523,no,"<p>n.m. I am mistaken. You take the max over the (A1B1, ..., AnBn) space</p>
<p></p>",2019-11-21T23:19:48Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39c8rfakin60i,2019-11-21T23:19:48Z,{},project3
4524,no,"<p>&#34; 2 Q-tables&#34; &gt; Littman wrote that he used only 1 table. This would be the centralized method of action selection. You could pit friend vs friend I suppose, which is really just Q versus Q using joint actions.</p>",2019-11-21T23:24:07Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39ceba7hl3q4,2019-11-21T23:24:07Z,{},project3
4525,no,"<p>My environment sounds exactly the same as Aida&#39;s. I am doing the same thing now. One observation is that in later iterations, every episode max out the max_steps ( I set to 100 max steps per episode). However, my curve looks like always 0 expect some values in 20-50 episodes.</p>",2019-11-21T23:24:11Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39cee8a3y679,2019-11-21T23:24:11Z,{},project3
4526,no,"<p>Same here! I have a spike on the step = 2326, but everything else is 0. Iterated over 100,000 steps. Doing 10,000,000 now. Maybe I&#39;m wrong though. Maybe we need to make the second payer play random game</p>",2019-11-21T23:38:18Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39cwjq2efx677,2019-11-21T23:38:18Z,{},project3
4527,no,"<p>If you are using a second friend-q agent, then are you updating using Greenwald 2003 equation (5) ?</p>
<p></p>
<p>$$V_{1}(s) = max_{\sigma_{1}\in\Sigma_{1}(s)} min_{a_{2}\in A_{2}(s)} Q_1(s,\sigma_1,a_2) = -V_2(s)$$</p>
<p></p>
<p>Remember to keep them zero sum.</p>
<p></p>",2019-11-22T00:19:27Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39edgwk4y01f0,2019-11-22T00:19:27Z,{},project3
4528,no,"<p>I simply have 2 updating equations for Q value. The reward is opposite in the 2 equations. &#64;Jacob, how do you update the other agent&#39;s Q table? Does the second player play random or friend Q as well?</p>",2019-11-22T05:37:06Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39ppyr8od82xm,2019-11-22T05:37:06Z,{},project3
4529,no,<p>I used Friend Q as the opponent. </p>,2019-11-23T20:24:28Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3c0uz4dw8z6yj,2019-11-23T20:24:28Z,{},project3
4530,no,So you have two separate Q tables? And two separate V’s? I currently have one but not updating correct ,2019-11-21T22:26:10Z,41,Week 11/17 - 11/23,followup,,jc6mqevhagl262,k39abs3rh391r5,2019-11-21T22:26:10Z,{},project3
4531,no,"<p>Yes, each Q would have its own V.</p>",2019-11-22T00:19:58Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39ee54sz8l1v4,2019-11-22T00:19:58Z,{},project3
4532,no,"<p>My friend vs friend converges similar to the paper. However when I have them play against each other (as in testing after training them), none of the players score. This seems to be intuitive as the two players have opposing goals and at the same time are friends (strange!). Is my understanding correct or your algorithms differ?</p>
<p></p>",2019-11-21T22:52:08Z,41,Week 11/17 - 11/23,followup,,j6ll2xkiDJf,k39b962j4y347e,2019-11-21T22:52:08Z,{},project3
4533,no,<p>Are you starting each game in state $$s$$ with player A moving South and player B Sticking?  Because in this case I would think that the player with the ball would tend to score since he has a clear path to the goal.</p>,2019-11-22T00:05:34Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39dvly3uy949s,2019-11-22T00:05:34Z,{},project3
4534,no,"<p>But if your friend is A, then B should always move back towards A&#39;s goal! In Friend-Q the friend player should get the most goals consistently. That&#39;s my understanding.</p>",2019-11-22T00:21:10Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39efo051xv430,2019-11-22T00:21:10Z,{},project3
4535,no,"<p>Here is the rendering of the game after training two friend-Q players:</p>
<p></p>
<p>-----------------<br />environment reset<br />-----------------<br />|    | B*| A |   |<br />|    |     |    |   |<br />-----------------<br />[aMove, bMove]: (&#39;N&#39;, &#39;E&#39;)<br />Player A moves first<br />-----------------<br />|    | B | A*|   |<br />|    |     |    |   |<br />-----------------<br />[aMove, bMove]: (&#39;W&#39;, &#39;S&#39;)<br />Player A moves first<br />-----------------<br />|   |    | A |    |<br />|   | B*|    |    |<br />-----------------<br />[aMove, bMove]: (&#39;N&#39;, &#39;X&#39;)<br />Player B moves first<br />-----------------<br />|   |     | A |    |<br />|   | B*|    |     |<br />-----------------</p>
<p></p>
<p>It then gets stuck at this for the remainder of the game. (A wanting to</p>
<p>go North and B is sticking</p>",2019-11-22T02:45:32Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39jlcao2cn6px,2019-11-22T02:45:32Z,{},project3
4536,no,"<p>One thing is, you implement your environment differently than Jacob and I do (I had something like yours initially but ended up copying Jacob&#39;s approach).  In your third render, both Jacob and I would have A moved over to the left one spot.</p>
<p></p>
<p>Now, regardless of that, I&#39;m not sure why B is sticking in that final position.  Even in Friend Q, he should WANT to score, which means he should move East.  Don&#39;t you think?</p>
<p></p>
<p>Edit: what is your exploration rate when training?  If it&#39;s too low, your Q values may not have converged.</p>",2019-11-22T03:15:53Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39kodbeikv4h6,2019-11-22T03:15:53Z,{},project3
4537,no,"<p></p><pre>[aMove, bMove]: (&#39;W&#39;, &#39;S&#39;)
Player A moves first
-----------------
|   |    | A |    |
|   | B*|    |    |
-----------------</pre>
<p>if A moves W, and B stays put, why does B get the ball? Are you changing possession every time one of the players gets stuck?</p>",2019-11-22T03:45:52Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39lqxhwbme1zh,2019-11-22T03:45:52Z,{},project3
4538,no,"<p>I thought I might have had a bug but it seems right.</p>
<p>A has the ball, it moves 1st to W and hits B --&gt; A remains in its cell and</p>
<p>B gets the ball and goes south.</p>",2019-11-22T03:51:07Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39lxnya56o43y,2019-11-22T03:51:07Z,{},project3
4539,no,"<p>No, it&#39;s not a bug.  It&#39;s an implementation decision.  This is what I think happened.  Kamran rolled and A went first.  A bumps into B, hands the ball over, and stays put in his original position.  Now B moves South.</p>
<p></p>
<p>This is why I elaborated on how different people can have different environments.  One&#39;s not necessarily right or wrong. But one may be better for replicating the graphs.</p>",2019-11-22T03:56:47Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39m4ygpy4z5b1,2019-11-22T03:56:47Z,{},project3
4540,no,"<p>yes, thanks for pointing it out. </p>",2019-11-22T03:58:12Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39m6rsp9vk59a,2019-11-22T03:58:12Z,{},project3
4541,no,"<p>This is not how I understand the environment; or maybe i&#39;m not understanding your move list... From the following, I understand that from this condition, the player A moves first, and then B.</p>
<p>-----------------<br />|    | B | A*|   |<br />|    |     |    |   |<br />-----------------<br />[aMove, bMove]: (&#39;W&#39;, &#39;S&#39;)<br />Player A moves first</p>
<p></p>
<p>As such, A would try to move west first, a collision would occur, and then B (the second player) would not move. Since the experiment states that on collisions, only the first player moves. But again, that is just my interpretation and implementation of the environment. It might not be correct.</p>",2019-11-22T03:59:58Z,41,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k39m921fovqgd,2019-11-22T03:59:58Z,{},project3
4542,no,<p>OK so are you guys seeing similar moves? it seems that they quickly learn to basically do nothing so no one scores....</p>,2019-11-22T04:00:03Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39m966j10iix,2019-11-22T04:00:03Z,{},project3
4543,no,<p>My learners end up with more draws when I let them run out. I changed it to use LIttman&#39;s 10% rule for draws without short circuiting the game.</p>,2019-11-22T04:01:24Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39maw5ydju2aa,2019-11-22T04:01:24Z,{},project3
4544,no,<p>The paper says that FriendQ is a irrational equilibrium for a zero sum game such as soccer. I am just trying to make sense of the behavior of the two players here. In other words friendship in a competitive (zero-sum) game where the two players have competing goals does not make sense....</p>,2019-11-22T04:06:23Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39mhb3n7ovtw,2019-11-22T04:06:23Z,{},project3
4545,no,"<p>George, my understanding is that when two players collide, the player with the ball who moved into the player without it, loses the ball to his opponent and remains still in his position, the other player is now able to proceed with his move.  </p>",2019-11-22T04:10:50Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39mn0z29x85y3,2019-11-22T04:10:50Z,{},project3
4546,no,"<p>Jacob, I&#39;ll try that but do not know if makes any difference. It just forces a draw for ever. And the players do not score... Will let you know</p>",2019-11-22T04:13:09Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39mq0jb7bz10v,2019-11-22T04:13:09Z,{},project3
4547,no,"<p>Kamran,</p>
<p></p>
<p>As I pointed out earlier, I think B should move east when he has a clear path to the goal, even in Friend-Q.  Moving east maximizes his expected reward.  Sticking or going south does not.  Are you doing exploration during training?</p>",2019-11-22T04:42:22Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39nrkm6oga1nw,2019-11-22T04:42:22Z,{},project3
4548,no,"<p>Kamran, yea, I guess I didn&#39;t put it but, yea the ball would change hands based on my understanding as well, just that B would not get to move after the ball changed hands. Just differences in our interpretations is all.</p>",2019-11-22T05:35:11Z,41,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k39pnhxc47sxk,2019-11-22T05:35:11Z,{},project3
4549,no,"<p>Vahe, I am doing epsilon greedy type of exploration/exploitation. Similar to the one in littman 1994. </p>",2019-11-22T05:50:54Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39q7plh9g4203,2019-11-22T05:50:54Z,{},project3
4550,no,"<p>Yeah, if your $$\epsilon=0.2$$ like in Littman 1994, you should be exploring enough...</p>",2019-11-22T05:55:26Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39qdjwvil7503,2019-11-22T05:55:26Z,{},project3
4551,no,"<p>George, I am wondering if you get the figure3c in the paper? Mine is close but not identical. wondering if the difference in my approach is causing it...</p>",2019-11-22T05:56:58Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39qfim4ze52hv,2019-11-22T05:56:58Z,{},project3
4552,no,"<p>vahe, yes I am using 0.2</p>",2019-11-22T05:57:29Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39qg6v0t2o3j0,2019-11-22T05:57:29Z,{},project3
4553,no,"<p>Kamran, I get something close to it. I don&#39;t think it&#39;s exact, but the behavior is the same.</p>",2019-11-23T02:02:17Z,41,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k3axhkdeeon69f,2019-11-23T02:02:17Z,{},project3
4554,no,"<p>So, does the second player update its Q table and choose its action as Friend-Q algorithm? </p>",2019-11-22T05:40:41Z,41,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k39pukyrmfl7bn,2019-11-22T05:40:41Z,{},project3
4555,no,<p>That is what I am doing. Each player has a Q table for joint actions.</p>,2019-11-22T05:51:50Z,41,Week 11/17 - 11/23,feedback,,j6ll2xkiDJf,k39q8x25iq82ql,2019-11-22T05:51:50Z,{},project3
4556,no,"<p>&#64;Vahe, yep, absolutely. I&#39;ve edited my question to reflect this.</p>",2019-11-21T17:41:28Z,52,Week 11/17 - 11/23,followup,,jl3oi5v7qkSk,k3905nq6tcgkh,2019-11-21T17:41:28Z,{},project3
4557,no,<p>Have you tested it?  Is it not working?</p>,2019-11-21T17:42:18Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3906pwf8cz72u,2019-11-21T17:42:18Z,{},project3
4558,no,"<p>Still not working:</p>
<p></p>
<pre>LP HAS NO PRIMAL FEASIBLE SOLUTION</pre>
<p>I&#39;m pretty sure this means that there is no solution for all the constraints I&#39;ve given it, so the fact I&#39;ve skipped the probability constraints shouldn&#39;t be affecting it.</p>
<p></p>
<p>The other matrices I have are:</p>
<p></p>
<pre>c = [0.0, 0.0, 0.0, 0.0, -1.0]
h = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</pre>
<p>which I think are correct because for <em>c</em> we&#39;re trying to minimise $$V$$, and for <em>h</em> we&#39;ve got the right hand side of all the equations set to zero.</p>",2019-11-21T17:45:18Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k390al0od5m3un,2019-11-21T17:45:18Z,{},project3
4559,no,"<p>I also have:</p>
<pre>A = [[1.0], [1.0], [1.0], [1.0], [0.0]]
b = [1.0]</pre>
<p>because I want all of the probabilities ($$x_1, x_2, x_3, x_4$$) to sum to 1.</p>",2019-11-21T17:55:51Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k390o5ijcpg306,2019-11-21T17:55:51Z,{},project3
4560,no,"<p>You could also have probability constraints for each of your variables - that each one is greater than or equal to $$0$$.  But if you&#39;re not not finding a feasible solution without those, you won&#39;t get one with them.</p>",2019-11-21T18:06:54Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3912dh7tpb5ms,2019-11-21T18:06:54Z,{},project3
4561,no,"<p>Oh, wait, I had a bug with my &#96;A&#96; matrix. Fixing it (to the value shown in the comment above) now gives me:</p>
<p></p>
<pre>LP HAS UNBOUNDED PRIMAL SOLUTION</pre>
<p>which is much better. I&#39;m about to add the probability constraints.</p>",2019-11-21T18:07:18Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k3912vmlffw63q,2019-11-21T18:07:18Z,{},project3
4562,no,"<p>Boom. Adding probability constraints are now giving me answers. Not sure how much they make sense though:<br /><br />$$x_1 = 0$$</p>
<p>$$x_2 = 0.778$$</p>
<p>$$x_3 = 0.222$$</p>
<p>$$x_4 = 0$$</p>
<p></p>
<p>$$V = 1.56$$</p>",2019-11-21T18:18:47Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k391hnbq1y040r,2019-11-21T18:18:47Z,{},project3
4563,no,"<p>By the way, your constraints 6 and 7 isolate the minimum expected return for the row player.  This is what the normal maximin objective (Littman 1994) is asking us to maximize.  But I don&#39;t think the egalitarian objective is exactly the same thing.</p>
<p></p>
<p>The minimum is over all players, not just over the opponent&#39;s actions.</p>",2019-11-21T18:19:05Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k391i0wedzb4e9,2019-11-21T18:19:05Z,{},project3
4564,no,"<p>Ok, so how would I expand that to all players? Does it just mean that I should use the smallest reward that any of the players would get? So if the second player (the column player) chooses action $$L$$, equation 6 becomes:</p>
<p></p>
<p>6.    $$-6x_1 &#43; 0x_2 - 2x_3 &#43; 0x_4 &#43; 1V \leq 0$$</p>
<p></p>
<p>because of the two rewards for the BL quadrant, 2 (the col player&#39;s reward) is smaller than 7 (the row player&#39;s reward)? Equation 7 would then stay the same:<br /><br />7.    $$0x_1 - 2x_2 &#43; 0x_3 &#43; 0x_4 &#43; 1V \leq 0$$</p>
<p></p>
<p>because it&#39;s already using the smallest values.</p>",2019-11-21T18:29:57Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k391w0dkuok2b5,2019-11-21T18:29:57Z,{},project3
4565,no,"<p>The <em>summation</em> is not over opponent actions anymore either - it&#39;s over joint actions.  So I think all variables need to be populated for each constraint, one constraint for each player.</p>",2019-11-21T18:40:44Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3929vix6v638z,2019-11-21T18:40:44Z,{},project3
4566,no,"<p>But if we&#39;re using the minimum (for the <em>egalitarian</em> approach) of the players&#39; actions, Won&#39;t they be the same? So:</p>
<p></p>
<p>6.    $$-6x_1 - 2x_2 - 2x_3 &#43; 0x_4 &#43; 1V \leq 0$$  (Row player)</p>
<p>7.    $$-6x_1 - 2x_2 - 2x_3 &#43; 0x_4 &#43; 1V \leq 0$$  (Column player)</p>",2019-11-21T18:47:48Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k392iyriwkj7o5,2019-11-21T18:47:48Z,{},project3
4567,no,"<p>Although I suppose that would be different if the rewards weren&#39;t symmetrical. So for a game matrix where the <strong>minimum</strong> values were:</p>
<p></p>
<table cellspacing=""0"" cellpadding=""6px"" border=""1""><tbody><tr><td></td><td>L</td><td>R</td></tr><tr><td>T</td><td>a</td><td>b</td></tr><tr><td>B</td><td>c</td><td>d</td></tr></tbody></table>
<p></p>
<p>are you saying you would have the equations:</p>
<p></p>
<p>$$-ax_1 - bx_2 - cx_3 - dx_4 - 1V \leq 0$$ (Row player)</p>
<p>$$-ax_1 - cx_2 - bx_3 - dx_4 - 1V \leq 0$$ (Column player)</p>",2019-11-21T18:51:37Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k392nve73tt4tp,2019-11-21T18:51:37Z,{},project3
4568,no,"<p>Huh, that does indeed make the final values come out as expected:</p>
<p></p>
<p>$$x_1 = 0.5$$</p>
<p>$$x_2 = 0.25$$</p>
<p>$$x_3 = 0.25$$</p>
<p>$$x_4 = 0.0$$</p>
<p></p>
<p>$$V = 4.0$$</p>",2019-11-21T18:56:49Z,52,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k392ujv6ada5dl,2019-11-21T18:56:49Z,{},project3
4569,no,"<p>I think for CEQ, we shouldn&#39;t use the value V as one of the X vector. We are just optimizing c.T*X, where x is 25 in this case for the joint probability of all actions. We can pick (qa &#43; qb) as c, so that we can maximize the utalatarian criteria. However, for the basic chicken example, my solve doesn&#39;t give me the correct probabilities. </p>",2019-11-24T18:11:48Z,51,Week 11/24 - 11/30,followup,,j6ln9puq99s5uv,k3dbk7x3xp61ij,2019-11-24T18:11:48Z,{},project3
4570,no,"<p>does anybody know where larger, better quality versions of the graphs we are supposed to be reproducing, exist? I appreciate the need to save ink on a paper.  but these graphs, are -tiny-.</p>",2019-11-21T22:51:05Z,52,Week 11/17 - 11/23,followup,,jzivtxcbl6964n,k39b7txalp75t,2019-11-21T22:51:05Z,{},project3
4571,no,<p>You can zoom in on the PDFs digitally and they their equality. I believe the graphs are vector graphics.</p>,2019-11-22T20:55:02Z,52,Week 11/17 - 11/23,feedback,,jqnardlwW3NE,k3amifgbtup108,2019-11-22T20:55:02Z,{},project3
4572,no,"<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3aos60dbqf0%2Ffig3b.svg"" target=""_blank"" rel=""noopener noreferrer"">fig3b.svg</a> - svg scalable version of figure 3b</p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3aosoxf3ue2%2Ffig3a.svg"" target=""_blank"" rel=""noopener noreferrer"">fig3a.svg</a> - svg scalable version of figure 3a</p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3ap1v7z1rjg%2Ffig3c.svg"" target=""_blank"" rel=""noopener noreferrer"">fig3c.svg</a> - svg of figure 3c</p>
<p></p>
<p><a href=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3ap268l866n%2Ffig3d.svg"" target=""_blank"" rel=""noopener noreferrer"">fig3d.svg</a> - svg of figure 3d</p>
<p></p>
<p>Everything produced in this post is not copyrighted by me or controlled by me. This is all derived from the Greenwald 2003 paper. You can use it as you see fit.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3ap4i8i8ejn%2Fevidenceofskippingpoints.png"" alt="""" /></p>",2019-11-22T21:59:10Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3aoswq22kkkz,2019-11-22T21:59:10Z,{},project3
4573,no,<p>Their x stride is variable. That means they were only plotting changes and not consecutively the same values. That&#39;s where their plots look very different than ours.</p>,2019-11-22T22:11:14Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3ap8fvxoq85a,2019-11-22T22:11:14Z,{},project3
4574,no,"<p>Jacob, that is fantastically helpful.  Thank you very very much! :)</p>
<p></p>",2019-11-23T23:03:19Z,52,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k3c6j98t99k5c5,2019-11-23T23:03:19Z,{},project3
4575,no,<p>Can you explain why you choose actions randomly? Shouldn&#39;t Q learner always choose actions to maximize its expected value? </p>,2019-11-22T06:04:37Z,52,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k39qpcxaz161y5,2019-11-22T06:04:37Z,{},project3
4576,no,<p>The paper stated that the Friend-Q was doen off-policy which as I understand it basically means it selects using any policy not necessarily the one it is learning. Random is easy. </p>,2019-11-22T18:13:54Z,52,Week 11/17 - 11/23,feedback,,i4lzw47xh1u57w,k3agr81036c5hu,2019-11-22T18:13:54Z,{},project3
4577,no,"<p>I dont know if you guys solved this, but I found that CVXOPT has a default tolerance, so its best to set your hard constraints to that, that way the solver does have a bad time.</p>
<p></p>
<p><a href=""https://stackoverflow.com/questions/12339425/python-cvxopt-ignores-constraints"">https://stackoverflow.com/questions/12339425/python-cvxopt-ignores-constraints</a></p>
<p></p>
<p></p>",2019-11-24T23:42:52Z,51,Week 11/24 - 11/30,followup,,is5x0kzrzjpg7,k3dndywvhaa3ib,2019-11-24T23:42:52Z,{},project3
4578,stud,Woah! That makes sense. Thanks very much for sharing.,2019-11-25T01:47:14Z,51,Week 11/24 - 11/30,feedback,a_0,,k3drtwyrs88629,2019-11-25T01:47:14Z,{},project3
4579,no,"<p>I would put this under implementation detail. From the perspective of the LP solver, anything within the precision tolerance is satisfied to be solution. I don&#39;t know the detail of the value you have, but 1e-6 is usually the range of precision error that a float number could have. In my case, I ignore any negative number below that range and re-normalize my solution.</p>
<p></p>
<p>Even in Littman 94 he mentioned the solution of the LP does not need to be accurate, it only needs to be good enough as an approximation, we are dealing with stochastic process anyway.</p>",2019-11-25T01:13:17Z,51,Week 11/24 - 11/30,followup,,jl5wq8mca7o0,k3dqm8u5gd15k1,2019-11-25T01:13:17Z,{},project3
4580,no,"<p>Here is the longitudinal view of the uCE-Q</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk398yb1at42q%2Fceq_epochsshare.png"" alt="""" /></p>",2019-11-21T21:47:49Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k398ygg43ob4sy,2019-11-21T21:47:49Z,{},project3
4581,no,"<p>1. Are both players μ<span style=""vertical-align:-0.336em""></span>CE, or just player A?</p>
<p></p>
<p>Yes. I am using CEQ for both players.</p>
<p></p>
<p>2. Can you provide some landmarks for your α<span style=""vertical-align:-0.09em""></span>, e.g., at what x-axis value does α<span style=""vertical-align:-0.09em""></span> hit 0.01, for example, so we have an idea for the convergence envelope.</p>
<p></p>
<p>You bet. I&#39;d give you guys all of it, if the staff would allow it.</p>
<p></p>
<p>666,683 - 666,697:</p>
<table width=""128"" cellspacing=""0"" cellpadding=""0"" border=""0""><tbody><tr height=""20""><td width=""64"" height=""20"">1.06E-08</td><td width=""64"">0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">1.03E-08</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">1.11E-08</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">1.16E-08</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">1.12E-08</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr><tr height=""20""><td height=""20"">0</td><td>0.01</td></tr></tbody></table>
<p></p>
<p>I rewrote my algo to use Greenwald&#39;s algorithm verbatim. I did not use anything from Littman. I recomputed the alpha decay to make 1 -&gt; 0.001 in 1e6 iterations. I had many conversations about LP with other students. Every conversation helps. This calculation took about 3 hours to complete I think.</p>
<p></p>",2019-11-21T21:54:44Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k3997cwptly3gi,2019-11-21T21:54:44Z,{},project3
4582,no,<p>Cool thanks!  I only asked for a landmark so that we know how to prepare our own graphs for comparison.</p>,2019-11-21T21:58:36Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k399ccbitnn33y,2019-11-21T21:58:36Z,{},project3
4583,no,"<p>Using the same parameters:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk39a2a4ufxcb%2FJacob.png"" alt="""" /></p>",2019-11-21T22:23:09Z,52,Week 11/17 - 11/23,followup,,jzfsa4a37jf4aq,k39a7wyrf0ctn,2019-11-21T22:23:09Z,{},project3
4584,no,<p>nice. My first one was using the default solver. Are you using glpk?</p>,2019-11-21T22:58:43Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39bhnb2q4t388,2019-11-21T22:58:43Z,{},project3
4585,no,"<p>Yep, only because it&#39;s like 5x faster.  On the downside, I have a whole bunch of hacks to deal with numerical instability, that I don&#39;t have when using the default solver.</p>
<p></p>
<p>The graph looks a lot better than it actually is, because of the high rate of decay of $$\alpha$$.</p>
<p></p>
<p>Try this:  $$\alpha = 0.1,$$ decay rate$$= 1.0$$.</p>
<p></p>
<p>It&#39;s not nearly as nice looking (for me), maybe yours will still look beautiful:</p>
<p></p>",2019-11-21T23:00:42Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39bk791pxz743,2019-11-21T23:00:42Z,{},project3
4586,no,<p>How are you plotting this? I&#39;m confused how to plot Player A q-values vs PlayerB q-values for various actions when my q-table is sectioned by states and they are separated (decentralized). Are these q-values you&#39;re plotting from a specific state? I feel like I&#39;m missing something here</p>,2019-11-21T23:28:50Z,52,Week 11/17 - 11/23,feedback,,j6m1jeidndu6wq,k39ckdo2q9w5im,2019-11-21T23:28:50Z,{},project3
4587,no,"<p>Each Q value represents a particular state-action pair.  The graphs in Figure 3 represent the state action &#34;pair&#34; (s, South, Stick).  The plots are for errors in those Q values over the run. See Section 5 of Greenwald 2003 for a description of the methodology for how these graphs were generated.</p>",2019-11-21T23:40:09Z,52,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39cyxe39fj10v,2019-11-21T23:40:09Z,{},project3
4588,no,<p>Relative to the starting position &#39;s&#39; as shown in Figure 4 of Greenwald 2003.</p>,2019-11-22T00:22:06Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39egvjmfpf5ft,2019-11-22T00:22:06Z,{},project3
4589,no,<p>Thanks guys!</p>,2019-11-22T02:01:32Z,52,Week 11/17 - 11/23,feedback,,j6m1jeidndu6wq,k39i0r36xb35be,2019-11-22T02:01:32Z,{},project3
4590,no,"<p>do we use the same error values on steps between the pair (s, South, Stick) in the plot? </p>",2019-11-22T02:49:59Z,52,Week 11/17 - 11/23,feedback,,j6ilcw6hxoc77h,k39jr1z8ods4z3,2019-11-22T02:49:59Z,{},project3
4591,no,"<p>Every iteration, you test the sample state of (s,South,Stick) to see if it has changed value, or how its value is changing.</p>",2019-11-22T03:43:32Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39lnx3g9jd767,2019-11-22T03:43:32Z,{},project3
4592,stud,"<p>In many steps, the state action pair (s, South,Stick) is not encountered and therefore not updated at all. So how do you handle this? My plot has a lot of zeros...</p>",2019-11-23T16:20:09Z,52,Week 11/17 - 11/23,feedback,a_0,,k3bs4sc1zpy558,2019-11-23T16:20:09Z,{},project3
4593,no,<p>I have a lot of zeros too</p>,2019-11-23T20:21:41Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3c0rehjwo14iu,2019-11-23T20:21:41Z,{},project3
4594,no,<p>Do you all get your Q table for uCE the same as Foe-Q? I can&#39;t get both Q tables the same and I can&#39;t figure which one is wrong.</p>,2019-11-22T23:39:49Z,52,Week 11/17 - 11/23,followup,,jc6xvgjncoey,k3asecyh8ta3bs,2019-11-22T23:39:49Z,{},project3
4595,no,<p>Are you doing (9) for sure? There was a discussion about how (10) might be what you are doing by mistake unless you add a slack variable to force all of the rationality constraints to be maximized.</p>,2019-11-22T23:47:37Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3asoded11f39y,2019-11-22T23:47:37Z,{},project3
4596,no,"<p>Jacob,</p>
<p></p>
<p>Sorry, I didn&#39;t get what you mean by (9) and (10).</p>
<p></p>
<p>Edited:</p>
<p></p>
<p>Maybe you refer to the different types of CE-Q. If so, I&#39;m using (9). I think my uCE is working (or at least my LP of my uCE is working), since it passes some test cases that Alejandro posted in another thread.</p>
<p></p>
<p></p>
<p></p>",2019-11-23T02:46:00Z,52,Week 11/17 - 11/23,feedback,,jc6xvgjncoey,k3az1sa4773787,2019-11-23T02:46:00Z,{},project3
4597,no,<p>That&#39;s correct. Eqs (9) - utilitarian CE and (10) egalitarian CE. Are we certain that chicken is uCEQ or is it eCEQ?</p>,2019-11-23T20:21:24Z,52,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3c0r121zhp455,2019-11-23T20:21:24Z,{},project3
4598,no,"<p>Jacob,</p>
<p></p>
<p>I think Greenwald-Hall didn&#39;t describe which one they used for chicken, I guess they could use either uCEQ or eCEQ.</p>
<p></p>
<p>I actually didn&#39;t get how your question is related to my first one.</p>",2019-11-24T02:15:51Z,51,Week 11/24 - 11/30,feedback,,jc6xvgjncoey,k3cdev6y8zj6zd,2019-11-24T02:15:51Z,{},project3
4599,no,"<p>&#34;Do you all get your Q table for uCE the same as Foe-Q&#34; - that depends on which algorithm (equation) you implement. If you are doing the minmax that is eq 10, then you might get foeQ. If you are doing the max-max that is eq 9, then I don&#39;t think you would get foe-q.</p>
<p></p>",2019-11-24T13:48:50Z,51,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d261jyw1h79o,2019-11-24T13:48:50Z,{},project3
4600,no,"<p>Why Q_A is not -Q_B?</p>
<p></p>
<p>In Foe-Q, V_A=-V_B and R_A=-R_B =&gt; Q_A=-Q_B</p>
<p></p>
<p>according to Greenwald: &#34;CE-Q learns Q-values (and policies) that coincide exactly with those of foe- Q.&#34; So, I expect Q_A=-Q_B for eCE-Q (as well as uCE-Q)</p>",2019-11-23T09:55:51Z,52,Week 11/17 - 11/23,followup,,jzhghvgidip601,k3beekzvl374kt,2019-11-23T09:55:51Z,{},project3
4601,no,"<p>That&#39;s a good point, and you should expect that as the simulation runs out to infinite steps.  That&#39;s also assuming that Greenwald&#39;s description of her algorithm is accurate.</p>
<p></p>
<p>if Q_A = -Q_B, then &#34;The sum of the rewards&#34;  would be zero, and we&#39;re back at Vahe&#39;s original observation - what&#39;s the point? Doing max(0) is always ... wait for it .... ZERO.</p>",2019-11-24T13:50:50Z,51,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d28mlou2i125,2019-11-24T13:50:50Z,{},project3
4602,no,"<p>Vahe, are you initializing the Q tables to zero or to 1 at the beginning?</p>",2019-11-24T02:12:22Z,51,Week 11/24 - 11/30,followup,,j6ll2xkiDJf,k3cdadowc683qx,2019-11-24T02:12:22Z,{},project3
4603,no,"<p>So, I&#39;ve found it doesn&#39;t matter, hardly at all, whether you initialize to $$0$$, $$1$$, or random values.  I&#39;ve tried all three.  Convergence takes slightly longer for me when I initialize to random values vs all zeros.  So depending on what your goals are (replicating the graphs, or converging fast), you have some freedom to see which you like.</p>
<p></p>
<p>Are you noticing drastic differences when you initialize differently?</p>",2019-11-24T02:31:21Z,51,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3cdysrk9u25w3,2019-11-24T02:31:21Z,{},project3
4604,no,"<p>no, I have used all zeros and ones. They differ but not that much. My problem is that my tests run along time so it is hard to do it by trial and error. I am using cvxopt which seems much slower than glpk that you are using? It too late any way. my QQ, Foe-Q, and Frnd-Q resemble the paper and I am currently running the uCE-Q which will take another 10 hours or so to complete! I am hopeful as time is running out!</p>",2019-11-24T02:49:37Z,51,Week 11/24 - 11/30,feedback,,j6ll2xkiDJf,k3cema3djsc1c0,2019-11-24T02:49:37Z,{},project3
4605,no,"<p>Yeah, fortunately, GLPK has no issues for Foe-Q, for me, so it&#39;s a no-brainer to use there since it&#39;s at least five times faster than the default solver.  For $$\mu$$CE, I have had to do some hacking to get it to behave.  I may run the regular solver for the graph that I include in my report, but, yeah, doing trial and error runs on something that takes hours to complete is no fun.</p>",2019-11-24T02:54:41Z,51,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3cessuqn5d6w5,2019-11-24T02:54:41Z,{},project3
4606,no,"<p>I graphed the uCE-Q algorithm over constant alphas. There is one - alpha = 0.6 - that seems to look pretty good as compared to their results (shape-wise that is). This is using a slack variable..... so $$row_i - row_{-i} &gt;= V$$</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3d20yd7n7vn%2Fceqalphasshare.png"" alt="""" /></p>",2019-11-24T13:45:00Z,51,Week 11/24 - 11/30,followup,,jc554vxmyuy3pt,k3d214f7z3w16y,2019-11-24T13:45:00Z,{},project3
4607,no,"<p>Then i also did the same survey for CEQ without the slack variable (egalitarian? or just P1 == P2).</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjc554vxmyuy3pt%2Fk3d4ol5k1hic%2Feceqalphasshare.png"" alt="""" /></p>",2019-11-24T14:59:21Z,51,Week 11/24 - 11/30,followup,,jc554vxmyuy3pt,k3d4oq54ja02s1,2019-11-24T14:59:21Z,{},project3
4608,no,<p>This looks great. Thanks for sharing. are you using CEQ for both players? or one is CEQ and the other is random? </p>,2019-11-24T15:13:45Z,51,Week 11/24 - 11/30,feedback,,j6ilcw6hxoc77h,k3d57994w8b4ow,2019-11-24T15:13:45Z,{},project3
4609,no,"<p>I am using player A for all action selections, and I am using argmax PI (not cumsum distribution of pi) to select that action. So always the action that is best suited for A.</p>",2019-11-24T15:24:50Z,51,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d5li1614o5e7,2019-11-24T15:24:50Z,{},project3
4610,no,"<p>Thanks. the paper appears to say uCE-Q. sorry, I was not able to follow all discussions. I use random player for both foeQ and ceQ charts, and neither appear to converge properly. The errors go down but that is simply driven by the alpha decay schedule, and I only realized the problem till i use fixed alpha value. </p>
<p></p>
<p>what do you use for Player B? the Q values for B have effect on the Pi, right? </p>",2019-11-24T15:29:59Z,51,Week 11/24 - 11/30,feedback,,j6ilcw6hxoc77h,k3d5s4ccnn51jy,2019-11-24T15:29:59Z,{},project3
4611,no,"<p>These are correlated, right? So each time we compute the LP equilibrium we are computing the joint action (a,b) so you can take the action from that tuple. You can also do CE vs CE with two independent Q calculations. I bet that would do the same thing as using the joint actions from player A&#39;s CE.</p>
<p></p>
<p>I am computing both in each iteration, just not using player B&#39;s.</p>",2019-11-24T15:32:08Z,51,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d5uwb88x36p2,2019-11-24T15:32:08Z,{},project3
4612,no,<p>Thanks. </p>,2019-11-24T15:40:34Z,51,Week 11/24 - 11/30,feedback,,j6ilcw6hxoc77h,k3d65qinkmt5xe,2019-11-24T15:40:34Z,{},project3
4613,no,"<p>The sum of the Q&#39;s for A and B would be zero, if you were either (1) seeing this at infinite iterations, or (2) using $$V_{2}(s) = -V_{1}(s)$$. There will always be some drift between the two player&#39;s Q matrices in a finite horizon because the MDPs may not be fully sampled.</p>
<p></p>",2019-11-22T03:42:23Z,52,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k39lmfyekhb5yd,2019-11-22T03:42:23Z,{},project3
4614,stud,"<p>Fair point! I suppose I could change the dynamics from the current indoor caged soccer-like rules (ball can be bounced off walls; defenders can bolster themselves against walls if advantageous) to regulation soccer rules (ball turned over to opponent if dribbled/kicked out of bounds; if the player not in possession leaves the field, opponent can beeline to goal, perhaps translated as a -100 to the player).</p>
<p></p>
<p>I just reread the relevant parts of Littman &#39;94, Greenwald &#39;03 &amp; &#39;05, and it doesn&#39;t seem like any of them explicitly specify rules governing player behavior adjacent to boundaries, so, I&#39;m guessing this is one of those decisions we get to make ourselves and justify in the paper.</p>",2019-11-21T22:58:57Z,41,Week 11/17 - 11/23,followup,a_0,,k39bhxzs5yp33g,2019-11-21T22:58:57Z,{},project3
4615,no,"<p>Thank you. I had the same question and setting ylimit definitely makes the graph bit similar to the paper.</p>
<p></p>
<p>I see values for foe-q ranging 0-100 based on reward=100 in a state. Others are seeing such values too ?</p>",2019-11-22T09:28:27Z,41,Week 11/17 - 11/23,followup,,jfzaqnqvtQ1m,k39xzhl867g3wh,2019-11-22T09:28:27Z,{},project3
4616,stud,"<p>I had forgotten to use Greenwald&#39;s update formula, where the reward is multiplied by (1 - gamma), instead of the standard/Littman&#39;s. With gamma = 0.9, the reward gets sizably shrunken.</p>",2019-11-23T04:00:06Z,41,Week 11/17 - 11/23,feedback,a_0,,k3b1p32tec15bd,2019-11-23T04:00:06Z,{},project3
4617,no,<p>Thank you. I would have never noticed it!</p>,2019-11-23T05:48:08Z,41,Week 11/17 - 11/23,feedback,,jfzaqnqvtQ1m,k3b5k03tz7l4ad,2019-11-23T05:48:08Z,{},project3
4618,no,"<p>When you say we are essentially doing Q(s,.) max(), you do mean the argmax right?</p>
<p></p>
<p>If I&#39;m understanding right, we are still doing argmax over the Q_tables, but in CE, the Q_tables are updated differently depending on the CE variants we use.</p>",2019-11-22T01:48:05Z,41,Week 11/17 - 11/23,followup,,k0bpd3l41jf63x,k39hjg9mqtn2d7,2019-11-22T01:48:05Z,{},project3
4619,no,"<p>The V(s) is calculated using a max value in mimimax and in CEQ. This has the effect of doing a Q.max(), but over all of the reachable states from $$(s,\vec{a})$$</p>",2019-11-22T03:38:49Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39lhv64tri5ts,2019-11-22T03:38:49Z,{},project3
4620,no,"<p>It wouldn&#39;t be argmax, right? Otherwise wouldn&#39;t that lead to a deterministic policy that can always be defeated. I think your action should be a probability distribution, and you choose it from this distribution. </p>",2019-11-22T02:02:35Z,41,Week 11/17 - 11/23,followup,,jl3we43d3bp15p,k39i23uvaoi4jv,2019-11-22T02:02:35Z,{},project3
4621,no,"<p>Yeah, I think so. We should update pi (like in Litman 1994) and chose an action based on this pi - probability distribution for all actions for single player</p>",2019-11-22T02:07:30Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39i8f5247o4ls,2019-11-22T02:07:30Z,{},project3
4622,no,"<p>Yeah, that&#39;s another point of confusion. Littman says to choose a with probability p. Does that mean the max p gives you a, or does that mean you choose a RANDOM action with probability p...</p>",2019-11-22T03:39:41Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k39liyls6w26tv,2019-11-22T03:39:41Z,{},project3
4623,no,"<p>Jacob, I think you need to pick randomly with probability p. If you pick max p, then you don&#39;t pick actions based on probability, you pick them deterministically (where you pick max(p) with probability=1, basically you always pick max(p) which is incorrect) </p>",2019-11-22T05:27:41Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k39pduz7t3t1te,2019-11-22T05:27:41Z,{},project3
4624,no,"<p>Jacob, could you answer the above question directly? I agree with Aida, and seems like you are talking about something else I don&#39;t quite get. Thanks. </p>",2019-11-22T05:56:54Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39qfffr8e26mz,2019-11-22T05:56:54Z,{},project3
4625,no,"<p>I think the whole point of finding an equilibrium, mixed strategy (via linear programming), is to actually <em>use</em> it.  This means picking each of the possible next actions with the probability assigned to it by your equilibrium solution.</p>",2019-11-22T06:01:02Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39qkr9lrbx202,2019-11-22T06:01:02Z,{},project3
4626,no,"<p>Sounds good. another related questions is for Friend Q, how do you choose actions? Currently my friend Q always shows 0 error expect a few spikes. Any idea why it does that/ </p>",2019-11-22T06:06:06Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k39qr9q0kti6i8,2019-11-22T06:06:06Z,{},project3
4627,no,<p>Because it&#39;s converging to the correct Q values very quickly?</p>,2019-11-22T06:26:13Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39rh4kgf2367g,2019-11-22T06:26:13Z,{},project3
4628,no,"<p>&#64;Zhenning Tan, I wish I had a definitive answer. I&#39;ve been doing the pi.max() action selection. Can&#39;t say that&#39;s been all that successful.</p>",2019-11-22T17:04:24Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3ae9u5rvtd28j,2019-11-22T17:04:24Z,{},project3
4629,no,"<p>Thanks. based on the above instructors&#39; answer, looks like we can just choose randomly. at the end of the day, we just want to see learners to converge. I think it doesn&#39;t matter if we follow policy or completely random. we are just updating the table. correct me if I am wrong. </p>",2019-11-22T19:07:23Z,41,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k3ainznrs28nq,2019-11-22T19:07:23Z,{},project3
4630,no,"<p>Yeah I agree.  For off-policy training, it shouldn&#39;t matter how we choose the next action.</p>",2019-11-22T19:50:22Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3ak79riahd73b,2019-11-22T19:50:22Z,{},project3
4631,no,"<p>When I switched my Foe-Q to using the max action (not distributed most likely), it behaved much better. I agree, though, that it should not matter, so long as it&#39;s not random.</p>",2019-11-22T21:56:35Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3aopl9zdv75o6,2019-11-22T21:56:35Z,{},project3
4632,no,"<p>nevermind. my password was expired and I had to renew it!</p>
<p></p>",2019-11-22T01:44:37Z,52,Week 11/17 - 11/23,followup,,j6ll2xkiDJf,k39hezdfjlk4t8,2019-11-22T01:44:37Z,{},logistics
4633,stud,Would these grades happen to be missing the &#43;10 points if we had a question accepted for James&#39; office hours?,2019-11-22T12:07:18Z,24,Week 11/17 - 11/23,followup,a_0,,k3a3ns0zveo45c,2019-11-22T12:07:18Z,{},project2
4634,no,<p>No we will fix that.</p>,2019-11-23T00:35:27Z,24,Week 11/17 - 11/23,feedback,,hz7meu55mi8sd,k3audw3pi8n2h3,2019-11-23T00:35:27Z,{},project2
4635,no,<p>I&#39;ll work on that this weekend. Thanks for the note.</p>,2019-11-27T16:50:56Z,23,Week 11/24 - 11/30,feedback,,hyx9thiqa6j4nn,k3hizscvknf5gk,2019-11-27T16:50:56Z,{},project2
4636,stud,<p>Friendly reminder it appears this is still pending.</p>,2019-12-06T20:28:43Z,22,Week 12/1 - 12/7,feedback,a_1,,k3ulqillnhi26s,2019-12-06T20:28:43Z,{},project2
4637,no,"<p>Yeah, sorry about this. I had my youngest one at the hospital, with an ambulance ride and everything. It&#39;ll tackle this shortly.</p>",2019-12-06T23:05:52Z,22,Week 12/1 - 12/7,feedback,,hyx9thiqa6j4nn,k3urcmd2kkp4ru,2019-12-06T23:05:52Z,{},project2
4638,stud,<p>Yikes -- sending positive thoughts for your youngest to be well soon.</p>,2019-12-06T23:11:56Z,22,Week 12/1 - 12/7,feedback,a_1,,k3urkfdpy6s2yp,2019-12-06T23:11:56Z,{},project2
4639,no,<p>Sorry to hear this Miguel!</p>,2019-12-06T23:44:07Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3uspt4ibfn1j4,2019-12-06T23:44:07Z,{},project2
4640,no,"<p>Thanks, Sergei. He&#39;s good now, it seems to have been more a big scare than anything. Time will tell.</p>",2019-12-06T23:51:43Z,22,Week 12/1 - 12/7,feedback,,hyx9thiqa6j4nn,k3uszkrwsw55rg,2019-12-06T23:51:43Z,{},project2
4641,no,<p>ISA.</p>,2019-12-07T20:19:42Z,22,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3w0us2k5wb7jc,2019-12-07T20:19:42Z,{},project2
4642,no,<p>Sorry to hear about this as well. Hope all the best for your family!</p>,2019-12-08T07:39:13Z,22,Week 12/1 - 12/7,feedback,,jl5wq8mca7o0,k3wp4myx47p6kk,2019-12-08T07:39:13Z,{},project2
4643,no,"<p>Thanks, that&#39;s why I understand as well. Looks like we need to choose the action based on the distribution solved by linear programing. </p>",2019-11-22T05:44:12Z,34,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k39pz3vkkln55r,2019-11-22T05:44:12Z,{},project3
4644,no,"<p>Ups, sorry. I didn&#39;t read carefully your question and replayed. Now I&#39;m trying to understand as well how to update Vs and your question actually made it little bit clear.</p>",2019-11-22T20:23:59Z,34,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3alei4x81j5l1,2019-11-22T20:23:59Z,{},project3
4645,no,<p>I also saw another option that pick the action with highest probability. I don&#39;t have any change/update of Q table using current method. Not sure if picking action from probabilistic distribution is wrong. </p>,2019-11-22T07:20:24Z,34,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k39tet8pcpx5l4,2019-11-22T07:20:24Z,{},project3
4646,no,"<p>update: here is one of the A matrix used to solve linear programing. It&#39;s not symmetric. How come? Basically I take the Q[s, :] which is 25 element and reshape it to 5x5, then pad with some values to suit linear programing calculation. </p>
<pre>[[ 1.          0.          0.          0.         28.30867691  0.        ]
 [ 1.          0.          0.          0.         28.88083912  0.        ]
 [ 1.          0.          0.          0.         28.05491556  0.        ]
 [ 1.          0.          0.          0.         29.46456561  0.        ]
 [ 1.          0.          0.          0.         73.11248898  0.        ]
 [ 0.          1.          1.          1.          1.          1.        ]
 [ 0.         -1.         -1.         -1.         -1.         -1.        ]]</pre>
<p></p>",2019-11-22T07:36:37Z,34,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k39tznrf2bt142,2019-11-22T07:36:37Z,{},project3
4647,no,"<p>Hi Folks,</p>
<p>Does it means we maintain 2 Q tables here ?  Else otherwise, we will end up with same action probabilities ?</p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>
<p></p>",2019-11-23T06:42:41Z,34,Week 11/17 - 11/23,followup,,jfzaqnqvtQ1m,k3b7i5r7lni47a,2019-11-23T06:42:41Z,{},project3
4648,no,"<p>I have two Q tables, but I am not done yet this is my understanding though</p>",2019-11-23T09:50:12Z,34,Week 11/17 - 11/23,feedback,,jc6mqevhagl262,k3be7b4gztg5s1,2019-11-23T09:50:12Z,{},project3
4649,no,"<p>I realized that Foe-Q is off-policy. So we have to pick an action based on Q tables, not based on distribution. Same for CE-Q, but our Q-tables is num_states, 25. So do we need to decode after? That&#39;s kinda doesn&#39;t make sense. I see some people do argmax(pi). Not sure how it related though to off-policy, but even it&#39;s then for ce-Q is again 25 array. So we decode again? </p>",2019-11-23T21:21:01Z,34,Week 11/17 - 11/23,followup,,jzih0fdt4sn1cq,k3c2vp4wy3e1xw,2019-11-23T21:21:01Z,{},project3
4650,no,what&#39;s definition of off policy? it means that you can use any behaviour policy for data sampling for training right. ,2019-11-23T21:29:53Z,34,Week 11/17 - 11/23,feedback,,is5gzbotXmz,k3c373i1rm9he,2019-11-23T21:29:53Z,{},project3
4651,no,"<p>oh, right. Thanks!</p>",2019-11-23T21:47:06Z,34,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3c3t8onz4s53r,2019-11-23T21:47:06Z,{},project3
4652,no,"<p>So my Q-tables for friend-Q vs friend-Q are not symmetrical and they are not Q_a != -Q_b and V_a != -V_b, because let&#39;s say they are symmetrical at some point (we initialize Qs and Vs as 0). Now at the next step, we are at the state s wher Q_a[s] = [0,0...0,7,0..0] and Q_b = [0..0,-7,0...0]. Then V_a will become V_a=7 and V_b=0. So our update for Qs will be different which makes Q tables and V tables are not symmetrical at all for friend-Q algorithm.</p>
<p></p>
<p>I haven&#39;t checked for foe and ce-Q yet.</p>",2019-11-22T07:49:47Z,41,Week 11/17 - 11/23,followup,,jzih0fdt4sn1cq,k39uglp6rg342g,2019-11-22T07:49:47Z,{},project3
4653,no,"<p>I believe you are correct. (what I mentioned in OH about symmetry is incorrect). I tested this on all 3 friend, foe, uCEq.</p>",2019-11-22T09:05:53Z,41,Week 11/17 - 11/23,feedback,,jl1acpoc4HA9,k39x6gj5wrt7r,2019-11-22T09:05:53Z,{},project3
4654,no,"<p>Thanks, Farrukh!</p>",2019-11-22T20:19:01Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3al84j96v16p4,2019-11-22T20:19:01Z,{},project3
4655,stud,"<p>Thanks for sharing. Very interesting. </p>
<p></p>
<p>I am not sure I understand &#34;When doing the project, I didn&#39;t quite realize how sensitive the performance of a particular set of weights can be as a function of time.&#34; Did you mean that the scores in testing on the saved weights have big standard deviation? </p>",2019-11-22T05:30:19Z,18,Week 11/17 - 11/23,followup,a_0,,k39ph94ffni2gc,2019-11-22T05:30:19Z,{},project2
4656,stud,"<p>&#34;Sometimes, even weights from two adjacent time steps can be of substantially different quality, even when the rewards from those two time steps are both high,&#34; </p>
<p></p>
<p>How did you define quality? Mean score on testing?</p>
<p></p>
<p>For the above, did you mean that the two sets of weights can have very different performance for a specific starting state and seed? </p>
<p></p>
<p>I think sometimes we can turn this around onto its head... </p>
<p></p>
<p>Thanks again for sharing.</p>",2019-11-22T05:35:26Z,18,Week 11/17 - 11/23,feedback,a_0,,k39pnu155xsb7,2019-11-22T05:35:26Z,{},project2
4657,no,"<p>It is true that the scores in testing on the saved weights have big standard deviation.  My point went beyond that - weights from parts of the training run that look similar, or weights from adjacent or close-to-adjacent time steps in the training run, could have quite different mean rewards during testing.</p>
<p></p>
<p>So, saving weights from one particular part of your training run - say the part that had the highest 100-episode trailing mean, is a vastly inferior strategy to saving not only that weight, but maybe 10 weights on either side of that weight, and testing them all.  Maybe this is obvious, but as someone completely new to reinforcement learning, I hadn&#39;t really considered my strategy for saving weights, and it&#39;s not until after project 2 finished that I realized that just picking, for example, the final weights of your training run, is a pretty silly strategy.</p>
<p></p>
<p>&#34;<em>How did you define quality? Mean score on testing?&#34;</em></p>
<p></p>
<p>Yes, that was my metric.</p>",2019-11-22T05:37:07Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39ppzozvvp1r8,2019-11-22T05:37:07Z,{},project2
4658,no,"<p>&#34;<em>For the above, did you mean that the two sets of weights can have very different performance for a specific starting state and seed?</em>&#34;</p>
<p></p>
<p>As you can see from the graph for the trained agent, the results are extremely stable when not learning or exploring.  You don&#39;t really need to set seeds to replicate the mean performance as long as you run it long enough (law of large numbers)  100k episodes is pretty long.</p>",2019-11-22T05:40:17Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39pu1ztdna6xa,2019-11-22T05:40:17Z,{},project2
4659,no,"<p>Also, to clarify.  The first graph, of the 293.1, is pretty meaningless to me.  I only included it because that&#39;s what Project 2 asks for.  It&#39;s the second graph that matters - the <em>true</em> mean reward of the trained agent (or a good estimate of it).</p>",2019-11-22T05:52:50Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39qa74h84v3b9,2019-11-22T05:52:50Z,{},project2
4660,stud,<p>I agree. It would be interesting to make it a histogram. </p>,2019-11-22T05:54:24Z,18,Week 11/17 - 11/23,feedback,a_0,,k39qc8423bz8c,2019-11-22T05:54:24Z,{},project2
4661,no,"<p>I ran two more 100k episode testing runs.  Here are the histograms (first one&#39;s y-axis was mistitled):</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3asa82y3u3s%2FRun1_DeployedHist20591_1114_023242.png"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3asb3hjfoo9%2FRun2_DeployedHist20591_1114_023242.png"" alt="""" /></p>
<p></p>
<p></p>
<p>Incidentally, a new single-episode high reward of 334.85 was observed in the first of these runs.</p>",2019-11-22T23:38:45Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3asczhkukk4ni,2019-11-22T23:38:45Z,{},project2
4662,no,"<p>That&#39;s awesome.  My issue was trying to eliminate those pesky bad landing, those tiny blue dots on the left of your graph which represent cases when things don&#39;t go as planned ...</p>
<p>Any idea how to do that?</p>",2019-11-23T00:50:57Z,18,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k3auxu8e7655a8,2019-11-23T00:50:57Z,{},project2
4663,no,"<p>1. It may not be possible without sacrificing quite a bit on the mean score.</p>
<p></p>
<p>2. There are different levels of bad landings.  For example, almost none of mine have negative scores, and most are in the 25-125 range.</p>
<p></p>
<p>3. I&#39;m pretty close.  I think it&#39;s misleading to treat the presence of &#39;bad landings&#39; as a binary phenomenon (you either have them or you don&#39;t).  Reducing their frequency is, I think, the main engineering challenge, not eliminating every single one.  That&#39;s why I think the mean reward (over all 100k episodes, not just 100) is such a good metric.  Getting that mean from 250, to 260, to 270, to 280, etc. are major milestones that, I think, are more representative of the quality of the lander.</p>
<p></p>
<p>4. The time horizon of testing matters.  For example, if you want to see a graph with no bad landings, just look at the first one at the top of the page.  It&#39;s easy for me to generate many of those, even over a thousand episodes, because my non-perfect landings are so rare.</p>",2019-11-23T01:17:31Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3avvzn5f2x5x,2019-11-23T01:17:31Z,{},project2
4664,no,"<p>I agree but if AI wants to replace people one day, one would think we should be able to land something &#39;so&#39; simple without ever crashing or missing the landing area (or not being able to get back to it).</p>
<p>I wish I had the time to run 100k episodes.  Maybe after the finals, because this project is really interesting.  </p>",2019-11-23T01:25:34Z,18,Week 11/17 - 11/23,feedback,,jzh6k6o994a6dh,k3aw6ccylvv20w,2019-11-23T01:25:34Z,{},project2
4665,no,"<p>I think landing without crashing is probably easier if you allow for much lower average scores.  You have to remember that Lunar Lander feeds us a reward structure that we&#39;re then using as an input to learning.  Maximizing expected future rewards != not crashing.</p>
<p></p>
<p>If you don&#39;t care about rewards as defined by OpenAI and only care about crashing, then I think a better option is to work on creating your own custom Lunar Lander environment where crashing is more costly, but one where learning is still possible.  It&#39;s a non-trivial balancing act.  I&#39;m sure the folks at OpenAI put a lot of work into creating environments and reward structures that actually work for learning.</p>",2019-11-23T01:44:11Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3awuaiylew44v,2019-11-23T01:44:11Z,{},project2
4666,no,"<p>It&#39;s interesting to see your results. My mean was 274 when evaluating over 100 consecutive runs using DDQN, from weights taken after training for ~6700 episodes. I&#39;m sure I could have gotten a bit better, but it took 4-5 hours each cycle to run, so I could only do so many hyper param tests. My training graph was much less stable then your run was.</p>",2019-11-22T05:31:27Z,18,Week 11/17 - 11/23,followup,,ixty1midfufhd,k39pipnhufi5tw,2019-11-22T05:31:27Z,{},project2
4667,no,<p>Possibly a benefit of avoiding the deadly triad - SARSA is on-policy</p>,2019-11-22T05:37:52Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39pqyewy4u2i3,2019-11-22T05:37:52Z,{},project2
4668,no,"<p>Vahe, thank you so much for your thoughts and exploration of the Lunar Lander environment, but also for your discussions in the class as a whole. I feel like I would not have the understanding of reinforcement learning I do without your contributions and I greatly appreciate that. </p>
<p></p>
<p>Unfortunately I have been too busy focusing on completing the pending assignment at hand to keep analyzing previous projects, but you bring up a point that I am very interested in moving forward: further exploring existing solutions. I feel there is a great deal to be learned by further optimizing our solutions for the problems presented. However; I am also very interested in how we can best utilize existing solutions to solve new problems. We can develop and tune the best possible solution to the lunar lander problem, but how easily will this solution hold up to other, unknown or unexplored environments? What can we do with the solutions we developed for the Lunar Lander project to make it applicable in other environments? Once I&#39;ve wrapped up project 3 and taken the final, I&#39;d like to throw my lunar lander agent at the bipedal walker environment in open AI gym to see how much variation is necessary to get something semi functional. I know this class has focused heavily on developing agents for existing environments but I&#39;m also wondering what sort of generality we can explore with our developed agents across different pre-existing or real-world environments? Project 3 has been a step in the direction of also building the environment for the agents, but I have been wondering if there was general sense for utilizing/modifying the agents we&#39;ve learned about and developed in a more real world scenario? I think it was mentioned earlier in the course that the focus in this course was more on developing agents but I feel a more general discussion on environment development could be useful for moving forward.</p>",2019-11-22T06:17:01Z,18,Week 11/17 - 11/23,followup,,jqi9pwn26my6g2,k39r5avhcmp4js,2019-11-22T06:17:01Z,{},project2
4669,no,<p>I realized this is less applicable to the original discussion and probably better served as its own conversation thread. I will repost my questions here in their own thread tomorrow.</p>,2019-11-22T06:20:45Z,18,Week 11/17 - 11/23,feedback,,jqi9pwn26my6g2,k39ra3o1iogc1,2019-11-22T06:20:45Z,{},project2
4670,no,"<p>No, I&#39;m glad you posted.  I think your desires are more general and in a sense more important than the focus of this thread.  With regard to your wish that we did more of certain aspects of RL, at least for me, I&#39;ve been drinking from a firehose this semester.  I can&#39;t imagine learning very much more than I have, especially given the immense amount of time I&#39;ve put into this class.</p>
<p></p>
<p>I will say that one positive side-effect of optimizing an existing solution like Lunar Lander (for us newbies) is that we can still learn a lot that will help us for those grander plans that you have.  For instance, after tweaking Lunar Lander, I feel like I&#39;m much better prepared to adapt existing agents to other problems.  If, say, we were already researchers in RL, then I would probably be less enthralled with dumping a lot of time into &#34;toy&#34; environments.  But I feel like there&#39;s still a lot I don&#39;t know, as well as a lot I don&#39;t realize I don&#39;t know.</p>",2019-11-22T06:36:38Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k39ruj9spek3tc,2019-11-22T06:36:38Z,{},project2
4671,no,"<p>Oh, I agree 100%. There&#39;s also been a firehose directed my direction this semester (also totally a newbie). It&#39;s not that I wish we&#39;d done more of certain aspects, I definitely couldn&#39;t have handled that with everything else. I&#39;m more interested in figuring out how to best continue learning moving forward and I know there are certain aspects I&#39;d like to further explore. This has been my most interesting and favorite course in the program so far, so I was more interested in a general &#34;next steps&#34; type conversation which felt like it could be its own thread because I agree entirely that the things I want to look into next are such an incredibly small portion of the things I don&#39;t know. </p>",2019-11-22T07:13:33Z,18,Week 11/17 - 11/23,feedback,,jqi9pwn26my6g2,k39t60e9x2ypf,2019-11-22T07:13:33Z,{},project2
4672,no,"hey &#64;Vahe, those are some nice graphs and some dedication to run all of those episodes! I have a few things to say that might be helped with respect to the stability of the network validation test. While the amount of parameters in the NN may be Daunting, I think it is somewhat more of an art than a science trying to understand how many layers, where and when, what activations, what loss function, what optimizer ... oh boy we could go all day. Not to mention the hyperparams like learning rate and epsilon, batch size, etc. I think a good chunk of my time was spent “honing in” on the right parameters. I do this every day for my research trying to improve image classifiers AUC (accuracy metric) just by one or two % once it seems like they’ve peaked out (these models have gained 1-2% in the past 2-3 months and in the prior 3 months they went from 70-88%. However, I can say one thing I’ve learned is that there is a realm of hyperparameters where the algorithm doesn’t converge, and no matter how much you change the architecture or tweak params it will appear hopeless. I think a good method is going to a safe zone and starting with a very low learning rate and high epsilon annealing, and observe a slow learner and ramp it up. In this realm your changes and tweaks \textit{will} make a difference tha is noticeable and not seemingly hopeless as it appears to be in stochastic naughty nets. Do everything you can to smooth it out and help it. I did some preprocessing to ensure batches had samples from all over the state space, high epsilon, even having several nets precede the main one and contribute to it as teacher nets (I averaged 3-5 networks parameters where each was trained no longer than 200 episodes).<p></p><div><br /></div><div><p></p><img src=""https://piazza.com/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzj4sh1p7pf5af%2Ffarsvbrvdvyx%2Fpublic_2019_11_22.png"" /><br /></div><div><br /></div><div>Y axis is normalized regret (best possible score in those 500 epochs-actual score)/100 </div><div>X axis is number of episodes to look into the future before stopping and saving the peak and different lines represent different number of sample epochs considered as a smoothed peak. </div><div><br /></div><div><br /></div><div>For my research I prepared these graphs of the image classification nets showing the best early stopping methods. Averaging a test score at every epoch over 10-20 of them, saving the net at the midpoint episode at peak value, and only saving it if no better moving average is encountered for 50 epochs into the future, provided the best results for me. (This is for image classification, so really I was averaging over 10-20 validation set scores and comparing those, and then getting the best possible results on my test set. This study was retroactive so I saved 500 epochs test set and validation set scores, and simulated all of these scenarios. Although the Lunar Lander problem is different, maybe testing a lander after every episode for just one episode can provide some measure of how stable the net, and save it when these scores have a certain average rather than a peak.</div><div><br /></div><div>Stability is key, even if you gotta let the laptop run for four days it may be worth it :)</div><p></p>",2019-11-22T08:22:33Z,18,Week 11/17 - 11/23,followup,,jzj4sh1p7pf5af,k39vmqmvlzb3di,2019-11-22T08:22:33Z,{},project2
4673,no,"<p>Daniel,</p>
<p></p>
<p>Thanks for all the advice.  I definitely should start to use more sophisticated algorithms for saving the weights.  I used just the 100-episode trailing average as my metric, and sometimes, the best weights came from the leadup to the peak of that metric, or even from the come-down after the peak.</p>
<p></p>
<p>You&#39;re right that stability is very important for Lunar Lander.  In my best solutions (like the one in the graphs above), the Lander is almost perfectly stable.  If you notice, there are essentially no negative scores, and all the sub-230s are fluky.  The issue here is that, if the lander wants to score above 300, it needs to land quickly, and once in a blue moon it will land just slightly too quickly, a consequence of living on the knife&#39;s edge.  There&#39;s a very big difference between a lander that averages 275 and 280.  And a huge difference between one that averages 280 and one that averages 285.</p>
<p></p>
<p>Your job sounds cool - I also like the process of optimization: squeezing those few percentage points out over time, moving to regions of better and better local optima, and in the process gaining better understanding of the underlying problem, definitely sounds like a lot of fun.</p>
<p></p>
<p>I had a little experience with supervised learning from taking Andrew Ng&#39;s Coursera course (which I loved btw) over the summer, and that actually helped me a lot in terms of intuition for the deep-learning part of Project 2.  Stability in SL is a cakewalk compared to what we have to deal with in RL, don&#39;t you think?</p>
<p></p>
<p>I actually did what you suggested for my project, which was to try to hone in on regions of hyperparameter settings that made intuitive sense, but there was still so much that was left untested because of the sheer search space.</p>
<p></p>
<p>For example:</p>
<p></p>
<p>1. I had almost equally good results with a single layer (228) node NN as with a two layer (44,44) one.  But who knows what else could have worked?</p>
<p></p>
<p>2. I sampled the s**t out of my experience tuples for the memory replay.  My minibatch size was 128 (the DQN paper used 16), so I basically turned my RL problem into a supervised learning problem where every MDP transition was learned on hundreds of times.  As a corollary, I found that L2 regularization helped - maybe it&#39;s because of how many epochs each transition went through.  But here too - how much larger could I have made my batch size?  How much does tweaking the regularization amount help?  Also, I split my minibatch of 128 into 8 mini-minibatches of 16, to speed up my runs.  Would it have performed better with pure SGD?  A larger mini-minibatch size?  I don&#39;t know.</p>
<p></p>
<p>3. I never even deviated from MSE for a loss function.  I wonder if I was missing out there.</p>
<p></p>
<p>4. I used Adam, which kind of alleviated some of the pressure with learning rates, since it adapts.  I just chose a very small value: 0.00025.  But even here, I noticed that if I made it too small, I would get degradation in performance.</p>
<p></p>
<p>There were many other unexplored ideas, and even with all that, the average score for my trained agent was 279, within striking distance of the SARSA solution that I have above.</p>
<p></p>
<p>I do wonder though, if the extra complexity of DQN vs SARSA really results in better performance.  I could never prove it either way, but maybe I&#39;ll get back to my DQN solution and try optimizing that some more.</p>",2019-11-22T21:08:49Z,18,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3an05my39h2l9,2019-11-22T21:08:49Z,{},project2
4674,no,<p>Thank you very much guys for sharing this information!</p>,2019-11-22T16:45:57Z,18,Week 11/17 - 11/23,followup,,jqkxzdmmolGf,k3adm3zy6m05xf,2019-11-22T16:45:57Z,{},project2
4675,no,"<p>Thanks for the feedback Vahe.  I&#39;m impressed Sarsa worked.  </p>
<p></p>
<p>For me, I tricked DQN to learn the heuristic function first, it converged really really quick (a few hundred episodes in an hour), then it plateaus (prob the limit of the heuristic model), so I let it fly solo at episode 1,000, and it kept improving.   </p>
<p>Even after training for 2-3,000 episodes, I couldn&#39;t iron out the crash in 1000 flights and the 7-8 sub-par landings.  </p>
<p>It&#39;s probably because those cases are rare, and if I had had the time, I should have logged the difficult cases and forced DQN to learn from them... </p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2cup76148ef%2Ffull_simulation_with_rewards_plot_using__3300262_5h2.png"" alt="""" />   <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk2c083gnsihg%2F1000_rewards_with_NN3300_with_seed_7007.png"" alt="""" /></p>
<p>I did the simulations from using the Q matrix after 2300 episodes, but it&#39;s really intriguing to see that, if I let the algo run a few more thousand episodes, there are a few very bad &#39;dips&#39; and then it recovers, but the performance decreases slightly too...</p>
<p>I couldn&#39;t explain it.  </p>
<p>After the deadline, I kept trying tweaking hyperparameters, neural net sizes, and there were many cases where it couldn&#39;t recover when I dropped the heuristic training.</p>
<p>The score average kept going down no matter what... I think that if it doesn&#39;t recover quickly, maybe it gets the replay memory totally corrupted... but of course, I never let it try past, say, 5-7,000 episodes because it was taking too much time.  </p>
<p>Interesting nonetheless.</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj1ac98y0q%2Ftests_full_20_hours.png"" alt="""" /></p>
<p>Yet, I had some awesome landings and impressive recoveries.  </p>
<p>What was amazing is to see new behaviors appear the more the algo was learning.</p>
<p>For instance, there were many cases where the lander was landing off base and trying to use its side propellers to get to the landing area, but they are not strong enough.</p>
<p>So, after a while, I noticed that the lander was using its booster and tilting a bit to more horizontally.  </p>
<p></p>
<p>I really loved this project, and learned a lot.  </p>
<p></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj543j8u1g%2Fopenaigym.video.0.21643.video000000.gif"" alt="""" />  <img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj6gd4hvzd%2Fopenaigym.video.0.26073.video000008.mp4"" alt="""" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj78c7oqg5%2Fopenaigym.video.0.26073.video000008.mp4"" alt="""" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj6gd4hvzd%2Fopenaigym.video.0.26073.video000008.mp4"" alt="""" /><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3ajbk2wzk14%2Fopenaigym.video.0.71871.video000000.mp4"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzh6k6o994a6dh%2Fk3aj8gzpgqza%2Fopenaigym.video.0.26073.video000008.mp4"" alt="""" /></p>",2019-11-22T19:33:59Z,18,Week 11/17 - 11/23,followup,,jzh6k6o994a6dh,k3ajm7exv3d5qv,2019-11-22T19:33:59Z,{},project2
4676,no,"<p>I believe each player should know the other players&#39; location, but I&#39;m not 100% sure, maybe only 95%</p>",2019-11-23T01:40:47Z,41,Week 11/17 - 11/23,followup,,jl284xdcifz44g,k3awpwwqsj31pf,2019-11-23T01:40:47Z,{},project3
4677,no,<p>Do you do alpha decay? </p>,2019-11-22T19:04:18Z,41,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k3aik1hllix2ym,2019-11-22T19:04:18Z,{},project3
4678,no,"<p>No, that&#39;s his whole point.  He froze $$\alpha$$ to various values and observed strange behavior for a constant $$\alpha = 0.07$$.</p>",2019-11-22T19:48:33Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3ak4xrxb2l3mx,2019-11-22T19:48:33Z,{},project3
4679,no,"<p>That is correct, but in this case, the aberrant behavior was from 6,000,000 iterations of the solver (well, more like 6 billion if you count the internal iterations). I think it suffered some kind of overflow, which I suspect was memory related. But it could have been numerical overflow as I think about it more.</p>",2019-11-22T21:53:28Z,41,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3aolkremyu2jc,2019-11-22T21:53:28Z,{},project3
4680,no,<p>What I also found was that glpk consistently locked-up in an infinite loop at a specific iteration. Turns out that every solver had an issue with the LP at this iteration and only glpk crashed on it. I did not capture the state of the LP when this happened.</p>,2019-11-23T20:23:29Z,41,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k3c0tprfhw164q,2019-11-23T20:23:29Z,{},project3
4681,no,<p>I think I had this issue once and I found out that I should have transposed my Q_table when creating the matrix A in the Linear Programming. Check if you also need to transpose it.</p>,2019-11-22T23:31:29Z,33,Week 11/17 - 11/23,followup,,jc6xvgjncoey,k3as3mw3pwd4x7,2019-11-22T23:31:29Z,{},project3
4682,no,"<p>No luck, but thanks for the suggestion</p>",2019-11-23T01:39:15Z,33,Week 11/17 - 11/23,feedback,,jl284xdcifz44g,k3awny0nkw1751,2019-11-23T01:39:15Z,{},project3
4683,stud,<p>Have you found the exact reason now? I am having the same issue</p>,2019-11-24T11:07:25Z,33,Week 11/17 - 11/23,feedback,a_0,,k3cwegjnrd33ae,2019-11-24T11:07:25Z,{},project3
4684,no,"In my case, this problem appeared when I used the wrong reward when updating Q2. Assuming you&#39;re using separate Q tables for player 1 and 2, and updating them separately, then beware that the reward used to update Q2 needs to be the player 2&#39;s reward, and which is not the same as player 1&#39;s reward.",2019-11-25T04:33:00Z,32,Week 11/24 - 11/30,followup,,hvoy5qp1cnk4xx,k3dxr32wb76774,2019-11-25T04:33:00Z,{},project3
4685,stud,"<p>They are negate of each other, right? I am still trying to resolve this...</p>",2019-11-25T10:32:39Z,32,Week 11/24 - 11/30,feedback,a_0,,k3ealljplwl71x,2019-11-25T10:32:39Z,{},project3
4686,no,Correct.,2019-11-25T12:00:01Z,32,Week 11/24 - 11/30,feedback,,hvoy5qp1cnk4xx,k3edpytmyki6bv,2019-11-25T12:00:01Z,{},project3
4687,no,"<p>There was a discussion on Slack about archiving Piazza posts off-line and someone mentioned that it could be against class policy, so I would get a reply from instructors first. If I understood correctly technically it&#39;s possible (although not as straightforward as just pressing a Download button).</p>",2019-11-23T00:17:08Z,48,Week 11/17 - 11/23,followup,,jqkxzdmmolGf,k3atqbwfs664u5,2019-11-23T00:17:08Z,{},project3
4688,no,"<p>Just as a note, All my previous class piazza threads are still available for me to go browse as long as I had a link to the piazza thread, even well after I finished the class. So they might keep the piazza threads up for quite a while. Although I&#39;m not sure what will happen once I graduate. So there likely is not a need for an offline archive. </p>",2019-11-23T02:15:09Z,48,Week 11/17 - 11/23,followup,,ixty1midfufhd,k3axy3z751l73n,2019-11-23T02:15:09Z,{},project3
4689,no,"<p>does this mean, I need to bookmark every piazza thread? or can we access the forum as-is and browse?</p>",2019-11-23T23:08:11Z,48,Week 11/17 - 11/23,feedback,,jzivtxcbl6964n,k3c6pj4rna8lj,2019-11-23T23:08:11Z,{},project3
4690,no,"<p>You can access the entire posting, just like you do now. With full searching ability etc. I don&#39;t think you are allowed to post new messages however, but I have not tried that.</p>",2019-11-23T23:18:46Z,48,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k3c7354qiso5pt,2019-11-23T23:18:46Z,{},project3
4691,no,<p>excellent to know ;) this has been a very handy feature in online learning.</p>,2019-11-25T16:14:12Z,47,Week 11/24 - 11/30,feedback,,jzivtxcbl6964n,k3emsu3bs101it,2019-11-25T16:14:12Z,{},project3
4692,no,"<p>What is slack here? When you train your model, do you also train the other agent using FoeQ (maximin) algorithm?</p>
<p>Also, how do you pick your action? (a, choose randomly, b: choose based on probability distribution (pi as shown above), c: choose argmax of pi) Thanks.</p>",2019-11-22T19:02:48Z,27,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k3aii3r3qam4nx,2019-11-22T19:02:48Z,{},project3
4693,no,<p>This was Foe-Q vs Random. This time I was choosing actions that are random over pi.</p>,2019-11-22T21:11:10Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3an36mlnw26nk,2019-11-22T21:11:10Z,{},project3
4694,no,<p>The slack variable is the &#39;V&#39; value in the inequalities.</p>,2019-11-22T21:12:48Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3an5a4ocoy11k,2019-11-22T21:12:48Z,{},project3
4695,no,"<p>If the rows represent B&#39;s actions and the columns represent A&#39;s actions (which is what you wrote), and we&#39;re evaluating the policy for player A, then I get the same results you do.</p>
<p></p>
<p>Hey Jacob, for FoeQ vs FoeQ, does your final Q table for player A at state $$s$$ match mine?</p>
<p></p>
<p>playerA x playerB</p>
<p></p>
<p>-2 -3  2 10 -2</p>
<p>-3 -2 -9 10 -3</p>
<p>-3 -7 -9 10 -3</p>
<p>-2 -7 -9 10 -2</p>
<p>-2 -3  2 10 -2</p>
<p></p>
<p>Here&#39;s a convergence graph for state $$s$$, South, Stick, which is entry (1,4) (0-indexed) above:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3akmzsbqlj0%2FsSouthStick.png"" alt="""" /></p>",2019-11-22T19:46:46Z,27,Week 11/17 - 11/23,followup,,jzfsa4a37jf4aq,k3ak2nfmdam17y,2019-11-22T19:46:46Z,{},project3
4696,no,"<p>Nice! Thanks for confirming my numbers Vahe. That also confirms another observation I made about this environment and Foe-Q. Do 10 runs of constant alpha from 0.1 to 1.0 and graph it. You&#39;ll see the same thing I see.</p>
<p></p>
<p>I&#39;ll try a Foe-Q vs Foe-Q next with random-over-pi action selection.</p>",2019-11-22T21:12:26Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3an4tp9eigm4,2019-11-22T21:12:26Z,{},project3
4697,no,"<p>You mean from 0.01 to 0.1, right?</p>",2019-11-22T21:15:36Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3an8vm2al04or,2019-11-22T21:15:36Z,{},project3
4698,no,"<p>No, really 0.1 to 1.0. You could do 0.01 to 0.1 too, it doesn&#39;t really matter, so long as you keep the learning rate constant.</p>",2019-11-22T23:03:11Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3ar38d8emm3wx,2019-11-22T23:03:11Z,{},project3
4699,no,<p>My state 81 is all 1.0&#39;s for alpha = 1.0 decaying to 0.001.</p>,2019-11-22T23:12:40Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3arffzn1zm7ec,2019-11-22T23:12:40Z,{},project3
4700,no,"<p>Hopefully stupid question: given the reward for getting a goal (or the opponent scoring an own goal) is 100, why is your 4th column 10 and not 100?</p>
<p></p>
<p>I think I&#39;m correct in saying that the fourth column is agent B stepping to the left and scoring an own goal, thus getting agent A 100 points.</p>",2019-11-22T23:58:19Z,27,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k3at253ts5n7jf,2019-11-22T23:58:19Z,{},project3
4701,no,"<p>Yup, that&#39;s correct. The factor of ten reduction comes from the $$(1-\gamma)$$ factor Greenwald uses in her Q update rule.</p>
<p></p>
<p>&#64;Jacob Is your State 81 $$(s,$$ South, Stick)?  So you&#39;re saying you get 1.0 instead of the -3 that I get?</p>",2019-11-23T00:11:55Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3atjmhk6s97on,2019-11-23T00:11:55Z,{},project3
4702,no,"<p>Jacob, I ran FoeQ vs Random at a constant $$\alpha=0.01$$. It&#39;s a bit disturbing that there&#39;s so much noise in my &#34;convergence.&#34;  I ran it for 25 million time steps, and it seems like after about 2 million, it&#39;s already as converged as it ever gets, but as you can see it fluctuates a good $$\pm 0.5$$.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3attmya41lq%2FJacob.png"" alt="""" /></p>
<p></p>
<p>Are your Q values settling down to noiseless values when $$\alpha$$ is held constant?</p>
<p></p>
<p>I&#39;ve been plotting the Q values instead of the error, as Farrukh recommended in office hours yesterday, because there&#39;s too much noise in my error signal to figure out if I&#39;m converging.</p>
<p></p>
<p>Another plotting issue is that when we overlay errors from, say, $$\alpha=0.1$$ and $$\alpha=0.01$$, the latter plot will be drowned out by the former because values are just changing an order of magnitude less due to the lower learning rate, which causes the maximum errors at each time step to also be less.</p>",2019-11-23T00:41:21Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3aulhjnfge1l0,2019-11-23T00:41:21Z,{},project3
4703,no,"<p>&#64;Vahe, do you mind sharing what actions your indices align with? I&#39;m getting a very different final Q matrix and I&#39;d love to hone in on one or two actions to see where the differences might be coming from:</p>
<p></p>
<pre>[[-5.427  0.    -6.561 10.    -5.652]<br /> [-6.561 -8.1   -6.561 10.    -6.561]<br /> [-5.967 -8.1   -5.968 10.    -5.972]<br /> [-5.908 -2.767 -6.561 10.    -5.749]<br /> [-5.956  0.    -6.561 10.    -5.83 ]]</pre>
<p><br />My indices are:</p>
<p></p>
<p>0: North</p>
<p>1: East</p>
<p>2: South</p>
<p>3: West</p>
<p>4: Stay</p>",2019-11-23T00:54:57Z,27,Week 11/17 - 11/23,feedback,,jl3oi5v7qkSk,k3av2yutx801j5,2019-11-23T00:54:57Z,{},project3
4704,no,"<p>Vahe, you have found the same conclusions that I have. This thing doesn&#39;t really converge at all. Its another trick of the decay parameter.</p>
<p></p>
<p>My &#39;s&#39; is always 1 at the end. During the iteration, the value changes consistently and produces a fun graph (like I&#39;ve shown in other posts).</p>
<p></p>
<p>Ben, I am using Table 1 update calculations, which scales the reward by a tenth. it&#39;s weird.</p>
<p></p>
<p>Vahe, I searched my Q for any &#34;-2&#34; or &#34;-3&#34; values, nada. Not a single one. I have some negative values in there. There is no indication of convergence when alpha is constant. It just fluctuates like Q-on-policy does.....</p>",2019-11-23T02:07:08Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3axnt2yalu1xq,2019-11-23T02:07:08Z,{},project3
4705,no,"<p>Ben,</p>
<p></p>
<p>Mine are:</p>
<p></p>
<p>0: North</p>
<p>1: South</p>
<p>2: East</p>
<p>3: West</p>
<p>4: Stick</p>
<p></p>
<p>You know something, our values aren&#39;t quite as different as they appear.  There potentially are differences in the way we&#39;ve coded our environments that explain it.</p>
<p></p>
<p>Let&#39;s look critically at a few values.  When player A stays in the same cell and player B passes him the ball (cells (0,1) and (4,1) in your matrix), then player A&#39;s expected future rewards are $$0$$. That&#39;s one of the few bright spots for player A, besides B scoring an own goal.</p>
<p></p>
<p>If you look at my matrix, it&#39;s a similar situation.  As in your case, those two cells are the only non-negative entries (besides the own goals), but in my matrix the value is shifted a little higher for A (&#43;2 instead of 0).</p>",2019-11-23T02:51:29Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3az8tqxds83zw,2019-11-23T02:51:29Z,{},project3
4706,no,"<p>How do we update V here? I&#39;m trying to implement foe and confused. Let&#39;s say player A is columns, player B - rows. Then in the first example, 1st player B is picking the row with the smallest Q(s,sigma1,o) which is the row 3? Or player B knows player A&#39;s pi distribution and knows the player A will be picking the last column likely, then the player B picks the row 2?</p>",2019-11-22T20:07:38Z,27,Week 11/17 - 11/23,followup,,jzih0fdt4sn1cq,k3akth81h5w1gm,2019-11-22T20:07:38Z,{},project3
4707,no,"<p>I assume by $$V$$ you mean the value function, and not the &#34;slack variable&#34;, which is also sometimes denoted $$V$$.</p>
<p></p>
<p>It&#39;s the same in principle as the Rock-Paper-Scissors example in Littman 1994.  If you are trying to determine $$\pi$$ (and $$V$$) for Player B, then you need to maximize the minimum &#34;slack&#34; value over all of A&#39;s actions.  At least that&#39;s how Littman does it.  You can see a little tutorial I wrote (scroll down towards the end of the thread where I have a long post) here: &#64;908.</p>",2019-11-22T20:19:23Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3al8lcoz0h4q2,2019-11-22T20:19:23Z,{},project3
4708,no,"<p>If you have trouble with the LP, then look at this: <a href=""https://github.com/axonal/cvxopt-tutorial/blob/master/Linear%20Programming.pdf"">https://github.com/axonal/cvxopt-tutorial/blob/master/Linear%20Programming.pdf</a></p>",2019-11-22T21:15:20Z,27,Week 11/17 - 11/23,feedback,,jc554vxmyuy3pt,k3an8jtjd7c32m,2019-11-22T21:15:20Z,{},project3
4709,no,"<p>Thanks, Vahe. I think my confusion comes from the fact that in the Greenwald paper V1 is:<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzih0fdt4sn1cq%2Fk3an6a2c7msw%2FScreen_Shot_20191122_at_1.13.09_PM.png"" alt="""" />  While in the Littman 94&#39;s paper:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzih0fdt4sn1cq%2Fk3an8ds0oacx%2FScreen_Shot_20191122_at_1.14.33_PM.png"" alt="""" /></p>
<p></p>
<p>I think I can understand LP for the Littman&#39;s case and CE-Q, but looking at the equation 5 in the Grenwald&#39;s paper I&#39;m puzzled how to write down the inequalities. Intuitively, Littman&#39;s looks right and I can see what inequalities I can write down there (same as in HW6). </p>",2019-11-22T21:18:19Z,27,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3ancdkt6yn5mn,2019-11-22T21:18:19Z,{},project3
4710,no,"<p>Those two equations are actually the same.  You left out a key piece of the Greenwald one:</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3ankqkedg8d%2FAida.PNG"" alt="""" /></p>",2019-11-22T21:25:08Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3anl5mqsyh439,2019-11-22T21:25:08Z,{},project3
4711,no,"<p>oh, I simply ignored it. Now makes sense! Thank you, Vahe! Your &#64;908 is superb!!</p>",2019-11-22T21:49:37Z,27,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3aogmpq9hs8p,2019-11-22T21:49:37Z,{},project3
4712,no,"<p>Hey, thank you</p>",2019-11-22T23:43:35Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3asj77r6kz5jw,2019-11-22T23:43:35Z,{},project3
4713,no,"<p>Just to validate my minimax functions. In the above example, you used BxA Q matrix, but the pi and slack values are for A. Is that correct? </p>
<p></p>
<p>I am assuming it is AxB Q matrix and calculating the slack and pi for A. I got the following results. </p>
<pre>[[ 1.         -8.02888831  1.         -8.18802091  0.85585958]
 [ 1.          0.90045555  1.          1.          0.90038332]
 [ 1.          0.84883616  1.          0.67762618  0.84912303]
 [-9.0987744  -9.09999236 -9.09710303 -9.00183109  1.        ]
 [ 0.90022355  0.90085698  0.90091449  0.90085767  1.        ]]

-0.8513443321567549

[0.         0.         0.98527739 0.01472261 0.        ]</pre>
<p></p>",2019-11-23T08:16:14Z,27,Week 11/17 - 11/23,followup,,j6ln9puq99s5uv,k3baugm6q5p3xf,2019-11-23T08:16:14Z,{},project3
4714,no,"<p>I think I am confused here. For example, if the payoff matrix is A for player 1. The Q table is for player 1 (row) vs player 2 (column), when we define the constrain for linear programing, do we use A or A.transpose() ? Looks like we should use A.transpose() when we build the constrain matrix, is that correct?</p>
<p></p>
<p></p>",2019-11-23T09:09:41Z,27,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k3bcr7l6r922n5,2019-11-23T09:09:41Z,{},project3
4715,no,<p>You can print out the constraint matrix before you call the solver function and see if it looks like what it should.</p>,2019-11-23T16:06:57Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3brnt23pfh64n,2019-11-23T16:06:57Z,{},project3
4716,no,"<p>Is the result from &#64;Jacob correct? Right now, I am not sure if my minimax function finds the correct value. With some modifications, I can repeat &#64;Jacob&#39;s result. However, the Q values are not converging....</p>",2019-11-23T20:42:25Z,27,Week 11/17 - 11/23,feedback,,j6ln9puq99s5uv,k3c1i26geagdz,2019-11-23T20:42:25Z,{},project3
4717,no,"<p>I think the best anyone can tell you is that if you match Jacob then either both of you are correct, or both of you are wrong.</p>",2019-11-23T22:07:50Z,27,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c4jwmwu1o3wc,2019-11-23T22:07:50Z,{},project3
4718,stud,<p>the coefficient of rewards R is different. should it be (1-gamma)*R or just R?</p>,2019-11-23T20:17:13Z,41,Week 11/17 - 11/23,followup,a_0,,k3c0lnrplh54mj,2019-11-23T20:17:13Z,{},project3
4719,no,"<p>Since we are reproducing Greenwald, do what is in the paper.</p>",2019-11-23T20:27:23Z,41,Week 11/17 - 11/23,feedback,,jzih0fdt4sn1cq,k3c0yqgygnez1,2019-11-23T20:27:23Z,{},project3
4720,no,<p>See &#64;932</p>,2019-11-23T21:09:14Z,41,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c2gjtjwdp2t1,2019-11-23T21:09:14Z,{},project3
4721,no,"<p>Btw, for the code, it&#39;s cleaner to follow the Greenwald update aka update V,  then Q as there are on-policy off-policy procedures.</p>",2019-11-23T20:28:24Z,41,Week 11/17 - 11/23,followup,,jzih0fdt4sn1cq,k3c101lynh025j,2019-11-23T20:28:24Z,{},project3
4722,no,"<p>I am including this in the analysis. It&#39;s more interesting than &#34;their plots are (x,y) of skipped points.&#34;</p>",2019-11-22T22:21:38Z,25,Week 11/17 - 11/23,followup,,jc554vxmyuy3pt,k3aplssbg6v4q0,2019-11-22T22:21:38Z,{},project3
4723,no,"<p>I also have had issues with GLPK, but not related to it finding a solution early. There was a chance that it would enter an infinite loop while trying to solve which i started to try and handle by forcibly terminating it with another thread, but having never multithreaded with python it was quickly becoming more effort than it was worth. Ended up having to use the default solver instead which increased the runtime from about 30 minutes to 5 hours but oh well.</p>",2019-11-23T18:06:15Z,24,Week 11/17 - 11/23,followup,,jr46k9bbb1g5ju,k3bvx8gfo0f4v3,2019-11-23T18:06:15Z,{},project3
4724,no,<p>interesting. good find.</p>,2019-11-25T01:38:59Z,36,Week 11/24 - 11/30,followup,,jl1acpoc4HA9,k3drjalm50o4rj,2019-11-25T01:38:59Z,{},other
4725,no,"<p>One thing I&#39;ve noticed is that ML or ML-adjacent professors seem to often have &#34;side-hustles&#34;. They consult, they start companies, they collaborate with or are directly employed by companies. I wonder how that arrangement usually works exactly, and how the time is split.</p>",2019-11-27T06:13:45Z,36,Week 11/24 - 11/30,followup,,ixr4jvzzg1zba,k3gw8clipfq1nh,2019-11-27T06:13:45Z,{},other
4726,no,"<p>I took it to mean that alpha was decaying from some value (likely 1.0), by some amount y, down to a min of 0.001.</p>",2019-11-23T17:04:34Z,41,Week 11/17 - 11/23,followup,,ixty1midfufhd,k3btpwnxhzh2ac,2019-11-23T17:04:34Z,{},project3
4727,no,Ok thank you. I thought of this too after posting. Somehow decaying to a horizontal asymptote of 0.001. Or maybe just decaying linearly to a minimum of 0.001. ,2019-11-23T17:06:23Z,41,Week 11/17 - 11/23,feedback,,jzhnj3cj4pl2uy,k3bts8v4avt2fn,2019-11-23T17:06:23Z,{},project3
4728,no,"<p>Just as a note, the alpha decay schedules are discussed more in the technical papers, (2005\2007) and in the Littmann 1994 paper where the soccer game was pulled from; so I would look at those as well.</p>",2019-11-23T17:13:13Z,41,Week 11/17 - 11/23,feedback,,ixty1midfufhd,k3bu11afjqi1mn,2019-11-23T17:13:13Z,{},project3
4729,no,"<p>exactly, alpha decays down to a minimum of 0.001.</p>",2019-11-23T17:31:02Z,41,Week 11/17 - 11/23,feedback,,jl1acpoc4HA9,k3bunxwa48l2pl,2019-11-23T17:31:02Z,{},project3
4730,no,"Great, thanks!",2019-11-23T18:07:06Z,41,Week 11/17 - 11/23,feedback,,jzhnj3cj4pl2uy,k3bvybwi9eh4f7,2019-11-23T18:07:06Z,{},project3
4731,no,"<p>Just remember that Littman&#39;s paper decayed to 0.01 (10x that of Greenwald). So if you use his decay parameter, you won&#39;t hit 0.001. You have to compute the decay parameter that reaches 0.001.</p>",2019-11-24T14:46:48Z,40,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d48l7pf616dm,2019-11-24T14:46:48Z,{},project3
4732,no,<p>Also.. Epsilon Decay and Epsilon minimum is important as it took me a long long long time to figure out why my Friend Q is not converging as fast as the example given by the paper hahaha. It was still going on an exploration even after the optimal Friend-Q-Learning has been reached</p>,2019-11-24T03:40:07Z,40,Week 11/24 - 11/30,followup,,hzoi2qsuCAd,k3cgf8bni9o6v6,2019-11-24T03:40:07Z,{},project3
4733,no,"<p>Sungyoung,</p>
<p></p>
<p>Are you saying that when you explored more, the Q-value for ($$s$$, South, Stick) - the value whose error we&#39;re plotting - wasn&#39;t converging as quickly?</p>
<p></p>
<p>Or are you saying that it un-converged itself?</p>",2019-11-24T03:54:09Z,40,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3cgx9z8iyb4y6,2019-11-24T03:54:09Z,{},project3
4734,stud,<p>I think it is exactly the problem that he treats the other player as part of the stationary environment but in reality it is another strategic player. </p>,2019-11-23T21:45:49Z,37,Week 11/17 - 11/23,followup,a_0,,k3c3rlaezg72dv,2019-11-23T21:45:49Z,{},other
4735,stud,"<p>That the other player is strategic is, IMHO, implied in the text. It mentioned that the &#39;minmax&#39; method is not ideal, because it fails to exploit the other player&#39;s imperfection. Another way to read this is that despite the existence of imperfection, it recognizes the strategic nature. :-) </p>",2019-11-23T21:53:55Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c420lma174rz,2019-11-23T21:53:55Z,{},other
4736,no,"<p>I&#39;m not sure what you mean by the other player being &#34;strategic.&#34;  If you mean that he&#39;s adapting his play, I don&#39;t think so, at least not until Sutton mentions the point I have in (2) above.  I think his point is that minimax fails to properly exploit a player who does <em>not</em> play optimally.</p>
<p></p>
<p>Game theory methods are <em>severely</em> lacking in real-world game play, in the context of playing against non-optimal players.</p>
<p></p>
<p>In games we have two common notions of optimality of strategy:</p>
<p></p>
<p>1. Game-theoretic optimal (found by the minimax solution)</p>
<p></p>
<p>2. Maximally exploitative</p>
<p></p>
<p>The second one is the true notion of optimality.  In the case where your opponent also plays optimally, #2 will become the special case of #1.  But in the vast majority of games in the real world, the opponent does <em>not</em> play anywhere close to optimally, and if you employ a game-theoretic strategy against him, you won&#39;t do nearly as well as you could do if you maximally exploited him - took full advantage of all of his mistakes.</p>",2019-11-23T22:02:08Z,37,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c4ckkv85b4ss,2019-11-23T22:02:08Z,{},other
4737,stud,"<p>‘strategic’ is a game theory term. it roughly means that agents care not only about their own actions, but also about the actions taken by other agents. the mere mention of minmax, in my mind, recognizes that the other player is strategic. </p>
<p></p>",2019-11-23T22:10:23Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c4n78r9ii318,2019-11-23T22:10:23Z,{},other
4738,no,"<p>He only mentions the word &#39;minimax&#39; at the beginning of his analysis, to point out that the minimax solution is <em>not</em> correct against an imperfect player.  I don&#39;t think it has anything to do with the opponent in his analysis that follows.  I think that opponent is the fixed, imperfect player (non-adapting) that I mention above.  But that is my interpretation.  Here is Sutton&#39;s phrasing:</p>
<p></p>
<p>&#34;...let us assume that we are playing against an imperfect player, one whose play is sometimes incorrect and allows us to win.&#34;</p>
<p></p>
<p>&#34;...the classical “minimax” solution from game theory is not correct here because it assumes a particular way of playing by the opponent.&#34;</p>",2019-11-23T22:34:46Z,37,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c5ijob3s67k,2019-11-23T22:34:46Z,{},other
4739,stud,"<p>&#34;...let us assume that we are playing against an imperfect player, one whose play <strong>is sometimes incorrect and allows us to win.</strong>&#34;</p>
<p></p>
<p>The bold part also can also be read as &#34;<strong>sometimes the opponent plays correctly</strong>&#34; or in other words strategically. That is how I read it.</p>
<p></p>
<p>Anyway, this is way too vague to make much inference out of it.  If the other player is random, then yes, Q-learning works. My real question is that given he recognizes that the other player is only sometimes random, why does he say that Q-learning still works well?</p>
<p></p>
<p></p>",2019-11-23T22:39:23Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c5ohjtt2o3de,2019-11-23T22:39:23Z,{},other
4740,no,"<p>I&#39;m not sure where you&#39;re getting &#34;random&#34; from.  Just because a player doesn&#39;t play optimally does not mean they play randomly.</p>
<p></p>
<p>Tabular Q-learning will converge to an optimal solution if the environment (including the other player) is stationary and you decay the learning rate appropriately.</p>",2019-11-23T23:14:50Z,37,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c6y2pner93xh,2019-11-23T23:14:50Z,{},other
4741,stud,"<p>Maybe I was not very clear with my question. I feel Sutton recognizes that this is a strategic setting, which means that the other player, although sometimes plays imperfectly, can and, most likely, plays strategically in a lot of the situations. That is to say that the other player considers what the first player is likely to respond when it decides what to play. In this situation, my understanding is that Q-learning is not likely to work well. Yet the book seems to say that it nevertheless works. I feel there might be more here and want to dig a bit deeper. </p>",2019-11-23T23:19:26Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c73zm9sfy1hs,2019-11-23T23:19:26Z,{},other
4742,stud,"<p>Think I know what you mean. I think I have some confusion around fixed/adaptive vs. strategic/non-strategic. Question remains. It seems you think as long as the other agent is not adaptive, Q-learning will converge? You may be right! Prompts me to read more about that. </p>",2019-11-23T23:36:34Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c7q0z9v8a1mp,2019-11-23T23:36:34Z,{},other
4743,stud,"<p>Okay. Pending more investigation and reading on my part, now I think there is above 90% probability you are right. :-) </p>",2019-11-23T23:51:15Z,37,Week 11/17 - 11/23,feedback,a_0,,k3c88wnv18j3v3,2019-11-23T23:51:15Z,{},other
4744,no,"<p>Adaptive-ness is the real killer - it leads to a non-stationary environment.  Randomness leads to a stochastic environment, which is no different than something like the Frozen Lake or Lunar Lander environments we played with.</p>
<p></p>
<p>To deal with opponent adaptation, Sutton suggests at the end that keeping $$\alpha$$ constant will help Q-learning, not to converge, but to still give good policies even for a changing opponent.</p>
<p></p>
<p>We can also handle adaptation of the opponent by switching to game theory methods (e.g. minimax), but what happens there is that we will arrive at a strategy that beats every other strategy, but doesn&#39;t <em>optimize</em> against any of them (except the one case where the opponent is also optimal).  It handles adaptation by ignoring it and just trying to beat an optimal player.</p>
<p></p>
<p>This is the issue that I think Sutton was addressing with this example, and how TD methods can do better than game theory if the opponent isn&#39;t optimal.</p>
<p></p>",2019-11-23T23:54:25Z,37,Week 11/17 - 11/23,feedback,,jzfsa4a37jf4aq,k3c8cz78qln7a9,2019-11-23T23:54:25Z,{},other
4745,stud,"<p>Yes to your points. </p>
<p></p>
<p>The $\alpha$ thing, I think, is often used to deal with nonstationarity. We are not looking for convergence in this case but practically works well. </p>
<p></p>
<p>However, I am still now sure Sutton is talking about a fixed opponent. Another thing I am suspecting, though, even when faced with an adaptive agent, there are situations &#34; Q-learning might still converge.  I am having a pretty good idea... :-) </p>
<p></p>
<p></p>
<p></p>",2019-11-24T02:41:38Z,36,Week 11/24 - 11/30,feedback,a_0,,k3cec0wz6m4ci,2019-11-24T02:41:38Z,{},other
4746,stud,"<p>Okay, the book actually specifically mentioned that &#34;this method converges for a fixed opponent.&#34;  Haha, that makes sense. </p>
<p></p>
<p>Interesting discussion, though! It cleared me up on the difference between being strategic and adaptive and opened up a few more questions. :-) </p>
<p></p>
<p>Thanks, Vahe!</p>",2019-11-24T02:59:48Z,36,Week 11/24 - 11/30,feedback,a_0,,k3cezdtorzl67j,2019-11-24T02:59:48Z,{},other
4747,no,"<p>I&#39;d say think back to the rock-paper-scissor problem Littman introduced in his paper (we used it for homework 6). There we don&#39;t just pick argmax of Q. Instead we came up with a probability distribution over our actions, called pi. When we chose an action, we chose it with the probability assigned to it in pi. I think your problem is your using that argmax directly, which leads to a completely deterministic policy. This converges really quick, and hence your graph. </p>",2019-11-24T00:18:11Z,51,Week 11/24 - 11/30,followup,,jl3we43d3bp15p,k3c97jw0rea3hm,2019-11-24T00:18:11Z,{},project3
4748,no,"<p>Andrew, I think V_i should be solved also through LP </p>",2019-11-24T01:42:56Z,51,Week 11/24 - 11/30,followup,,jzih0fdt4sn1cq,k3cc8j3b82u5l0,2019-11-24T01:42:56Z,{},project3
4749,no,"<p>So I also solve <br />P_a[new_state_val] and P_b[new_state_value] using linear programming</p>
<p></p>
<p>and then use these to get V_a and V_b where</p>
<p><br />V_a = np.max(Q_a[new_state_val] * P_a[new_state_val])</p>
<p></p>
<p>V_b = np.max(Q_b[new_state_val] * P_b[new_state_val])</p>
<p></p>
<p>is this more on the right track?</p>
<p> </p>",2019-11-24T02:11:36Z,51,Week 11/24 - 11/30,feedback,,jc6mqevhagl262,k3cd9ea2khrlb,2019-11-24T02:11:36Z,{},project3
4750,no,"<p>Andrew,</p>
<p></p>
<p>I think you can&#39;t use np.max. You should calculate the Sum of the probabilities*Q.</p>
<p></p>
<p>See equation (5) of Greenwald-Hall 2003 (look below equation 5, after <strong>&#34;where Q(s, ...&#34;</strong>)</p>",2019-11-24T17:38:15Z,51,Week 11/24 - 11/30,feedback,,jc6xvgjncoey,k3dad356fkm322,2019-11-24T17:38:15Z,{},project3
4751,no,"<p>I&#39;m using the pseudo code provided in Littman &#39;94 and I&#39;m using LP to get the probability distribution but I&#39;m confused about updating V(s&#39;). Is this a completely different LP problem that needs another system of constraint equations???<img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fj6m1jeidndu6wq%2Fk3djg2bt2mmv%2FScreen_Shot_20191124_at_4.52.23_PM.png"" alt="""" width=""544"" height=""314"" /></p>",2019-11-24T21:52:46Z,51,Week 11/24 - 11/30,feedback,,j6m1jeidndu6wq,k3djgdwyp2u4mt,2019-11-24T21:52:46Z,{},project3
4752,no,"<p>It can be framed in such as way that the game value (V) and pi distribution come from one LP solution.  See below for a decent tutorial:</p>
<p></p>
<p><a href=""https://www2.cs.duke.edu/courses/fall12/cps270/lpandgames.pdf"">https://www2.cs.duke.edu/courses/fall12/cps270/lpandgames.pdf</a></p>
<p></p>
<p>There, U is the same as V here.</p>",2019-11-25T03:58:35Z,51,Week 11/24 - 11/30,feedback,,hz3hplpvI9c,k3dwiu4pug66oh,2019-11-25T03:58:35Z,{},project3
4753,no,<p>Thanks Jonathan!</p>,2019-11-25T04:24:41Z,51,Week 11/24 - 11/30,feedback,,j6m1jeidndu6wq,k3dxge6josf74l,2019-11-25T04:24:41Z,{},project3
4754,no,"<p><a href=""/class/jzh9tkzzxkd7ph?cid=971"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=971</a> </p>
<p></p>
<p>Also to check your algo for uCE...</p>",2019-11-25T05:28:17Z,51,Week 11/24 - 11/30,feedback,,hz3hplpvI9c,k3dzq6vfxdd6uo,2019-11-25T05:28:17Z,{},project3
4755,no,"<p>Final piece in case you haven&#39;t seen it, for LP and the game of chicken in the paper :)</p>
<p></p>
<p><a href=""https://github.com/axonal/cvxopt-tutorial/blob/master/Linear%20Programming.pdf"">https://github.com/axonal/cvxopt-tutorial/blob/master/Linear%20Programming.pdf</a> </p>",2019-11-25T05:33:46Z,51,Week 11/24 - 11/30,feedback,,hz3hplpvI9c,k3dzx8e2f8q1d0,2019-11-25T05:33:46Z,{},project3
4756,no,"<p>For other 3 algorithms, the final Q value for the start position is 80 - 90 range, which I think makes sense. Now, I start to suspect my FoeQ is not working properly. </p>
<p></p>
<p>Has anyone checked their Q values?</p>",2019-11-24T05:12:53Z,40,Week 11/24 - 11/30,followup,,j6ln9puq99s5uv,k3cjqivamft370,2019-11-24T05:12:53Z,{},project3
4757,no,<p>Same I have no idea how I am getting that and its really annoying!!</p>,2019-11-24T07:39:07Z,23,Week 11/24 - 11/30,followup,,jc6mqevhagl262,k3coyl1ghuf5t6,2019-11-24T07:39:07Z,{},project3
4758,no,"<p>boundary constraints? Make sure you have 2 boundary constraints.</p>
<p></p>
<p>sum(pi) &gt;= 0</p>
<p>sum(-pi) &lt;= 0</p>",2019-11-24T14:48:50Z,23,Week 11/24 - 11/30,feedback,,jc554vxmyuy3pt,k3d4b79pifg4hf,2019-11-24T14:48:50Z,{},project3
4759,no,"<p>I think it should be:</p>
<p>sum(pi) &gt;= 1</p>
<p>sum(-pi) &lt;= -1</p>",2019-11-24T17:21:51Z,23,Week 11/24 - 11/30,feedback,,jc6xvgjncoey,k3d9rz8un5216d,2019-11-24T17:21:51Z,{},project3
4760,no,"<p></p>
<p>sum(pi) &lt;= 1</p>
<p>sum(-pi) &lt;= -1</p>",2019-11-24T17:30:21Z,23,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3da2x2adla3ea,2019-11-24T17:30:21Z,{},project3
4761,no,<p>ya I have both of those and converge the same way as JWA does in post &#64;1007</p>,2019-11-24T17:50:12Z,23,Week 11/24 - 11/30,feedback,,jc6mqevhagl262,k3dasgdx7cj58a,2019-11-24T17:50:12Z,{},project3
4762,no,"<p>Right, are these sum(pis) the policy of a player for a given action, or are you meaning sum pies over the total 25?</p>",2019-11-24T21:52:24Z,23,Week 11/24 - 11/30,feedback,,is5x0kzrzjpg7,k3djfwlmjcn71i,2019-11-24T21:52:24Z,{},project3
4763,no,"<p>Thanks, I understand how to make the graphs, I&#39;m just referring to the fact that the TA suggested we should not plot any points for which the error is zero, and alluded to the fact that there are other points we shouldn&#39;t plot. I assume this is to produce a cleaner, more readable graph, but I can&#39;t find instructions anywhere about which points to omit. </p>",2019-11-24T19:01:26Z,40,Week 11/24 - 11/30,followup,,jl284xdcifz44g,k3ddc1veo7hd4,2019-11-24T19:01:26Z,{},project3
4764,no,We don&#39;t know how the original plots were generated. These are assumptions you&#39;ll need to make and discuss in your report based on your experimentation.,2019-11-24T22:30:46Z,40,Week 11/24 - 11/30,feedback,,i4op5p9vfbq5yz,k3dkt96gke9fs,2019-11-24T22:30:46Z,{},project3
4765,no,"<p>My understanding is that with libertarian-CE particularly, it wouldn&#39;t really turn into a LP problem. l-CE basically selects the action that max the player&#39;s reward, without considering opponent&#39;s reward. So, it is basically a rephrase of Q-value.</p>
<p>And yes, for it, you would need to get a value for each individual player.</p>",2019-11-24T18:25:32Z,24,Week 11/24 - 11/30,followup,,jl5wq8mca7o0,k3dc1vvvkw545t,2019-11-24T18:25:32Z,{},project3
4766,stud,"<p>That&#39;s an interesting interpretation. If it&#39;s true, I wonder if the graph produced by l-CE, using my current interpretation of the equation above, matches that of vanilla Q-learning. Thanks for responding.</p>",2019-11-24T18:49:19Z,24,Week 11/24 - 11/30,feedback,a_0,,k3dcwh5bhh9617,2019-11-24T18:49:19Z,{},project3
4767,no,"<p>If you are able to implement it, please share your finding. I didn&#39;t actually implement it, and would like to know if it matches my understanding as well.</p>
<p></p>
<p>Edit: now after putting more thought into it, you would still need to solve an LP for finding the Correlated-Equilibrium. l-CE is one type of criteria selection. A result of that criteria selection, in a context of CE, would result in different value than a vanilla Q-learning, which is done disregard of CE. Sorry for leading you off the wrong path. I think you will need to solve the LP for each player.</p>",2019-11-24T19:00:59Z,24,Week 11/24 - 11/30,feedback,,jl5wq8mca7o0,k3ddbh6f7jg1s7,2019-11-24T19:00:59Z,{},project3
4768,stud,"<p>No worries. The separate LP problem per player point is made explicitly/clearly in the &#39;05 paper, page 17, which I only recently came across. I&#39;m still not entirely sure what the $$\prod $$ operator does in the l-CE equation above. Do they mean we take the sigma/pi from the 2 LP solutions and elementwise-mutiply those mixed strategies (and normalize the product so sum = 1)?</p>",2019-11-25T03:58:09Z,24,Week 11/24 - 11/30,feedback,a_0,,k3dwia7587669t,2019-11-25T03:58:09Z,{},project3
4769,no,"<p>In the 2005 paper she doesn&#39;t use the $$\prod$$ symbol.  In fact the objective function is simply to maximize the dictatorial agent&#39;s rewards, over the distributions present in the set of correlated equilibria.  This makes sense.  Pick the best correlated equilibrium for <em>you</em>.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3e23sjby91o%2FCapture.PNG"" alt="""" /></p>
<p></p>
<p>My guess is that $$\prod$$ refers to the <em>Cartesian product</em> of the individual distributions, i.e., the set of ordered tuples of distributions.</p>",2019-11-25T06:40:16Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3e2ara94fr4kq,2019-11-25T06:40:16Z,{},project3
4770,no,"<p>Hi Vahe, in the mood of being confused by notation, in this: (Section 2 in the paper)</p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjl5wq8mca7o0%2Fk3eohw24pypj%2Fsigma.png"" alt="""" /></p>
<p></p>
<p>Should the first 2 $$\sum$$ be interpreted as <em>set </em>of distributions, rather than a <em>sum</em>? Or in this context they are equivalent?</p>
<p>And the last $$\sum_{a1}$$ is actually a sum?</p>",2019-11-25T17:04:36Z,24,Week 11/24 - 11/30,feedback,,jl5wq8mca7o0,k3eolnd7vxl2ce,2019-11-25T17:04:36Z,{},project3
4771,no,<p>Correct.  The first two instances of $$\sum$$ are <em>not</em> summations - they&#39;re a set of action distributions at state $$s$$.  The last instance of $$\sum$$ actually <em>is</em> a sum over the component actions <em>in</em> a particular distribution.</p>,2019-11-25T17:14:49Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3eoysy4vmr6wx,2019-11-25T17:14:49Z,{},project3
4772,no,<p>Thank you Vahe!</p>,2019-11-26T18:40:15Z,24,Week 11/24 - 11/30,feedback,,jl5wq8mca7o0,k3g7gi8c4na424,2019-11-26T18:40:15Z,{},project3
4773,stud,"<p>Vahe, thanks for pointing out the Cartesian product and the clarification in the 2005 paper.</p>
<p></p>
<p>If you don&#39;t mind sparing a little more time, I have a few more questions.</p>
<p></p>
<p>So the $$\prod $$ operator would produce a 25 x 25 array by Cartesian multiplying the two separate 25-item CE policies, of which one maximizes A&#39;s rewards and the other maximizes B&#39;s. I can&#39;t figure out the probability math to then flatten that 25 x 25 Cartesian product back down to a shared libertarian CE policy to return.</p>
<p></p>
<p>Is the rebranding of libertarian to dictatorial (and the omission of the $$\prod $$ operator) in the 2005 paper to clarify/correct that, in this mode, CE-Q isn&#39;t meant to be used to find a policy that let&#39;s all agent be harmoniously libertarian relative to one another? That maybe it should only be used to learn policies that correlate agents&#39; actions to benefit only one specific agent (the dictator)?</p>",2019-11-27T04:17:18Z,24,Week 11/24 - 11/30,feedback,a_0,,k3gs2ly4vwg2ec,2019-11-27T04:17:18Z,{},project3
4774,no,"<p>Firstly, I think the role of Cartesian product here is as a placeholder for the two agents&#39; equilibrium policies.  There&#39;s no actual arithmetic multiplication going on.  For example, the ordered pair $$(3,2)$$ holds two different integers, in a specific order, but we never combine them (multiply them).  The &#34;Cartesian product&#34; of two sets is just the set of all ordered pairs where the first element is from the first set and the second element is from the second set.</p>
<p></p>
<p>As for rebranding, my original take was that the republican -&gt; plutocrat and libertarian -&gt; dictatorial switches had nothing to do with the content of the ideas, but for political correctness, as the former are actual political parties in the United States and maybe she had second thoughts on using those words :) </p>
<p></p>
<p>But I think you may be right in the case of libertarian -&gt; dictatorial.  After re-reading, it seems like dictatorial is <em>picking</em> one particular agent, making him the <i>dictator</i>, exactly as you say.  Then all players are choosing the correlated equilibrium that maximizes <em>his</em> expected return.  The 2003 paper just stored <em>all of those possible dictators</em> in a Cartesian product, whereas in 2005, she&#39;s just picking one of them, maybe for simplicity of exposition.</p>
<p></p>
<p>Pretty cool and thanks for forcing me to look at this more closely!</p>",2019-11-27T05:33:12Z,24,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3gus7jcozz22g,2019-11-27T05:33:12Z,{},project3
4775,stud,"<p>Thanks, I did end up finding it. Also turns out that CVXOPT has a tolerance setting and you need to force your constraints to that tolerance!</p>
<p></p>
<p></p>",2019-11-25T02:23:04Z,23,Week 11/24 - 11/30,followup,a_0,,k3dt405ehdu4re,2019-11-25T02:23:04Z,{},project3
4776,no,<p>freaky</p>,2019-11-25T16:10:16Z,23,Week 11/24 - 11/30,feedback,,jzivtxcbl6964n,k3emnrrw7m41fa,2019-11-25T16:10:16Z,{},project3
4777,no,"<p>Thank you Farrukh! Just was wondering about external reference practice that I&#39;ve described in the question and if it won&#39;t be penalized. The charts that I wanted to put into Git repo and refer to in the report are to support my claims explaining noticed differences in the replicated results. This of course isn&#39;t essential, but makes my claims less handwavy. It&#39;s OK, I&#39;ll probably leave them out as my report already has 6 figures in it.</p>",2019-11-25T02:23:52Z,40,Week 11/24 - 11/30,followup,,jqkxzdmmolGf,k3dt517z13k5fr,2019-11-25T02:23:52Z,{},project3
4778,no,<p>Well now I want to replicate that paper. MuZero looks cool.</p>,2019-11-26T17:45:23Z,28,Week 11/24 - 11/30,followup,,ixr4jvzzg1zba,k3g5hyodrbbsg,2019-11-26T17:45:23Z,{},logistics
4779,no,<p>Next year&#39;s Project 3? ;)</p>,2019-11-27T14:16:53Z,28,Week 11/24 - 11/30,feedback,,jl3oi5v7qkSk,k3hdho78x6de1,2019-11-27T14:16:53Z,{},logistics
4780,no,"<p>For record, here is the book that Don was talking about:</p>
<p><a href=""https://web.mit.edu/dimitrib/www/RLbook.html"">https://web.mit.edu/dimitrib/www/RLbook.html</a></p>
<p></p>",2019-11-30T19:02:55Z,19,Week 11/24 - 11/30,followup,,jl5wq8mca7o0,k3ly12v3if38y,2019-11-30T19:02:55Z,{},logistics
4781,no,"<p>Thanks Quang!</p>
<p></p>
<p>Here is their entire library: <a href=""http://www.athenasc.com/"">http://www.athenasc.com/</a></p>
<p></p>
<p>Their &#34;Introduction to Probability&#34; book is the textbook for their EdX probability course &#34;Probability - The Science of Uncertainty and Data&#34;, which is a good course for a solid introduction to probability theory.  It used to be completely free (I took it several years ago).  Now you can audit it for free but you won&#39;t be able to complete the assessments, I think.</p>",2019-11-30T19:40:41Z,19,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3lzdn7kbak3wi,2019-11-30T19:40:41Z,{},logistics
4782,no,"<p>This is what Mosek replied:</p>
<p></p>
<p>&#34;Cvxopt has not seen much development recently and it doesn&#39;t look like it was adapted to MOSEK 9.<br /><br />You will have to edit the file<br /><br />src/python/coneprog.py<br /><br />in your cvxopt installation and remove all references to mosek.solsta.near_optimal in lines:<br /><br /><a href=""https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L2895"" target=""_blank"" rel=""noopener noreferrer"">https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L2895</a><br /><a href=""https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L3353"" target=""_blank"" rel=""noopener noreferrer"">https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L3353</a><br /><a href=""https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L4368"" target=""_blank"" rel=""noopener noreferrer"">https://github.com/cvxopt/cvxopt/blob/master/src/python/coneprog.py#L4368</a><br /><br />Just leave mosek.solsta.optimal</p>
<p>&#34;</p>
<p>I replaced the 3 &#34;if solsta in (mosek.solsta.near_optimal, mosek.solsta.optimal) by if solsta is mosek.solsta.optimal&#34; and it works.  </p>",2019-11-27T08:18:01Z,23,Week 11/24 - 11/30,followup,,jzh6k6o994a6dh,k3h0o5p79bd3qn,2019-11-27T08:18:01Z,{},project3
4783,no,Has anyone seen their hw grades in canvas yet?,2019-11-28T02:45:08Z,51,Week 11/24 - 11/30,followup,,jh701barLeiB,k3i47xthdjk1d7,2019-11-28T02:45:08Z,{},logistics
4784,no,<p>not yet</p>,2019-12-02T01:57:02Z,50,Week 12/1 - 12/7,feedback,,jqtti9gfnagH,k3ns9gw0359g2,2019-12-02T01:57:02Z,{},logistics
4785,no,"<p>I now see grades for all the homeworks. (Dec 1, 2019; 6:12pm PST). Note they might come in waves.</p>",2019-12-02T02:12:40Z,50,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3nstkpsc7o4qg,2019-12-02T02:12:40Z,{},logistics
4786,stud,"Do not worry. Just focus on the final, and do the best you can, as it is the only thing you can control. I know it is easier said than done, but want to say it anyway.<div><br /></div><div>There seems some stats on lite.gatech.edu . You can search “grades” on piazza to find the exact link. It seems a large percentage of people (&gt;40%) got an A historically. At least that is my impression.</div>",2019-11-27T13:48:26Z,20,Week 11/24 - 11/30,followup,a_0,,k3hch2uxqt76jk,2019-11-27T13:48:26Z,{},other
4787,stud,<p>I would add that I don&#39;t think that a &#39;B&#39; in one of Isbell&#39;s courses (especially this one) is anything to scoff at. This is challenging material. Also agree with other &#64;anon above. </p>,2019-11-28T15:31:49Z,20,Week 11/24 - 11/30,feedback,a_1,,k3ivlvuiquaz7,2019-11-28T15:31:49Z,{},other
4788,no,<p>Sweet! Nice answer.</p>,2019-11-28T23:30:10Z,20,Week 11/24 - 11/30,feedback,,jzygktecb0e3i1,k3jcp1lfskg2fy,2019-11-28T23:30:10Z,{},other
4789,no,"<p>wait... so if we all just skip the exam (and all get zeros).... ;)</p>
<p></p>
<p>hahahahahaha.  RL in RL.</p>
<p></p>",2019-12-02T07:13:35Z,19,Week 12/1 - 12/7,followup,,jzivtxcbl6964n,k3o3kk9vqqh1ba,2019-12-02T07:13:35Z,{},other
4790,no,<p>If we all decide to skip I can benefit from unilaterally taking the exam</p>,2019-12-02T07:27:11Z,19,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3o421pc2pz4sn,2019-12-02T07:27:11Z,{},other
4791,no,<p>Not if I can find an appropriate side-bet to make with you so you flunk it ;)</p>,2019-12-02T16:00:55Z,19,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3omepxllcs4mq,2019-12-02T16:00:55Z,{},other
4792,no,"<p>Vahe - having studied the material in this course and with your grasp of the material (and I presume, a lead in the points) :)</p>
<p></p>
<p>... I&#39;m willing to bet u&#39;re smart enough to go along, than &#34;unilaterally&#34; decide to break the detente ;)</p>
<p></p>
<p>Team optimal, beats personally optimal under competitive assumption, no ? :)</p>
<p></p>
<p>anyway it was a funny thought on a late night ;)</p>
<p></p>
<p>It would be a fun game to run though; just to see the outcome; I&#39;ld be curious; if the material taught in the class; predisposed the class, to team optimal, for example.</p>
<p></p>
<p>Even more interesting, would be to examine the value of skipping an exam; versus the value of possible improvement in points, vis-a-vis, agent expectation...</p>
<p></p>
<p>An interesting experiment in econometrics. ;)</p>",2019-12-04T08:01:31Z,19,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3r05x02kyt3ki,2019-12-04T08:01:31Z,{},other
4793,no,"<p>I think the utility of skipping the exam may be a more complex function of variables than it seems.</p>
<p></p>
<p>Let&#39;s say the utility of an A to a student is 4, the utility of a B is 3, etc, and let&#39;s say our goal was to maximize the $$\textit{sum}$$ of the utilities of all the students in the class.</p>
<p></p>
<p>Everyone skipping the final may then be problematic.  Professor Isbell may get annoyed, parse the distribution of grades in an unfriendly way, and that sum of utilities we&#39;re trying to maximize may end up being much lower than if we had just taken the exam.</p>
<p></p>
<p>On the other hand, it would take an impossible level of coordination to pull that off that feat, and it would definitely be an achievement that should impress anyone who hears about it.</p>
<p></p>
<p>It&#39;s also worth some utility to each student to think about the professor&#39;s reaction if all 300&#43; people skipped the final.</p>",2019-12-04T08:48:56Z,19,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3r1uw2gvhf6th,2019-12-04T08:48:56Z,{},other
4794,no,"<p>&gt;  Professor Isbell may get annoyed, parse the distribution of grades in an unfriendly way, and that sum of utilities we&#39;re trying to maximize may end up being much lower than if we had just taken the exam.</p>
<p></p>
<p>this makes to presumptions:</p>
<p></p>
<p>1) that we have not been evaluated sufficiently by the homeworks on a broad subset of the topics (debatable; since the metric is replication of complex topical papers written by masters, phd students or higher) - i.e. that a sufficiently rational basis for actually grading the classroom as a whole, does not exist.  I would warrant that a sufficient number of points (1/2 of the class) have been assigned already.  The question then is:  does further testing significantly change the outcome?  And much like in sampling; there is some number n, after which additional samples yield marginal results...</p>
<p></p>
<p>2) that Prof. isbell, doesn&#39;t like games.  I would hazard to guess; that a man who spends his life studying the reinforcement techniques based on reward structures, in fact would find this an interesting variant.</p>
<p></p>
<p>&gt; It&#39;s also worth some utility to each student to think about the professor&#39;s reaction if all 300&#43; people skipped the final.</p>
<p></p>
<p>that is an argument then of persuasion; not intrinsic reward, no?  and how much nicer would it be for the graders, and the professor, to simply knock off early. ;) same as for the students.</p>
<p></p>
<p>it is, fun to think about.     much like in RL, occasionally the agent stumbles onto an unorthodox or unconventional way to solve the problem; why should this then, be limited to the set of agents devised?  shouldn&#39;t a person, rationally having studied these topics, aspire to the utility gained from that insight ?</p>
<p></p>
<p>;)</p>
<p></p>
<p></p>
<p></p>",2019-12-07T01:59:14Z,19,Week 12/1 - 12/7,feedback,,jzivtxcbl6964n,k3uxjkexwpz2ui,2019-12-07T01:59:14Z,{},other
4795,no,"<p>yes, I was wondering if there are any lecture videos in udacity going over it. It seems that at some point there were some videos but they have been removed?</p>",2019-11-30T21:03:35Z,51,Week 11/24 - 11/30,followup,,j6ll2xkiDJf,k3m2c9btfec8l,2019-11-30T21:03:35Z,{},other
4796,no,"<p>It was mentioned in &#64;790 that the course used to use Burlap, so it&#39;s very possible that there used to be Burlap lectures, but that they&#39;re not applicable to this course anymore.</p>
<p></p>
<p>My guess is that Open AI Gym has supplanted Burlap for CS7642.</p>",2019-11-30T21:54:49Z,51,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3m4652mlx64xe,2019-11-30T21:54:49Z,{},other
4797,no,<p>just also added &#34;review&#34; tag</p>,2019-11-29T03:21:11Z,23,Week 11/24 - 11/30,followup,,jqkxzdmmolGf,k3jky54lpkb4tx,2019-11-29T03:21:11Z,{},final_exam
4798,no,"<p>Thanks! I did consider it, but wasn&#39;t sure if the &#39;review&#39; tag was for posts asking for grade reviews. I guess it doesn&#39;t matter too much as this is clearly not a grade review :P</p>",2019-12-02T15:59:40Z,22,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3omd433q82x1,2019-12-02T15:59:40Z,{},final_exam
4799,no,thanks!!,2019-11-29T05:18:48Z,23,Week 11/24 - 11/30,followup,,jzj4sh1p7pf5af,k3jp5e0zukj1s1,2019-11-29T05:18:48Z,{},final_exam
4800,no,<p>You&#39;re welcome ^_^</p>,2019-12-02T15:58:41Z,22,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3ombu6kf8v1tp,2019-12-02T15:58:41Z,{},final_exam
4801,no,<p>Thank you for posting this!</p>,2019-11-30T15:32:53Z,23,Week 11/24 - 11/30,followup,,jl3mf50dr12i,k3lqiyypvuo3uk,2019-11-30T15:32:53Z,{},final_exam
4802,no,<p>You&#39;re welcome ^_^</p>,2019-12-02T15:58:44Z,22,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3ombwzz8rj205,2019-12-02T15:58:44Z,{},final_exam
4803,stud,<p>why are they categorized as 1 to 5 does that mean they are from specific lectures</p>,2019-12-01T21:20:51Z,22,Week 12/1 - 12/7,followup,a_0,,k3nieat8dx6zd,2019-12-01T21:20:51Z,{},final_exam
4804,no,"<p>Alec posted them under Review #1, #2,...,#6 topic names, so the categorization left from there.</p>",2019-12-02T02:02:14Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3nsg5ys8of37v,2019-12-02T02:02:14Z,{},final_exam
4805,no,"<p>Yup, exactly. This was literally just copy pasta from Alex&#39;s posts :)</p>",2019-12-02T15:58:24Z,22,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3ombhcyh6snx,2019-12-02T15:58:24Z,{},final_exam
4806,no,"<p>I found this:</p>
<p></p>
<p><a href=""https://quizlet.com/392097252/weekly-reinforcement-learning-questions-flash-cards/"">https://quizlet.com/392097252/weekly-reinforcement-learning-questions-flash-cards/</a></p>",2019-12-05T00:32:47Z,22,Week 12/1 - 12/7,followup,,jc554vxmyuy3pt,k3rzkoku930pg,2019-12-05T00:32:47Z,{},final_exam
4807,no,"<p>Funny!  Be careful though, some are wrong...</p>",2019-12-05T01:52:11Z,22,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3s2esocv1151v,2019-12-05T01:52:11Z,{},final_exam
4808,no,<p>haha. I saw that too!</p>,2019-12-05T01:52:36Z,22,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3s2fc1nrb740w,2019-12-05T01:52:36Z,{},final_exam
4809,no,<p>Was looking for the same thing. Would be incredibly helpful. </p>,2019-11-30T06:51:52Z,40,Week 11/24 - 11/30,followup,,jzozvpx25to679,k3l7wxvnfir1pm,2019-11-30T06:51:52Z,{},final_exam
4810,stud,I would take a look at the cliff walk example in Sutton and Barto.,2019-11-29T22:55:16Z,45,Week 11/24 - 11/30,followup,a_0,,k3kqw0psia02k6,2019-11-29T22:55:16Z,{},final_exam
4811,no,"<p>There seems to be a lot of confusion on this topic.</p>
<p></p>
<p>First of all, I believe &#39;on-policy&#39; and &#39;off-policy&#39; are the standard terms here.  &#39;Online&#39; and &#39;offline&#39; refer to something else, at least in Sutton and Barto (state-value updating, and, in general, performance, <strong>during an episode</strong>, as opposed to after an episode is completed).</p>
<p></p>
<p>Secondly, the cliff-walker example seems to be misunderstood by at least some people. In no way does it show the superiority of on-policy algorithms over off-policy algorithms.  It does show that, if someone were stupid enough to forget to shut off exploration when deploying a trained Q-learning agent in a stationary, deterministic, environment, then that agent would do worse than a trained SARSA agent. The purpose of this example is to show that Q-learning can learn the optimal policy <strong>offline</strong> while doing badly <strong>online.</strong> But the algorithm must be purposely sabotaged to actually do worse than SARSA during evaluation.</p>",2019-11-29T23:40:14Z,45,Week 11/24 - 11/30,followup,,jzfsa4a37jf4aq,k3kshug68hh1gz,2019-11-29T23:40:14Z,{},final_exam
4812,stud,"I think the point of cliff walk is to show that online performance of SARSA can be better when the algorithm is exploring and exploiting at the same time, which is the environment in which most RL algorithms operate. In addition, the difference will be more fundamental when the environment is slowly changing (not very stationary), because then it is impossible to train a model and then go to exploit only mode. <div><br /></div><div>Also, on-policy algo can only work online but off-policy algo can work off line, at least theoretically. I don’t think a lot of them would work well. </div>",2019-11-30T01:07:18Z,45,Week 11/24 - 11/30,feedback,a_0,,k3kvlt6z6ki13r,2019-11-30T01:07:18Z,{},final_exam
4813,stud,"<p>I want to clarify my answer a bit. While the above is true for SARSA vs, Q-learning, there are other ways to achieve similar effects with off-policy algorithms. One example is exploration-sensitive MDP. </p>",2019-12-05T00:55:18Z,44,Week 12/1 - 12/7,feedback,a_0,,k3s0dnj4srp6qw,2019-12-05T00:55:18Z,{},final_exam
4814,no,"<p>I think Vahe points out a key point of confusion here.  Is the question asking about &#34;offline&#34; vs. &#34;online&#34; or off-policy vs. on-policy.  I see those as being two completely different terms.  </p>
<p></p>
<p>It is my understanding, that offline and online are terms that are associated with how the algorithm takes in data. <strong>Would be nice to get some confirmation on how accurate this is or is not.</strong></p>
<p></p>
<p>An <em>online</em> algorithm would be taking in data essentially from a live (possibly near-real-time) stream of data and would be designed to make decisions on that live stream of data.  In ML4T these terms came up and were used in this way.  Think of an algorithm that is designed to predict or analyze stock market behavior and is analyzing fresh data straight from the live market.</p>
<p></p>
<p>An <em>offline</em> algorithm would be designed to analyze historical data or even data that was of a static environment where &#34;live&#34; changes were not being made to the dataset.  In this way you could devise training sets and split up the static data for different analysis and validation purposes.</p>",2019-11-30T00:48:47Z,45,Week 11/24 - 11/30,followup,,is9so9huTMp,k3kuy0gqdj8465,2019-11-30T00:48:47Z,{},final_exam
4815,no,"<p>Yes Nick your understanding of online/offline algos is more or less correct. Example is the mean calculation of some value in online and offline fashion: for offline calculation you need to know all N values (let&#39;s say 10,000 of them) and calculate it as sum/N. For online you need to know only last N, last_mean, and new value, and the new mean will be [(N*last_mean)&#43;new_val]/(N&#43;1).</p>",2019-11-30T02:38:17Z,45,Week 11/24 - 11/30,feedback,,jqkxzdmmolGf,k3kyuta4r2x5i0,2019-11-30T02:38:17Z,{},final_exam
4816,no,"<p>Thanks &#64;Sergei,  if I follow what you&#39;re saying.  For online we use a rolling window of data (in your example the window is &#34;last N&#34; data points long).  But for offline we have the entire set of data, whatever that is defined as.</p>",2019-11-30T03:06:59Z,45,Week 11/24 - 11/30,feedback,,is9so9huTMp,k3kzvq8rpb27gs,2019-11-30T03:06:59Z,{},final_exam
4817,no,"<p>For online you don&#39;t need a rolling window, only the last calculated value of the mean (last_mean), counter of values in the mean (N), and the new value that you want to add to the mean (new_val) - 3 numbers.</p>
<p>You can try it yourself:</p>
<p></p>
<pre>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; arr = np.array([1, 2, 3, 4, 5])
&gt;&gt;&gt; arr.mean() # this is off-line mean
3.0
&gt;&gt;&gt; arr2 = np.array([1, 2, 3, 4])
&gt;&gt;&gt; arr2.mean() # this is off-line too
2.5
&gt;&gt;&gt;
&gt;&gt;&gt; ((4 * 2.5) &#43; 5)/(4&#43;1) # this is on-line mean
3.0
&gt;&gt;&gt;</pre>
<p></p>",2019-11-30T11:42:23Z,45,Week 11/24 - 11/30,feedback,,jqkxzdmmolGf,k3liajbetc061b,2019-11-30T11:42:23Z,{},final_exam
4818,no,"True, I guess in offline vs online I was comparing a &#34;off policy vs on policy&#34;, and asking whether e-greedy as a policy counts when the epsilon is only 1.0, and it&#39;s policy is purely random actions. In this case, this policy encapsulates some off-policy since every action is random?",2019-11-30T00:51:08Z,45,Week 11/24 - 11/30,followup,,jzj4sh1p7pf5af,k3kv10vxorm6ci,2019-11-30T00:51:08Z,{},final_exam
4819,no,"<p>I agree... It would be nice to know if the original poster of the question meant offline or off-policy because those terms have context in the ML and RL world, and as far as I know they have completely different meaning.  I would hate to see this as a point of contention for the final exam.</p>",2019-11-30T00:52:58Z,45,Week 11/24 - 11/30,feedback,,is9so9huTMp,k3kv3dj97gp3uy,2019-11-30T00:52:58Z,{},final_exam
4820,no,"<p>Daniel,</p>
<p></p>
<p>Not sure if I&#39;m answering your question, but $$\epsilon$$-greedy with $$\epsilon=1$$ definitely counts as a valid policy.  In an off-policy setting it&#39;s actually a quite useful one.  The Taxi MDP was a good example of an environment whose state-space was small enough that using on off-policy algorithm (Q-learning) with a behavior policy that was $$\epsilon$$-greedy with $$\epsilon=1.0$$ was a good way to get to the optimal policy, and even the optimal value function, of that environment.</p>
<p></p>
<p>Now, if you&#39;re doing <strong>on-policy</strong> learning, then $$\epsilon$$-greedy with $$\epsilon=1.0$$ is certainly valid but not very useful, since you&#39;d be &#34;learning&#34; the random policy, which you already know.</p>",2019-11-30T01:07:34Z,45,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3kvm5v5lkwrm,2019-11-30T01:07:34Z,{},final_exam
4821,no,"Curious about this one myself Vahe.  I thought “on policy” means the agent acts according to some notion of its “knowledge base” (I.e) it uses the q table.  Outside of semantics, can we consider an eps greedy agent with eps = 1 to really be learning “on policy”?  Would be nice if instructors have some guidance as well.",2019-11-30T05:24:03Z,45,Week 11/24 - 11/30,feedback,,hz3hplpvI9c,k3l4s01j25r3tf,2019-11-30T05:24:03Z,{},final_exam
4822,no,"<p>Once you decide that an algorithm using an $$\epsilon$$-greedy policy with $$\epsilon=0.99999$$ can be &#34;on-policy&#34; (which it can), I feel like it&#39;s not so important to classify the case where $$\epsilon=1$$.   It&#39;s a singularity, and it&#39;s not very useful (for an on-policy learning algorithm).</p>",2019-11-30T05:32:24Z,45,Week 11/24 - 11/30,feedback,,jzfsa4a37jf4aq,k3l52qgknju3ot,2019-11-30T05:32:24Z,{},final_exam
4823,stud,"<p>I just want to point out that $$\epsilon=1$$ does not make SARSA off-policy, as it is still updating the value function only from the experience gained from executing the $$\epsilon$$- greedy policy. </p>",2019-11-30T17:36:54Z,45,Week 11/24 - 11/30,feedback,a_0,,k3luygaxp33q,2019-11-30T17:36:54Z,{},final_exam
4824,stud,"<p>Also, I am not sure if the infinite state/action MDP described in OP&#39;s question would converge. Convergence results I have seen, for example, Singh, Jaakkola, Littman,  and Szepesvari (2000), all seem to require a finite number of states. </p>
<p></p>
<p></p>
<p></p>",2019-11-30T07:59:15Z,45,Week 11/24 - 11/30,followup,a_0,,k3labkwkjmg4vq,2019-11-30T07:59:15Z,{},final_exam
4825,no,"Hm, but the lunar lander had infinite states and it could converge no ?",2019-12-01T17:00:53Z,44,Week 12/1 - 12/7,feedback,,jzj4sh1p7pf5af,k3n93zurko61r9,2019-12-01T17:00:53Z,{},final_exam
4826,stud,That’s a good question.  I was thinking about the tabular case. I haven’t given much thought about convergence in the continuous state space. ,2019-12-01T18:15:56Z,44,Week 12/1 - 12/7,feedback,a_0,,k3nbsi9oi0k5a5,2019-12-01T18:15:56Z,{},final_exam
4827,no,"<p>No matter how continuous the state space, it is always being discretized in some fashion. Table driven discretization is a crude way to do this, and the ANN method is a more sophisticated method. The linear approximation method with non-linear basis functions is also an elegant way of discretizing the space.</p>
<p></p>
<p>ANN is discretized because there is a finite number of connections between the node layers. This is no different than a tabular discretizer, it just has more fidelity. For instance, 3 x 1000 node layers would be able to represent 1 billion states. This is doable with commodity desktop hardware.</p>
<p></p>
<p>N/L-BF is also discretizing by way of the basis functions. They approximate the continuous state space by virtue of how the basis functions represent the sampling space. For instance, the FFT basis function is a cyclical function. The periodicity is the control bound on the fidelity of the discretization. You could use a hyperbolic basis function which would never converge IMHO because it would be just a scalar transformation of your already infinite space.</p>
<p></p>
<p>These techniques, when designed to converge, will all converge for some problems, but none of them converge on <em><strong>ALL</strong> problems in general. Keyword - <strong>ALL</strong></em>.</p>
<p></p>",2019-12-06T00:51:34Z,44,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3tfop7656025,2019-12-06T00:51:34Z,{},final_exam
4828,stud,<p>I was referring to the OP&#39;s original question about Q-learning convergence under infinite-state MDPs. I feel it is pretty easy to come up with an infinite state MDPs with discrete states where Q-learning does not converge. I haven&#39;t given it much thought about it for Lunar Lander. </p>,2019-12-06T03:10:32Z,44,Week 12/1 - 12/7,feedback,a_0,,k3tknep43b3559,2019-12-06T03:10:32Z,{},final_exam
4829,stud,"BTW: nice post, Jacob !<div><br /></div>",2019-12-06T03:16:56Z,44,Week 12/1 - 12/7,feedback,a_0,,k3tkvn0lna1fp,2019-12-06T03:16:56Z,{},final_exam
4830,stud,Can we get a student version of windows  from Georgia Tech?,2019-11-30T15:44:02Z,21,Week 11/24 - 11/30,followup,a_0,,k3lqxavz49x4ei,2019-11-30T15:44:02Z,{},final_exam
4831,no,"<p>Yes.  I believe so if you go to <a href=""http://software.oit.gatech.edu/"">http://software.oit.gatech.edu/</a>  I think you have to be connected to the GA Tech VPN to access the site.</p>",2019-11-30T15:46:09Z,21,Week 11/24 - 11/30,feedback,,hz7meu55mi8sd,k3lr00qqu5f5zm,2019-11-30T15:46:09Z,{},final_exam
4832,no,"<p>I agree with everything you wrote.</p>
<p></p>
<p>I&#39;m also not clear on why the player being $$20\%$$ incorrect means that</p>
<p></p>
<p>$$P(H=x|O=y) = P(H=x|O=z) = \frac15$$ instead of</p>
<p></p>
<p>$$P(H=x|O=y)&#43;P(H=x|O=z)=\frac15$$.</p>
<p></p>
<p>Really nice write up btw.</p>",2019-12-01T18:30:49Z,19,Week 12/1 - 12/7,followup,,jzfsa4a37jf4aq,k3ncbn8y156hm,2019-12-01T18:30:49Z,{},other
4833,stud,Where can we schedule the test during the final window Dec 6-9? And how long is the final?,2019-12-02T02:01:59Z,22,Week 12/1 - 12/7,followup,a_1,,k3nsfuh3uikg8,2019-12-02T02:01:59Z,{},final_exam
4834,no,"<p>You are not scheduling the test, you just login into proctor track when you ready (not too close to the deadline as you may experience issues and won&#39;t be able to resolve them on-time). Final is 90 minutes</p>",2019-12-02T02:05:52Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3nsku1486d29d,2019-12-02T02:05:52Z,{},final_exam
4835,stud,Thanks!,2019-12-02T02:10:54Z,22,Week 12/1 - 12/7,feedback,a_1,,k3nsrb4d3gr2vl,2019-12-02T02:10:54Z,{},final_exam
4836,no,"<p>Oops, have you seen new email and Piazza post, this exam is 60 minutes only</p>",2019-12-03T02:15:39Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3p8d9xemj51b2,2019-12-03T02:15:39Z,{},final_exam
4837,stud,"<p>saw it, thanks!</p>",2019-12-04T02:09:36Z,22,Week 12/1 - 12/7,feedback,a_1,,k3qnlckbktd1l4,2019-12-04T02:09:36Z,{},final_exam
4838,stud,<p>Could anyone kindly advise me how can I do the on-boarding test?  There is nothing in my Canvas under Proctortrack.  Thanks a lot.</p>,2019-12-02T03:05:07Z,22,Week 12/1 - 12/7,followup,a_2,,k3nup11yhai6a4,2019-12-02T03:05:07Z,{},final_exam
4839,no,"<p>Please see my reply below, I suspect that you&#39;re on the wrong account, if not please contact instructors as they are who adding tests to your account.</p>",2019-12-02T16:11:09Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3omrvmlop1r6,2019-12-02T16:11:09Z,{},final_exam
4840,stud,"<p>i clicked on &#34;Proctortrack2&#34; found in Piazza side menu and on the proctortrack webpage , i see the following msg: you are not assigned an onboarding test or the onboaring test assigned to you has expired. Contact you instructor&#34; am i supposed to see sth else?</p>",2019-12-02T14:11:46Z,22,Week 12/1 - 12/7,followup,a_0,,k3oiiccm6e36hz,2019-12-02T14:11:46Z,{},final_exam
4841,stud,<p>I am having the same issue.</p>,2019-12-02T14:55:00Z,22,Week 12/1 - 12/7,feedback,a_3,,k3ok1ydoh2t51d,2019-12-02T14:55:00Z,{},final_exam
4842,stud,<p>same here</p>,2019-12-02T15:26:00Z,22,Week 12/1 - 12/7,feedback,a_2,,k3ol5tct8fr217,2019-12-02T15:26:00Z,{},final_exam
4843,no,"<p>Have you created your ProctorTrack account? As far as I know this has nothing to do with Piazza (have no idea on what level they are integrated).<br />Here is the link that may be helpful (please see &#34;Getting Started with Proctortrack&#34; part, but the PT login area is on the top now, not on the right) <a href=""http://omscs6750.gatech.edu/fall-2016/test-1/"">http://omscs6750.gatech.edu/fall-2016/test-1/</a></p>",2019-12-02T16:07:40Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3omnerfnju58e,2019-12-02T16:07:40Z,{},final_exam
4844,stud,"<p>I had the same issue, but I logged out whatever account that was logged in and click forget password and use official Gatech email, the issue was not there anymore.</p>
<p></p>",2019-12-02T18:33:15Z,22,Week 12/1 - 12/7,feedback,a_5,,k3orumlxac54f6,2019-12-02T18:33:15Z,{},final_exam
4845,stud,"same here but I asked instructor in a private post, the response I get is to wait until they make an announcement.",2019-12-02T19:18:43Z,22,Week 12/1 - 12/7,feedback,a_1,,k3oth306xfn2n7,2019-12-02T19:18:43Z,{},final_exam
4846,stud,<p>Is anyone else seeing &#34;This test starts at 2019-12-05 14:00:00&#43;00:00. You can start a test session only upto 1 minutes before the test starts.&#34; for the onboarding test?</p>,2019-12-02T18:32:25Z,22,Week 12/1 - 12/7,followup,a_4,,k3ortjxmikr4dy,2019-12-02T18:32:25Z,{},final_exam
4847,stud,<p>I had the same message. </p>,2019-12-02T18:34:41Z,22,Week 12/1 - 12/7,feedback,a_5,,k3orwgzw4466nq,2019-12-02T18:34:41Z,{},final_exam
4848,no,"<p>The same for me, have posted on Slack yesterday</p>
<p>It&#39;s related to the &#34;Final (Test your setup)&#34; that is enabled on Canvas, but not on ProctorTrack for some reason.</p>",2019-12-02T18:42:27Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3os6gg0h1j6f6,2019-12-02T18:42:27Z,{},final_exam
4849,stud,"<p>Does anyone know if we&#39;re allowed to take breaks during the ProctorTrack final? Is there a way to pause the exam and grab water, take a bathroom break, etc.?</p>",2019-12-02T19:01:54Z,22,Week 12/1 - 12/7,followup,a_4,,k3osvgpaiy077u,2019-12-02T19:01:54Z,{},final_exam
4850,no,<p>There are generally no breaks allowed during a ProctorTrack exam</p>,2019-12-02T23:48:15Z,22,Week 12/1 - 12/7,feedback,,jh701barLeiB,k3p33pw0u5k5nj,2019-12-02T23:48:15Z,{},final_exam
4851,no,"<p>No, nothing during exam, no food/drinks/bathroom</p>",2019-12-03T02:16:38Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3p8eiyotpc2cp,2019-12-03T02:16:38Z,{},final_exam
4852,no,"<p>Sorry I couldn&#39;t resist an old joke from my previous class: You just are not allow to leave the webcam view, for restroom need...</p>",2019-12-08T08:57:56Z,22,Week 12/1 - 12/7,feedback,,jl5wq8mca7o0,k3wrxvj6zaw5uv,2019-12-08T08:57:56Z,{},final_exam
4853,stud,"<p>When I try the &#34;Final (Test your setup)&#34; it asks for a access code:</p>
<p></p>
<p>&#96;This is a quiz to test your setup. Find the exam &#34;CS 7642 - Fall 2019 - Test your Setup&#34; to get the access code for it.&#96;</p>
<p></p>
<p>How do I find it?</p>",2019-12-03T00:36:04Z,22,Week 12/1 - 12/7,followup,a_6,,k3p4t7qe197jh,2019-12-03T00:36:04Z,{},final_exam
4854,no,"<p>The code is on ProctorTrack website that was still inactive as far as I know, not sure when &#34;Test your Setup&#34; will be accessible on PT</p>",2019-12-03T02:17:50Z,22,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3p8g2z5zy34xp,2019-12-03T02:17:50Z,{},final_exam
4855,no,"<p>I just tried to get there and the &#34;CS 7642 - Fall 2019 - Test your Setup&#34; is not open until the same time window that the actual final exam is...  Not sure if this same window of time was intentional or not.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fis9so9huTMp%2Fk3pdvxqnxi6a%2FCapture.PNG"" alt="""" /></p>",2019-12-03T04:50:37Z,22,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3pdwjweh6m4hi,2019-12-03T04:50:37Z,{},final_exam
4856,no,"<p>If there is no stochasticity,  then a given deterministic policy should result in the same exact path and therefore same exact target each time.  So you can set your learning rate to 1.0 and learn the value function for that policy very quickly by making sure to hit each state, or state-action pair, once.</p>",2019-12-03T01:32:27Z,30,Week 12/1 - 12/7,followup,,jzfsa4a37jf4aq,k3p6tpwqglc3vh,2019-12-03T01:32:27Z,{},other
4857,no,"<p>Yeah... I&#39;m actually thinking of adding this one to the Q&amp;A pool (for later semesters, of course.)</p>",2019-12-03T02:09:44Z,30,Week 12/1 - 12/7,feedback,,hyx9thiqa6j4nn,k3p85nqrzul2yf,2019-12-03T02:09:44Z,{},other
4858,no,"<p>I found this interesting paper about offline learning and task hierarchies.</p>
<p></p>
<h1>Offline reinforcement learning with task hierarchies</h1>
<p><a href=""https://link.springer.com/article/10.1007/s10994-017-5650-8"">https://link.springer.com/article/10.1007/s10994-017-5650-8</a></p>
<p></p>
<p>&#34;<span style=""display:!important"">Task hierarchies can allow more efficient sample collection from large MDPs, while offline algorithms can learn better policies than the so-called “recursively optimal” or even hierarchically optimal policies learned by standard hierarchical RL algorithms.</span>&#34;</p>",2019-12-06T00:40:15Z,19,Week 12/1 - 12/7,followup,,jc554vxmyuy3pt,k3tfa4vwfbd2c,2019-12-06T00:40:15Z,{},other
4859,no,"<p>Interesting article, thanks for sharing.</p>",2019-12-02T23:22:15Z,50,Week 12/1 - 12/7,followup,,jzfsa4a37jf4aq,k3p269w99ym31y,2019-12-02T23:22:15Z,{},other
4860,no,"<p>Isn&#39;t this true for all forms of research? As in, not ever grad student produces ground breaking work but in many cases still has to produce a paper/s.</p>",2019-12-06T05:24:56Z,50,Week 12/1 - 12/7,followup,,gx3c8l7z7r72zl,k3tpg96vey7on,2019-12-06T05:24:56Z,{},other
4861,no,"<p>Yeah, I was thinking the same thing.  I think a lot of these problems are general to many fields, but I wouldn&#39;t be surprised if it&#39;s much worse in ML because of the factors outlined in the paper.</p>
<p></p>
<p>1. It&#39;s a very fast-growing field - lot of competition to produce results, reviewers spread thin, perhaps average quality of researchers is lower because it&#39;s so hot and everyone wants to do it and there&#39;s a lot of demand for people</p>
<p></p>
<p>2. It&#39;s laden with hyperparameter tuning, which means people can pass off lazy tuning for research, OR they can concoct elegant theories for their results, when the real cause was lazy hyperparameter tuning.</p>
<p></p>
<p>I also think the statistical nature of ML probably makes it tempting to flub results.  You can &#34;cheat&#34; with statistics.</p>",2019-12-06T06:04:02Z,50,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3tquisjqab2sy,2019-12-06T06:04:02Z,{},other
4862,no,"<p>Just as a note since you are planning to take it the last day, for the final, ensure you start early enough to have enough time to finish the final before the 6pm deadline. Also, if interruptions occur (i.e. power loss, network loss, site crashes, etc), ensure you leave enough time to contact support and get loaded back in before the deadline.(i.e. don&#39;t plan to start at 5pm est on the last day...)</p>",2019-12-04T18:37:49Z,19,Week 12/1 - 12/7,followup,,ixty1midfufhd,k3rmw6ywpm759a,2019-12-04T18:37:49Z,{},final_exam
4863,no,<p>Thanks. How long is it?</p>,2019-12-05T19:10:50Z,24,Week 12/1 - 12/7,followup,,j6ll2xkiDJf,k3t3iir4d694vo,2019-12-05T19:10:50Z,{},final_exam
4864,no,<p>This is in the announcement on Canvas. The exam is one hour.</p>,2019-12-05T19:23:42Z,24,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3t3z270keo1er,2019-12-05T19:23:42Z,{},final_exam
4865,no,<p>thanks</p>,2019-12-05T22:50:54Z,24,Week 12/1 - 12/7,feedback,,j6ll2xkiDJf,k3tbdiavw4p22h,2019-12-05T22:50:54Z,{},final_exam
4866,no,<p>Is final a multiple choice or essay like questions quiz ?</p>,2019-12-03T04:04:37Z,16,Week 12/1 - 12/7,followup,,jc9nkspumds4ki,k3pc9en2kx2y0,2019-12-03T04:04:37Z,{},final_exam
4867,no,<p>NVM. I got my answer</p>,2019-12-03T04:11:03Z,16,Week 12/1 - 12/7,feedback,,jc9nkspumds4ki,k3pchofugk7543,2019-12-03T04:11:03Z,{},final_exam
4868,stud,<p>What was the answer?</p>,2019-12-03T22:25:37Z,16,Week 12/1 - 12/7,feedback,a_1,,k3qflax8si524v,2019-12-03T22:25:37Z,{},final_exam
4869,no,<p>&#64;1069</p>,2019-12-04T07:12:20Z,16,Week 12/1 - 12/7,feedback,,jc9nkspumds4ki,k3qyeo0ricz7hw,2019-12-04T07:12:20Z,{},final_exam
4870,stud,"<p>Find the exam &#34;CS 7642 - Fall 2019 - Test your Setup&#34; to get the access code for it.</p>
<p></p>
<p>Where to find the exam to get the access code?</p>",2019-12-03T15:59:48Z,16,Week 12/1 - 12/7,followup,a_0,,k3q1t4z1mx18k,2019-12-03T15:59:48Z,{},final_exam
4871,no,<p>I have the same question. I couldn&#39;t find the access code.</p>,2019-12-04T14:33:57Z,16,Week 12/1 - 12/7,feedback,,jc6xvgjncoey,k3re6ld3hdf6dx,2019-12-04T14:33:57Z,{},final_exam
4872,stud,<p>I&#39;ve tried contacting the instructors but haven&#39;t hear back yet. I&#39;ve been trying to test the set up for a couple days...</p>,2019-12-04T17:12:07Z,16,Week 12/1 - 12/7,feedback,a_3,,k3rjtzr5yvl21k,2019-12-04T17:12:07Z,{},final_exam
4873,no,Did you start the exam on ProctorTrack? It should be called &#34;CS 7642 - Fall 2019 - Test your Setup Onboarding&#34;,2019-12-05T02:15:40Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s38zxlcot3e2,2019-12-05T02:15:40Z,{},final_exam
4874,stud,"<p>I cannot see any link from Canvas? What I did was to go to canvas --&gt; Prototrack 2. Below is the current screenshot.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhs489b10i78c%2Fk3s3e4hudhtv%2Fpp1.PNG"" alt="""" /></p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzhs489b10i78c%2Fk3s3f2zr1k9i%2Fpp2.PNG"" alt="""" /></p>
<p></p>",2019-12-05T02:23:56Z,16,Week 12/1 - 12/7,feedback,a_2,,k3s3jmm3wv298,2019-12-05T02:23:56Z,{},final_exam
4875,stud,<p>Just showed up 2 seconds ago on Canvas for me. So never mind. :-) </p>,2019-12-05T02:25:44Z,16,Week 12/1 - 12/7,feedback,a_2,,k3s3lxzz5bl3k2,2019-12-05T02:25:44Z,{},final_exam
4876,no,Cool,2019-12-05T02:26:19Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s3mp61r4w327,2019-12-05T02:26:19Z,{},final_exam
4877,no,<p>What time zone is that range valid for?? </p>,2019-12-04T02:05:18Z,16,Week 12/1 - 12/7,followup,,hzoi2qsuCAd,k3qnftasyz22vt,2019-12-04T02:05:18Z,{},final_exam
4878,no,<p>If you set your timezone correctly in Canvas then the time shown there for the exam will be the correct time relative to you.</p>,2019-12-04T12:41:55Z,16,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3ra6idhgd2m5,2019-12-04T12:41:55Z,{},final_exam
4879,stud,"<p>I still cannot test my setup. Anyone else has any luck?</p>
<p></p>
<p>Also, this is my first exam. There are nightmare stories on Reddit about room scans, etc.  Does anyone have any tips? </p>
<p></p>
<p>Do I need an absolutely quiet room? What if my wife/kid run around outside?</p>",2019-12-04T16:40:43Z,16,Week 12/1 - 12/7,followup,a_2,,k3ripm4lkq15f6,2019-12-04T16:40:43Z,{},final_exam
4880,no,"It&#39;s ok. If PT shows these warnings, we can go and check. If your kids are not running around with answers to help you out, you are fine.",2019-12-05T02:37:36Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s4173q3tl752,2019-12-05T02:37:36Z,{},final_exam
4881,no,"<p>I can&#39;t open the exam.</p>
<p></p>
<p>Error: Minified React error #200; visit <a href=""https://reactjs.org/docs/error-decoder.html?invariant=200"" target=""_blank"" rel=""noopener noreferrer"">https://reactjs.org/docs/error-decoder.html?invariant=200</a> for the full message or use the non-minified dev environment for full errors and additional helpful warnings. </p>
<p></p>
<p>Also this cookie warning:</p>
<p></p>
<p>Request to access cookie or storage on “<a href=""https://stats.g.doubleclick.net/r/collect?v=1&amp;aip=1&amp;t=dc&amp;_r=3&amp;tid=UA-9138420-1&amp;cid=904595880.1534894359&amp;jid=743667778&amp;_gid=760821457.1575510216&amp;gjid=775956624&amp;_v=j79&amp;z=1720517932"" target=""_blank"" rel=""noopener noreferrer"">https://stats.g.doubleclick.net/r/collect?v=1&amp;aip=1&amp;t=dc&amp;_r=…_gid=760821457.1575510216&amp;gjid=775956624&amp;_v=j79&amp;z=1720517932</a>” was blocked because it came from a tracker and content blocking is enabled.</p>",2019-12-05T01:47:53Z,16,Week 12/1 - 12/7,followup,,jc554vxmyuy3pt,k3s299oosz65kt,2019-12-05T01:47:53Z,{},final_exam
4882,no,"<p>I turned off enhanced protections for the site, so the cookie warning went away. The react error is still present.</p>",2019-12-05T01:52:18Z,16,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3s2ey00rhj3le,2019-12-05T01:52:18Z,{},final_exam
4883,no,"<p>Jacob, where you getting this error from? On Canvas or PT? I didn&#39;t have any of these while trying &#34;Final (Test your setup)&#34; (I use Windows 7 &#43; Chrome browser). I&#39;ve just got notification from PT that the access is not available until Dec 5&#39;th and no errors.</p>",2019-12-05T02:07:48Z,16,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3s2yw0nty8oq,2019-12-05T02:07:48Z,{},final_exam
4884,no,<p>This was from the &#39;test&#39; exam assignment. This was not from the undated assignment that was just posted. You have to view the web console to see the errors.</p>,2019-12-05T03:16:46Z,16,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3s5fkbd73w2cg,2019-12-05T03:16:46Z,{},final_exam
4885,stud,"<p>I am new to proctortrack. </p>
<p></p>
<p>My desktop computer has two monitors. Is a dual monitor setup considered okay?</p>
<p></p>
<p>Also, I occasionally look at the keyboard when I type. Is that a problem?</p>
<p></p>
<p>Thanks.</p>",2019-12-05T03:56:15Z,16,Week 12/1 - 12/7,followup,a_2,,k3s6uc68erk47z,2019-12-05T03:56:15Z,{},final_exam
4886,stud,"<p>Also, my understanding is that a room scan is not required. Is that correct?</p>
<p></p>
<p>Thanks and I apologize for so many questions. </p>",2019-12-05T04:25:54Z,16,Week 12/1 - 12/7,feedback,a_2,,k3s7wh3xgye5b2,2019-12-05T04:25:54Z,{},final_exam
4887,no,The monitors might be a problem. Disconnect one of them.,2019-12-05T06:09:13Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sblcee8oq7l2,2019-12-05T06:09:13Z,{},final_exam
4888,stud,<p>Sounds good</p>,2019-12-05T06:34:30Z,16,Week 12/1 - 12/7,feedback,a_2,,k3schuk3th2g7,2019-12-05T06:34:30Z,{},final_exam
4889,no,"<p><strong attention=""hgq7wx2g0LO"">&#64;Pushkar Kolhe</strong>  I have a second monitor, I was planning to simply turn it off and show it is off.  Is that going to be a problem?</p>
<p>Because, even if I disconnect it, how will they know if I really did it? </p>
<p>In the end, don&#39;t their app record everything from our screen(s)?</p>",2019-12-06T16:51:20Z,16,Week 12/1 - 12/7,feedback,,jzh6k6o994a6dh,k3udyylrqjw3k4,2019-12-06T16:51:20Z,{},final_exam
4890,no,"<p>There are a couple of questions here that I&#39;ll answer from my experience from past exams.</p>
<p></p>
<p><strong>(1) My desktop computer has two monitors. Is a dual monitor setup considered okay? and I have a second monitor, I was planning to simply turn it off and show it is off.  Is that going to be a problem?</strong></p>
<p></p>
<p>If the computer detects a second monitor, then it will require you to unplug it from the computer. (i.e. even me just having the dvi to vga agapter without a vga cable was enough for it to flag it as unacceptable). The same test that is run during the onboarding will also run during the regular test, so if having the second monitor is flagged then, it will be flagged in the test. I suggest physically unplugging the second (or third, or fourth, etc) monitor from the computer to avoid any issues. This is the part when the software loads up and is scanning through all the check boxes before you start with the face scan and such.</p>
<p></p>
<p><strong>(2) Because, even if I disconnect it, how will they know if I really did it?  In the end, don&#39;t their app record everything from our screen(s)?</strong></p>
<p></p>
<p>When you run the proctor track software it does more then just screen recording. It looks at what hardware is plugged into the machine, makes sure you only have a single mouse, keyboard, monitor, etc. </p>
<p></p>
<p><strong>(3) Also, I occasionally look at the keyboard when I type. Is that a problem?</strong></p>
<p></p>
<p>As far as occasionally looking down at the keyboard. It will likely flag a violation (really looking away for anything will likely flag), however, the TAs review all the violations and determine if they are real or not. I have occasionally looked down as well when trying to get the correct symbol key and have had no issues. I&#39;ve also accidentally glanced out the window when a loud car drove by without issue. If you stare off screen for an extended period of time, that will likely be a problem. I find this a bit hard sometimes, since I tend to lean back and look at the ceiling when I&#39;m thinking, but it&#39;s something I have to suppress when I take these tests.</p>",2019-12-06T19:22:57Z,16,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3ujdxy0ark75n,2019-12-06T19:22:57Z,{},final_exam
4891,no,"<p>thx a lot &#64;George Cox... I would have never thought of disconnecting my second mouse and remote keyboard.... </p>
<p>Gosh, I usually look up when I think hard, but yeah, I&#39;ll force myself to focus ... </p>
<p>Oh wait, I have allergies, I cough and sneeze all the time, and I take tissues from a box every 5 minutes, so I guess I&#39;m toast... </p>
<p><strong attention=""hz7meu55mi8sd"">&#64;Timothy Bail</strong>  any advice about this? what are we allowed to have on the desk?</p>
<p></p>
<p></p>
<div>
<div></div>
</div>",2019-12-07T18:00:08Z,16,Week 12/1 - 12/7,feedback,,jzh6k6o994a6dh,k3vvvaeca5a4d1,2019-12-07T18:00:08Z,{},final_exam
4892,no,"<p>Also, do we have to leave our microphone on?</p>
<p>It would make sense they want to know if we&#39;re not sharing our screen with someone else to get help for instance.  </p>",2019-12-07T18:01:39Z,16,Week 12/1 - 12/7,feedback,,jzh6k6o994a6dh,k3vvx8fo5mh68e,2019-12-07T18:01:39Z,{},final_exam
4893,no,"<p>You&#39;re welcome JP.</p>
<p></p>
<p>I&#39;m guessing a box of tissues is likely ok, but if I needed it, I would just pull a handful out and set them loose on the desk instead.</p>
<p></p>
<p><strong>Also, do we have to leave our microphone on?</strong></p>
<p><strong></strong></p>
<p>Yes, you will need to leave the microphone on so that they can ensure no one is telling you answers and you are not talking to someone...</p>",2019-12-07T19:37:57Z,16,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3vzd2ya9l84m2,2019-12-07T19:37:57Z,{},final_exam
4894,no,"<p>thanks again &#64;George... I hear there&#39;s no room scan, so I guess I&#39;ll be able to pick a tissue from the box on my left without watching... haha... I&#39;ve never seen anything like this in my life lol... </p>",2019-12-08T21:47:34Z,15,Week 12/8 - 12/14,feedback,,jzh6k6o994a6dh,k3xjfm97v8v4ez,2019-12-08T21:47:34Z,{},final_exam
4895,no,<p>Am I allowed to wear noise cancelling headphones during the exam?</p>,2019-12-05T15:35:22Z,16,Week 12/1 - 12/7,followup,,jl1b27fpaYkv,k3svtewdktu338,2019-12-05T15:35:22Z,{},final_exam
4896,no,I think so.,2019-12-05T17:54:36Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3t0sgx02zm3l5,2019-12-05T17:54:36Z,{},final_exam
4897,no,"<p>Dang, just went thru the onboarding &#34;test your setup&#34;, and it said headphones are not allowed.</p>",2019-12-05T18:29:46Z,16,Week 12/1 - 12/7,feedback,,jl1b27fpaYkv,k3t21ozy9fu395,2019-12-05T18:29:46Z,{},final_exam
4898,stud,"<p>I noticed when I took the trial exam that Proctortrack was only recording (blue frame around screen) the desktop space in my system (running MacOS Catalina) and not any of the other active <a href=""https://support.apple.com/guide/mac-help/work-in-multiple-spaces-mh14112/mac"" target=""_blank"" rel=""noopener noreferrer"">spaces</a>. I&#39;m guessing that means that the browser window in which I take the exam should only be open in the desktop space. Can someone please confirm?</p>",2019-12-05T21:10:26Z,16,Week 12/1 - 12/7,followup,a_4,,k3t7sbfpprs1gx,2019-12-05T21:10:26Z,{},final_exam
4899,no,<p>Can you ask this to PT support and let us know the answer?</p>,2019-12-06T16:46:52Z,16,Week 12/1 - 12/7,feedback,,hyx9thiqa6j4nn,k3udt89orn7v2,2019-12-06T16:46:52Z,{},final_exam
4900,stud,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>, I emailed them a while ago but am yet to hear back. I&#39;m planning on taking the exam tomorrow, so if I don&#39;t hear from them in time, I will just assume I need to take it in a browser window open in the Desktop space (the space Proctortrack draws a border around).</p>",2019-12-07T01:29:43Z,16,Week 12/1 - 12/7,feedback,a_4,,k3uwhlpw7phbx,2019-12-07T01:29:43Z,{},final_exam
4901,no,<p>Thanks for checking on this.</p>,2019-12-09T06:46:10Z,15,Week 12/8 - 12/14,feedback,,hyx9thiqa6j4nn,k3y2o9lk7iv2py,2019-12-09T06:46:10Z,{},final_exam
4902,no,"<p>Just to confirm, there is no room scan for this exam? Just did the test exam and it was not mentioned. I&#39;ve heard about mirrors and stuff and I would need to get one if that&#39;s the case.</p>",2019-12-06T11:05:55Z,16,Week 12/1 - 12/7,followup,,jzjwcq2u8o7110,k3u1mrfvrr96c,2019-12-06T11:05:55Z,{},final_exam
4903,no,<p>I had Proctortrack before on a different class and the room scan was required. I just used a small mirror. I learned later that they might do random sample on this. it may be better to have one there just in case unless someone sees clear instructions on that. </p>,2019-12-06T15:24:39Z,16,Week 12/1 - 12/7,feedback,,j6ilcw6hxoc77h,k3uavhvjzdh7oq,2019-12-06T15:24:39Z,{},final_exam
4904,no,<p>In my previous classes the room scan was not required.</p>,2019-12-06T17:10:59Z,16,Week 12/1 - 12/7,feedback,,jqkxzdmmolGf,k3ueo8gmv234tg,2019-12-06T17:10:59Z,{},final_exam
4905,no,"<p>Out of the 8 classes I&#39;ve taken, only one required a room scan. And it was good enough to just pick up the webcam and turn it around so that it scanned all the spots of the room. For those with laptops I&#39;ve heard that they were able to pick up the laptop and spin it around as well. I just picked up a cheap dollar store hand mirror just in case, but it wasn&#39;t needed, but I had it on hand just in case.</p>",2019-12-06T18:55:40Z,16,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3uiev0dywk1xo,2019-12-06T18:55:40Z,{},final_exam
4906,no,"No room scan. If you want to show your music studio setup to us, we can enable it.",2019-12-07T17:27:30Z,16,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3vupbnfxf14wz,2019-12-07T17:27:30Z,{},final_exam
4907,stud,"<p>Hi TAs, can you reset my Final (test your setup)? I successfully did not but want to change the computer and so would like to redo it. Thanks.</p>",2019-12-08T05:02:12Z,15,Week 12/8 - 12/14,followup,a_2,,k3wjipg8tug3m9,2019-12-08T05:02:12Z,{},final_exam
4908,no,"<p>If you didn&#39;t get your reply from TAs yet, you can test your setup by running on-boarding test that is always available on the ProctorTrack website (this one is not integrated with Canvas)</p>",2019-12-08T14:28:26Z,15,Week 12/8 - 12/14,feedback,,jqkxzdmmolGf,k3x3qw7nnsr5g9,2019-12-08T14:28:26Z,{},final_exam
4909,stud,I know. I just also need to test typing into canvas ,2019-12-08T14:50:40Z,15,Week 12/8 - 12/14,feedback,a_2,,k3x4jhqs7tm2y7,2019-12-08T14:50:40Z,{},final_exam
4910,no,"<p>I re-ran the onboarding test 3 times, from the canvas proctortrack 2 link, It just won&#39;t let you submit the quiz again, but you can go through the entire flow, end the proctoring, and it will still send you an email that it was completed successfully.</p>",2019-12-08T17:48:09Z,15,Week 12/8 - 12/14,feedback,,ixty1midfufhd,k3xavqh88hk5zf,2019-12-08T17:48:09Z,{},final_exam
4911,no,"I reset an attempt for Zhenyang Zhu, if that is indeed you.",2019-12-09T20:50:47Z,15,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3ywug98c0s5kw,2019-12-09T20:50:47Z,{},final_exam
4912,no,"<p><strong attention=""hgq7wx2g0LO"">&#64;Pushkar Kolhe</strong>  I don&#39;t know if it is within our ability to improve these for future final exam, but here are something in my wishlist:</p>
<p></p>
<p>- Display remaining time and question as percentage. Right now, time is displayed at the top of the exam, and I find myself scrolling up and down every 15 minutes to have accurate remaining time. It would be nice to have a &#34;progress bar&#34; that displays <em>% remaining time</em> vs <em>% unanswered questions</em>. In my previous class, I draw that into my scratch paper, just to make I am on right pace. We don&#39;t allow scratch paper for this exam, and mentally doing that is distracting.</p>
<p></p>
<p>- Some writable text area in the exam. Since we are not allowing to use scratch paper, but I need to note down the questions I am not sure about, to them later after finishing other questions. We roughly have 2 minutes per question to think and type answer, so when 2 minutes is up we would have to move on to next question. I find myself using the URL bar to note down the questions that need 2nd look. While it works, it is fragile. Any wrong click would have navigate away and throw away that note.</p>",2019-12-09T20:45:32Z,15,Week 12/8 - 12/14,followup,,jl5wq8mca7o0,k3ywnpcx6c42iu,2019-12-09T20:45:32Z,{},final_exam
4913,no,"Thanks for the feedback. Something related to Canvas UI, I can&#39;t change... but I can share this feedback.",2019-12-09T20:48:55Z,15,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3yws1l38a428v,2019-12-09T20:48:55Z,{},final_exam
4914,no,"<p>Will I be penalized if I don&#39;t complete the &#34;test your setup&#34; exam?</p>
<p></p>
<p>I submitted the final already. I thought I did everything right for the &#34;test your setup&#34; quiz but it&#39;s not showing up in canvas.</p>",2019-12-10T21:47:13Z,15,Week 12/8 - 12/14,followup,,k0i2gsf6my96ge,k40eaw0d86u6dz,2019-12-10T21:47:13Z,{},final_exam
4915,no,<p>no it was just for your benefit to test your setup.</p>,2019-12-11T02:02:16Z,15,Week 12/8 - 12/14,feedback,,hz7meu55mi8sd,k40nevndm4u1o8,2019-12-11T02:02:16Z,{},final_exam
4916,no,<p>Same for me.... Annoying as I am trying to test my setup</p>,2019-12-03T15:01:37Z,50,Week 12/1 - 12/7,followup,,jzj7y1ofgsro1,k3pzqbj6z471ai,2019-12-03T15:01:37Z,{},final_exam
4917,no,<p>Same for me.</p>,2019-12-04T05:29:25Z,50,Week 12/1 - 12/7,feedback,,jzjwhmhsghg5cy,k3quqbcsuf82ab,2019-12-04T05:29:25Z,{},final_exam
4918,no,"<p>When I log into Proctor track from <a href=""https://testing.verificient.com/"">https://testing.verificient.com/</a> I see the correct information for the two cs7642 tests (final and setup). If I use the ProctorTrack 2 link from canvas, I see what you see; so I am guessing the canvas link is incorrect. I would use the Proctortrack website I listed and log in with your proctortrack credentials (this might be you gatech login)... I don&#39;t remember...</p>
<p></p>
<p>As a reference, that site is what I&#39;ve used for all the classes I&#39;ve taken so far for finding tests. </p>
<p></p>
<p>EDIT: I see that the instructions from Pushkar in the canvas announcement, say that the test should be via the proctor track 2 link in canvas. Maybe it just doesn&#39;t show up until Dec 6th.</p>",2019-12-04T18:47:41Z,50,Week 12/1 - 12/7,followup,,ixty1midfufhd,k3rn8vs34nf4ny,2019-12-04T18:47:41Z,{},final_exam
4919,no,"<p>Interesting, when I login to ProctorTrack I don&#39;t see any exam info at all</p>
<p></p>",2019-12-04T20:14:49Z,50,Week 12/1 - 12/7,feedback,,jhqqvwpyvXsQ,k3rqcxsn94bns,2019-12-04T20:14:49Z,{},final_exam
4920,no,"<p>Every guide on proctortrack seems to rely on having an onboarding test available under &#34;onboarding data&#34; as shown in the following guide. <a href=""https://verificient.freshdesk.com/support/solutions/articles/1000223641-canvas-student-guide"">https://verificient.freshdesk.com/support/solutions/articles/1000223641-canvas-student-guide</a></p>
<p>I also am unable to see any onboarding tests and am unsure how to move forward.</p>",2019-12-04T14:42:04Z,39,Week 12/1 - 12/7,followup,,jr46k9bbb1g5ju,k3reh0xoyt85ii,2019-12-04T14:42:04Z,{},final_exam
4921,no,Can you check now?,2019-12-05T02:50:04Z,39,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s4h8f0k2d4ih,2019-12-05T02:50:04Z,{},final_exam
4922,no,<p>This may help.  &#64;1097  or more importantly &#64;1098</p>,2019-12-05T06:18:29Z,39,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3sbx9k6l6q1qx,2019-12-05T06:18:29Z,{},final_exam
4923,no,"<p>I&#39;m also seeing the same, not assigned a test.</p>",2019-12-04T14:55:31Z,39,Week 12/1 - 12/7,followup,,jzjzgz6lrdv431,k3reybrxxuw2j4,2019-12-04T14:55:31Z,{},final_exam
4924,no,You should see both the tests now. See &#64;1098.,2019-12-05T14:07:28Z,39,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3ssodnef725hp,2019-12-05T14:07:28Z,{},final_exam
4925,no,"Oh, it&#39;s Dec 5. You cannot see the final yet.",2019-12-05T14:10:00Z,39,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3ssrmyzcgs2ev,2019-12-05T14:10:00Z,{},final_exam
4926,no,"<p>I&#39;ve got the same issue. &#64;1082 seems to suggest that nothing will be available until sometime tomorrow (Dec 5th)? Proctortrack mentions that onboarding must be completed at least 48 hours ahead of time, which should have been today?</p>
<p></p>",2019-12-04T20:12:52Z,39,Week 12/1 - 12/7,followup,,jhqqvwpyvXsQ,k3rqafhp4r3669,2019-12-04T20:12:52Z,{},final_exam
4927,no,You should see both the tests now. See &#64;1098.,2019-12-05T14:07:36Z,39,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3ssojmoi2l5ln,2019-12-05T14:07:36Z,{},final_exam
4928,no,"Oh, it&#39;s Dec 5. You cannot see the final yet. But you should see the Practice Exam.",2019-12-05T14:10:10Z,39,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3ssruqefvf2ie,2019-12-05T14:10:10Z,{},final_exam
4929,no,"<p>This is great! Someone, not me, once suggested that RL doesn&#39;t overfit. I don&#39;t remember who said that, but it&#39;s not correct.</p>
<p></p>
<p>I&#39;ve been playing with my ML4T Q-Learner and various optimization and algo tricks learned in this class, and I can get it to learn the optimal trading policy for in-sample periods like magic. For instance, one stock learns a 14X return on the training period. When I apply the same Q (Double-Q in this instance) to the out-sample period, it performs horribly (20% return, -20% return sometimes).</p>
<p></p>
<p>This is all off-line learning. I haven&#39;t tried an on-line learning method yet.</p>",2019-12-05T00:00:41Z,49,Week 12/1 - 12/7,followup,,jc554vxmyuy3pt,k3ryfekt3it17c,2019-12-05T00:00:41Z,{},other
4930,no,<p>Is that using function approximation?</p>,2019-12-05T01:27:49Z,49,Week 12/1 - 12/7,feedback,,jzfsa4a37jf4aq,k3s1jgxowdm5f4,2019-12-05T01:27:49Z,{},other
4931,no,<p>no. i am using zcut to bin my indicators.</p>,2019-12-05T03:28:55Z,49,Week 12/1 - 12/7,feedback,,jc554vxmyuy3pt,k3s5v7afm15va,2019-12-05T03:28:55Z,{},other
4932,no,"<p>you definately need the model (learned, or otherwise). </p>
<p></p>
<p>&#34;policy flow&#34; is just nothing other than a network, with conservation of flow, capacity constraints (i.e. the q.sa) and an &#34;initial&#34; dumping of utility into the network until it finds its steady state (similar to randomly traversing an mdp until u get its steady state) -- which due to the constraints and the minimizing/maximizing, will be the correct answer.  since there isn&#39;t an explicit sink; the evaporation (discount factor? I can&#39;t remember right now) serves as a loss at each node.</p>",2019-12-07T20:32:14Z,22,Week 12/1 - 12/7,followup,,jzivtxcbl6964n,k3w1awenv7953x,2019-12-07T20:32:14Z,{},other
4933,no,Thank you! Very helpful ,2019-12-04T08:15:36Z,50,Week 12/1 - 12/7,followup,,jfzaqnqvtQ1m,k3r0o0vhd2324p,2019-12-04T08:15:36Z,{},final_exam
4934,no,"<p>I&#39;m also interested in recommendations of RL newsletters if they exist. I&#39;ve been subbed to an RL oriented Google group for years, but it&#39;s not been particularly great.</p>
<p></p>
<p>One resource I recommend is the YT channel Two Minute Papers which isn&#39;t focused on RL exclusively, but does a great job covering advances in RL as they come up: <a href=""https://www.youtube.com/user/keeroyz"">https://www.youtube.com/user/keeroyz</a></p>
<p></p>
<p>Also downloaded the arXiv Android app and subbed to RL publications there, not with a goal of reading N papers, but with skimming publication titles every day or so, reading abstracts of interesting titles, and reading papers of interesting abstracts. It&#39;s been a decent way of filtering things down while keeping an eye on the pulse of what&#39;s published.</p>",2019-12-05T00:29:52Z,50,Week 12/1 - 12/7,followup,,isde332xcka1m0,k3rzgxyr1m1wq,2019-12-05T00:29:52Z,{},other
4935,no,"<p>Follow a few key people in Researchgate and Google Scholar, and you&#39;ll get notifications not only of their latest publications but also those who quoted their papers.  </p>
<p></p>
<div>
<div></div>
</div>",2019-12-06T10:49:15Z,50,Week 12/1 - 12/7,followup,,jzh6k6o994a6dh,k3u11bq8ocx35j,2019-12-06T10:49:15Z,{},other
4936,no,"There are lots of nice article here:<div><a href=""https://thegradient.pub/"">https://thegradient.pub/</a></div><div><br /></div><div>But it’s not limited to RL.</div>",2019-12-06T13:46:45Z,50,Week 12/1 - 12/7,followup,,jl1b27fpaYkv,k3u7dlcspyw7gp,2019-12-06T13:46:45Z,{},other
4937,no,"<p>As a follow-up, I&#39;m struggling to even understand even Vahe&#39;s answer. As Anon said earlier in that thread:</p>
<p></p>
<p>False, Given that the R(s) = -.04 for all states except for terminating the states which are 1,-1 respectively; An increase of 10 to all states would incentivize the learner to traverse the non-terminating states collecting .96 reward forever, instead of moving towards the positive terminating state.</p>
<p></p>
<p>I believe they meant 9.96 rather than 0.96, but still, as long as the reward for moving back and forth between two states is positive, why would the agent seek a terminal state?</p>",2019-12-04T19:57:30Z,28,Week 12/1 - 12/7,followup,,jl284xdcifz44g,k3rpqo9goiv3r4,2019-12-04T19:57:30Z,{},final_exam
4938,no,"I believe the assumption is that the agent would still try to get to the terminal state as soon as possible, and then it respawns, and tries to get to the terminal state as soon as possible, and then respawns, and this repeats. This is better than going back and forth between two non-terminal states, assuming the agent gets to continue playing forever even after reaching a terminal state.",2019-12-12T21:23:39Z,27,Week 12/8 - 12/14,feedback,,jzj4sh1p7pf5af,k438c9usm8z1il,2019-12-12T21:23:39Z,{},final_exam
4939,no,"Generally, would you say this is only true in a finite horizon case if, in the original reward function, there is at least one reward r that is greater than -10 but smaller than 0? 
Because if this was so, then states which originally yield negative reward would now yield positive reward, and become desirable to the learner. Otherwise, the preference over actions at each state would remain unchanged, thus the policy remains the same. Is this correct?",2019-12-07T08:52:44Z,28,Week 12/1 - 12/7,followup,,jl0d29plhbs6cl,k3vcbcal8x22cz,2019-12-07T08:52:44Z,{},final_exam
4940,no,"<p>We saw this in Greenwald 2003 in Table 1 where she deducts $$\gamma R_i$$ from the reward. This only scales down the final Q value that you saw in your graphs. I saw the values go from 90 down to 9.</p>
<p></p>
<p>$$Q_i(s,\vec{a}) = (1-\alpha)Q_i(s,\vec{a}) &#43; \alpha[(1-\gamma)R_i &#43; \gamma V_i(s&#39;)]$$</p>
<p></p>
<p>This did not change the policy in any way that I could detect. The graphs were all the same, just scaled down.</p>
<p></p>
<p>This makes sense intuitively. Any value $$R_i$$ for a reward is just the sum of some other values $$\sum_n{\hat{R_n}}$$, correct? For instance, a reward of 3 is just a reward of 0 &#43; 3, etc. You can characterize any sum like this, just like you can characterize the multiplication as a bunch of sums. If multiplication does not affect the policy  then neither can addition because $$nR$$ is just $$R &#43; R &#43; R &#43; R .... &#43; R_n$$, correct?</p>
<p></p>
<p></p>",2019-12-12T22:07:20Z,27,Week 12/8 - 12/14,followup,,jc554vxmyuy3pt,k439wg3nygn57k,2019-12-12T22:07:20Z,{},final_exam
4941,no,<p>Also note that Greenwald removed this scaling in her submission to AAAI in 2002.</p>,2019-12-12T22:08:19Z,27,Week 12/8 - 12/14,feedback,,jc554vxmyuy3pt,k439xply7au7ki,2019-12-12T22:08:19Z,{},final_exam
4942,no,"<p>I noted this before, but I believe the $$(1-\gamma)$$ scaling on the reward in Greenwald 2003 was superfluous because it only applied to the terminal state of each episode (since all other rewards were $$0$$).</p>
<p></p>
<p>So I agree that there should be no effect from it other than to make smaller numbers on graphs.</p>",2019-12-12T22:26:12Z,27,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k43akps8vt1p,2019-12-12T22:26:12Z,{},final_exam
4943,no,"<p><strong attention=""hgq7wx2g0LO"">&#64;Pushkar Kolhe</strong>  -- Thanks. It&#39;s still showing the following error message for me:</p>
<p><br />&#34;This test starts at 2019-12-05 14:00:00&#43;00:00. You can start a test session only upto 1 minutes before the test starts.&#34;</p>
<p></p>
<p>Or should we be expecting this for the set up exam? I was under the impression that it should be open.</p>",2019-12-05T03:02:46Z,50,Week 12/1 - 12/7,followup,,jqq4kjckTEy3,k3s4xknzz2cdc,2019-12-05T03:02:46Z,{},final_exam
4944,no,"<p><strong attention=""hgq7wx2g0LO"">&#64;Pushkar Kolhe</strong>  -- The issue I was having related to logging into Proctortrack directly, instead of through Canvas. I see now that taking the test through Canvas is the correct way, as you said here: <a href=""/class/jzh9tkzzxkd7ph?cid=1097"">https://piazza.com/class/jzh9tkzzxkd7ph?cid=1097</a></p>
<p></p>
<p>Thanks for your help!</p>",2019-12-05T03:21:25Z,50,Week 12/1 - 12/7,feedback,,jqq4kjckTEy3,k3s5ljqqw4j2gf,2019-12-05T03:21:25Z,{},final_exam
4945,no,"<p>Hi Pushkar, I posted a private Piazza message to you but thought I would respond to this post.</p>
<p></p>
<p>When I attempt to login on any of the &#34;Test Your Setup&#34; assignments through Canvas I get this.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fis9so9huTMp%2Fk3s88dxkqhpx%2FPT.png"" alt="""" width=""644"" height=""314"" /></p>",2019-12-05T04:36:10Z,50,Week 12/1 - 12/7,followup,,is9so9huTMp,k3s89oayafs3tn,2019-12-05T04:36:10Z,{},logistics
4946,no,Thank you for letting me know. I am checking into this.,2019-12-05T04:36:42Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s8acyss0n1yu,2019-12-05T04:36:42Z,{},logistics
4947,no,"<p>It worked for me by going to the &#34;Proctortrack 2&#34; option in canvas, and it showed both the final and the test setup. However I got the same error as Nick accessing through the Assignments option.</p>",2019-12-05T04:41:34Z,50,Week 12/1 - 12/7,feedback,,is6e83bsfvk,k3s8gmpr2yb27u,2019-12-05T04:41:34Z,{},logistics
4948,no,&#43;1. I am getting this screen as well. I was trying to test my set up,2019-12-05T04:52:39Z,50,Week 12/1 - 12/7,feedback,,jvfpllmsggt7p4,k3s8uvkwe4d65z,2019-12-05T04:52:39Z,{},logistics
4949,no,"You can access PT through the &#34;ProctorTrack2&#34; link and get the access code for the exam. I just verified the setup.

For some reason, the &#34;Get Access Code&#34; button doesn&#39;t work. I am trying to fix it.",2019-12-05T05:01:34Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s96cbk51d5gq,2019-12-05T05:01:34Z,{},logistics
4950,no,"<p>OK... when I used the ProctorTrack2 link in the bottom of the links in Canvas it did bring me to the ProctorTrack application within Canvas.  I was able to see the &#34;secret code&#34; and was prompted to ask a question.  I&#39;m not sure if I was able to actually finish the practice exam though.  It seems I got further by using this method.</p>
<p></p>
<p>When I click the &#34;Get Access Code&#34; it brought me to the screen/error I showed above.</p>
<p></p>",2019-12-05T05:03:28Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3s98sih3bp19d,2019-12-05T05:03:28Z,{},logistics
4951,no,"I changed the &#34;Get Access Code&#34; link so that it goes to the ProctorTrack2 page. You should be able to click on &#34;Go to Test&#34; there and start the PT session. 

After you have started the session, you will see the code on the top right.",2019-12-05T05:20:15Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s9udbmirp4z2,2019-12-05T05:20:15Z,{},logistics
4952,no,"<p>Once we see the code on the top right, where should we enter that code?</p>
<p></p>",2019-12-05T05:20:53Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3s9v6lxy2448r,2019-12-05T05:20:53Z,{},logistics
4953,no,Go back to Canvas to the Exam. Below the Get Access Code is the &#34;Take Quiz&#34; link. There you will enter it.,2019-12-05T05:21:31Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3s9w0d32ra6tv,2019-12-05T05:21:31Z,{},logistics
4954,no,<p>I do not see that Take Quiz button anymore but did during my earlier iteration.  I do not think I entered anything after that though.  If we don&#39;t enter the right &#34;code&#34; will we need to re-take the practice quiz?</p>,2019-12-05T05:27:13Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3sa3c4mn6o13g,2019-12-05T05:27:13Z,{},logistics
4955,no,I don&#39;t think so. I think you can enter the code as many times as you can. Let me check that too.,2019-12-05T05:30:47Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sa7x3wges30s,2019-12-05T05:30:47Z,{},logistics
4956,no,"Yup, you can submit a lot of times.",2019-12-05T05:31:30Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sa8ukx2qw519,2019-12-05T05:31:30Z,{},logistics
4957,no,<p>The problem I have now then is that I don&#39;t see the &#34;Take Quiz&#34; button anymore.  Might you be able to enable that again?</p>,2019-12-05T05:32:43Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3saaeql4l977d,2019-12-05T05:32:43Z,{},logistics
4958,no,"Doesn&#39;t seem like I can see your submission. Let me check anyways.

&#64;1097",2019-12-05T05:59:35Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sb8y97s5x56t,2019-12-05T05:59:35Z,{},logistics
4959,no,"Nick, your PT session has ended. So you don&#39;t see the link.",2019-12-05T06:01:50Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sbbudupl669u,2019-12-05T06:01:50Z,{},logistics
4960,no,"<p>I ended up trying it multiple times, I have the email messages of the submissions but did not enter the code.</p>
<p></p>
<p>Should I try again?   Sorry, I don&#39;t mean to be such a pain but hope to work out the kinks with this practice test.</p>",2019-12-05T06:02:42Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3sbcyi7f1w4gt,2019-12-05T06:02:42Z,{},logistics
4961,no,"Yup, I have given you another attempt at it.",2019-12-05T06:04:00Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3sben0bnc41ac,2019-12-05T06:04:00Z,{},logistics
4962,no,"<p>Great, I believe it worked that time!  Does it look good on your end?  I want to thank you for looking into this before the weekend starts.  Hopefully this Piazza post will help others.</p>",2019-12-05T06:09:19Z,50,Week 12/1 - 12/7,feedback,,is9so9huTMp,k3sblgoo3yj5fp,2019-12-05T06:09:19Z,{},logistics
4963,no,I hope so too.,2019-12-05T14:05:55Z,50,Week 12/1 - 12/7,feedback,,hgq7wx2g0LO,k3ssme5cimr4f0,2019-12-05T14:05:55Z,{},logistics
4964,no,<p>I am trying to take onboarding test and after entering access code I don&#39;t see the question I just see the screen with access code  input nothing else it redirects to the same page</p>,2019-12-07T01:34:51Z,50,Week 12/1 - 12/7,feedback,,jqtti9gfnagH,k3uwo7wh48g1re,2019-12-07T01:34:51Z,{},logistics
4965,no,<p>I am trying to take onboarding test and after entering access code I don&#39;t see the question I just see the screen with access code  input nothing else it redirects to the same page</p>,2019-12-07T01:35:03Z,50,Week 12/1 - 12/7,followup,,jqtti9gfnagH,k3uwogpnxkb1wa,2019-12-07T01:35:03Z,{},logistics
4966,no,This is resolved,2019-12-07T01:56:56Z,50,Week 12/1 - 12/7,feedback,,jqtti9gfnagH,k3uxgmazpzu2o4,2019-12-07T01:56:56Z,{},logistics
4967,no,"<p>How did you resolve this? I currently have the same issue</p>
<p></p>",2019-12-07T22:36:17Z,50,Week 12/1 - 12/7,feedback,,jzm01jjgolv32u,k3w5qf6psbx3x4,2019-12-07T22:36:17Z,{},logistics
4968,no,"<p>Turns out the access code was just the two words and not three. Other instructions threw me off. Works fine</p>
<p></p>",2019-12-07T22:40:56Z,50,Week 12/1 - 12/7,feedback,,jzm01jjgolv32u,k3w5wecqnc83je,2019-12-07T22:40:56Z,{},logistics
4969,no,"<p>For other who are trying this, onboarding has two works with a space. test has three words with a space between each. I&#39;ve noticed that it could be on multiple lines as well.</p>",2019-12-08T00:23:37Z,49,Week 12/8 - 12/14,feedback,,ixty1midfufhd,k3w9kg2l91a4hn,2019-12-08T00:23:37Z,{},logistics
4970,no,I agree space between the words throw me off,2019-12-08T01:25:21Z,49,Week 12/8 - 12/14,feedback,,jqtti9gfnagH,k3wbru854tl4zw,2019-12-08T01:25:21Z,{},logistics
4971,no,"Yes. <strong>The test exam has 2 words with space. The actual exam has 3 words with a space between each.</strong>

Next time, I would not add a space at all. I did not know we would have such problems with how the access code is shown in PT.",2019-12-08T02:56:41Z,49,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3wf1ayph4u4pv,2019-12-08T02:56:41Z,{},logistics
4972,no,"<p></p><blockquote>
<p>Next time, I would not add a space at all.</p>
</blockquote>
<p>Please do. This has been driving me nut for the last 20 minutes.</p>",2019-12-08T08:41:44Z,49,Week 12/8 - 12/14,feedback,,jl5wq8mca7o0,k3wrd11ldgl4a7,2019-12-08T08:41:44Z,{},logistics
4973,no,"<p>I am able to start a session in Protractor but the access code is blank what do I do?</p>
<p></p>",2019-12-09T10:36:11Z,49,Week 12/8 - 12/14,feedback,,jlrcqhl2UKpE,k3yaw2yzfk3631,2019-12-09T10:36:11Z,{},logistics
4974,no,"Hi Andrzej, were you able to resolve this? You should ask PT if they can share the access code because you should see it in their app.",2019-12-09T17:30:34Z,49,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3ypoz2kyhs30g,2019-12-09T17:30:34Z,{},logistics
4975,no,"<p>yes thanks!</p>
<p></p>",2019-12-09T21:49:21Z,49,Week 12/8 - 12/14,feedback,,jlrcqhl2UKpE,k3yyxruhm1m2zm,2019-12-09T21:49:21Z,{},logistics
4976,no,"<p></p><div>
<div>
<div>
<div>
<h1>We couldn&#39;t find what you<br />were looking for...</h1>
</div>
</div>
</div>
</div>
<div>
<div>
<div>
<h1>403 error</h1>
<p>It looks like the page you&#39;re looking for doesn&#39;t exist, or you don&#39;t have permission. Please try to enable cookies and log in again to your LMS</p>
<p>Try going back or refreshing the page you were on previously. If you&#39;re totally lost, check out the options below.</p>
<a href=""http://verificient.freshdesk.com/"" target=""_blank"" rel=""noopener noreferrer""> Help Center</a></div>
<div></div>
<div><span style=""text-decoration:underline""><strong>In canvas when I try to access the test your setup page</strong></span></div>
</div>
</div>",2019-12-07T20:31:18Z,18,Week 12/1 - 12/7,followup,,jcg0nzvdk8272b,k3w19orz2jt77c,2019-12-07T20:31:18Z,{},final_exam
4977,no,"<p>Update: nvm, turns out you have to run with PT open</p>",2019-12-07T20:34:42Z,18,Week 12/1 - 12/7,feedback,,jcg0nzvdk8272b,k3w1e2i5xkm43t,2019-12-07T20:34:42Z,{},final_exam
4978,no,"Note: The access code is 3 words separated by a space. I realized that on certain computer resolution, you might not see the 3rd word. The third word is brain.",2019-12-07T21:28:44Z,18,Week 12/1 - 12/7,followup,,hgq7wx2g0LO,k3w3bjxl4439c,2019-12-07T21:28:44Z,{},final_exam
4979,no,"<p>So, is the access code is &#34;really secret brain&#34; ?</p>",2019-12-08T07:52:02Z,18,Week 12/1 - 12/7,feedback,,jl8j7vzvUNs2,k3wpl447a5e6op,2019-12-08T07:52:02Z,{},final_exam
4980,no,"<p>I want to bump this up as well. Just want to have a confirmation, since both space-separated and nonspace do not work for me.</p>",2019-12-08T08:31:33Z,18,Week 12/1 - 12/7,feedback,,jl5wq8mca7o0,k3wqzxt12lt5jb,2019-12-08T08:31:33Z,{},final_exam
4981,no,"<p>Copied from &#64;1097:</p>
<blockquote>
<p><strong>The test exam has 2 words with space. The actual exam has 3 words with a space between each.</strong></p>
</blockquote>",2019-12-08T08:42:27Z,18,Week 12/1 - 12/7,feedback,,jl5wq8mca7o0,k3wrdysnjpd4pq,2019-12-08T08:42:27Z,{},final_exam
4982,no,"<p>Hi, I think I followed a different flow for the onboarding. Would anyone be able to confirm whether it went through? I went directly through the exam link in the &#34;Proctortrack 2&#34; section, and there seemed to be some weird bugginess with the &#34;access code&#34; button on the exam page.</p>
<p></p>
<p></p>
<p>From the message it sounds like it submitted alright, but just wanted to double check. It&#39;s now saying:</p>
<p>&#34;Your quiz has been muted<br />Your quiz score is unavailable because your instructor has not finished grading it. When your instructor finishes grading this quiz, your score will be available on this page.&#34;</p>
<p></p>",2019-12-08T07:15:03Z,18,Week 12/1 - 12/7,followup,,jznnnvgcyam1mv,k3wo9kgz10p4tx,2019-12-08T07:15:03Z,{},final_exam
4983,no,"<p>click on &#39;Finals&#39; under &#39;Grade&#39;, you&#39;ll see that your final is submitted when. that is how I confirmed it&#39;s submitted. hope it&#39;s ok.</p>",2019-12-08T21:40:00Z,17,Week 12/8 - 12/14,feedback,,j6ilcw6hxoc77h,k3xj5wfhhj1r,2019-12-08T21:40:00Z,{},final_exam
4984,no,<p>Yes that will allow you to confirm it was submitted.</p>,2019-12-09T01:51:51Z,17,Week 12/8 - 12/14,feedback,,hz7meu55mi8sd,k3xs5rwh6e5t2,2019-12-09T01:51:51Z,{},final_exam
4985,no,"<p>I haven&#39;t taken an exam through Canvas before, only on ProctorTrack directly where you submit and the session is ended for you. For the onboarding quiz, I just ended the proctor session myself after submission. Is that correct, or is there another way to end the proctor session?</p>",2019-12-09T18:12:07Z,17,Week 12/8 - 12/14,followup,,jc6y8iyomz41me,k3yr6etfdid1e,2019-12-09T18:12:07Z,{},final_exam
4986,no,<p>Vahe answered many of them.</p>,2019-12-06T05:08:32Z,24,Week 12/1 - 12/7,followup,,gx3c8l7z7r72zl,k3tov5e1kpe5rs,2019-12-06T05:08:32Z,{},final_exam
4987,stud,<p>Not all answers are the same with the instructor&#39;s expectation </p>,2019-12-06T16:43:34Z,24,Week 12/1 - 12/7,feedback,a_0,,k3udoyw8upd5uh,2019-12-06T16:43:34Z,{},final_exam
4988,no,"<p>Right, Vahe&#39;s answers are very helpful, but I&#39;d feel much more comfortable seeing a TA confirm them</p>",2019-12-06T16:46:10Z,24,Week 12/1 - 12/7,feedback,,jl284xdcifz44g,k3udsbh8y8s1cp,2019-12-06T16:46:10Z,{},final_exam
4989,no,<p>Isn&#39;t Vahe a TA?</p>,2019-12-07T17:21:43Z,24,Week 12/1 - 12/7,feedback,,gx3c8l7z7r72zl,k3vuhvs322s7fv,2019-12-07T17:21:43Z,{},final_exam
4990,no,"<p>No, Vahe is not a TA, he is a student just like us taking the class. However, his answers are very helpful and often right on point. If you want to see who the TAs are, they are listed in the Resources tab, under staff.</p>
<p></p>
<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fixty1midfufhd%2Fk3vyv736c4i1%2Fstaff.PNG"" alt="""" /></p>",2019-12-07T19:28:36Z,24,Week 12/1 - 12/7,feedback,,ixty1midfufhd,k3vz1200wgk1sw,2019-12-07T19:28:36Z,{},final_exam
4991,no,"<p>lol.  no wonder why this reminded me of the look-ahead question in TD-0 vs TD-1.  slightly different focus (on constraining upper and lower bounds of error); but the essence of it is essentially the same question solved by TD-lambda; just focusing more on the notion of interval bounding estimates via bootstrapped samples, intermediate estimates and discounted reward sequences and extending optimality to include the spread on the bounds with respect to value-estimation ordering.</p>
<p></p>
<p>extended from infinite game sequence-based learning, brought down to finite-step runs.  there should be a direct analogue then, to TD-lambda.  in fact it rather reminds me of some interesting restrictions in nash-q  (in the vague sense that it should be possible to mathematically prove there must exist some intermediate step along a finite-horizon, that approximates the performance of a truncated infinite game horizon).</p>
<p></p>
<p>kitschy.  </p>
<p></p>
<p>and the stationary plan assumption has some additional interesting properties that are a bit, over-strict (can be relaxed).</p>",2019-12-06T09:47:57Z,23,Week 12/1 - 12/7,followup,,jzivtxcbl6964n,k3tyuhvupsg53r,2019-12-06T09:47:57Z,{},other
4992,no,"<p>Actually Anon brings up a really good point that I don&#39;t think is properly addressed in the video.</p>
<p></p>
<p>$$k$$ is a random variable.</p>
<p></p>
<p>The errata captions at the bottom of the video help a little by saying that the expectations are over &#34;all sequences of length $$k$$,&#34; but it&#39;s still not clear from that whether the sequences are random but have fixed length $$k$$, or whether $$k$$ is also random. </p>
<p></p>
<p>If we really want to treat an option as an abstract action, from one arbitrary point to another, then we need the number of atomic time steps composing that option to be random, so this formulation makes sense.</p>
<p></p>
<p>My interpretation is based on Sutton and Barto page 462.</p>",2019-12-06T07:13:58Z,32,Week 12/1 - 12/7,followup,,jzfsa4a37jf4aq,k3ttcgn5e058s,2019-12-06T07:13:58Z,{},review
4993,stud,<p>Thanks Vahe. Reviewing the bit from S&amp;B you referenced cleared it up for me.</p>,2019-12-07T05:44:47Z,32,Week 12/1 - 12/7,feedback,a_0,,k3v5lmc7n7i7i1,2019-12-07T05:44:47Z,{},review
4994,no,"<p>Beta interrupts actions; therefore k is supplied post-fact (I suspect); it is neither a random variable, nor a k-step estimator (although either could be plausible -- the former due to the probablistic termination stats; the latter due to the rollup).<br /><br />the simplest thing: keep track of when ur option started; index to the common time frame; pass in as a parameter to R &amp;&amp; F.<br /><br />the fact that the option (t ... t&#43;k) can be terminated at t &#43;k -m, means ur k is never set until it actually runs.<br /><br />there is an alternate formulation where options are seen as goal abstraction (rather than temporal abstraction) where the options are running in parallel that might lend itself to k-step estimator rollup (estimated final discount) -- with respect to evaluation for selection).  but this is because the options are allowed to explicitly interrupt each other.   no such mention is made in temporally extended actions (which presumably, upon initial selection, will run according to their termination state without agent-based (non-environmental) pre-emption).</p>",2019-12-07T20:27:12Z,32,Week 12/1 - 12/7,followup,,jzivtxcbl6964n,k3w14f5v86b2d3,2019-12-07T20:27:12Z,{},review
4995,no,"<p><strong attention=""hyxsfbkeit22m2"">&#64;Alec Feuerstein</strong>  </p>",2019-12-06T16:51:01Z,24,Week 12/1 - 12/7,followup,,hyx9thiqa6j4nn,k3udyjrwbry604,2019-12-06T16:51:01Z,{},final_exam
4996,no,Im down! Used to do this at NYU all the time. Definitely helpful. ,2019-12-06T17:30:42Z,24,Week 12/1 - 12/7,followup,,jzj4sh1p7pf5af,k3ufdl3bpe01me,2019-12-06T17:30:42Z,{},final_exam
4997,no,<p>I&#39;m in</p>,2019-12-06T20:45:55Z,24,Week 12/1 - 12/7,followup,,j6m1jeidndu6wq,k3umcmuvn7e6u0,2019-12-06T20:45:55Z,{},final_exam
4998,no,<p>I&#39;ll post a google hangouts link here tomorrow around 1pm ET. Thanks for the interest so far!</p>,2019-12-06T23:40:34Z,24,Week 12/1 - 12/7,followup,,jl284xdcifz44g,k3usl8dboww5cm,2019-12-06T23:40:34Z,{},final_exam
4999,no,"<p><a href=""https://hangouts.google.com/call/DRHsw3ZoTc2wQQEigjZYAEEE"">https://hangouts.google.com/call/DRHsw3ZoTc2wQQEigjZYAEEE</a></p>
<p></p>
<p>Here&#39;s the link for anyone who wants to join!</p>",2019-12-07T18:00:07Z,24,Week 12/1 - 12/7,followup,,jl284xdcifz44g,k3vvv9x27xd7p0,2019-12-07T18:00:07Z,{},final_exam
5000,no,"<p>Dalton, here&#39;s the Rmax lecture we were talking about: <a href=""https://www.youtube.com/watch?v=z64Exu8Uxbg"">https://www.youtube.com/watch?v=z64Exu8Uxbg</a></p>
<p></p>",2019-12-07T19:08:11Z,24,Week 12/1 - 12/7,followup,,j6m1jeidndu6wq,k3vyasyqhsp5fb,2019-12-07T19:08:11Z,{},final_exam
5001,no,<p>&#64;1094</p>,2019-12-07T01:13:43Z,31,Week 12/1 - 12/7,followup,,jzfsa4a37jf4aq,k3uvx1imknx40w,2019-12-07T01:13:43Z,{},other
5002,no,"<p>Ah, cool (I&#39;ve been ignoring review question discussions so far). So I think what I&#39;ve understood is that in a continuous discounted problem, adding a constant to each state will not change the optimal policy, but it can change it in an episodic problem?</p>",2019-12-07T16:53:39Z,31,Week 12/1 - 12/7,feedback,,jl3oi5v7qkSk,k3vthsuf1i03kd,2019-12-07T16:53:39Z,{},other
5003,no,<p>Thanks!</p>,2019-12-08T00:22:17Z,18,Week 12/1 - 12/7,followup,,ixty1midfufhd,k3w9iqs1f4w30x,2019-12-08T00:22:17Z,{},final_exam
5004,no,Any ETA on project 3 grade ??,2019-12-13T23:09:47Z,17,Week 12/8 - 12/14,followup,,jqtti9gfnagH,k44rkm6wpla1db,2019-12-13T23:09:47Z,{},final_exam
5005,no,"Interesting. This is similar to what I found when creating batches from instances in the state space which had the furthest distance from each other. By increasing the variance in the state space sampling, as by completely random exploring, it produces the least bias in which  areas of the state space are explored and updated with gradient descent.<div><br /></div><div>I like to think of it as wrinkles in a sheet where you want to flatten it as much as possible, so to do so you pat out the entire sheet equally or parts of it will be depressed and distorted out of shape quickly. This blanket represents the accuracy or how close the function approximator is to ideal. By flatten it out evenly we ensure that while we increase the accuracy in some sections of it places far away don’t fall out of wack, quickly. </div>",2019-12-08T18:05:04Z,15,Week 12/8 - 12/14,followup,,jzj4sh1p7pf5af,k3xbhhcp65n5h,2019-12-08T18:05:04Z,{},project2
5006,no,"<p>Nice post.  I have a few comments.</p>
<p></p>
<p>&#34;<em>Specifically, that the best results I got were by setting ϵ=1 and keeping it there.&#34;</em></p>
<p></p>
<p>You mean $$\epsilon = 0$$, right?  Or do you mean you were the opposite of greedy, and acted randomly?  For me, $$\epsilon=0$$ worked, but was inferior to a very small exploration rate ($$\epsilon=0.2$$ or $$0.1$$ to start, and a steady-state of $$0.01$$ when searching for good weights), both in training speed and training quality.</p>
<p></p>
<p>&#34;<em>but they all eventually lead to the optimal value function&#34;</em></p>
<p></p>
<p>How can you be sure what the optimal value function or optimal policy is?  Surely not by just eyeballing the landing?  Scores for &#34;good&#34; landings can range from 235 to 335.  Where in that range would a globally optimal lander&#39;s scores be distributed?</p>
<p></p>
<p>The metric I used to evaluate landers was the average score of the trained agent over many episodes (many more than 100), i.e. an estimate of the true average reward.  This seemed to be the most logical metric, since maximizing expected reward is what the typical RL objective is.  But I have no idea what the globally optimal lander would average.</p>
<p></p>
<p>You may be right that it&#39;s only incrementally better than what&#39;s been achieved and may have a histogram that looks like the one I posted in my LL post.  But it&#39;s also possible that there&#39;s a lander out there that can land 300&#43; landings every time.</p>
<p></p>
<p>&#34;<em>I prefilled the replay buffer with the agent randomly doofing around, which I think gave it some additional &#34;free exploration&#34;.</em>&#34;</p>
<p></p>
<p>I tried this, with various amounts of prefill, and didn&#39;t notice much of a difference either way.  Like you pointed out there are a lot of sources of stochasticty in Lunar Lander.  The fact that a flat $$\epsilon=0$$ can even work reasonably well is proof of that.</p>",2019-12-08T21:49:00Z,15,Week 12/8 - 12/14,followup,,jzfsa4a37jf4aq,k3xjhgspe6158g,2019-12-08T21:49:00Z,{},project2
5007,no,"<p>My exam was submitted over 12 hours ago and I still haven&#39;t received any sort of email.  </p>
<p></p>
<p>&#64;Pushkar maybe a clue as to what we should expect to see confirming our exam was received.  I&#39;m sure you&#39;re getting a lot of these &#34;did you get my test&#34; inquiries.</p>
<p></p>
<p></p>",2019-12-08T22:08:31Z,21,Week 12/8 - 12/14,followup,,is9so9huTMp,k3xk6klfd2j46q,2019-12-08T22:08:31Z,{},final_exam
5008,no,"Someone else said that if they go to &#34;ProctorTrack2&#34; they see the submission is done.

I don&#39;t think students get an email.. but I also don&#39;t know how students can verify that submission was done yet.",2019-12-09T02:41:06Z,21,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3xtx3u7vso16y,2019-12-09T02:41:06Z,{},final_exam
5009,no,"<p><img src=""/redirect/s3?bucket=uploads&amp;prefix=attach%2Fjzh9tkzzxkd7ph%2Fjzfsa4a37jf4aq%2Fk3xu2aolp5b%2FCapture.PNG"" alt="""" /></p>",2019-12-09T02:45:28Z,21,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k3xu2qcocmb1uv,2019-12-09T02:45:28Z,{},final_exam
5010,no,&#64;1129,2019-12-09T02:48:38Z,21,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3xu6sro8iq718,2019-12-09T02:48:38Z,{},final_exam
5011,no,"<p>Yeah, the above is an alternate page where you can view confirmation of submission.</p>",2019-12-09T02:53:04Z,21,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k3xuci9lx16103,2019-12-09T02:53:04Z,{},final_exam
5012,no,"<p>Yes, this is really confusing. Looks like it is a new version of proctortrack.</p>
<p></p>
<p>I spent 30 min on screen share with proctortrack team for setup exam and they could not tell me if my data was uploaded successfully or not.</p>
<p></p>
<p>The test does not end by itself anymore, as earlier and does not say anything on if all results were saved. It did show upload successful.</p>
<p></p>
<p>I contacted them again after finishing the exam. They still could not confirm if they can see my test data. </p>
<p></p>
<p>I see what Pushkar mentioned in the other post to verify in my canvas but I am still not sure if my test results are saved. Really confusing.</p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>
<p></p>
<p></p>
<p></p>
<p></p>",2019-12-09T02:57:57Z,21,Week 12/8 - 12/14,followup,,jfzaqnqvtQ1m,k3xuis5am733dc,2019-12-09T02:57:57Z,{},final_exam
5013,no,<p>Yeah this is different than in years past.  Maybe next semester they will fix the process flow here.  Thanks to all the TA&#39;s who helped out and confirmed exams were uploaded.</p>,2019-12-09T03:52:50Z,21,Week 12/8 - 12/14,feedback,,is9so9huTMp,k3xwhdbiwk0294,2019-12-09T03:52:50Z,{},final_exam
5014,no,We have both of your answers.,2019-12-09T17:35:08Z,21,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3ypuulbu6m3qx,2019-12-09T17:35:08Z,{},final_exam
5015,no,"<p>Yeah I hit the time limit for the exam so I&#39;ve no idea if it submitted or not. Can&#39;t see any clear indication on canvas. Maybe it would be obvious if there was an &#34;incomplete&#34; Canvas exam page I could compare it to.<br /><br />I know proctortrack got my data, but that&#39;s monitoring data, not the exam itself. You could be monitored successfully but still not have successfully completed the exam since the data goes to two different parties.</p>",2019-12-09T15:37:32Z,21,Week 12/8 - 12/14,followup,,ixr4jvzzg1zba,k3ylnlpzdwujm,2019-12-09T15:37:32Z,{},final_exam
5016,no,We have your answers as well.,2019-12-09T17:35:46Z,21,Week 12/8 - 12/14,feedback,,hgq7wx2g0LO,k3ypvo5gb8d4m0,2019-12-09T17:35:46Z,{},final_exam
5017,no,Same here. I got a verification email for the test set up but not for the exam. Canvas says it was successfully submitted however. Can instructors please comment ?<p></p><p></p>,2019-12-08T22:32:35Z,17,Week 12/8 - 12/14,followup,,i4jbttw9ru63ot,k3xl1i9utg217a,2019-12-08T22:32:35Z,{},final_exam
5018,no,"If you click procktertrack2, you should see that both tests were attempted and there are 2 uploads. That’s some confirmation. I didn’t get an email for the real test as well though.",2019-12-08T22:43:05Z,17,Week 12/8 - 12/14,feedback,,jl1b27fpaYkv,k3xlf0fgjaa6ta,2019-12-08T22:43:05Z,{},final_exam
5019,no,"<p>I couldn&#39;t find what you were talking about, but I did speak to proctortrack chat and they said it was submitted.</p>",2019-12-08T22:54:23Z,17,Week 12/8 - 12/14,feedback,,i4jbttw9ru63ot,k3xltjo52ja2if,2019-12-08T22:54:23Z,{},final_exam
5020,no,Proctortrack doesn’t always send out confirmation emails.,2019-12-10T16:54:51Z,17,Week 12/8 - 12/14,feedback,,jc5n7m3r8yc3br,k403uvt772014e,2019-12-10T16:54:51Z,{},final_exam
5021,no,"<p>500MBs.. wow.. I don&#39;t think I&#39;ve ever seen it upload more then a 20 MB for me... even on the ones that took me hours to do... your webcam must be way better then mine.</p>
<p>But yea. I really wish it provided an email that the answers were uploaded correctly... I&#39;m just hoping they are since the protortrack website days it was uploaded.</p>",2019-12-11T01:47:07Z,17,Week 12/8 - 12/14,followup,,ixty1midfufhd,k40mvdxlmiv681,2019-12-11T01:47:07Z,{},final_exam
5022,no,"<p>Well that makes me nervous, somehow both real test and onboarding test say the uploaded files are around 1 MB. I have a pretty new laptop with a decent webcam, maybe the program determined to either compress a lot or deteriorate the resolution a lot for some reasons? And I have not received any confirmation from the proctor side through email.</p>",2019-12-13T18:10:06Z,17,Week 12/8 - 12/14,followup,,jzjb6ojxtuc1d3,k44gv7sukhy50l,2019-12-13T18:10:06Z,{},final_exam
5023,no,"<p>Thanks Nick.</p>
<p></p>
<p>Here&#39;s my profile:</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/anurag-tangri/"">https://www.linkedin.com/in/anurag-tangri/</a></p>
<p></p>
<p>Thanks,</p>
<p>Anurag Tangri</p>",2019-12-09T03:06:59Z,25,Week 12/8 - 12/14,followup,,jfzaqnqvtQ1m,k3xuue9rvoe556,2019-12-09T03:06:59Z,{},other
5024,no,"<p>good idea. <a href=""https://www.linkedin.com/in/nethiprudhvi/"">https://www.linkedin.com/in/nethiprudhvi/</a></p>",2019-12-09T04:36:17Z,25,Week 12/8 - 12/14,followup,,jl7dp4gg87s5k1,k3xy18ezjup3ij,2019-12-09T04:36:17Z,{},other
5025,no,"<p><a href=""https://www.linkedin.com/in/alanarmen/"">https://www.linkedin.com/in/alanarmen/</a></p>",2019-12-09T05:13:16Z,25,Week 12/8 - 12/14,followup,,jqq4kjckTEy3,k3xzcsoaq944q5,2019-12-09T05:13:16Z,{},other
5026,no,"<p><a href=""https://www.linkedin.com/in/tannerlund/"">https://www.linkedin.com/in/tannerlund/</a></p>",2019-12-09T05:28:05Z,25,Week 12/8 - 12/14,followup,,ixr4jvzzg1zba,k3xzvuu06q9st,2019-12-09T05:28:05Z,{},other
5027,no,"<p>I wouldn&#39;t mind endorsing you if you let me know.</p>
<p><a href=""https://www.linkedin.com/in/mimoralea"">https://www.linkedin.com/in/mimoralea</a></p>
<p></p>",2019-12-09T06:35:47Z,25,Week 12/8 - 12/14,followup,,hyx9thiqa6j4nn,k3y2axdcesx3vd,2019-12-09T06:35:47Z,{},other
5028,no,"<p>i would be grateful if you endorse me:  https://<a href=""https://www.linkedin.com/in/keroles-hakem?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B45F0LYVVQRKN6T10Z5YKbw%3D%3D"">linkedin.com/in/keroles-hakem</a></p>",2019-12-09T19:01:45Z,25,Week 12/8 - 12/14,feedback,,jzlwxqswu5m2e7,k3ysy8r5ucp3gz,2019-12-09T19:01:45Z,{},other
5029,no,"<p>It would mean a lot to me too since I&#39;m going through a total 180 career change from EE.</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/jean-pierre-bianchi-baa62a/"">https://www.linkedin.com/in/jean-pierre-bianchi-baa62a/</a></p>",2019-12-10T09:56:17Z,25,Week 12/8 - 12/14,feedback,,jzh6k6o994a6dh,k3zowlq15re5k2,2019-12-10T09:56:17Z,{},other
5030,no,"<p>It&#39;d be really helpful. Thanks!<br /><br /><a href=""https://www.linkedin.com/in/ishan-maheshwari/"">https://www.linkedin.com/in/ishan-maheshwari/</a></p>",2019-12-10T18:14:14Z,25,Week 12/8 - 12/14,feedback,,jzqia0ganjm1ou,k406ozjqppd2ad,2019-12-10T18:14:14Z,{},other
5031,no,"<p>I would really appreciate that:</p>
<p><a href=""https://www.linkedin.com/in/danilo-martins-14144627/"">https://www.linkedin.com/in/danilo-martins-14144627/</a></p>",2019-12-10T22:53:50Z,25,Week 12/8 - 12/14,feedback,,jc6xvgjncoey,k40gojp3sh31ha,2019-12-10T22:53:50Z,{},other
5032,no,"<p>I would really appreciate it. Thanks</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/gzhu/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/gzhu/</a></p>
<p></p>
<p></p>",2019-12-12T04:04:32Z,25,Week 12/8 - 12/14,feedback,,jzhs489b10i78c,k4277z0hc5w5sf,2019-12-12T04:04:32Z,{},other
5033,no,"I would also appreciate it! Endorsements never hurt.

www.linkedin.com/in/daniel-luci-48a260111",2019-12-12T16:53:31Z,25,Week 12/8 - 12/14,feedback,,jzj4sh1p7pf5af,k42yovim65z57z,2019-12-12T16:53:31Z,{},other
5034,no,"<p>Got you all, except Ishan. I think you need to &#34;friend me&#34; before I can endorse you.</p>",2019-12-12T20:19:16Z,25,Week 12/8 - 12/14,feedback,,hyx9thiqa6j4nn,k4361h39s7od,2019-12-12T20:19:16Z,{},other
5035,no,"<p>I would really appreciate it as well professor!</p>
<p></p>
<p>https://www.linkedin.com/in/aparagas/</p>",2019-12-12T20:28:06Z,25,Week 12/8 - 12/14,feedback,,jqnardlwW3NE,k436cua7qvc4xx,2019-12-12T20:28:06Z,{},other
5036,no,<p>https://www.linkedin.com/in/jacobwanderson/</p>,2019-12-12T21:03:35Z,25,Week 12/8 - 12/14,feedback,,jc554vxmyuy3pt,k437mh1jojy5eb,2019-12-12T21:03:35Z,{},other
5037,no,"<p>I would also really appreciate being endorsed by you!<br /><br /><a href=""https://www.linkedin.com/in/jonathanbeltranus/"">https://www.linkedin.com/in/jonathanbeltranus/</a></p>",2019-12-12T21:23:57Z,25,Week 12/8 - 12/14,feedback,,jqnu4ff8DlD1,k438co5i33u31k,2019-12-12T21:23:57Z,{},other
5038,no,"<p>Same here! Thanks. <a target=""""> </a><a href=""https://www.linkedin.com/in/joaquin-bur-22691685"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/joaquin-bur-22691685</a><a target=""""></a></p>",2019-12-13T00:43:37Z,25,Week 12/8 - 12/14,feedback,,jziegiv4xls3zn,k43fhfl3zrj74,2019-12-13T00:43:37Z,{},other
5039,no,"<p>Joaquin, you might need to add the Reinforcement Learning skill to your profile before I can endorse you.</p>",2019-12-13T06:27:18Z,25,Week 12/8 - 12/14,feedback,,hyx9thiqa6j4nn,k43rrelq3wg4fo,2019-12-13T06:27:18Z,{},other
5040,no,"<p>I&#39;d really appreciate it as well -- thank you!!</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/sylin07/"">https://www.linkedin.com/in/sylin07/</a></p>",2019-12-13T12:56:45Z,25,Week 12/8 - 12/14,feedback,,jzifg1e23c29s,k445o96qor350r,2019-12-13T12:56:45Z,{},other
5041,no,"<p><a href=""https://www.linkedin.com/in/xiangnan-shawn-he-phd-65b97021/"">https://www.linkedin.com/in/xiangnan-shawn-he-phd-65b97021/</a></p>
<p></p>",2019-12-14T04:35:51Z,25,Week 12/8 - 12/14,feedback,,j6mhdov3jg75yk,k4537xo7h0k5oj,2019-12-14T04:35:51Z,{},other
5042,no,"<p>Thanks Miguel, I would also appreciate being endorsed by you.</p>
<p><a href=""https://www.linkedin.com/in/manu-gopalakrishnan-a898306/"">https://www.linkedin.com/in/manu-gopalakrishnan-a898306/</a></p>
<p></p>
<p></p>",2019-12-15T01:00:20Z,25,Week 12/8 - 12/14,feedback,,i4t1oo4aALG,k46ayn7eplc5xg,2019-12-15T01:00:20Z,{},other
5043,stud,"<p><a href=""https://www.linkedin.com/in/george-sherry-8a50a87/"">https://www.linkedin.com/in/george-sherry-8a50a87/</a> </p>",2019-12-15T23:01:10Z,24,Week 12/15 - 12/21,feedback,a_0,,k47m58f6ioc6xg,2019-12-15T23:01:10Z,{},other
5044,no,"<p><a href=""https://www.linkedin.com/in/syl-nktaylor/"">https://www.linkedin.com/in/syl-nktaylor/</a> Cheers! Thanks for a great class</p>",2019-12-15T23:32:33Z,24,Week 12/15 - 12/21,feedback,,jc7wfm89uv844j,k47n9lnr3hz1x9,2019-12-15T23:32:33Z,{},other
5045,no,Thanks for a great semester. Your endorsement will definitely help.<div><br /></div><div>https://www.linkedin.com/in/ravikishoreboggarapu</div>,2019-12-16T01:38:24Z,24,Week 12/15 - 12/21,feedback,,jqi9pq31o4p6dt,k47rrg23j972w3,2019-12-16T01:38:24Z,{},other
5046,no,"<p>Thanks a lot Miguel for the semester. I&#39;d definitely your endorsement</p>
<p><a href=""https://www.linkedin.com/in/shailesh-tappe-66a2973/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/shailesh-tappe-66a2973/</a> </p>",2019-12-16T02:39:39Z,24,Week 12/15 - 12/21,feedback,,jqlnrz9q91d6ko,k47ty7ifm2a5kg,2019-12-16T02:39:39Z,{},other
5047,no,"A bit late to the thread and my profile is definitely in need of some updating (currently my plan for the break between classes), but I&#39;d appreciate the connection and endorsement:<div>https://www.linkedin.com/in/christopherebright</div><div>Thanks again for a great semester.</div><div><br /></div>",2019-12-16T14:44:32Z,24,Week 12/15 - 12/21,feedback,,jqi9pwn26my6g2,k48jufc8i196vz,2019-12-16T14:44:32Z,{},other
5048,no,"<p>Great idea everyone! <a href=""https://www.linkedin.com/in/christophertperry2/"">https://www.linkedin.com/in/christophertperry2/</a></p>",2019-12-16T23:14:17Z,24,Week 12/15 - 12/21,feedback,,jzttp1ojahj6ju,k4921yf3wvu28b,2019-12-16T23:14:17Z,{},other
5049,no,"<p><a href=""https://www.linkedin.com/in/espinosaalex/"">https://www.linkedin.com/in/espinosaalex/</a></p>
<p>Thanks everyone! Best class so far.</p>",2019-12-17T01:34:36Z,24,Week 12/15 - 12/21,feedback,,jl9q3tj6G53J,k4972eoxdq24a5,2019-12-17T01:34:36Z,{},other
5050,no,"<p>Thank you for this great semester!</p>
<p><a href=""https://www.linkedin.com/in/quang-vu"">https://www.linkedin.com/in/quang-vu</a></p>
<p></p>",2019-12-17T18:18:06Z,24,Week 12/15 - 12/21,feedback,,jl5wq8mca7o0,k4a6wwnlyco50a,2019-12-17T18:18:06Z,{},other
5051,no,"Hey Miguel, thanks for a womderful start to OMSCS. I would love to have your endorsement <div><br /></div><div>https://www.linkedin.com/in/raghavg10</div><div><br /></div><div>Thanks</div>",2019-12-19T08:30:03Z,24,Week 12/15 - 12/21,feedback,,jzzw8pi7a7t3d4,k4cgsdiwj262i0,2019-12-19T08:30:03Z,{},other
5052,no,"<p>For me to be able to endorse you, I need you to (1) add me as a connection, (2) add &#34;Reinforcement Learning&#34; as a skill, and (3) ping me on that platform.</p>
<p></p>
<p>I&#39;m archiving Piazza.</p>
<p></p>
<p>Thank you all!</p>",2019-12-20T15:43:43Z,24,Week 12/15 - 12/21,feedback,,hyx9thiqa6j4nn,k4ebpxbsy6c5cm,2019-12-20T15:43:43Z,{},other
5053,no,"<p><a href=""https://www.linkedin.com/in/syl-nktaylor/"">https://www.linkedin.com/in/syl-nktaylor/</a></p>",2019-12-09T14:50:03Z,25,Week 12/8 - 12/14,followup,,jc7wfm89uv844j,k3yjyk3xi1l2q2,2019-12-09T14:50:03Z,{},other
5054,no,"<p><a href=""https://www.linkedin.com/in/anuzis/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/anuzis/</a></p>",2019-12-09T15:07:25Z,25,Week 12/8 - 12/14,followup,,isde332xcka1m0,k3ykkw2j20h47o,2019-12-09T15:07:25Z,{},other
5055,no,"<p><a href=""https://www.linkedin.com/in/alan-cespedes/"">https://www.linkedin.com/in/alan-cespedes/</a></p>",2019-12-09T15:51:04Z,25,Week 12/8 - 12/14,followup,,j6jxj5zix3746q,k3ym50bsfxxye,2019-12-09T15:51:04Z,{},other
5056,no,https://www.linkedin.com/in/minzhu2019/,2019-12-09T17:15:50Z,25,Week 12/8 - 12/14,followup,,jl11i2neyt7G,k3yp611qz4m13n,2019-12-09T17:15:50Z,{},other
5057,no,"<p><a href=""https://www.linkedin.com/in/dtadmor/"">https://www.linkedin.com/in/dtadmor/</a></p>",2019-12-09T18:50:06Z,25,Week 12/8 - 12/14,followup,,jzyjfo5dosy7o3,k3ysj9aeuqv9a,2019-12-09T18:50:06Z,{},other
5058,no,"<p>https://<a href=""https://www.linkedin.com/in/keroles-hakem?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_contact_details%3B45F0LYVVQRKN6T10Z5YKbw%3D%3D"">linkedin.com/in/keroles-hakem</a></p>",2019-12-09T19:01:55Z,25,Week 12/8 - 12/14,followup,,jzlwxqswu5m2e7,k3ysyg22agv3oj,2019-12-09T19:01:55Z,{},other
5059,no,"<p><a href=""https://www.linkedin.com/in/guilhermesmi/"">https://www.linkedin.com/in/guilhermesmi/</a></p>",2019-12-09T21:31:08Z,25,Week 12/8 - 12/14,followup,,is6e83bsfvk,k3yyacqwxmt25h,2019-12-09T21:31:08Z,{},other
5060,no,"<p>here is my link: <a href=""https://www.linkedin.com/in/jonathanbeltranus/"">https://www.linkedin.com/in/jonathanbeltranus/</a><br /><br />I am going to have an exciting opportunity soon to use MDPs and Reinforcement Learning to apply it to the domain of software testing!</p>",2019-12-09T22:38:28Z,25,Week 12/8 - 12/14,followup,,jqnu4ff8DlD1,k3z0oxwqlg84d6,2019-12-09T22:38:28Z,{},other
5061,no,"<p><a href=""https://www.linkedin.com/in/daniel-vasilyonok-18369a32/"">https://www.linkedin.com/in/daniel-vasilyonok-18369a32/</a></p>",2019-12-09T22:38:59Z,25,Week 12/8 - 12/14,followup,,jl1b27fpaYkv,k3z0pla7byt50k,2019-12-09T22:38:59Z,{},other
5062,no,"<p><a href=""https://www.linkedin.com/in/aparagas/"">https://www.linkedin.com/in/aparagas/</a></p>
<p></p>
<p>Also GitHub! https://github.com/alastairparagas</p>",2019-12-09T22:39:41Z,25,Week 12/8 - 12/14,followup,,jqnardlwW3NE,k3z0qi8dqcj1d0,2019-12-09T22:39:41Z,{},other
5063,no,"<p><a href=""https://www.linkedin.com/in/maayankil/"">https://www.linkedin.com/in/maayankil/</a></p>",2019-12-09T23:34:44Z,25,Week 12/8 - 12/14,followup,,jzium5mrhbn6x9,k3z2payfj7y1zb,2019-12-09T23:34:44Z,{},other
5064,no,https://www.linkedin.com/in/ben-lomax-9b01367a/,2019-12-10T00:04:12Z,25,Week 12/8 - 12/14,followup,,jl3oi5v7qkSk,k3z3r6gclfv1ju,2019-12-10T00:04:12Z,{},other
5065,no,"<p><a href=""https://www.linkedin.com/in/alson-yap/"">https://www.linkedin.com/in/alson-yap/</a></p>
<p></p>
<p>Happy to connect with everyone :)</p>",2019-12-10T00:53:55Z,25,Week 12/8 - 12/14,followup,,jqmjhlg7ktv5a,k3z5j45o5aup7,2019-12-10T00:53:55Z,{},other
5066,no,"<p>Thank you Professors, Miguel, Tim &amp; TAs bringing me this challenging &amp; exciting RL journey! Much appreciated!</p>
<p></p>
<p>Happy to connect with you all~</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/jesmerwong"">https://www.linkedin.com/in/jesmerwong</a><a href=""https://www.linkedin.com/in/alson-yap/"" target=""_blank"" rel=""noopener noreferrer""></a></p>
<p></p>
<p>Thank you!</p>
<p>With best regards,</p>
<p>Jesmer Wong.</p>",2019-12-10T02:22:24Z,25,Week 12/8 - 12/14,followup,,jznmonjuzvi67j,k3z8owme5mi28n,2019-12-10T02:22:24Z,{},other
5067,no,"<p><a href=""https://www.linkedin.com/in/matthew-lemons/"">https://www.linkedin.com/in/matthew-lemons/</a></p>",2019-12-10T02:54:38Z,25,Week 12/8 - 12/14,followup,,i4lzw47xh1u57w,k3z9ud68fb73fi,2019-12-10T02:54:38Z,{},other
5068,no,"<p>Thanks, all! </p>
<p></p>
<p><a href=""https://www.linkedin.com/in/kevin-stewart-4597496a/"">https://www.linkedin.com/in/kevin-stewart-4597496a/</a></p>",2019-12-10T03:42:19Z,25,Week 12/8 - 12/14,followup,,idghbt86wqe,k3zbjp6o6un4xc,2019-12-10T03:42:19Z,{},other
5069,no,"<p><a href=""https://www.linkedin.com/in/conorlivingston/"">https://www.linkedin.com/in/conorlivingston/</a></p>",2019-12-10T04:42:13Z,25,Week 12/8 - 12/14,followup,,ixnwq0s4ozg,k3zdoprznaj4ze,2019-12-10T04:42:13Z,{},other
5070,no,"<p><a href=""https://www.linkedin.com/in/neeraj-s-9094114/"">https://www.linkedin.com/in/neeraj-s-9094114/</a></p>",2019-12-10T05:26:51Z,25,Week 12/8 - 12/14,followup,,jc9nkspumds4ki,k3zfa4kl8v738x,2019-12-10T05:26:51Z,{},other
5071,no,"<p><a href=""https://www.linkedin.com/in/ashoydok/"">https://www.linkedin.com/in/ashoydok/</a></p>",2019-12-10T06:40:28Z,25,Week 12/8 - 12/14,followup,,jzih0fdt4sn1cq,k3zhwslbc4ll7,2019-12-10T06:40:28Z,{},other
5072,no,"<p><a href=""https://www.linkedin.com/in/bok-zhuang-qiang-626a4328/"">https://www.linkedin.com/in/bok-zhuang-qiang-626a4328/</a></p>
<p></p>
<p>Thanks for starting this thread! Let&#39;s all keep in contact!</p>",2019-12-10T07:03:15Z,25,Week 12/8 - 12/14,followup,,ijctp4ucNy8,k3ziq3i3ynl57s,2019-12-10T07:03:15Z,{},other
5073,no,"<p>I&#39;ve invited everyone here and I&#39;ll give you an endorsement if you reply.</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/jean-pierre-bianchi-baa62a/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/jean-pierre-bianchi-baa62a/</a></p>",2019-12-10T10:25:01Z,25,Week 12/8 - 12/14,followup,,jzh6k6o994a6dh,k3zpxkkvw3o68s,2019-12-10T10:25:01Z,{},other
5074,no,"<p><a href=""https://www.linkedin.com/in/purakjain/"">https://www.linkedin.com/in/purakjain/</a></p>",2019-12-10T10:51:08Z,25,Week 12/8 - 12/14,followup,,j6npc6xyzr32ht,k3zqv5q7slm6kv,2019-12-10T10:51:08Z,{},other
5075,no,"<p><a href=""https://www.linkedin.com/in/daniel-klingman-74a44064/"">https://www.linkedin.com/in/daniel-klingman-74a44064/</a></p>",2019-12-10T15:28:39Z,25,Week 12/8 - 12/14,followup,,jl561222orGT,k400s1eyr58480,2019-12-10T15:28:39Z,{},other
5076,no,"https://www.linkedin.com/in/tom-zhang-8206843b/<div><br /><div><br /></div><div>Thanks, all!</div></div>",2019-12-10T16:02:25Z,25,Week 12/8 - 12/14,followup,,jzjabmjd3l2103,k401zgj0z284km,2019-12-10T16:02:25Z,{},other
5077,no,<p>https://www.linkedin.com/in/jacobwanderson/</p>,2019-12-10T18:09:19Z,25,Week 12/8 - 12/14,followup,,jc554vxmyuy3pt,k406io1mtv9701,2019-12-10T18:09:19Z,{},other
5078,no,"<p>Thank you all!</p>
<p></p>
<p><a href=""https://www.linkedin.com/in/danilo-martins-14144627/"">https://www.linkedin.com/in/danilo-martins-14144627/</a></p>",2019-12-10T23:05:34Z,25,Week 12/8 - 12/14,followup,,jc6xvgjncoey,k40h3mp2r9l1lq,2019-12-10T23:05:34Z,{},other
5079,no,"<p><a href=""https://www.linkedin.com/in/gzhu/"">https://www.linkedin.com/in/gzhu/</a></p>
<p></p>
<p>Thank you all</p>",2019-12-11T01:40:02Z,25,Week 12/8 - 12/14,followup,,jzhs489b10i78c,k40mma7fmyd2j0,2019-12-11T01:40:02Z,{},other
5080,no,"<p><a href=""https://www.linkedin.com/in/mrinal-venkatesh-45897813/"">https://www.linkedin.com/in/mrinal-venkatesh-45897813/</a></p>
<p></p>",2019-12-11T07:46:59Z,25,Week 12/8 - 12/14,followup,,jqob6okzSK1c,k40zq6p3cijsg,2019-12-11T07:46:59Z,{},other
5081,no,"<p><a href=""https://www.linkedin.com/in/shimlee-sengupta-9a0892102/"">https://www.linkedin.com/in/shimlee-sengupta-9a0892102/</a></p>",2019-12-11T15:20:27Z,25,Week 12/8 - 12/14,followup,,jl2bv84c6FEX,k41fxc5eq1w3gz,2019-12-11T15:20:27Z,{},other
5082,no,"<p><a href=""https://www.linkedin.com/in/akrishnamurthy1/"">https://www.linkedin.com/in/akrishnamurthy1/</a></p>",2019-12-11T17:16:57Z,25,Week 12/8 - 12/14,followup,,jqrr36mqfm8M,k41k3655oylid,2019-12-11T17:16:57Z,{},other
5083,no,"<p><a href=""https://www.linkedin.com/in/xiangnan-shawn-he-phd-65b97021/"">https://www.linkedin.com/in/xiangnan-shawn-he-phd-65b97021/</a></p>
<p></p>",2019-12-11T23:34:01Z,25,Week 12/8 - 12/14,followup,,j6mhdov3jg75yk,k41xk2l7nd75j3,2019-12-11T23:34:01Z,{},other
5084,no,"<p><a href=""https://www.linkedin.com/in/david-wong-65806955/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/david-wong-65806955/</a></p>",2019-12-11T23:51:25Z,25,Week 12/8 - 12/14,followup,,jr7165ipLPTP,k41y6glbi3g1tb,2019-12-11T23:51:25Z,{},other
5085,no,<p></p>,2019-12-12T02:21:53Z,25,Week 12/8 - 12/14,feedback,,jl2bw1yzg5w1ej,k423jyrv1e15c5,2019-12-12T02:21:53Z,{},other
5086,no,"<p><a href=""https://www.linkedin.com/in/shuvalaxmi-panda-4143ba99/"">https://www.linkedin.com/in/shuvalaxmi-panda-4143ba99/</a></p>
<p></p>",2019-12-12T02:22:17Z,25,Week 12/8 - 12/14,followup,,jl2bw1yzg5w1ej,k423kh2mwfn5t5,2019-12-12T02:22:17Z,{},other
5087,no,"<p><a href=""https://www.linkedin.com/in/sriram-palutla/"">https://www.linkedin.com/in/sriram-palutla/</a></p>",2019-12-12T13:02:16Z,25,Week 12/8 - 12/14,followup,,jl8j7vzvUNs2,k42qfheewte5qr,2019-12-12T13:02:16Z,{},other
5088,no,"www.linkedin.com/in/daniel-luci-48a260111

Throw me an endorsement / add I&#39;ll return the favor!",2019-12-12T16:53:56Z,25,Week 12/8 - 12/14,followup,,jzj4sh1p7pf5af,k42ypf6sgx5rf,2019-12-12T16:53:56Z,{},other
5089,no,"<p><a href=""https://www.linkedin.com/in/evgenii-ignatev-210965190/"" target=""_blank"" rel=""noopener noreferrer"">https://www.linkedin.com/in/evgenii-ignatev-210965190/</a></p>",2019-12-12T20:45:23Z,25,Week 12/8 - 12/14,followup,,jzhwdil7ssn443,k436z2asg93n8,2019-12-12T20:45:23Z,{},other
5090,no,"<p><a href=""https://www.linkedin.com/in/kurtis-streutker/"">https://www.linkedin.com/in/kurtis-streutker/</a></p>
<p></p>
<p><a href=""https://github.com/SC4RECOIN"">https://github.com/SC4RECOIN</a></p>",2019-12-12T23:00:46Z,25,Week 12/8 - 12/14,followup,,jzm01jjgolv32u,k43bt5umgqbjr,2019-12-12T23:00:46Z,{},other
5091,no,"<p><a href=""https://www.linkedin.com/in/joshorourke/"">https://www.linkedin.com/in/joshorourke/</a></p>
<p></p>
<p><a href=""https://github.com/jpo"" target=""_blank"" rel=""noopener noreferrer"">https://github.com/jpo</a></p>",2019-12-13T02:10:07Z,25,Week 12/8 - 12/14,followup,,jcavbualf0l23o,k43ikogmbmbaz,2019-12-13T02:10:07Z,{},other
5092,no,"<p><a href=""https://www.linkedin.com/in/sylin07/"">https://www.linkedin.com/in/sylin07/</a></p>
<p></p>
<p>Thanks everyone!</p>",2019-12-13T12:53:57Z,25,Week 12/8 - 12/14,followup,,jzifg1e23c29s,k445knizsmq3h7,2019-12-13T12:53:57Z,{},other
5093,no,<md><br />[https://linkedin.com/in/manishrw](https://linkedin.com/in/manishrw)<br /></md><br />Endorsements will be reciprocated,2019-12-13T14:58:53Z,25,Week 12/8 - 12/14,followup,,jm666efxmod97,k44a1b2uuby3k5,2019-12-13T14:58:53Z,{},other
5094,no,"<p><a href=""https://www.linkedin.com/in/jing-wang-342b8375/"">https://www.linkedin.com/in/jing-wang-342b8375/</a></p>
<p></p>
<p>Endorsements will be reciprocated.</p>",2019-12-13T20:04:27Z,25,Week 12/8 - 12/14,followup,,jzjb6ojxtuc1d3,k44ky9vfp3652m,2019-12-13T20:04:27Z,{},other
5095,no,<md><br />[https://linkedin.com/in/manishrw](https://linkedin.com/in/manishrw)<br /></md><br />Endorsements will be reciprocated.,2019-12-13T20:05:49Z,25,Week 12/8 - 12/14,followup,,jm666efxmod97,k44l016wbeh7d8,2019-12-13T20:05:49Z,{},other
5096,no,"<p>thanks for the great semester. <a href=""http://www.linkedin.com/in/farrukh-rahman"" target=""_blank"" rel=""noopener noreferrer"">www.linkedin.com/in/farrukh-rahman</a></p>",2019-12-13T22:38:56Z,25,Week 12/8 - 12/14,followup,,jl1acpoc4HA9,k44qgy8gkqi4aw,2019-12-13T22:38:56Z,{},other
5097,no,"<p><a href=""https://www.linkedin.com/in/george-sherry-8a50a87/"">https://www.linkedin.com/in/george-sherry-8a50a87/</a></p>",2019-12-15T22:58:47Z,24,Week 12/15 - 12/21,followup,,j6m1j6i41cz6ru,k47m25ynsjf422,2019-12-15T22:58:47Z,{},other
5098,no,"<p><a href=""https://www.linkedin.com/in/shailesh-tappe-66a2973/"">https://www.linkedin.com/in/shailesh-tappe-66a2973/</a></p>",2019-12-16T01:33:40Z,24,Week 12/15 - 12/21,followup,,jqlnrz9q91d6ko,k47rlcs8qzy5pp,2019-12-16T01:33:40Z,{},other
5099,no,"<p><a href=""https://www.linkedin.com/in/stshining/"">https://www.linkedin.com/in/stshining/</a></p>",2019-12-16T03:32:28Z,24,Week 12/15 - 12/21,followup,,jzj0om7qnbd4yf,k47vu4wydim57i,2019-12-16T03:32:28Z,{},other
5100,no,<p>https://www.linkedin.com/in/shravankumarsambaraju</p>,2019-12-16T03:36:11Z,24,Week 12/15 - 12/21,followup,,jzu2o3mdsd9198,k47vywvsvl55jn,2019-12-16T03:36:11Z,{},other
5101,no,"<p><a href=""https://www.linkedin.com/in/hazel-john-6382833/"">https://www.linkedin.com/in/hazel-john-6382833/</a> </p>",2019-12-16T13:26:37Z,24,Week 12/15 - 12/21,followup,,j6pmq1sglzo35i,k48h27vcntl586,2019-12-16T13:26:37Z,{},other
5102,no,"<p><a href=""https://www.linkedin.com/in/muzaffer-e-42822b157/"">https://www.linkedin.com/in/muzaffer-e-42822b157/</a></p>
<p></p>",2019-12-16T13:35:10Z,24,Week 12/15 - 12/21,followup,,jqud6q7bqlagv,k48hd7wu8sc2l2,2019-12-16T13:35:10Z,{},other
5103,no,"A bit late to the thread and my profile is definitely in need of some updating (currently my plan for the break between classes), but I&#39;d appreciate connections and trading endorsements:<div>https://www.linkedin.com/in/christopherebright</div><div><br /></div>",2019-12-16T14:43:19Z,24,Week 12/15 - 12/21,followup,,jqi9pwn26my6g2,k48jsv1znpu52j,2019-12-16T14:43:19Z,{},other
5104,no,"<p>Let&#39;s link!</p>
<p>I am in bayarea, ca</p>
<p><a href=""https://www.linkedin.com/in/kamranmostafavi/"">https://www.linkedin.com/in/kamranmostafavi/</a></p>",2019-12-17T00:42:13Z,24,Week 12/15 - 12/21,followup,,j6ll2xkiDJf,k49571k6iwq1yu,2019-12-17T00:42:13Z,{},other
5105,no,"<p><a href=""https://www.linkedin.com/in/michaeldilks/"">https://www.linkedin.com/in/michaeldilks/</a></p>
<p></p>",2019-12-17T03:15:32Z,24,Week 12/15 - 12/21,followup,,jc8bshn91pFK,k49ao7hk7dt1yl,2019-12-17T03:15:32Z,{},other
5106,no,"<p><a href=""https://www.linkedin.com/in/chewxyu/"">https://www.linkedin.com/in/chewxyu/</a></p>",2019-12-17T13:43:10Z,24,Week 12/15 - 12/21,followup,,jzjbneqoyho1hu,k49x3cirk3c1zv,2019-12-17T13:43:10Z,{},other
5107,no,"<p><a href=""https://www.linkedin.com/in/ella-jiao-wu-9ba082199/"">https://www.linkedin.com/in/ella-jiao-wu-9ba082199/</a></p>
<p></p>
<p>Happy to connect with everyone! willing to reciprocate endorsements!</p>",2019-12-18T01:16:39Z,24,Week 12/15 - 12/21,followup,,ijex0qpofma6zh,k4alv65pcp731,2019-12-18T01:16:39Z,{},other
5108,no,"<p><a href=""https://www.linkedin.com/in/kai-yu-tan-835888a2/"">https://www.linkedin.com/in/kai-yu-tan-835888a2/</a></p>
<p></p>
<p>Happy to connect and reciprocate endoresement too!</p>
<p>Thanks TAs and fellow students for the fruitful learning experience!</p>",2019-12-18T14:03:45Z,24,Week 12/15 - 12/21,followup,,k012haa329t4gp,k4bd9o687iq4lg,2019-12-18T14:03:45Z,{},other
5109,no,"Hi guys,<div><br /></div><div>Let&#39;s connect.</div><div><br /></div><div>https://www.linkedin.com/in/raghavg10</div><div><br /></div><div>Thanks.</div>",2019-12-19T08:31:17Z,24,Week 12/15 - 12/21,followup,,jzzw8pi7a7t3d4,k4cgtyppsmp35g,2019-12-19T08:31:17Z,{},other
5110,no,"<p><a href=""https://www.linkedin.com/in/maximilian-roquemore/"">https://www.linkedin.com/in/maximilian-roquemore/</a></p>",2019-12-20T06:21:20Z,24,Week 12/15 - 12/21,followup,,jcg0nzvdk8272b,k4drmpqctwpko,2019-12-20T06:21:20Z,{},other
5111,stud,"<p>Sorry. I was not clear. I clarified the original question. Also, my understanding can be totally off. Let me know.</p>
<p></p>
<p>Thanks. </p>",2019-12-16T05:29:48Z,30,Week 12/15 - 12/21,followup,a_0,,k480118b5pw33t,2019-12-16T05:29:48Z,{},other
5112,no,"<p>thanks for clarification on the question, btw I didn&#39;t notice the original Experience replay paper actually came out before Baird&#39;s residual grad one, my bad there.</p>
<p></p>
<p>It does share some similarity to Experience replay. As Vahe mentioned, each time conditions for learning are satisfied the 2 transitions used should not be correlated because they are not sequences but rather returning to the same state.</p>",2019-12-17T00:47:22Z,30,Week 12/15 - 12/21,feedback,,jl1acpoc4HA9,k495do1p19n5se,2019-12-17T00:47:22Z,{},other
5113,stud,"<p>In fact, what Baird mentioned was different from EP. Baird&#39;s method requires that each state be visited at least twice. Given the large number of states, one would expect that the majority of the states experience zero or one transition. That is why it is hard to use EP to draw past transitions from the same state because they most likely don&#39;t exist. That is when Baird said that the algo needs to &#34;intentionally performing actions to return to a given state.&#34; to intentionally generate the second transitions. </p>
<p></p>
<p>The correlation thing in my last paragraph was my slip-up. Yes, two transitions are conditionally independent given the state. :-) </p>",2019-12-17T01:41:53Z,30,Week 12/15 - 12/21,feedback,a_0,,k497bs8x70o3jq,2019-12-17T01:41:53Z,{},other
5114,stud,"<p>&#64;vehe, you are right that the two transitions should be conditionally independent given the state. </p>
<p></p>
<p>My original question was if we can just keep a buffer to draw past transitions from the same state to pair with the current transitions. When I first read the paper, Experience Replay jumped to my mind. :-) On re-reading it, though, I now see what he was saying about &#34;intentionally performing actions to return to a given state,&#34;  and why it&#39;s different from EP. There are many many states, and it is not good enough to have a transition in a nearby state. His method calls for returning to the exact state where there was the first transition.</p>",2019-12-16T16:31:43Z,30,Week 12/15 - 12/21,followup,a_0,,k48no971byx4cd,2019-12-16T16:31:43Z,{},other
5115,stud,"<p>In EP, most recorded states have perhaps only a single recorded transition, despite the large buffer size.  </p>",2019-12-16T16:34:13Z,30,Week 12/15 - 12/21,feedback,a_0,,k48nrh701oo6us,2019-12-16T16:34:13Z,{},other
5116,no,"<p>Yeah, good point :)</p>",2019-12-16T16:37:51Z,30,Week 12/15 - 12/21,feedback,,jzfsa4a37jf4aq,k48nw5b2x041t,2019-12-16T16:37:51Z,{},other
5117,no,"<p>studying for GIOS ;) another late night ;) sent u some pm&#39;s.  respond as u like. I will be async for a day or two until I clear this exam.</p>
<p></p>",2019-12-11T10:53:37Z,20,Week 12/8 - 12/14,followup,,jzivtxcbl6964n,k416e7596zk10b,2019-12-11T10:53:37Z,{},other
5118,no,"<p>The transition is deterministic because the states are the training samples. As you train the agent you go from state $$s_1$$ to $$s_2$$ with 100% probability. If this was a stochastic environment, then you would go from $$s_1$$ to $$s_2$$ with some $$\epsilon$$ probability. Such would be the case, say, if you shuffled your data inline of the training. Then you would pick a next state, $$s_n$$, based upon some probability which is proportional to the mixing that you are trying to implement.</p>
<p></p>
<p><em>When the new episode begins, the environment shuffles the order of samples in the training data set.</em></p>
<p><em></em></p>
<p>The samples are shuffled once, and then the order that they are presented to the RL agent is the same. That means the sequence of training samples is deterministic, and thus the states are deterministic. We can always predict the next state from the current state.</p>
<p></p>
<p>Also remember that the actions are the classification. The RL agent is not taking any kind of action that leads to another state. Each presentation of a state is just a simple one-shot output (didn&#39;t we read papers on one-shot games?).</p>
<p></p>
<p>Because this is a one-shot game and the states are not dependent on each other, the shuffling of the data likely does not contribute anything appreciable to the output. The paper should have provided a graph of the loss function, eq 15, comparing both in-order and shuffled inputs.</p>
<p></p>
<p>Their Table IV is misleading. They don&#39;t report repeated presentations of the data set and do not report any statistical analysis of their work. We don&#39;t know if their results are statistically significant. The F score (F-measure) is a good metric here, but they only give a single value and not a statistical range for the F score. In my opinion the technique only improves classification for Mnist and IMDB. The others appear to be no better than state of the art.</p>
<p></p>
<p>Like many papers we&#39;ve read, this paper is written as a &#34;look at my wonderful idea,&#34; rather than &#34;I came up with a cool idea, and this is how I shot it down ...&#34;</p>",2019-12-11T14:01:27Z,18,Week 12/8 - 12/14,followup,,jc554vxmyuy3pt,k41d3r8w8rz6s8,2019-12-11T14:01:27Z,{},other
5119,no,"<p>without commenting on the quality of the paper; its not a bad idea.  given the nature of how q-learning actually works; instead of trying to match the bootstrapping probability for an optimal policy; you are trying to find a reward function s.t. the optimal bootstrapped distribution compensates for the skew in likelihood of encountering the class.  which, is itself a mixing function.</p>",2019-12-12T05:50:06Z,18,Week 12/8 - 12/14,feedback,,jzivtxcbl6964n,k42azq0wo6936z,2019-12-12T05:50:06Z,{},other
5120,no,&#64;jacob yes I always find it funny when they say their idea beats EVERY other network and method 1st place except for one. I find it hard to believe and probably biased.,2019-12-12T21:05:55Z,18,Week 12/8 - 12/14,feedback,,jzj4sh1p7pf5af,k437phccugz1fk,2019-12-12T21:05:55Z,{},other
5121,no,What is the difference between doing something like this and weighting the cross-entropy by class according to the imbalance ratio?,2019-12-12T21:05:14Z,18,Week 12/8 - 12/14,followup,,jzj4sh1p7pf5af,k437ol99d0j7n,2019-12-12T21:05:14Z,{},other
5122,no,"<p>I don&#39;t think it&#39;s fair for me to really evaluate this paper without putting time into the details and maybe trying to replicate it myself.  </p>
<p></p>
<p>But from a very high level, I&#39;m suspicious as to why injecting a false temporal structure to data samples which in reality are $$\textit{not }$$ temporally related, would improve classification performance.</p>
<p></p>
<p>The primary innovation here is that the reward is different for classifying minority vs majority samples correctly.  I&#39;m being asked to believe that this one insight is superior to the entire weight of the supervised learning paradigm on a problem with no temporal structure.</p>",2019-12-12T22:04:51Z,18,Week 12/8 - 12/14,followup,,jzfsa4a37jf4aq,k439t94cq9d1zh,2019-12-12T22:04:51Z,{},other
5123,no,Vahe that&#39;s what I think. It seems to be mathematically equivalent to just weighting the cross entropy equation by the imbalance ratio.,2019-12-12T22:05:58Z,18,Week 12/8 - 12/14,feedback,,jzj4sh1p7pf5af,k439uorlayj38u,2019-12-12T22:05:58Z,{},other
5124,no,"<p>Yeah, you&#39;re probably right.  I wonder why they didn&#39;t do that in their paper as a comparison approach.</p>",2019-12-12T22:20:07Z,18,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k43acvxlapk2md,2019-12-12T22:20:07Z,{},other
5125,no,"<p>gentleman: simple.  a fixed weight, isn&#39;t adaptive; and also requires a priori knowledge.<br /><br />it really isn&#39;t a bad idea.  I don&#39;t know if it &#34;beats every other idea out there&#34;.  but it does leverage (if somewhat inelegantly) the main benefits of reinforcement learning.<br /><br />u could (just as easily) for a fixed data set simply normalize the data-set.  this would be no different; other than it does so dynamically; and would work, on just about any machine-learning algorithm without having to modify it.</p>
<p><br />think of the misclassification error (due to imbalance) as a dynamically applied reward shaping guiding u to the &#34;true distribution&#34;. ;)</p>
<p></p>
<p></p>",2019-12-13T07:22:41Z,18,Week 12/8 - 12/14,feedback,,jzivtxcbl6964n,k43tqmvlpqu3b2,2019-12-13T07:22:41Z,{},other
5126,no,"<p>&#34;<em>a fixed weight, isn&#39;t adaptive; and also requires a priori knowledge.&#34;</em></p>
<p></p>
<p>But in this paper they don&#39;t seem to be deriving the imbalance ratio from some adaptive process.  It&#39;s just a hyperparameter they &#34;tune&#34;;  it&#39;s more like they&#39;re using a priori knowledge of the imbalance ratio from the dataset to show us that that ratio happens to be the best possible value for the reward of the majority class.</p>",2019-12-13T07:43:59Z,18,Week 12/8 - 12/14,feedback,,jzfsa4a37jf4aq,k43ui1bte5737m,2019-12-13T07:43:59Z,{},other
5127,no,"<p>then there is a new paper to write ;) say with three other authors (jwa, vahe, d.luci)? :)<br /><br />I skimmed the paper... once I read JWA&#39;s comment about the shuffling and the &#39;reward modification&#39; I sort of lost interest (in their particular implementation; though I think the idea, in fact is good); not because the shuffling means it has no effect; but that simply, it indicates a lack of good theoretic knowledge on the bellman operators in Q-learning.   Apparently they did not take Isbell/Littman? :)<br /><br />I would hazard to say that the shuffling is there attempt to do a &#34;simultaneous access on a turn-based (sequentialized) game&#34; similar to the technique I used on one of the homework assignments i.e. randomize rankings on the start of each turn with in-order selection.   Which works, but is more a hack, than elegant.<br /><br />I think this is a better version of bootstrapping in the &#34;machine learning&#34; sense i.e. testing with cross-validation with k-fold cuts; in that it obviates the need for cross-validation&#43;&#43;&#43; and provides a better proximate estimate to discrete choices (their relative frequencies) and how selection, informs optimality (with respect to the trained neural network).  <br /><br />This is after all, the purpose of bootstrapping (generating samples with replacement from the data set) -- and using the maximal of the k-fold, cross-validated ML (on the receiver curves&#43;&#43;)<br /><br />Considering the serious (pervasive) hazards involved in statistical (and by proxy, statistical learning) with respect to misclassification due to relative frequency; I would say it would be a very valid addition to the core techniques for ML.  Particularly since it leverages the strengths of RL with ML; and is as I noted, likely broadly (and easily) applicable.<br /><br />In fact, now that I think about it; it may be superior; in the sense that the probability distribution with respect the sampling itself, is heavily influenced by the imbalance.  In that sense the k-fold cutting itself is an estimate of the imbalance; and here, the imbalance (properly formulated as a variant of Q-learning) could uncover a more tunable, epsilon-style estimate (similar to the k-fold estimators, in fact!) :)<br /><br />I&#39;ll have to think about it.  My brain is slightly fried right now.<br /><br />I am available on slack if y&#39;all want to discuss it...  I&#39;m serious about the paper ;)  its worth publishing, and done right, could be ******* for some -very specific- technical reasons. ;)<br /><br />&#43;&#43; although I  note, there is an imbalanced receiver curve specifically to handle this type of misclassification (I forget the name at the moment).<br /><br />&#43;&#43;&#43; not obviates per se; but makes a portion of the purpose of the cross-validation, redundant.</p>",2019-12-14T01:08:16Z,18,Week 12/8 - 12/14,feedback,,jzivtxcbl6964n,k44vsz79mu670x,2019-12-14T01:08:16Z,{},other
5128,no,"Regardless, you can have an adaptive weight that adjusts on each data sample based on how often you see it or how ‘hard’ the example is, how much of an outlier it is.<div><br /></div><div>I would be curious to see a good formulation of RL applied to supervised learning however so far a lot of examples are DQN with some hurdles in between, but essentially the loss function does the same thing in the end through gradient descent and it is just a neural network doing classification in disguise. </div>",2019-12-13T14:07:50Z,18,Week 12/8 - 12/14,followup,,jzj4sh1p7pf5af,k4487nl3wba3m2,2019-12-13T14:07:50Z,{},other
5129,no,"<p>you are much more familiar with ML :) <br /><br />&gt; but essentially the loss function does the same thing in the end through gradient descent and it is just a neural network doing classification in disguise. <br /><br />&#34;bounded regret&#34;; the question is:  which, is more correct. ;)<br /></p>
<p></p>",2019-12-14T19:09:05Z,18,Week 12/8 - 12/14,feedback,,jzivtxcbl6964n,k45yex184cx75a,2019-12-14T19:09:05Z,{},other
5130,stud,"<p>Are answers to the final exam released like review questions? How long will we see the grade on buzz port?</p>
<p></p>
<p>Thanks again to all the teaching staff for this class.</p>",2019-12-15T19:16:35Z,17,Week 12/15 - 12/21,followup,a_0,,k47e4fdv4435qa,2019-12-15T19:16:35Z,{},logistics
5131,no,<p>Answers to the final won&#39;t be posted. Final grades in Buzzport should be Tuesday/Wednesday</p>,2019-12-15T19:30:00Z,17,Week 12/15 - 12/21,feedback,,jl1acpoc4HA9,k47elo5uh0f4m,2019-12-15T19:30:00Z,{},logistics
5132,no,<p>Thanks for your efforts and other TAs. I enjoyed the class and learned a lot from the resources provided.</p>,2019-12-15T19:30:55Z,17,Week 12/15 - 12/21,followup,,j6ll2xkiDJf,k47emuhrqz82t2,2019-12-15T19:30:55Z,{},logistics
5133,no,<p>Thank you everyone for your hard work! :) Happy to have learned a lot this semester.</p>,2019-12-15T19:33:42Z,17,Week 12/15 - 12/21,followup,,jzhq0wae6o1uh,k47eqg05b3s5fp,2019-12-15T19:33:42Z,{},logistics
5134,no,<p>Same! It was a great exerience</p>,2019-12-15T19:59:00Z,17,Week 12/15 - 12/21,feedback,,jl45xzdvwqdJ,k47fmza9qpq6au,2019-12-15T19:59:00Z,{},logistics
5135,no,<p>Will the final exam statistics be released?</p>,2019-12-15T20:00:19Z,17,Week 12/15 - 12/21,followup,,jl45xzdvwqdJ,k47foo2xmy47l9,2019-12-15T20:00:19Z,{},logistics
5136,no,"<p>they have it. mean 52, high 90, low 0</p>",2019-12-15T20:54:01Z,17,Week 12/15 - 12/21,feedback,,jqrr36mqfm8M,k47hlpwc5r2209,2019-12-15T20:54:01Z,{},logistics
5137,no,<p>How about the homework and project 3?</p>,2019-12-15T21:52:01Z,17,Week 12/15 - 12/21,feedback,,jzhq0wae6o1uh,k47job1sccx1pr,2019-12-15T21:52:01Z,{},logistics
5138,no,<p>Wonder if the final grade will be curved given the final exam score mean.</p>,2019-12-15T22:52:55Z,17,Week 12/15 - 12/21,feedback,,hzoi2qsuCAd,k47lumgpbi32iw,2019-12-15T22:52:55Z,{},logistics
5139,no,<p>Found the answer: &#64;731</p>,2019-12-16T01:46:11Z,17,Week 12/15 - 12/21,feedback,,hzoi2qsuCAd,k47s1gauhgwe7,2019-12-16T01:46:11Z,{},logistics
5140,no,<p>agreed!</p>,2019-12-15T21:29:25Z,19,Week 12/15 - 12/21,followup,,jzhq0wae6o1uh,k47iv98oru73cu,2019-12-15T21:29:25Z,{},other
5141,no,<p>Agreed. Thanks to all Instructors and TAs for their hard work and prompt responses.</p>,2019-12-15T23:06:11Z,19,Week 12/15 - 12/21,followup,,jqob6okzSK1c,k47mbp7wha83ou,2019-12-15T23:06:11Z,{},other
5142,no,"<p>Yeah, just wanted to jump in last minute to say thank you. Daniel is absolutely correct, you&#39;ve all been some of the absolute best Instructors and TAs of the course I&#39;ve experienced so far, and I genuinely hope to run into some / all of you in the future. Good luck with future endeavors, both within OMSCS and outside it.</p>",2019-12-18T06:41:52Z,19,Week 12/15 - 12/21,followup,,jl3oi5v7qkSk,k4axhehxf013pn,2019-12-18T06:41:52Z,{},other
5143,no,"<p>awesome, thank you Miguel :) this is very helpful, particularly as a callout! :)<br /><br />thank you all again, for the wonderful teaching experience, it is much appreciated! :)</p>",2019-12-16T02:03:57Z,14,Week 12/15 - 12/21,followup,,jzivtxcbl6964n,k47soal82uz3s7,2019-12-16T02:03:57Z,{},logistics
5144,stud,<p>&#43;1</p>,2019-12-16T13:35:30Z,14,Week 12/15 - 12/21,feedback,a_0,,k48hdmwcwt2ut,2019-12-16T13:35:30Z,{},logistics
5145,no,"<p><strong attention=""hyx9thiqa6j4nn"">&#64;Miguel Morales</strong>  </p>
<p></p>
<p>When the forum is archived, will we still have access to view and review the posts here in the future?</p>
<p></p>
<p>Thanks,</p>
<p>George</p>",2019-12-16T04:45:15Z,14,Week 12/15 - 12/21,followup,,ixty1midfufhd,k47yfqkyk435y7,2019-12-16T04:45:15Z,{},logistics
5146,no,"<p>Yes, George. It just becomes inactive. So no follow-ups, new posts, etc.</p>",2019-12-16T04:57:33Z,14,Week 12/15 - 12/21,feedback,,hyx9thiqa6j4nn,k47yvk1mnzp79f,2019-12-16T04:57:33Z,{},logistics
5147,no,"<p>Thanks Miguel and all the TA&#39;s.</p>
<p>This was a very interesting and eye-opening course.  </p>
<p>I&#39;m so glad I trusted you at the beginning when you told me it could be done before ML.  </p>
<p>Too bad there isn&#39;t a second level, but I&#39;ll keep learning by myself or try other schools because, to me, RL is the real deal in terms of real artificial intelligence.  </p>
<p>All the best and hope to see you around.  </p>",2019-12-16T15:00:28Z,14,Week 12/15 - 12/21,followup,,jzh6k6o994a6dh,k48kewieflb476,2019-12-16T15:00:28Z,{},logistics
5148,no,<p>So long and thanks for all of the fish! This was a great experience.</p>,2019-12-16T16:14:10Z,14,Week 12/15 - 12/21,followup,,jc554vxmyuy3pt,k48n1opyhl2qy,2019-12-16T16:14:10Z,{},logistics
5149,no,Thanks Miguel &amp; all TAs! <div>This was one of the most rigorous courses I have done so far but very rewarding at the same time.</div><div>The help from TAs and students on slack &amp; piazza is only reason I sailed through &#x1f60b;</div><div><br /></div>,2019-12-17T00:20:06Z,14,Week 12/15 - 12/21,followup,,jfzaqnqvtQ1m,k494elip8dq2g7,2019-12-17T00:20:06Z,{},logistics
5150,no,"<p>Thank you TAs for your Office Hours! I got an excellent RL journey!</p>
<p></p>
<p>May I know if Bluejeans videos can be accessed even after the course?</p>
<p></p>
<p>All the best everyone!</p>",2019-12-17T03:21:34Z,14,Week 12/15 - 12/21,followup,,jznmonjuzvi67j,k49avz27b1h2sc,2019-12-17T03:21:34Z,{},logistics
5151,no,"<p>I do not know, but I&#39;d think so. Let me know if that is the case.</p>",2019-12-20T15:28:20Z,14,Week 12/15 - 12/21,feedback,,hyx9thiqa6j4nn,k4eb65oprcf4v8,2019-12-20T15:28:20Z,{},logistics
5152,no,"<p>Thank you all for the great introduction to RL, and for the time that TAs spent holding office hours / study sessions, explaining concepts on Piazza, and giving thoughtful project feedback. Looking forward to exploring the field a lot more!</p>",2019-12-17T14:16:53Z,14,Week 12/15 - 12/21,followup,,jzifg1e23c29s,k49yaplx85c2fr,2019-12-17T14:16:53Z,{},logistics
5153,no,"<p>This course has been a wild ride for me, with water-hose of information, endless frustration, half-sleep night with a dream of working code, to multiple joy and celebrations. (I have taken GA, it has more pressure, but in term of wildness, this one is one knot above). This course is also pushing hard on all the things I have been weak at, and makes it even more challenge. </p>
<p></p>
<p>At the end of it and looking back, all the learning and the incremental improvement are what I am proud of. I am a little better now, than I was 3 months ago: reading paper, writing paper, understanding research... More importantly, I learn to cooperate and learn from other people in the class, not just from paper and lecture. It would not be possible without an enthusiast community that always willing share their knowledge and address each other&#39;s question.</p>
<p></p>
<p>Despite all the advance material we went through in the course. I feel like this is just a beginning, opens new doors. I hope we all have a wonderful journey from here, and hope we would see each other again someday. Thanks for all the fish!</p>",2019-12-17T17:44:36Z,14,Week 12/15 - 12/21,followup,,jl5wq8mca7o0,k4a5pucxrfp5,2019-12-17T17:44:36Z,{},logistics
